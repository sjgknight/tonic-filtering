<!DOCTYPE html>
<html>
  <head>
    <title>My Collection | Principle of Safety, key considerations</title>
    <meta name="description" content=""Beyond safeguarding the sustainability of your AI project as it relates to its social impacts on individual wellbein...">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="stylesheets/application-39e33491.css" rel="stylesheet" />
    <link href="images/favicon.ico" rel="icon" type="image/ico" />
      <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
      :root {
        --main-color: #6366f1;
        --background-color: #e2e8f0;
        --font-family: Lato;
      }
    </style>
  </head>
  <body>
    <div id="header" class="flex justify-between w-full px-6 py-3 items-center">
  <div class="flex items-center">
      <a href="/" class="text-2xl text-white">My Collection</a>
  </div>
  <div class="flex gap-4">
    <div class="hidden sm:inline">
        <a href="#" class="nav-link">About Us</a>
        <a href="#" class="nav-link">Contact</a>
    </div>

  </div>
</div>

    

<div class="mt-6 mb-24 px-8 md:px-32 mx-auto max-w-screen-xl">
  <div class="flex justify-between items-center mb-4">
    <a href="#" onclick="back('/')">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="15 18 9 12 15 6"></polyline></svg>
 Back
    </a>
      <a href="#" onclick="toggleSharing()" class="ml-3">
  <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" y1="2" x2="12" y2="15"></line></svg>
 Share
</a>
<div id="sharing-options" class="dropdown hidden right-5 md:right-32 top-32">
  <div class="px-6 py-3">
    <p>Share this page</p>
    <div id="sharing-buttons" class="mb-4">
        <a data-sharer="facebook" href="#" title="Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 17.765625 1.03125 L 17.765625 4.546875 L 15.703125 4.546875 C 14.015625 4.546875 13.734375 5.34375 13.734375 6.46875 L 13.734375 9 L 17.671875 9 L 17.109375 12.984375 L 13.734375 12.984375 L 13.734375 23.15625 L 9.609375 23.15625 L 9.609375 12.984375 L 6.234375 12.984375 L 6.234375 9 L 9.609375 9 L 9.609375 6.09375 C 9.609375 2.71875 11.71875 0.84375 14.71875 0.84375 C 16.171875 0.84375 17.4375 0.984375 17.765625 1.03125 Z M 17.765625 1.03125 "></path>
</svg>

        </a>
        <a data-sharer="twitter" href="#" title="Twitter">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 21.515625 7.125 L 21.5625 7.734375 C 21.5625 14.25 16.59375 21.75 7.546875 21.75 C 4.78125 21.75 2.15625 20.953125 0 19.546875 C 2.613281 19.828125 5.234375 19.089844 7.3125 17.484375 C 5.109375 17.4375 3.328125 16.03125 2.71875 14.109375 L 3.609375 14.15625 C 4.078125 14.15625 4.5 14.109375 4.921875 14.015625 C 2.671875 13.546875 0.984375 11.578125 0.984375 9.1875 L 0.984375 9.09375 C 1.640625 9.46875 2.390625 9.703125 3.1875 9.75 C 1.007812 8.296875 0.347656 5.390625 1.6875 3.140625 C 4.078125 6.140625 7.734375 8.109375 11.8125 8.296875 C 11.71875 7.921875 11.71875 7.546875 11.71875 7.171875 C 11.726562 5.164062 12.957031 3.363281 14.820312 2.621094 C 16.6875 1.878906 18.816406 2.34375 20.203125 3.796875 C 21.328125 3.609375 22.40625 3.1875 23.34375 2.625 C 22.96875 3.75 22.171875 4.734375 21.1875 5.34375 C 22.171875 5.203125 23.109375 4.96875 24 4.546875 C 23.34375 5.53125 22.5 6.421875 21.515625 7.125 Z M 21.515625 7.125 "></path>
</svg>

        </a>
        <a data-sharer="linkedin" href="#" title="Linkedin">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 6.375 8.578125 L 6.375 21.84375 L 1.96875 21.84375 L 1.96875 8.578125 Z M 6.65625 4.453125 C 6.703125 5.71875 5.71875 6.75 4.171875 6.75 C 2.671875 6.75 1.734375 5.71875 1.734375 4.453125 C 1.734375 3.140625 2.71875 2.15625 4.21875 2.15625 C 5.71875 2.15625 6.65625 3.140625 6.65625 4.453125 Z M 22.265625 14.203125 L 22.265625 21.84375 L 17.859375 21.84375 L 17.859375 14.71875 C 17.859375 12.9375 17.25 11.71875 15.65625 11.71875 C 14.4375 11.71875 13.6875 12.5625 13.359375 13.359375 C 13.265625 13.640625 13.21875 14.015625 13.21875 14.4375 L 13.21875 21.84375 L 8.8125 21.84375 L 8.8125 8.578125 L 13.21875 8.578125 L 13.21875 10.5 C 13.78125 9.5625 14.8125 8.25 17.203125 8.25 C 20.109375 8.25 22.265625 10.171875 22.265625 14.203125 Z M 22.265625 14.203125 "></path>
</svg>

        </a>
        <a data-sharer="pinterest" href="#" title="Pinterest">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 12.5625 0.28125 C 7.734375 0.28125 3 3.515625 3 8.71875 C 3 12 4.875 13.875 6 13.875 C 6.421875 13.875 6.703125 12.5625 6.703125 12.234375 C 6.703125 11.765625 5.578125 10.828125 5.578125 9.046875 C 5.578125 5.25 8.484375 2.578125 12.1875 2.578125 C 15.375 2.578125 17.71875 4.40625 17.71875 7.734375 C 17.71875 10.21875 16.734375 14.90625 13.5 14.90625 C 12.328125 14.90625 11.34375 14.0625 11.34375 12.84375 C 11.34375 11.0625 12.5625 9.375 12.5625 7.546875 C 12.5625 4.40625 8.15625 4.96875 8.15625 8.71875 C 8.15625 9.515625 8.25 10.40625 8.625 11.109375 C 7.96875 13.921875 6.65625 18.046875 6.65625 20.90625 C 6.65625 21.796875 6.796875 22.6875 6.84375 23.578125 C 7.03125 23.71875 6.9375 23.71875 7.171875 23.625 C 9.5625 20.390625 9.46875 19.78125 10.546875 15.515625 C 11.109375 16.640625 12.609375 17.203125 13.78125 17.203125 C 18.75 17.203125 21 12.375 21 8.015625 C 21 3.328125 16.96875 0.28125 12.5625 0.28125 Z M 12.5625 0.28125 "></path>
</svg>

        </a>
        <a data-sharer="whatsapp" href="#" title="Whatsapp">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 19.359375 4.546875 C 15.664062 0.875 9.820312 0.503906 5.6875 3.675781 C 1.554688 6.847656 0.40625 12.589844 3 17.109375 L 1.453125 22.5 L 6.984375 21.046875 C 10.222656 22.785156 14.132812 22.707031 17.296875 20.835938 C 20.457031 18.96875 22.414062 15.582031 22.453125 11.90625 C 22.453125 9.140625 21.28125 6.515625 19.3125 4.546875 Z M 12 20.578125 C 10.453125 20.578125 8.90625 20.15625 7.59375 19.359375 L 7.265625 19.171875 L 3.984375 20.015625 L 4.875 16.828125 L 4.6875 16.5 C 2.402344 12.769531 3.253906 7.921875 6.671875 5.191406 C 10.09375 2.464844 15.011719 2.710938 18.140625 5.765625 C 19.734375 7.453125 20.765625 9.609375 20.71875 11.90625 C 20.765625 16.6875 16.78125 20.578125 12 20.578125 Z M 16.734375 14.109375 C 16.5 13.96875 15.1875 13.3125 14.953125 13.265625 C 14.71875 13.171875 14.53125 13.125 14.390625 13.359375 L 13.546875 14.390625 C 13.40625 14.578125 13.265625 14.578125 12.984375 14.484375 C 11.484375 13.6875 10.453125 13.078125 9.46875 11.390625 C 9.1875 10.921875 9.703125 10.921875 10.21875 9.9375 C 10.3125 9.75 10.265625 9.609375 10.171875 9.46875 L 9.375 7.546875 C 9.1875 7.078125 8.953125 7.125 8.8125 7.125 L 8.296875 7.125 C 8.109375 7.125 7.875 7.171875 7.59375 7.453125 C 7.359375 7.6875 6.703125 8.34375 6.703125 9.609375 C 6.703125 10.875 7.640625 12.140625 7.78125 12.28125 C 7.875 12.46875 9.609375 15.09375 12.1875 16.21875 C 13.875 16.921875 14.484375 17.015625 15.328125 16.875 C 15.84375 16.78125 16.875 16.21875 17.0625 15.609375 C 17.25 15 17.296875 14.484375 17.25 14.390625 C 17.15625 14.296875 17.015625 14.203125 16.734375 14.109375 Z M 16.734375 14.109375 "></path>
</svg>

        </a>
        <a data-sharer="telegram" href="#" title="Telegram">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 12 0.375 C 5.578125 0.375 0.375 5.578125 0.375 12 C 0.375 18.421875 5.578125 23.625 12 23.625 C 18.421875 23.625 23.625 18.421875 23.625 12 C 23.625 5.578125 18.421875 0.375 12 0.375 Z M 17.390625 8.296875 C 17.203125 10.125 16.453125 14.578125 16.078125 16.640625 C 15.890625 17.53125 15.609375 17.8125 15.28125 17.8125 C 14.625 17.90625 14.109375 17.390625 13.453125 16.96875 L 10.828125 15.234375 C 9.703125 14.4375 10.453125 14.0625 11.109375 13.359375 C 11.25 13.171875 14.25 10.5 14.296875 10.21875 L 14.25 10.03125 L 14.015625 9.984375 C 13.890625 10.015625 12.25 11.109375 9.09375 13.265625 C 8.625 13.578125 8.203125 13.71875 7.828125 13.6875 C 7.40625 13.6875 6.609375 13.453125 6.046875 13.265625 C 5.296875 13.03125 4.734375 12.9375 4.78125 12.515625 C 4.8125 12.296875 5.09375 12.078125 5.625 11.859375 L 12.421875 8.953125 C 15.65625 7.59375 16.3125 7.359375 16.734375 7.359375 C 16.828125 7.359375 17.0625 7.40625 17.203125 7.5 L 17.390625 7.828125 C 17.410156 7.984375 17.410156 8.140625 17.390625 8.296875 Z M 17.390625 8.296875 "></path>
</svg>

        </a>
        <a data-sharer="email" href="#" title="Email">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 21.75 3 C 22.710938 3.007812 23.558594 3.621094 23.867188 4.53125 C 24.171875 5.441406 23.871094 6.445312 23.109375 7.03125 L 12.890625 14.71875 C 12.375 15.09375 11.625 15.09375 11.109375 14.71875 L 0.890625 7.03125 C 0.128906 6.445312 -0.171875 5.441406 0.132812 4.53125 C 0.441406 3.621094 1.289062 3.007812 2.25 3 Z M 10.21875 15.890625 C 11.25 16.6875 12.75 16.6875 13.78125 15.890625 L 24 8.25 L 24 18 C 24 19.640625 22.640625 21 21 21 L 3 21 C 1.359375 21 0 19.640625 0 18 L 0 8.25 Z M 10.21875 15.890625 "></path>
</svg>

        </a>
    </div>
    <p><a href="#" onclick="copyToClipboard('#share_url')">Copy</a> to your clipboard</p>
    <p><input id="share_url" type="text"></p>
  </div>
</div>

  </div>

  <span>["Strategies"]</span>
  <h1>Principle of Safety, key considerations</h1>

  <div class="image-gallery">
  </div>

  <p><a href='?tags=reflection-discussion' class='tag'>reflection-discussion</a></p>


  <div class="text-gray-500">"Beyond safeguarding the sustainability of your AI project as it relates to its social impacts on individual wellbeing and public welfare, your project team must also confront the related challenge of technical sustainability or safety. A technically sustainable AI system is safe, accurate, reliable, secure, and robust. Securing these goals, however, is a difficult and unremitting task.Because AI systems operate in a world filled with uncertainty, volatility, and flux, the challenge of building safe and reliable AI can be especially daunting. This job, however, must be met head-on. Only by making the goal of producing safe and reliable AI technologies central to your project, will you be able to mitigate risks of your system failing at scale when faced with real-world unknowns and unforeseen events. The issue of AI safety is of paramount importance, because these potential failures may both produce harmful outcomes and undermine public trust.In order to safeguard that your AI system functions safely, you must prioritise the technical objectives of accuracy, reliability, security, and robustness. This requires that your technical team put careful forethought into how to construct a system that accurately and dependably operates in accordance with its designers’ expectations even when confronted with unexpected changes, anomalies, and perturbations. Building an AI system that meets these safety goals also requires rigorous testing, validation, and re-assessment as well as the integration of adequate mechanisms of oversight and control into its real-world operation._Accuracy, reliability, security, and robustness___It is important that you gain a strong working knowledge of each of the safety relevant operational objectives (accuracy, reliability, security, and robustness):·Accuracy and Performance Metrics: In machine learning, the accuracy of a model is the proportion of examples for which it generates a correct output. This performance measure is also sometimes characterised conversely as an error rate or the fraction of cases for which the model produces an incorrect output. Keep in mind that, in some instances, the choice of an acceptable error rate or accuracy level can be adjusted in accordance with the use case specific needs of the application. In other instances, it may be largely set by a domain established benchmark. As a performance metric, accuracy should be a central component to establishing and nuancing your team’s approach to safe AI. That said, specifying a reasonable performance level for your system may also often require you to refine or exchange your measure of accuracy. For instance, if certain errors are more significant or costly than others, a metric for total cost can be integrated into your model so that the cost of one class of errors can be weighed against that of another. Likewise, if the precision and sensitivity of the system in detecting uncommon events is a priority (say, in instances of the medical diagnosis of rare cases of a disease), you can use the technique of precision and recall. This method of addressing imbalanced classification would allow you to weigh the proportion of the system’s correct detections—both of frequent and of rare outcomes—against the proportion of actual detections of the rare event (i.e. the ratio of the true detections of the rare outcome to the sum of the true detections of that outcome and the missed detections or false negatives for that outcome).In general, measuring accuracy in the face of uncertainty is a challenge that must be given significant thought. The confidence level of your AI system will depend heavily on problems inherent in attempts to model a chaotic and changing reality. Concerns about accuracy must cope with issues of unavoidable noise present in the data sample, architectural uncertainties generated by the possibility that a given model is missing relevant features of the underlying distribution, and inevitable changes in input data over time.·Reliability: The objective of reliability is that an AI system behaves exactly as its designers intended and anticipated. A reliable system adheres to the specifications it was programmed to carry out. Reliability is therefore a measure of consistency and can establish confidence in the safety of a system based upon the dependability with which it operationally conforms to its intended functionality.·Security: The goal of security encompasses the protection of several operational dimensions of an AI system when confronted with possible adversarial attack. A secure system is capable of maintaining the integrity of the information that constitutes it. This includes protecting its architecture from the unauthorised modification or damage of any of its component parts. A secure system also remains continuously functional and accessible to its authorised users and keeps confidential and private information secure even under hostile or adversarial conditions.·Robustness: The objective of robustness can be thought of as the goal that an AI system functions reliably and accurately under harsh conditions. These conditions may include adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications). The measure of robustness is therefore the strength of a system’s integrity and the soundness of its operation in response to difficult conditions, adversarial attacks, perturbations, data poisoning, and undesirable reinforcement learning behaviour._Risks posed to accuracy and reliability:_ Concept Drift: Once trained, most machine learning systems operate on static models of the world that have been built from historical data which have become fixed in the systems’ parameters. This freezing of the model before it is released ‘into the wild’ makes its accuracy and reliability especially vulnerable to changes in the underlying distribution of data. When the historical data that have crystallised into the trained model’s architecture cease to reflect the population concerned, the model’s mapping function will no longer be able to accurately and reliably transform its inputs into its target output values. These systems can quickly become prone to error in unexpected and harmful ways.There has been much valuable research done on methods of detecting and mitigating concept and distribution drift, and you should consult with your technical team to ensure that its members have familiarised themselves with this research and have sufficient knowledge of the available ways to confront the issue. In all cases, you should remain vigilant to the potentially rapid concept drifts that may occur in the complex, dynamic, and evolving environments in which your AI project will intervene. Remaining aware of these transformations in the data is crucial for safe AI, and your team should actively formulate an action plan to anticipate and to mitigate their impacts on the performance of your system.Brittleness: Another possible challenge to the accuracy and reliability of machine learning systems arises from the inherent imitations of the systems themselves. Many of the high- performing machine learning models such as deep neural nets (DNN) rely on massive amounts of data and brute force repetition of training examples to tune the thousands, millions, or even billions of parameters, which collectively generate their outputs.However, when they are actually running in an unpredictable world, these systems may have difficulty processing unfamiliar events and scenarios. They may make unexpected and serious mistakes, because they have neither the capacity to contextualise the problems they are programmed to solve nor the common-sense ability to determine the relevance of new‘unknowns’. Moreover, these mistakes may remain unexplainable given the high- dimensionality and computational complexity of their mathematical structures. This fragility or brittleness can have especially significant consequences in safety-critical applications like fully automated transportation and medical decision support systems where undetectable changes in inputs may lead to significant failures. While progress is being made in finding ways to make these models more robust, it is crucial to consider safety first when weighing up their viability._Risks posed to security and robustness___Adversarial Attack: Adversarial attacks on machine learning models maliciously modify input data—often in imperceptible ways—to induce them into misclassification or incorrect prediction. For instance, by undetectably altering a few pixels on a picture, an adversarial attacker can mislead a model into generating an incorrect output (like identifying a panda as a gibbon or a ‘stop’ sign as a ‘speed limit’ sign) with an extremely high confidence. While a good amount of attention has been paid to the risks that adversarial attacks pose in deep learning applications like computer vision, these kinds of perturbations are also effective across a vast range of machine learning techniques and uses such as spam filtering and malware detection. These vulnerabilities of AI systems to adversarial examples have serious consequences for AI safety. The existence of cases where subtle but targeted perturbations cause models to be misled into gross miscalculation and incorrect decisions have potentially serious safety implication for the adoption of critical systems like applications in autonomous transportation, medical imaging, and security and surveillance. In response to concerns about the threats posed to a safe and trusted environment for AI technologies by adversarial attacks a field called adversarial machine learning has emerged over the past several years.Work in this area focuses on securing systems from disruptive perturbations at all points of vulnerability across the AI pipeline.One of the major safety strategies that has arisen from this research is an approach called model hardening, which has advanced techniques that combat adversarial attacks by strengthening the architectural components of the systems. Model hardening techniques may include adversarial training, where training data is methodically enlarged to include adversarial examples. Other model hardening methods involve architectural modification, regularisation, and data pre-processing manipulation. A second notable safety strategy is run- time detection, where the system is augmented with a discovery apparatus that can identify and trace in real-time the existence of adversarial examples. You should consult with members of your technical team to ensure that the risks of adversarial attack have been taken into account and mitigated throughout the AI lifecycle. A valuable collection of resources to combat adversarial attack can be found at [https://github.com/IBM/adversarial-](https://github.com/IBM/adversarial-robustness-toolbox) [robustness-toolbox](https://github.com/IBM/adversarial-robustness-toolbox) .Data Poisoning: A different but related type of adversarial attack is called data poisoning. This threat to safe and reliable AI involves a malicious compromise of data sources at the point of collection and pre-processing. Data poisoning occurs when an adversary modifies or manipulates part of the dataset upon which a model will be trained, validated, and tested. By altering a selected subset of training inputs, a poisoning attack can induce a trained AI system into curated misclassification, systemic malfunction, and poor performance. An especially concerning dimension of targeted data poisoning is that an adversary may introduce a‘backdoor’ into the infected model whereby the trained system functions normally until itprocesses maliciously selected inputs that trigger error or failure.In order to combat data poisoning, your technical team should become familiar with the state of the art in filtering and detecting poisoned data. However, such technical solutions are not enough. Data poisoning is possible because data collection and procurement often involves potentially unreliable or questionable sources. When data originates in uncontrollable environments like the internet, social media, or the Internet of Things, many opportunities present themselves to ill-intentioned attackers, who aim to manipulate training examples. Likewise, in third- party data curation processes (such as ‘crowdsourced’ labelling, annotation, and content identification), attackers may simply handcraft malicious inputs.Your project team should focus on the best practices of responsible data management, so that they are able to tend to data quality as an end-to-end priority.·Misdirected Reinforcement Learning Behaviour: A different set of safety risks emerges from the approach to machine learning called reinforcement learning (RL). In the more widely applied methods of supervised learning that have largely been the focus of this guide, a model transforms inputs into outputs according to a fixed mapping function that has resulted from its passively received training. In RL, by contrast, the learner system actively solves problems by engaging with its environment through trial and error. This exploration and‘problem-solving’ behaviour is determined by the objective of maximising a reward function that is defined by its designers.This flexibility in the model, however, comes at the price of potential safety risks. An RL system, which is operating in the real-world without sufficient controls, may determine a reward-optimising course of action that is optimal for achieving its desired objective but harmful to people. Because these models lack context-awareness, common sense, empathy, and understanding, they are unable to identify, on their own, scenarios that may have damaging consequences but that were not anticipated and constrained by their programmers. This is a difficult problem, because the unbounded complexity of the world makes anticipating all of its pitfalls and detrimental variables veritably impossible.Existing strategies to mitigate such risks of misdirected reinforcement learning behaviour include:oRunning extensive simulations during the testing stage, so that appropriate measures of constraint can be programmed into the systemoContinuous inspection and monitoring of the system, so that its behaviour can be better predicted and understoodoFinding ways to make the system more interpretable so that its decisions can be better assessedoHard-wiring mechanisms into the system that enable human override and system shut-downEnd-to-End AI SafetyThe safety risks you face in your AI project will depend, among other factors, on the sort of algorithm(s) and machine learning techniques you are using, the type of applications in which those techniques are going to be deployed, the provenance of your data, the way you are specifying your objective, and the problem domain in which that specification applies. As a best practice, regardless of this variability of techniques and circumstances, safety considerations of accuracy, reliability, security, and robustness should be in operation at every stage of your AI project lifecycle.This should involve both rigorous protocols of testing, validating, verifying, and monitoring the safety of the system and the performance of AI safety self-assessments by relevant members of your team at each stage of the workflow. Such self-assessments should evaluate how your team’s design and implementation practices line up with the safety objectives of accuracy, reliability, security, and robustness. Your AI safety self-assessments should be logged across the workflow on a single document in a running fashion that allows review and re-assessment."(Leslie, 2019, p26-30)### IEEE Report"Operational failures and, in particular, violations of a system’s embedded community norms, are unavoidable, both during system testing and during deployment. Not only are implementations never perfect, but A/IS with embedded norms will update or expand their norms over time (see Section 1, Issue 2) and interactions in the social world are particularly complex and uncertain. Thus, prevention and mitigation strategies must be adopted, and we sample four possible ones.First, anticipating the process of evaluation during the implementation phase requires defining criteria and metrics for such evaluation, which in turn better allows the detection and mitigation of failures. Metrics will include:•Technical variables, such as traceability and verifiability,•User-level variables such as reliability, understandable explanations, and responsiveness to feedback, and•Community-level variables such as justified trust (see Issue 2) and the collective belief that A/IS are generally creating social benefits rather than, for example, technological unemployment.Second, a systematic risk analysis and management approach can be useful (Oetzel and Spiekermann 201443) for an application to privacy norms. This approach tries to anticipate potential points of failure, e.g., norm violations, and, where possible, develops some ways to reduce or remove the effects of failures. Successful behavior, and occasional failures, can then iteratively improve predictions andmitigation attempts.Third, because not all risks and failures are predictable (Brundage et al 201844; Vanderelst and Winfield 201845), especially in complex human-machine interactions in social contexts, additional mitigation mechanisms must be made available. Designers are strongly encouraged to augment the architectures of their systems with components that handle unanticipated norm violations with a fail-safe, such as the symbolic “gateway” agents discussed in Section 2, Issue 1. Designers should identify a number of strict laws, that is, task- and community-specific norms that should never be violated, and the failsafe components should continuously monitor operations against possible violations of these laws. In case of violations, the higher-order gateway agent should take appropriate actions, such as safely disabling the system’s operation, or greatly limiting its scope of operation, untilthe source of failure is identified. The failsafe components need to be understandable, extremely reliable, and protected against security breaches, which can be achieved, for example, by validating them carefully and not letting them adapt their parameters during execution.Fourth, once failures have occurred, responsible entities, e.g., corporate, government, science, and engineering, shall create a publicly accessible database with undesired outcomes caused by specific A/IS systems. The database would include descriptions of the problem, background information on how the problem was detected, which context it occurred in, and how it was addressed.In summary, we offer the following recommendation.## RecommendationBecause designers and developers cannot anticipate all possible operating conditions and potential failures of A/IS, multiple strategies to mitigate the chance and magnitude of harmmust be in place.### Further Resources•M. Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garfunkel, A. Dafoe, P. Scharre, T. Zeitzo, et al. " “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,” CoRR abs/1802.07228 [cs.AI]. 2018. <https://arxiv.org/abs/1802.07228>•M. C. Oetzel and S. Spiekermann, “A Systematic Methodology for Privacy Impact Assessments: A Design Science Approach.” _European Journal of Information Systems_, vol._ _23, pp. 126–150, 2014. [https://link.springer. com/article/10.1057/ejis.2013.18](https://link.springer.com/article/10.1057/ejis.2013.18)•D. Vanderelst and A.F. Winfield, 2018 “The Dark Side of Ethical Robots,” In Proc. The First AAAI/ACM Conf. on Artificial Intelligence, Ethics and Society, New Orleans, LA, Feb. 1 -3, 2018."p.180-181</div>

  <div class="my-6">
      <div class="my-3">
        <b>Principles</b>
        <span class="text-gray-500">
            <a href="/?global=recOHnq45Fq7YWsRO" class="tag">Merit and Integrity</a> <a href="/?global=recy6hrMpKZ7TOn3Q" class="tag">Do no harm with these technologies and minimise the risks of their misuse or abuse</a>
        </span>
      </div>
      <div class="my-3">
        <b>Sources</b>
        <span class="text-gray-500">
            <a href="/?global=recfYC5jjPmpLfSlM" class="tag">Turing responsible design and implementation of AI systems in the public sector</a> <a href="/?global=recpXl48pJdKDhc6f" class="tag">IEEE</a>
        </span>
      </div>
      <div class="my-3">
        <b>Created At</b>
        <span class="text-gray-500">
              <a href="2023-05-19T11:51:22.000Z" target="_blank">2023-05-19T11:51:22.000Z</a>
        </span>
      </div>
      <div class="my-3">
        <b>Title</b>
        <span class="text-gray-500">
            Principle of Safety, key considerations
        </span>
      </div>
  </div>
</div>

    <footer class="fixed bottom-0 z-20 p-2 w-full border-t bg-white shadow-lg text-center">
  <span class="text-sm text-gray-500">
      My Collection | powered by <a href="https://github.com/Subgin/tonic">Tonic v0.17.0</a>
  </span>
</footer>


    <script src="application-b6b72200.js"></script>
    <script>
      window.config = JSON.parse(JSON.stringify({"title":"My Collection","detail_pages":true,"item_card_image":true,"sorting":{"default_order":"price asc"},"description":"This is an example of a \u003ca href=\"https://github.com/Subgin/tonic\"\u003eTonic\u003c/a\u003e project.","main_color":"#6366f1","background_color":"#e2e8f0","font_family":"Lato","links":[{"text":"About Us","url":"#"},{"text":"Contact","url":"#"}],"filters":{"exclude":["video","audio"]}}));
    </script>
  </body>
</html>
