<!DOCTYPE html>
<html>
  <head>
    <title>Ethics Principles, Challenges, Cases, and Strategies | Roles regarding ethics should have leadership capacity, to foster ethical culture across the organisation</title>
    <meta name="description" content="## RecommendationsCompanies should create roles for senior-level marketers, engineers, and lawyers who can collective...">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="../stylesheets/application-bc73e972.css" rel="stylesheet" />
    <link href="../images/favicon.ico" rel="icon" type="image/ico" />
      <link href="https://fonts.googleapis.com/css2?family=Arimo:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
      :root {
        --main-color: #0f4beb;
        --background-color: #ebebeb;
        --font-family: Arimo;
      }
    </style>
  </head>
  <body>
    <div id="header" class="flex justify-between w-full px-6 py-3 items-center">
  <div class="flex items-center">
      <a href="/" class="text-2xl text-white">Ethics Principles, Challenges, Cases, and Strategies</a>
  </div>
  <div class="flex gap-4">
    <div class="hidden sm:inline">
        <a href="/about" class="nav-link">About</a>
        <a href="/cards" class="nav-link">Cards</a>
        <a href="/guideline" class="nav-link">Guideline</a>
    </div>

  </div>

  <div>
    <div>
  <nav class="menu">
    <div class="menu-toggle">
      <button class="accordion bg-gray-300 hover:bg-gray-400 py-2 px-4 rounded">Pages</button>
    </div>
    <div class="menu-content hidden">
      <ul class="menu-items">
          <li>
            <a href="meta"
              class="block py-2 px-4 text-gray-700 hover:bg-gray-100 transition-colors duration-300 ease-in-out">
              Meta-data
            </a>
          </li>
          <li>
            <a href="introduction"
              class="block py-2 px-4 text-gray-700 hover:bg-gray-100 transition-colors duration-300 ease-in-out">
              Introduction
            </a>
          </li>
          <li>
            <a href="respect-for-persons"
              class="block py-2 px-4 text-gray-700 hover:bg-gray-100 transition-colors duration-300 ease-in-out">
              Respect for Persons and Human Dignity
            </a>
          </li>
          <li>
            <a href="beneficence"
              class="block py-2 px-4 text-gray-700 hover:bg-gray-100 transition-colors duration-300 ease-in-out">
              Beneficence
            </a>
          </li>
          <li>
            <a href="justice"
              class="block py-2 px-4 text-gray-700 hover:bg-gray-100 transition-colors duration-300 ease-in-out">
              Justice
            </a>
          </li>
          <li>
            <a href="worth"
              class="block py-2 px-4 text-gray-700 hover:bg-gray-100 transition-colors duration-300 ease-in-out">
              Worth
            </a>
          </li>
          <li>
            <a href="end"
              class="block py-2 px-4 text-gray-700 hover:bg-gray-100 transition-colors duration-300 ease-in-out">
              End-matter
            </a>
          </li>
          <li>
            <a href="about"
              class="block py-2 px-4 text-gray-700 hover:bg-gray-100 transition-colors duration-300 ease-in-out">
              About
            </a>
          </li>
      </ul>
    </div>
  </nav>
</div>

<script>
  // JavaScript to toggle accordion menu
  const menuToggle = document.querySelector('.menu-toggle button');
  const menuContent = document.querySelector('.menu-content');

  menuToggle.addEventListener('click', function () {
    menuContent.classList.toggle('hidden');
  });
</script>

<style>
/* CSS for accordion menu */
.menu-content {
  transition: max-height 0.3s ease-out;
  overflow: hidden;
  max-height: 0;
}

/* CSS for accordion menu */
.menu {
  font-family: 'Arimo'; /* Inherit font family from config.yaml */
  color: '#0f4beb'; /* Inherit color from config.yaml */
  width: fit-content; /* Adjust the width as needed */
  margin-right: 1rem; /* Add margin to separate from other header elements */
}

.menu-toggle {
  background-color: '#ebebeb'; /* Inherit background color from config.yaml */
  padding: 10px;
}

.menu-toggle button {
  background-color: transparent;
  border: none;
  font-size: 1.2em;
  color: #333;
  cursor: pointer;
}

.menu-content:not(.hidden) {
  max-height: none; /* Remove the fixed height to show all menu items */
  transition: max-height 0.3s ease-in;
}

.menu-items {
  list-style-type: none;
  padding: 0;
  margin: 0;
}

.menu-items li {
  padding: 10px;
}

.menu-items li a {
  display: block;
  padding: 10px;
  color: #333;
  text-decoration: none;
  transition: background-color 0.3s ease-in-out;
}

.menu-items li a:hover {
  background-color: #f5f5f5;
}

</style>


  </div>
</div>

    
<div class="mt-6 mb-24 px-8 md:px-32 mx-auto max-w-screen-xl">
  <div class="flex justify-between items-center mb-4">
    <a href="#" onclick="back('/')">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="15 18 9 12 15 6"></polyline></svg>
 Back
    </a>
      <a href="#" onclick="toggleSharing()" class="ml-3">
  <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path><polyline points="16 6 12 2 8 6"></polyline><line x1="12" y1="2" x2="12" y2="15"></line></svg>
 Share
</a>
<div id="sharing-options" class="dropdown hidden right-5 md:right-32 top-32">
  <div class="px-6 py-3">
    <p>Share this page</p>
    <div id="sharing-buttons" class="mb-4">
        <a data-sharer="facebook" href="#" title="Facebook">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 17.765625 1.03125 L 17.765625 4.546875 L 15.703125 4.546875 C 14.015625 4.546875 13.734375 5.34375 13.734375 6.46875 L 13.734375 9 L 17.671875 9 L 17.109375 12.984375 L 13.734375 12.984375 L 13.734375 23.15625 L 9.609375 23.15625 L 9.609375 12.984375 L 6.234375 12.984375 L 6.234375 9 L 9.609375 9 L 9.609375 6.09375 C 9.609375 2.71875 11.71875 0.84375 14.71875 0.84375 C 16.171875 0.84375 17.4375 0.984375 17.765625 1.03125 Z M 17.765625 1.03125 "></path>
</svg>

        </a>
        <a data-sharer="twitter" href="#" title="Twitter">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 21.515625 7.125 L 21.5625 7.734375 C 21.5625 14.25 16.59375 21.75 7.546875 21.75 C 4.78125 21.75 2.15625 20.953125 0 19.546875 C 2.613281 19.828125 5.234375 19.089844 7.3125 17.484375 C 5.109375 17.4375 3.328125 16.03125 2.71875 14.109375 L 3.609375 14.15625 C 4.078125 14.15625 4.5 14.109375 4.921875 14.015625 C 2.671875 13.546875 0.984375 11.578125 0.984375 9.1875 L 0.984375 9.09375 C 1.640625 9.46875 2.390625 9.703125 3.1875 9.75 C 1.007812 8.296875 0.347656 5.390625 1.6875 3.140625 C 4.078125 6.140625 7.734375 8.109375 11.8125 8.296875 C 11.71875 7.921875 11.71875 7.546875 11.71875 7.171875 C 11.726562 5.164062 12.957031 3.363281 14.820312 2.621094 C 16.6875 1.878906 18.816406 2.34375 20.203125 3.796875 C 21.328125 3.609375 22.40625 3.1875 23.34375 2.625 C 22.96875 3.75 22.171875 4.734375 21.1875 5.34375 C 22.171875 5.203125 23.109375 4.96875 24 4.546875 C 23.34375 5.53125 22.5 6.421875 21.515625 7.125 Z M 21.515625 7.125 "></path>
</svg>

        </a>
        <a data-sharer="linkedin" href="#" title="Linkedin">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 6.375 8.578125 L 6.375 21.84375 L 1.96875 21.84375 L 1.96875 8.578125 Z M 6.65625 4.453125 C 6.703125 5.71875 5.71875 6.75 4.171875 6.75 C 2.671875 6.75 1.734375 5.71875 1.734375 4.453125 C 1.734375 3.140625 2.71875 2.15625 4.21875 2.15625 C 5.71875 2.15625 6.65625 3.140625 6.65625 4.453125 Z M 22.265625 14.203125 L 22.265625 21.84375 L 17.859375 21.84375 L 17.859375 14.71875 C 17.859375 12.9375 17.25 11.71875 15.65625 11.71875 C 14.4375 11.71875 13.6875 12.5625 13.359375 13.359375 C 13.265625 13.640625 13.21875 14.015625 13.21875 14.4375 L 13.21875 21.84375 L 8.8125 21.84375 L 8.8125 8.578125 L 13.21875 8.578125 L 13.21875 10.5 C 13.78125 9.5625 14.8125 8.25 17.203125 8.25 C 20.109375 8.25 22.265625 10.171875 22.265625 14.203125 Z M 22.265625 14.203125 "></path>
</svg>

        </a>
        <a data-sharer="pinterest" href="#" title="Pinterest">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 12.5625 0.28125 C 7.734375 0.28125 3 3.515625 3 8.71875 C 3 12 4.875 13.875 6 13.875 C 6.421875 13.875 6.703125 12.5625 6.703125 12.234375 C 6.703125 11.765625 5.578125 10.828125 5.578125 9.046875 C 5.578125 5.25 8.484375 2.578125 12.1875 2.578125 C 15.375 2.578125 17.71875 4.40625 17.71875 7.734375 C 17.71875 10.21875 16.734375 14.90625 13.5 14.90625 C 12.328125 14.90625 11.34375 14.0625 11.34375 12.84375 C 11.34375 11.0625 12.5625 9.375 12.5625 7.546875 C 12.5625 4.40625 8.15625 4.96875 8.15625 8.71875 C 8.15625 9.515625 8.25 10.40625 8.625 11.109375 C 7.96875 13.921875 6.65625 18.046875 6.65625 20.90625 C 6.65625 21.796875 6.796875 22.6875 6.84375 23.578125 C 7.03125 23.71875 6.9375 23.71875 7.171875 23.625 C 9.5625 20.390625 9.46875 19.78125 10.546875 15.515625 C 11.109375 16.640625 12.609375 17.203125 13.78125 17.203125 C 18.75 17.203125 21 12.375 21 8.015625 C 21 3.328125 16.96875 0.28125 12.5625 0.28125 Z M 12.5625 0.28125 "></path>
</svg>

        </a>
        <a data-sharer="whatsapp" href="#" title="Whatsapp">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 19.359375 4.546875 C 15.664062 0.875 9.820312 0.503906 5.6875 3.675781 C 1.554688 6.847656 0.40625 12.589844 3 17.109375 L 1.453125 22.5 L 6.984375 21.046875 C 10.222656 22.785156 14.132812 22.707031 17.296875 20.835938 C 20.457031 18.96875 22.414062 15.582031 22.453125 11.90625 C 22.453125 9.140625 21.28125 6.515625 19.3125 4.546875 Z M 12 20.578125 C 10.453125 20.578125 8.90625 20.15625 7.59375 19.359375 L 7.265625 19.171875 L 3.984375 20.015625 L 4.875 16.828125 L 4.6875 16.5 C 2.402344 12.769531 3.253906 7.921875 6.671875 5.191406 C 10.09375 2.464844 15.011719 2.710938 18.140625 5.765625 C 19.734375 7.453125 20.765625 9.609375 20.71875 11.90625 C 20.765625 16.6875 16.78125 20.578125 12 20.578125 Z M 16.734375 14.109375 C 16.5 13.96875 15.1875 13.3125 14.953125 13.265625 C 14.71875 13.171875 14.53125 13.125 14.390625 13.359375 L 13.546875 14.390625 C 13.40625 14.578125 13.265625 14.578125 12.984375 14.484375 C 11.484375 13.6875 10.453125 13.078125 9.46875 11.390625 C 9.1875 10.921875 9.703125 10.921875 10.21875 9.9375 C 10.3125 9.75 10.265625 9.609375 10.171875 9.46875 L 9.375 7.546875 C 9.1875 7.078125 8.953125 7.125 8.8125 7.125 L 8.296875 7.125 C 8.109375 7.125 7.875 7.171875 7.59375 7.453125 C 7.359375 7.6875 6.703125 8.34375 6.703125 9.609375 C 6.703125 10.875 7.640625 12.140625 7.78125 12.28125 C 7.875 12.46875 9.609375 15.09375 12.1875 16.21875 C 13.875 16.921875 14.484375 17.015625 15.328125 16.875 C 15.84375 16.78125 16.875 16.21875 17.0625 15.609375 C 17.25 15 17.296875 14.484375 17.25 14.390625 C 17.15625 14.296875 17.015625 14.203125 16.734375 14.109375 Z M 16.734375 14.109375 "></path>
</svg>

        </a>
        <a data-sharer="telegram" href="#" title="Telegram">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 12 0.375 C 5.578125 0.375 0.375 5.578125 0.375 12 C 0.375 18.421875 5.578125 23.625 12 23.625 C 18.421875 23.625 23.625 18.421875 23.625 12 C 23.625 5.578125 18.421875 0.375 12 0.375 Z M 17.390625 8.296875 C 17.203125 10.125 16.453125 14.578125 16.078125 16.640625 C 15.890625 17.53125 15.609375 17.8125 15.28125 17.8125 C 14.625 17.90625 14.109375 17.390625 13.453125 16.96875 L 10.828125 15.234375 C 9.703125 14.4375 10.453125 14.0625 11.109375 13.359375 C 11.25 13.171875 14.25 10.5 14.296875 10.21875 L 14.25 10.03125 L 14.015625 9.984375 C 13.890625 10.015625 12.25 11.109375 9.09375 13.265625 C 8.625 13.578125 8.203125 13.71875 7.828125 13.6875 C 7.40625 13.6875 6.609375 13.453125 6.046875 13.265625 C 5.296875 13.03125 4.734375 12.9375 4.78125 12.515625 C 4.8125 12.296875 5.09375 12.078125 5.625 11.859375 L 12.421875 8.953125 C 15.65625 7.59375 16.3125 7.359375 16.734375 7.359375 C 16.828125 7.359375 17.0625 7.40625 17.203125 7.5 L 17.390625 7.828125 C 17.410156 7.984375 17.410156 8.140625 17.390625 8.296875 Z M 17.390625 8.296875 "></path>
</svg>

        </a>
        <a data-sharer="email" href="#" title="Email">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" width="24" height="24pt" viewBox="0 0 24 24" fill="currentColor">
<path d="M 21.75 3 C 22.710938 3.007812 23.558594 3.621094 23.867188 4.53125 C 24.171875 5.441406 23.871094 6.445312 23.109375 7.03125 L 12.890625 14.71875 C 12.375 15.09375 11.625 15.09375 11.109375 14.71875 L 0.890625 7.03125 C 0.128906 6.445312 -0.171875 5.441406 0.132812 4.53125 C 0.441406 3.621094 1.289062 3.007812 2.25 3 Z M 10.21875 15.890625 C 11.25 16.6875 12.75 16.6875 13.78125 15.890625 L 24 8.25 L 24 18 C 24 19.640625 22.640625 21 21 21 L 3 21 C 1.359375 21 0 19.640625 0 18 L 0 8.25 Z M 10.21875 15.890625 "></path>
</svg>

        </a>
    </div>
    <p><a href="#" onclick="copyToClipboard('#share_url')">Copy</a> to your clipboard</p>
    <p><input id="share_url" type="text"></p>
  </div>
</div>

  </div>

  <span>Strategies</span>
  <h1>Roles regarding ethics should have leadership capacity, to foster ethical culture across the organisation</h1>

  <div class="image-gallery">
  </div>

  <p></p>


  <div class="text-gray-500">

  


  <h1><br><br># RecommendationsCompanies should create roles for senior-level marketers, engineers, and lawyers who can collectively and pragmatically implement ethically aligned design. There is also a need for more in-house ethicists, or positions that fulfill similar roles. One potential way to ensure values are on the agenda in A/IS development is to have a Chief Values Officer (CVO), a role first suggested by Kay Firth-Butterfield, see “Further Resources”. However, ethical responsibility should not be delegated solely to CVOs. They can support the creation of ethical knowledge in companies, but in the end, all members of an organization will need to act responsibly throughout the design process.Companies need to ensure that their understanding of values-based system innovation is based on _de jure _and _de facto _international human rights standards.</h1>

<ul>
<li>K. Firth-Butterfield, “<a href="http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design">How IEEE Aims to Instill Ethics in Artificial Intelligence Design,</a>” The Institute. Jan. 19, <br></li>
<li><p>[Online]. Available: <a href="http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design">http://theinstitute.ieee.org/ieee-roundup/ blogs/blog/how-ieee-aims-to-instill-ethicsin-artificial-intelligence-design</a>. [Accessed October 28, 2018]. </p></li>
<li><p>United Nations, <a href="http://www.ohchr.org/Documents/Publications/GuidingPrinciplesBusinessHR_EN.pdf">Guiding Principles on Business and Human Rights: Implementing the United Nations “Protect, Respect and Remedy” Framework,</a> New York and Geneva: UN, <br></p></li>
<li></li>
<li><p>Institute for Human Rights and Business(IHRB), and Shift, ICT <a href="https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf">Sector Guide on </a><a href="https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf">Implementing the UN Guiding Principles on Business and Human Rights,</a> <br></p></li>
<li></li>
<li><p>C. Cath, and L. Floridi, “<a href="http://europepmc.org/abstract/med/27255607">The Design of the Internet’s Architecture by the Internet </a><a href="http://europepmc.org/abstract/med/27255607">Engineering Task Force (IETF) and Human Rights</a>.” <em>Science and Engineering Ethics, _vol.</em> _23, no. 2, pp. 449–468, Apr. <br></p></li>
<li><p>&quot;p.128</p></li>
</ul>



  </div>

  <div class="my-6">
      <div class="my-3">
        <b>Challenges</b>
        <span class="text-gray-500">
            <a href="/cards?global=recaBNAcact8Bz6dg" class="tag">How do leaders provide direction regarding promotion of human values in AI design?</a>
        </span>
      </div>
      <div class="my-3">
        <b>Sources</b>
        <span class="text-gray-500">
            <a href="/cards?global=recpXl48pJdKDhc6f" class="tag">IEEE</a>
        </span>
      </div>
      <div class="my-3">
        <b>Created At</b>
        <span class="text-gray-500">
              <a href="2023-06-05T10:29:17.000Z" target="_blank">2023-06-05T10:29:17.000Z</a>
        </span>
      </div>
      <div class="my-3">
        <b>Title</b>
        <span class="text-gray-500">
            Roles regarding ethics should have leadership capacity, to foster ethical culture across the organisation
        </span>
      </div>
  </div>
</div>

    <footer class="fixed bottom-0 z-20 p-2 w-full border-t bg-white shadow-lg text-center">
  <span class="text-sm text-gray-500">
      Follow Simon Knight on <a href="https://twitter.com/sjgknight">Twitter</a>
  </span>
</footer>


    <script src="../application-6e20ebee.js"></script>
    <script>
      window.config = JSON.parse(JSON.stringify({"title":"Ethics Principles, Challenges, Cases, and Strategies","detail_pages":true,"item_card_image":true,"text_box":"#fcfcfb","sorting":{"default_order":"Title asc"},"description":"This is an example of a \u003ca href=\"https://github.com/Subgin/tonic\"\u003eTonic\u003c/a\u003e project.\nYou can view the modified code at \u003ca href=\"https://github.com/sjgknight/tonic-filtering\"\u003esjgknight/tonic-fltering\u003c/a\u003e\n","main_color":"#0f4beb","background_color":"#ebebeb","font_family":"Arimo","links":[{"text":"About","url":"/about"},{"text":"Cards","url":"/cards"},{"text":"Guideline","url":"/guideline"}],"cardurl":"/cards","tagtitles":true,"autolink":true,"truncate":45,"filters":{"exclude":["video","audio","link","created_at"],"type":{"Rights":"tags","category":"radio_buttons","tags":"tags","title":"text","childOf":"text","equivalentTo":"text","Reference":"text","created_at":"date_range"}},"footer_content":"Follow Simon Knight on \u003ca href=\"https://twitter.com/sjgknight\"\u003eTwitter\u003c/a\u003e"}));

    if (window.location.pathname === '/cards') {
    window.collection = JSON.parse(JSON.stringify([{"Challenges":["test"],"Sources":["test"],"Strategies":["test"],"childOf":["test"],"tags":["test"],"equivalentTo":["test"],"PrincipleCollation":["test"],"Cases":["test"],"Principles":["test"],"Stakeholder-actors":["test"],"Stakeholders-impacted":["test"],"Rights":["test"],"Stakeholder-impacted":["test"],"Stakeholder-as-actor":["test"],"title":"test","description":"test","created_at":"test","name":"test","Reference":"test","Link":"test","category":"test","id":"test","dom_id":"item_test"},{"Challenges":["recCpTJXuuowqsqQa","recHHr97jsyNDnlsJ","recgZrgA8K7MmIzdF","recefglLZ3oJWw2SZ","recv6cN7XSt5GW32Y"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recJQF6QfQGlcSzDm","reczG9x5YfScZJdCo","recY9yr9vYcOAUSEA","recmhwo8kmYkBZ7Sy","recgswAsiepwEclOd","recNzXJwCfbwBLmVU","recAhcg8OdusJvk43","recB7MF7TeUKH3chO","recgNdOlcMTiussUk","rec55h8FhGKyGPNgw","reclplj55gpqdYGTr","recOo7Pmo4FBYcu7P","rec4oGONFgYMu3JGf","recAMa23XxDLEzMn8","recBUdrJ8n1tIJLTz","rec95I69E1YcGg0Sa","recTjwhqfrJoaRxYo","recq6GeKNegFQdzS9","recJyzFLE9ry4YEbb","reccAKmFQRH7IiuJi","recaTTvWN1olqx608"],"childOf":["recmzjcGKv3yNOxbl"],"title":"Minimise risks of harms","category":"Principles","name":"rec42P8U9usfYCtv9","tags":[],"created_at":"2023-05-18T14:25:20.000Z","id":"rec42p8u9usfyctv9","dom_id":"item_rec42p8u9usfyctv9"},{"Sources":["recfYC5jjPmpLfSlM"],"childOf":["recImuZ3T4iDiNP2B"],"equivalentTo":["recsvi4LnhEEPyQ1h"],"title":"Ensure their abilities to make free and informed decisions about their own lives","category":"Principles","name":"rec5216SybPPylYdD","tags":[],"created_at":"2023-05-19T09:39:12.000Z","id":"rec5216sybppylydd","dom_id":"item_rec5216sybppylydd"},{"Sources":["recpXl48pJdKDhc6f"],"childOf":["recuQpwelm0FwdAib"],"title":"Effectiveness","category":"Principles","name":"rec5N7PaVEFRs2o8R","tags":[],"created_at":"2023-06-03T18:23:44.000Z","description":"**Creators and operators shall provide evidence of the effectiveness and fitness for purpose of A/IS.**## BackgroundThe responsible adoption and deployment of A/IS are essential if such systems are to realize their many potential benefits to the well-being of both individuals and societies. A/IS will not be trusted unless they can be shown to be effective in use. Harms caused by A/IS, from harm to an individual through to systemic damage, can undermine the perceived value of A/IS and delay or prevent its adoption.Operators and other users will therefore benefit from measurement of the effectiveness of the A/IS in question. To be adequate, effective measurements need to be both valid and accurate, as well as meaningful and actionable. And such measurements must be accompanied by practical guidance on how to interpret and respond to them.## Recommendations1\\.Creators engaged in the development of A/IS should seek to define metrics or benchmarks that will serve as valid and meaningful gauges of the effectiveness of the system in meeting its objectives, adhering to standards and remaining within risk tolerances. Creators building A/IS should ensure that the results when the defined metrics are applied are readily obtainable by all interested parties, e.g., users, safety certifiers, and regulatorsof the system.2\\.Creators of A/IS should provide guidance on how to interpret and respond to the metrics generated by the systems.3\\.To the extent warranted by specific circumstances, operators of A/IS should follow the guidance on measurement provided with the systems, i.e., which metrics to obtain,how and when to obtain them, how to respond to given results, and so on.4\\.To the extent that measurements are sample based, measurements should account for the scope of sampling error, e.g., the reporting of confidence intervals associated with the measurements. Operators should be advised how to interpret the results.5\\.Creators of A/IS should design their systems such that metrics on specific deployments of the system can be aggregated to provide information on the effectiveness of the system across multiple deployments. For example, in the case of autonomous vehicles, metrics should be generated both for a specific instance of a vehicle and for a fleet of many instances of the same kind of vehicle.6\\.In interpreting and responding to measurements, allowance should be made for variation in the specific objectives and circumstances of a given deployment of A/IS. should work toward developing standards for the measurement and reporting on the effectiveness of A/IS.## Further Resources•R. Dillmann, [KA 1.10 Benchmarks for Robotics Research](https://www.researchgate.net/publication/250861011_KA_110_Benchmarks_for_Robotics_Research), 2010. •A. Steinfeld, T.W. Fong, D. Kaber, J. Scholtz, A. Schultz, and M. Goodrich, “[Common Metrics for Human-Robot Interaction](https://www.ri.cmu.edu/publications/common-metrics-for-human-robot-interaction/)”, 2006 Human-Robot Interaction Conference, March, 2006. •R. Madhavan, E. Messina, and E. Tunstel, Eds., [Performance Evaluation and Benchmarking of Intelligent Systems](https://link.springer.com/book/10.1007%2F978-1-4419-0492-8), Boston, MA: Springer, 2009.•_IEEE Robotics \u0026 Automation Magazine_, [Special Issue on Replicable and Measurable Robotics Research](https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=7254280), Volume 22, No. 3, September 2015.•C. Flanagin, [A Survey on Robotics Systems and Performance Analysis](https://www.cse.wustl.edu/~jain/cse567-11/ftp/robots/index.html), 2011.•[Transaction Processing Performance Council ](https://www.businesswire.com/news/home/20171212005281/en/Transaction-Processing-Performance-Council-TPC-Establishes-Artificial)[(TPC) Establishes Artificial Intelligence Working Group (TPC-AI)](https://www.businesswire.com/news/home/20171212005281/en/Transaction-Processing-Performance-Council-TPC-Establishes-Artificial) tasked with developing industry standard benchmarks for both hardware and software platforms associated with running Artificial Intelligence (AI) based workloads, 2017p.26-27","id":"rec5n7pavefrs2o8r","dom_id":"item_rec5n7pavefrs2o8r"},{"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recKDgUEDD2f1egyd","rec55h8FhGKyGPNgw","reclplj55gpqdYGTr"],"childOf":["recmzjcGKv3yNOxbl"],"title":"Maximise potential for benefits","category":"Principles","name":"rec6O9e1nYBJtQUTj","tags":[],"created_at":"2023-05-18T14:25:53.000Z","id":"rec6o9e1nybjtqutj","dom_id":"item_rec6o9e1nybjtqutj"},{"Sources":["recfYC5jjPmpLfSlM"],"Strategies":["rec0GhefNkhJqW0c2","recNHvwgyFPXERuJ8","recQPwyiQbPcN0G47"],"childOf":["recImuZ3T4iDiNP2B"],"equivalentTo":["recU6u0AZbcNj1ik9"],"title":"Safeguard their autonomy, their power to express themselves, and their right to be heard","category":"Principles","name":"rec6tz9Phzck0hvT8","tags":[],"created_at":"2023-05-19T09:39:25.000Z","id":"rec6tz9phzck0hvt8","dom_id":"item_rec6tz9phzck0hvt8"},{"Sources":["recfYC5jjPmpLfSlM"],"Strategies":["recmhwo8kmYkBZ7Sy","recgswAsiepwEclOd","recNzXJwCfbwBLmVU","recAhcg8OdusJvk43"],"equivalentTo":["recmzjcGKv3yNOxbl"],"title":"CARE for the wellbeing of each and all","category":"Principles","name":"rec7n2TGrH9RHYpQj","tags":[],"created_at":"2023-05-19T10:57:44.000Z","description":"“• Design and deploy AI systems to foster and to cultivate the welfare of all stakeholders whose interests are affected by their use • Do no harm with these technologies and minimise the risks of their misuse or abuse” (Leslie, 2019, p. 10)“• Prioritise the safety and the mental and physical integrity of people when scanning horizons of technological possibility and when conceiving of and deploying AI applications” (Leslie, 2019, p. 11)","id":"rec7n2tgrh9rhypqj","dom_id":"item_rec7n2tgrh9rhypqj"},{"Sources":["recpXl48pJdKDhc6f"],"equivalentTo":["recKdujFoPJr4ZAhZ","rec6tz9Phzck0hvT8","recQEiU22Qy1E0YuA"],"title":"Pillar of the Ethically Aligned Design Conceptual Framework: Political Self-Determination and Data Agency","category":"Principles","name":"rec9uueW1gq31CcBo","tags":[],"created_at":"2023-06-03T18:26:38.000Z","description":"\"A/IS—if designed and implemented properly— have a great potential to nurture political freedom and democracy, in accordance with the cultural precepts of individual societies, when people have access to and control over the data constituting and representing their identity. These systems can improve government effectiveness and accountability, foster trust, and protect our private sphere, but only when people have agency over their digital identity and their data is provably protected.\" p.7","id":"rec9uuew1gq31ccbo","dom_id":"item_rec9uuew1gq31ccbo"},{"Sources":["recfYC5jjPmpLfSlM"],"childOf":["rectPrYetyr4YIuq8"],"title":"Use the advancement and proliferation of AI technologies to strengthen the developmentally essential relationship between interacting human beings.","category":"Principles","name":"recArMNCiaj832hZ1","tags":[],"created_at":"2023-05-19T09:42:36.000Z","id":"recarmnciaj832hz1","dom_id":"item_recarmnciaj832hz1"},{"Sources":["recfYC5jjPmpLfSlM"],"Strategies":["recQPwyiQbPcN0G47"],"childOf":["rectPrYetyr4YIuq8"],"title":"Prioritise diversity, participation, and inclusion at all points in the design, development, and deployment processes of AI innovation.","category":"Principles","name":"recB9JaNSRmLbD8eE","tags":[],"created_at":"2023-05-19T09:41:36.000Z","id":"recb9jansrmlbd8ee","dom_id":"item_recb9jansrmlbd8ee"},{"Sources":["recfYC5jjPmpLfSlM"],"childOf":["rectPrYetyr4YIuq8"],"title":"Use AI technologies to foster this capacity to connect so as to reinforce the edifice of trust, empathy, reciprocal responsibility, and mutual understanding upon which all ethically wellfounded social orders rest","category":"Principles","name":"recC49LlmCoNq2Svy","tags":[],"created_at":"2023-05-19T10:57:01.000Z","id":"recc49llmconq2svy","dom_id":"item_recc49llmconq2svy"},{"Sources":["recpXl48pJdKDhc6f"],"childOf":["recuQpwelm0FwdAib"],"title":"Awareness of misuse","category":"Principles","name":"recCFSnZpjvw0PcUE","tags":[],"created_at":"2023-06-03T18:24:55.000Z","description":"**Creators shall guard against all potential misuses and risks of A/IS in operation.**## BackgroundNew technologies give rise to greater risk of deliberate or accidental misuse, and this is especially true for A/IS. A/IS increases the impact of risks such as hacking, misuse of personal data, system manipulation, or exploitation of vulnerable users by unscrupulous parties. Cases of A/IS hacking have already been widely reported, with [driverless cars](https://www.wired.com/2016/08/hackers-fool-tesla-ss-autopilot-hide-spoof-obstacles/),[ ](https://www.wired.com/2016/08/hackers-fool-tesla-ss-autopilot-hide-spoof-obstacles/)for example. The [Microsoft Tay AI chatbot](https://techcrunch.com/2016/03/24/microsoft-silences-its-new-a-i-bot-tay-after-twitter-users-teach-it-racism/) was famously manipulated when it mimicked deliberately offensive users. In an age where these powerful tools are easily available, there is a need for a new kind of education for citizens to be sensitized to risks associated with the misuse of A/IS. The EU’s General Data Protection Regulation (GDPR) provides measures to remedy the misuse of personal data.Responsible innovation requires A/IS creators to anticipate, reflect, and engage with users of A/IS. Thus, citizens, lawyers, governments, etc., all have a role to play through education and awareness in developing accountability structures (see Principle 6), in addition to guiding new technology proactively toward beneficial ends.## Recommendations1\\.Creators should be aware of methods of misuse, and they should design A/IS in ways to minimize the opportunity for these.2\\.Raise public awareness around the issues of potential A/IS technology misuse in an informed and measured way by:•Providing ethics education and security awareness that sensitizes society to the potential risks of misuse of A/IS. For example, provide “data privacy warnings” that some smart devices will collect their users’personal data.•Delivering this education in scalable and effective ways, including having experts with the greatest credibility and impact who can minimize unwarranted fear about A/IS.•Educating government, lawmakers, and enforcement agencies about these issues of A/IS so citizens can work collaboratively with these agencies to understand safe use of A/IS. For example, the same way police officers give public safety lectures in schools, they could provide workshops on safe use and interaction with A/IS.## Further Resources•A. Greenberg, “[Hackers Fool Tesla S’s\\_Autopilot to Hide and Spoof Obstacles](https://www.wired.com/2016/08/hackers-fool-tesla-ss-autopilot-hide-spoof-obstacles/),”Wired, August 2016.•C. Wilkinson and E. Weitkamp, _[Creative Research and Communication: Theory and Practice](http://www.manchesteruniversitypress.co.uk/9780719096518/)_, Manchester, UK: Manchester University Press, 2016 (in relation to Recommendation #2).•Engineering and Physical Sciences Research Council, “[Anticipate, Reflect, Engage and Act (AREA)](https://epsrc.ukri.org/research/framework/area/),” Framework for Responsible Research and Innovation, Accessed 2018.p.32-33","id":"reccfsnzpjvw0pcue","dom_id":"item_reccfsnzpjvw0pcue"},{"Sources":["recpXl48pJdKDhc6f"],"childOf":["recSwTdeEk6XbEgXU","rec9uueW1gq31CcBo","recuQpwelm0FwdAib"],"title":"Data agency","category":"Principles","name":"recD6uFlCKFiHl414","tags":[],"created_at":"2023-06-03T18:19:32.000Z","description":"**A/IS creators shall empower individuals with the ability to access and securely share their data, to maintain people’s capacity to have control over their identity.**## BackgroundDigital consent is a misnomer in its current manifestation. Terms and conditions or privacy policies are largely designed to provide legally accurate information regarding the usage of people’s data to safeguard institutional and corporate interests, while often neglecting the needs of the people whose data they process. “Consent fatigue”, the constant request for agreement to sets of long and unreadable data handling conditions, causes a majority of users to simply click and accept terms in order to access the services they wish to use. General obfuscation regarding privacy policies, and scenarios like the [Cambridge Analytica scandal](https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html) in 2018, demonstrate that even when individuals provide consent, the understanding of the value regarding their data and its safety is out of an individual’s control.This existing model of data exchange has eroded human agency in the algorithmic age. People don’t know how their data is being used at all times or when predictive messaging is honoring their existing preferences or manipulating them to create new behaviors.Regulations like the [EU General Data Protection Regulation ](https://eugdpr.org/)(GDPR) will help improve this lack of clarity regarding the exchange of personal data. But compliance with existing models of consent is not enough to safeguard people’s agency regarding their personal information. In an era where A/IS are already pervasive in society, governments must recognize that limiting the misuse of personal data is not enough.Society must also recognize that human rights in the digital sphere don’t exist until individuals globally are empowered with means—including tools and policies—that ensure their dignity through some form of sovereignty, agency, symmetry, or control regarding their identity and personal data. These rights rely on individuals being able to make their choices, outside of the potential influence of biased algorithmic messaging or bad actors. Society also needs to be confident that those who are unable to provide legal informed consent, including minors and people with diminished capacity to make informed decisions, do not lose their dignity due to this.## RecommendationOrganizations, including governments, should immediately explore, test, and implement technologies and policies that let individuals specify their online agent for case-by-case authorization decisions as to who can process what personal data for what purpose. For minors and those with diminished capacity to make informed decisions, current guardianship approaches should be viewed to determine their suitability in this context.The general solution to give agency to the individual is meant to anticipate and enable individuals to own and fully control autonomous and intelligent (as in capable of learning) technology that can evaluate data use requests by external parties and service providers. This technology would then provide a form of “digital sovereignty” and could issue limited and specific authorizations for processing of the individual’s personal data wherever it is held in acompatible system.## Further ResourcesThe following resources are designed to provide governments and other organizations—corporate, for-profit, not-for-profit, B Corp, or any form of public institution—basic information on services designed to provide user agency and/or sovereignty over their personal data.•The European Data Protection Supervisor [defines personal information management systems](https://edps.europa.eu/data-protection/our-work/subjects/personal-information-management-system_en) (PIMS) as:•“...systems that help give individuals more control over their personal data...allowing individuals to manage their personal data in secure, local or online storage systems and share them when and with whom they choose. Providers of online services and advertisers will need to interact with the PIMS if they plan to process individuals’ data. This can enable a human centric approach to personal information and new business models.” For further information and ongoing research regarding PIMS, visit [Crtl-Shift’s PIMS monthly archive](https://www.ctrl-shift.co.uk/tag/pims/). •IEEE P7006™, IEEE [Standards Project for Personal Data Artificial Intelligence (AI) Agent](https://standards.ieee.org/develop/project/7006.html) [describes the technical elements required to create and grant access to a personalized Artificial Intelligence that will comprise inputs, learning, ethics, rules, and values controlled by individuals. ](https://standards.ieee.org/develop/project/7006.html)IEEE P7012™, [IEEE Standards Project for Machine Readable Personal Privacy Terms](https://standards.ieee.org/project/7012.html) is designed to provide individuals with a means to proffer their own terms respecting personal privacy in ways that can be read, acknowledged, and be agreed to by machines operated by others in the networked worldp.25-26","id":"recd6uflckfihl414","dom_id":"item_recd6uflckfihl414"},{"Challenges":["recHHr97jsyNDnlsJ"],"Sources":["recfYC5jjPmpLfSlM"],"childOf":["recZToVrPeFlFq0Aw"],"title":"Treat all individuals equally and protect social equity","category":"Principles","name":"recDRQE1qQQNI65Xn","tags":[],"created_at":"2023-05-19T10:59:36.000Z","id":"recdrqe1qqqni65xn","dom_id":"item_recdrqe1qqqni65xn"},{"Sources":["recpXl48pJdKDhc6f"],"childOf":["recSwTdeEk6XbEgXU","rec9uueW1gq31CcBo","recuQpwelm0FwdAib"],"title":"Transparency","category":"Principles","name":"recFVLYl0cMH88Lzv","tags":[],"created_at":"2023-06-03T18:24:07.000Z","description":"**The basis of a particular A/IS decision should always be discoverable.**## BackgroundA key concern over autonomous and intelligent systems is that their operation must be transparent to a wide range of stakeholders for different reasons, noting that the level of transparency will necessarily be different for each stakeholder. Transparent A/IS are ones in which it is possible to discover how and why a system made a particular decision,or in the case of a robot, acted the way it did. The term “transparency” in the context ofA/IS also addresses the concepts of traceability, explainability, and interpretability. A/IS will perform tasks that are far more complex and have more effect on our world than prior generations of technology. Where the task is undertaken in a non-deterministic manner, it may defy simple explanation. This reality will be particularly acute with systems that interact with the physical world, thus raising the potential level of harm that such a system could cause. For example, some A/IS already have real consequences to human safety or well-being, such as medical diagnosis or driverless car autopilots. Systems such as these are safetycritical_ _systems. At the same time, the complexity of A/IS technology and the non-intuitive way in which it may operate will make it difficult for users of those systems to understand the actions of the A/IS that they use, or with which they interact. This opacity, combined with the often distributed manner in which the A/IS are developed, will complicate efforts to determine and allocate responsibility when something goes wrong. Thus, lack of transparency increases the risk and magnitude of harm when users do not understand the systems they are using, or there is a failure to fix faults and improve systems following accidents. Lack of transparency also increases the difficulty of ensuring accountability (see Principle 6— Accountability).Achieving transparency, which may involve a significant portion of the resources required to develop the A/IS, is important to each stakeholder group for the following reasons:1\\.For users, what the system is doing and why.2\\.For creators, including those undertaking the validation and certification of A/IS, the systems’ processes and input data.3\\.For an accident investigator, if accidents occur.4\\.For those in the legal process, to inform evidence and decision-making.5\\.For the public, to build confidence inthe technology.Develop new standards that describe measurable, testable levels of transparency, so that systems can be objectively assessed and levels of compliance determined. For designers, such standards will provide a guide for self-assessing transparency during development and suggest mechanisms for improving transparency. The mechanisms by which transparency is provided will vary significantly, including but not limited to, the following use cases:1\\.For users of care or domestic robots, a “whydid-you-do-that button” which, when pressed, causes the robot to explain the action itjust took.2\\.For validation or certification agencies, the algorithms underlying the A/IS and how they have been verified.3\\.For accident investigators, secure storage of sensor and internal state data comparable to a flight data recorder or black box. IEEE P7001™, [IEEE Standard for Transparency of Autonomous Systems](https://standards.ieee.org/project/7001.html) is one such standard, developed in response to this recommendationp.28-29","id":"recfvlyl0cmh88lzv","dom_id":"item_recfvlyl0cmh88lzv"},{"Sources":["recfYC5jjPmpLfSlM"],"Strategies":["recEHwRaLD44KPk8v"],"childOf":["recImuZ3T4iDiNP2B"],"equivalentTo":["recU6u0AZbcNj1ik9"],"title":"Secure their capacities to make well-considered and independent contributions to the life of the community","category":"Principles","name":"recGb4WZzr34RXKr0","tags":[],"created_at":"2023-05-19T09:39:33.000Z","id":"recgb4wzzr34rxkr0","dom_id":"item_recgb4wzzr34rxkr0"},{"Sources":["recfYC5jjPmpLfSlM"],"equivalentTo":["recLHILkx2JDFsLbX"],"title":"RESPECT the dignity of individual persons","category":"Principles","name":"recImuZ3T4iDiNP2B","tags":[],"created_at":"2023-05-19T09:38:40.000Z","description":"“• Ensure their abilities to make free and informed decisions about their own lives • Safeguard their autonomy, their power to express themselves, and their right to be heard • Secure their capacities to make well-considered and independent contributions to the life of the community • Support their abilities to flourish, to fully develop themselves, and to pursue their passions and talents according to their own freely determined life plans” (Leslie, 2019, p. 10)","id":"recimuz3t4idinp2b","dom_id":"item_recimuz3t4idinp2b"},{"Challenges":["recHHr97jsyNDnlsJ","recL5M2Hye2XHK7zg"],"Sources":["recnCULdYQ36cpZR7"],"equivalentTo":["recSqx6wklVpDzx3s"],"title":"Fairness","category":"Principles","name":"recK5zFiq18A3wHAE","tags":[],"created_at":"2023-05-25T18:06:39.000Z","description":"“The development, deployment and use of AI systems must be fair. While we acknowledge that there are many different interpretations of fairness, we believe that fairness has both a substantive and a procedural dimension. The substantive dimension implies a commitment to: ensuring equal and just distribution of both benefits and costs, and ensuring that individuals and groups are free from unfair bias, discrimination and stigmatisation. If unfair biases can be avoided, AI systems could even increase societal fairness. Equal opportunity in terms of access to education, goods, services and technology should also be fostered. Moreover, the use of AI systems should never lead to people being deceived or unjustifiably impaired in their freedom of choice. Additionally, fairness implies that AI practitioners should respect the principle of proportionality between means and ends, and consider carefully how to “balance competing interests and objectives.31 The procedural dimension of fairness entails the ability to contest and seek effective redress against decisions made by AI systems and by the humans operating them.32 In order to do so, the entity accountable for the decision must be identifiable, and the decision-making processes should be explicable.” (High-Level Expert Group on AI, 2019, p. 13-14)","id":"reck5zfiq18a3whae","dom_id":"item_reck5zfiq18a3whae"},{"Sources":["recpXl48pJdKDhc6f"],"Strategies":["reck6xeMUc9jtmLr3"],"childOf":["recuQpwelm0FwdAib"],"title":"Competence","category":"Principles","name":"recKWrfJzX52AXSIf","tags":[],"created_at":"2023-06-03T18:25:02.000Z","description":"**Creators shall specify and operators shall adhere to the knowledge and skill required for ****safe and effective operation.**## BackgroundA/IS can and often do make decisions that previously required human knowledge, expertise, and reason. Algorithms potentially can make even better decisions, by accessing more information, more quickly, and without the error, inconsistency, and bias that can plague human decision-making. As the use of algorithms becomes common and the decisions they make become more complex, however, the more normal and natural such decisions appear.Operators of A/IS can become less likely to question and potentially less able to question the decisions that algorithms make. Operators will not necessarily know the sources, scale, accuracy, and uncertainty that are implicit in applications of A/IS. As the use of A/IS expands, more systems will rely on machine learning where actions are not preprogrammed and that might not leave a clear record of the steps that led the system to its current state. Even if those records do exist, operators might not have access to them or the expertise necessary to decipher those records.Standards for the operators are essential.Operators should be able to understand howA/IS reach their decisions, the information and logic on which the A/IS rely, and the effects of those decisions. Even more crucially, operators should know when they need to question A/IS and when they need to overrule them.Creators of A/IS should take an active role in ensuring that operators of their technologies have the knowledge, experience, and skill necessary not only to use A/IS, but also to use it safely and appropriately, towards their intended ends. Creators should make provisions for the operators to override A/IS in appropriate circumstances.While standards for operator competence are necessary to ensure the effective, safe, and ethical application of A/IS, these standards are not the same for all forms of A/IS. The level of competence required for the safe and effective operation of A/IS will range from elementary, such as “intuitive” use guided by design, to advanced, such as fluency in statistics.## Recommendations1\\.Creators of A/IS should specify the types and levels of knowledge necessary to understand and operate any given application of A/IS. In specifying the requisite types and levels of expertise, creators should do so for the individual components of A/IS and for the entire systems.2\\.Creators of A/IS should integrate safeguards against the incompetent operation of their systems. Safeguards could include issuing notifications/warnings to operators in certain conditions, limiting functionalities for different levels of operators (e.g., novice vs. advanced), system shut-down in potentially risky conditions, etc.3\\.Creators of A/IS should provide the parties affected by the output of A/IS with information on the role of the operator, the competencies required, and the implications of operator error. Such documentation should be accessibleand understandable to both experts and the general public.4\\.Entities that operate A/IS should create documented policies to govern how A/IS should be operated. These policies should include the real-world applications for suchA/IS, any preconditions for their effective use, who is qualified to operate them, what training is required for operators, how to measure the performance of the A/IS, and what should be expected from the A/IS. The policies should also include specification of circumstancesin which it might be necessary for theoperator to override the A/IS.5.Operators of A/IS should, before operating a system, make sure that they have access to the requisite competencies. The operator need not be an expert in all the pertinent domains but should have access to individuals with the requisite kinds of expertise.## Further Resources•S. Barocas and A.D. Selbst, [The Intuitive Appeal of Explainable Machines](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3126971), Fordham Law Review, 2018.W. Smart, C. Grimm, and W. Hartzog, “[An Education Theory of Fault for Autonomous Systems](http://www.werobot2017.com/wp-content/uploads/2017/03/Smart-Grimm-Hartzog-Education-We-Robot.pdf)”, 2017. p.33-34","id":"reckwrfjzx52axsif","dom_id":"item_reckwrfjzx52axsif"},{"Challenges":["recdmBNNa98cN8Sda","recHsgB7ki6GknJnX","recGEFZ3EynXjRaa9","rec8hxSSEudaplJiA","reckPjhcWwLgjX9p5","recCkVZngvMA5I5uv","reczg5MObRbgzTeob"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recN2Lw4yXDBWLSYv","recf50wvya0NXxdxz","recELo3u36oZuujxN","recI5BV6v0BCK03VN","recOo7Pmo4FBYcu7P","recRTVqtvPcBS6zps","recNHvwgyFPXERuJ8","reczFKqCos9f1opXO","recJyzFLE9ry4YEbb","reccAKmFQRH7IiuJi","recaTTvWN1olqx608","recX4wcNFSw9YljDD","recWm6C6wOuVr6UCX"],"childOf":["recLHILkx2JDFsLbX"],"title":"Autonomy","category":"Principles","name":"recKdujFoPJr4ZAhZ","tags":[],"created_at":"2023-05-18T13:40:06.000Z","id":"reckdujfopjr4zahz","dom_id":"item_reckdujfopjr4zahz"},{"Sources":["recfYC5jjPmpLfSlM"],"childOf":["rec7n2TGrH9RHYpQj"],"title":"Prioritise the safety and the mental and physical integrity of people when scanning horizons of technological possibility and when conceiving of and deploying AI applications","category":"Principles","name":"recL7CrHMutQEBBu7","tags":[],"created_at":"2023-05-19T10:58:11.000Z","id":"recl7crhmutqebbu7","dom_id":"item_recl7crhmutqebbu7"},{"Challenges":["rec5VzvnIZcaNKqDh"],"PrincipleCollation":["recYP5QdyBcP0gruN","recW30jj9GzhNGTuX","recG3YFWxYITJ7HdM","recS88wQCG2Q3vCmN","recZQWPVfsVKa7q8t","recnatLtMuR1D5OXt","recqLh2ycsve2o5QO","recCTIw7k9DYnAN2g","recMBWhG4UXydd71J","rec1VlvAnhNs3MkbA","recbsOi9a2k1nJswY"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recgn2UvSD4OhzGI4","recnFHf9V2VtAVJMx","recinajIdAe7hRxjN","recegTm800wJXGjO1","rechaQXedBh3OsMjZ"],"equivalentTo":["recImuZ3T4iDiNP2B","recRbUmCzD09d6KZS","recSwTdeEk6XbEgXU"],"title":"Respect for persons","category":"Principles","name":"recLHILkx2JDFsLbX","tags":[],"created_at":"2023-05-18T13:40:06.000Z","description":"\"1.10 Respect for human beings is a recognition of their intrinsic value. In human research, this recognition includes abiding by the values of research merit and integrity, justice and beneficence. Respect also requires having due regard for the welfare, beliefs, perceptions, customs and cultural heritage, both individual and collective, of those involved in research.1.11 Researchers and their institutions should respect the privacy, confidentiality and cultural sensitivities of the participants and, where relevant, of their communities. Any specific agreements made with the participants or the community should be fulfilled.1.12 Respect for human beings involves giving due scope, throughout the research process, to the capacity of human beings to make their own decisions.1.13 Where participants are unable to make their own decisions or have diminished capacity to do so, respect for them involves empowering them where possible and providing for their protection as necessary.\" (NS 1.10-1.13)","id":"reclhilkx2jdfslbx","dom_id":"item_reclhilkx2jdfslbx"},{"Challenges":["recL5M2Hye2XHK7zg","recENQXCSOQQ3Y8et","recOeanQuUyB6Dx8g","reczg5MObRbgzTeob"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recKDgUEDD2f1egyd","rec55h8FhGKyGPNgw","reclplj55gpqdYGTr"],"childOf":["recSqx6wklVpDzx3s"],"title":"Fairness in distribution of burden and benefits of research","category":"Principles","name":"recMGB4iC5oaCtr5x","tags":[],"created_at":"2023-05-18T14:26:57.000Z","id":"recmgb4ic5oactr5x","dom_id":"item_recmgb4ic5oactr5x"},{"Challenges":["recOpn4I30te1qiKl"],"title":"Consider long-range and indirect impacts","category":"Principles","name":"recNB5h9bK4gEE9uc","tags":[],"created_at":"2023-06-05T05:32:52.000Z","id":"recnb5h9bk4gee9uc","dom_id":"item_recnb5h9bk4gee9uc"},{"Challenges":["recdmBNNa98cN8Sda","rech9r2f3QX8ZvmkP","recvQ90DajNCwPiGP","recJjTMjpfE0WHSWu"],"PrincipleCollation":["recl9ptNqbtslS9s6","rec2lbN7lCx1fQKnh","rec3tM5qcjsVoGf8o","recQEQ1NAj92jByY5","recQECJl1tr5x63ig","recNFvfNqfB2VwhZ8","recBuQv8QRql0Ow4q","recH0fKGdRvDSVpQm","recr4fWTm3a7tbJ4G","recZzygIMFc8xIAnW","recrpsLqCTq9nWSND"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["rec0GhefNkhJqW0c2","recGS9OVQ7qAjvIYE","recKWkdYO98PeEKDz","recZPU5yTmrjPXlF2","recAnT7HDpWYvdJ1V","recnFHf9V2VtAVJMx","recvVsXM40mlnmhqP","rec81gtnlFS5W2BBF","rec8dLtyDCwvjMWge","rec8cNT8sPSFtMSAc","recRTVqtvPcBS6zps","rec03QV2RUJkP3dfo","recBUdrJ8n1tIJLTz","rec95I69E1YcGg0Sa","reciw4r1bRvx6A6Fq","rec7m69DQyCw3rlFg","recqY2lEigz82Xmh5"],"title":"Merit and Integrity","category":"Principles","name":"recOHnq45Fq7YWsRO","tags":[],"created_at":"2023-05-18T14:26:57.000Z","description":"\"Unless proposed research has merit, and the researchers who are to carry out the research have integrity, the involvement of human participants in the research cannot be ethically justifiable.\" (NS, preamble)\"1.1 Research that has merit is:1. (a) justifiable by its potential benefit, which may include its contribution to knowledge and understanding, to improved social welfare and individual wellbeing, and to the skill and expertise of researchers. What constitutes potential benefit and whether it justifies research may sometimes require consultation with the relevant communities2. (b) designed or developed using methods appropriate for achieving the aims of the proposal3. (c) based on a thorough study of the current literature, as well as previous studies. This does not exclude the possibility of novel research for which there is little or no literature available, or research requiring a quick response to an unforeseen situation4. (d) designed to ensure that respect for the participants is not compromised by the aims of the research, by the way it is carried out, or by the results5. (e) conducted or supervised by persons or teams with experience, qualifications and competence that are appropriate for the research6. (f ) conducted using facilities and resources appropriate for the research.1.2 Where prior peer review has judged that a project has research merit, the question of its research merit is no longer subject to the judgement of those ethically reviewing the research.1.3 Research that is conducted with integrity is carried out by researchers with a commitment to:1. (a) searching for knowledge and understanding2. (b) following recognised principles of research conduct3. (c) conducting research honestly4. (d) disseminating and communicating results, whether favourable or unfavourable, in ways that permit scrutiny and contribute to public knowledge and understanding.\"(NS 1.1-1.3)","id":"recohnq45fq7ywsro","dom_id":"item_recohnq45fq7ywsro"},{"Challenges":["recO1L6GoFMkA6Lt4","recAoyMGCCLhaSqEb","recBc3GCNokDL220T"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recpUrzG3GpRiwqnz","recgWFCdfcVaeaPQO","reczG9x5YfScZJdCo","recJQF6QfQGlcSzDm","recY9yr9vYcOAUSEA","recypFGFqJzmgFmWV","reciw4r1bRvx6A6Fq","rec7m69DQyCw3rlFg","recwWov6KzmwU0FQW","recG86YsfXnKY9kNc"],"childOf":["recLHILkx2JDFsLbX"],"title":"Privacy","category":"Principles","name":"recPg7Ov0priGGtLm","tags":[],"created_at":"2023-05-19T07:41:54.000Z","description":"\"respect the privacy, confidentiality and cultural sensitivities of the participants and, where relevant, of their communities.\"","id":"recpg7ov0priggtlm","dom_id":"item_recpg7ov0priggtlm"},{"Challenges":["reciNc7OeTUmnsLqg","recA7Kh502s4UKWGo","recGRpoF7DA23ODSj","rec5nbWC0ZbVwnmxJ"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recinajIdAe7hRxjN","recELo3u36oZuujxN","recI5BV6v0BCK03VN","recTWhZ88TbQLcNaQ","reck6xeMUc9jtmLr3"],"childOf":["recOHnq45Fq7YWsRO"],"title":"ensuring respect for persons is maintained in the design","category":"Principles","name":"recQ9DIFEsOEkCx3O","tags":[],"created_at":"2023-05-18T14:28:05.000Z","id":"recq9difesoekcx3o","dom_id":"item_recq9difesoekcx3o"},{"Sources":["recnCULdYQ36cpZR7"],"Strategies":["recNHvwgyFPXERuJ8","recdtW2BdWmY6WSP5","recEHwRaLD44KPk8v","rec81gtnlFS5W2BBF"],"equivalentTo":["recKdujFoPJr4ZAhZ","rec6tz9Phzck0hvT8"],"title":"Respect for human autonomy","category":"Principles","name":"recQEiU22Qy1E0YuA","tags":[],"created_at":"2023-05-25T18:05:56.000Z","description":"“respect for human autonomy The fundamental rights upon which the EU is founded are directed towards ensuring respect for the freedom and autonomy of human beings. Humans interacting with AI systems must be able to keep full and effective self-determination over themselves, and be able to partake in the democratic process. AI systems should not unjustifiably subordinate, coerce, deceive, manipulate, condition or herd humans. Instead, they should be designed to augment, complement and empower human cognitive, social and cultural skills. The allocation of functions between humans and AI systems should follow human-centric design principles and leave meaningful opportunity for human choice. This means securing human oversight28 over work processes in AI systems. AI systems may also fundamentally change the work sphere. It should support humans in the working environment, and aim for the creation of meaningful work.” (High-Level Expert Group on AI, 2019, p. 12)","id":"recqeiu22qy1e0yua","dom_id":"item_recqeiu22qy1e0yua"},{"Sources":["recnCULdYQ36cpZR7"],"equivalentTo":["recKdujFoPJr4ZAhZ"],"title":"Freedom of the individual","category":"Principles","name":"recRBt2kSianfxfwe","tags":[],"created_at":"2023-05-25T18:03:04.000Z","description":"“Human beings should remain free to make life decisions for themselves. This entails freedom from sovereign intrusion, but also requires intervention from government and non-governmental organisations to ensure that individuals or people at risk of exclusion have equal access to AI’s benefits and opportunities. In an AI context, freedom of the individual for instance requires mitigation of (in)direct illegitimate coercion, threats to mental autonomy and mental health, unjustified surveillance, deception and unfair manipulation. In fact, freedom of the individual means a commitment to enabling individuals to wield even higher control over their lives, including (among other rights) protection of the freedom to conduct a business, the freedom of the arts and science, freedom of expression, the right to private life and privacy, and freedom of assembly and association” (High-Level Expert Group on AI, 2019, p. 10)","id":"recrbt2ksianfxfwe","dom_id":"item_recrbt2ksianfxfwe"},{"Sources":["recnCULdYQ36cpZR7"],"equivalentTo":["recLHILkx2JDFsLbX"],"title":"Respect for human dignity","category":"Principles","name":"recRbUmCzD09d6KZS","tags":[],"created_at":"2023-05-25T18:01:03.000Z","description":"“Human dignity encompasses the idea that every human being possesses an “intrinsic worth”, which should never be diminished, compromised or repressed by others – nor by new technologies like AI systems.19 In this context, respect for human dignity entails that all people are treated with respect due to them as moral subjects, rather than merely as objects to be sifted, sorted, scored, herded, conditioned or manipulated. AI systems should hence be developed in a manner that respects, serves and protects humans’ physical and mental integrity, personal and cultural sense of identity, and satisfaction of their essential needs.20” (High-Level Expert Group on AI, 2019, p. 10)","id":"recrbumczd09d6kzs","dom_id":"item_recrbumczd09d6kzs"},{"Challenges":["recHHr97jsyNDnlsJ"],"Sources":["recnCULdYQ36cpZR7"],"Strategies":["recTjwhqfrJoaRxYo","recq6GeKNegFQdzS9"],"equivalentTo":["reczVPIH1y2OMpAJH","recKdujFoPJr4ZAhZ"],"title":"Equality, non-discrimination and solidarity","category":"Principles","name":"recScYLR2TNiv7iKf","tags":[],"created_at":"2023-05-25T18:04:46.000Z","description":"“Equality, non-discrimination and solidarity - including the rights of persons at risk of exclusion. Equal respect for the moral worth and dignity of all human beings must be ensured. This goes beyond non-discrimination, which tolerates the drawing of distinctions between dissimilar situations based on objective justifications. In an AI context, equality entails that the system’s operations cannot generate unfairly biased outputs (e.g. the data used to train AI systems should be as inclusive as possible, representing different population groups). This also requires adequate respect for potentially vulnerable persons and groups,21 such as workers, women, persons with disabilities, ethnic minorities, children, consumers or others at risk of exclusion.” (High-Level Expert Group on AI, 2019, p. 11)","id":"recscylr2tniv7ikf","dom_id":"item_recscylr2tniv7ikf"},{"Challenges":["recENQXCSOQQ3Y8et","recL5M2Hye2XHK7zg"],"PrincipleCollation":["rec5tUjckYAzHmc7e","recO0PpscH27fFkXA"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recJQF6QfQGlcSzDm","recnFHf9V2VtAVJMx","recq6GeKNegFQdzS9","reccAKmFQRH7IiuJi"],"equivalentTo":["recZToVrPeFlFq0Aw"],"title":"Justice","category":"Principles","name":"recSqx6wklVpDzx3s","tags":[],"created_at":"2023-05-18T14:26:19.000Z","description":"\"1.4 In research that is just:1. (a) taking into account the scope and objectives of the proposed research, the selection, exclusion and inclusion of categories of research participants is fair, and is accurately described in the results of the research2. (b) the process of recruiting participants is fair3. (c) there is no unfair burden of participation in research on particular groups4. (d) there is fair distribution of the benefits of participation in research5. (e) there is no exploitation of participants in the conduct of research6. (f) there is fair access to the benefits of research.1.5 Research outcomes should be made accessible to research participants in a way that is timely and clear.\" (NS 1.4-1.5)","id":"recsqx6wklvpdzx3s","dom_id":"item_recsqx6wklvpdzx3s"},{"Sources":["recpXl48pJdKDhc6f"],"equivalentTo":["recLHILkx2JDFsLbX","recImuZ3T4iDiNP2B","reckb3cgfeDh1EeUP"],"title":"Pillar of the Ethically Aligned Design Conceptual Framework: Universal Human Values","category":"Principles","name":"recSwTdeEk6XbEgXU","tags":[],"created_at":"2023-06-03T18:26:10.000Z","description":"\"A/IS can be an enormous force for good in society provided they are designed to respect human rights, align with human values, and holistically increase well-being while empowering as many people as possible. They should also be designed to safeguard our environment and natural resources. These values should guide policy makers as well as engineers, designers, and developers. Advances in A/IS should be in the service of all people, rather than benefiting solely small groups, a single nation, or a corporation.\" p.7","id":"recswtdeek6xbegxu","dom_id":"item_recswtdeek6xbegxu"},{"Sources":["recnCULdYQ36cpZR7"],"title":"Citizens' rights","category":"Principles","name":"recTLqwMltPHHQDxH","tags":[],"created_at":"2023-05-25T18:05:21.000Z","description":"“Citizens benefit from a wide array of rights, including the right to vote, the right to good administration or access to public documents, and the right to petition the administration. AI systems offer substantial potential to improve the scale and efficiency of government in the provision of public goods and services to society. At the same time, citizens’ rights could also be negatively impacted by AI systems and should be safeguarded. When the term “citizens’ rights” is used here, this is not to deny or neglect the rights of third-country nationals and irregular (or illegal) persons in the EU who also have rights under international law, and – therefore in the area of AI systems.” (High-Level Expert Group on AI, 2019, p. 11)","id":"rectlqwmltphhqdxh","dom_id":"item_rectlqwmltphhqdxh"},{"Challenges":["recCpTJXuuowqsqQa"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recobVSYWj9jYvbgF","recY9yr9vYcOAUSEA","rec3q0xKZABgZf9Dg","rec09f7Nm4RTf6WjE","rec03QV2RUJkP3dfo","recspvYTySr0ANH6j","recEHwRaLD44KPk8v","recQPwyiQbPcN0G47"],"childOf":["recKdujFoPJr4ZAhZ"],"title":"Agency (including respect for views, opportunity to be heard (participation, input into research, fair representation)","category":"Principles","name":"recU6u0AZbcNj1ik9","tags":[],"created_at":"2023-05-18T14:19:44.000Z","id":"recu6u0azbcnj1ik9","dom_id":"item_recu6u0azbcnj1ik9"},{"Cases":["recrVkbG0XGe2Ca0v","recSXcY4cnofb4zTP","recskjpMFnQ0emSKI","recAlOHJhEy5nDwA6"],"Sources":["rec9jnxuHOioQn4DC"],"Strategies":["recOo7Pmo4FBYcu7P"],"title":"Privacy and data governance","category":"Principles","name":"recUYS0TFpk2MhVD7","tags":[],"created_at":"2023-06-08T06:30:26.000Z","description":"“Privacy and data governance including respect for privacy, quality and integrity of data, and access to data.” (European Commission, 2022, p. 18)","id":"recuys0tfpk2mhvd7","dom_id":"item_recuys0tfpk2mhvd7"},{"Cases":["recOOmVQviRyJGvea","recAlOHJhEy5nDwA6"],"Sources":["rec9jnxuHOioQn4DC"],"Strategies":["recTWhZ88TbQLcNaQ"],"title":"Technical robustness and safety","category":"Principles","name":"recWLcMWDPE9Fd1pE","tags":[],"created_at":"2023-06-08T06:34:37.000Z","description":"“Technical robustness and safety including resilience to attack, security and general safety, accuracy, reliability, and reproducibility.” (European Commission, 2022, p. 19)","id":"recwlcmwdpe9fd1pe","dom_id":"item_recwlcmwdpe9fd1pe"},{"Sources":["recpXl48pJdKDhc6f"],"childOf":["recSwTdeEk6XbEgXU","rec9uueW1gq31CcBo","recuQpwelm0FwdAib"],"title":"Accountability","category":"Principles","name":"recWlipA3L9QTydWS","tags":[],"created_at":"2023-06-03T18:24:44.000Z","description":"**A/IS shall be created and operated to provide an unambiguous rationale for decisions made.**## BackgroundThe programming, output, and purpose of A/IS are often not discernible by the general public. Based on the cultural context, application, and use of A/IS, people and institutions need clarity around the manufacture and deployment of these systems to establish responsibility and accountability, and to avoid potential harm. Additionally, manufacturers of these systems must be accountable in order to address legal issues of culpability. It should, if necessary, be possible to apportion culpability among responsible creators (designers and manufacturers) and operators to avoid confusion or fear within the general public.Accountability and partial accountability are not possible without transparency, thus this principle is closely linked with Principle 5–Transparency.## RecommendationsTo best address issues of responsibility and accountability:1\\.Legislatures/courts should clarify responsibility, culpability, liability, and accountability forA/IS, where possible, prior to development and deployment so that manufacturers and users understand their rights and obligations.2\\.Designers and developers of A/IS should remain aware of, and take into account, the diversity of existing cultural norms among the groups of users of these A/IS.3\\.Multi-stakeholder ecosystems including creators, and government, civil, and commercial stakeholders, should be developed to help establish norms where they do not exist because A/IS-oriented technology and their impacts are too new. These ecosystems would include, but not be limited to, representatives of civil society, law enforcement, insurers, investors, manufacturers, engineers, lawyers, and users. The norms can mature into best practicesand laws.4\\.Systems for registration and record-keeping should be established so that it is always possible to find out who is legally responsible for a particular A/IS. Creators, including manufacturers, along with operators,of A/IS should register key, high-level parameters, including:•Intended use,•Training data and training environment,if applicable,•Sensors and real world data sources,•Algorithms,•Process graphs,•Model features, at various levels,•User interfaces,•Actuators and outputs, and•Optimization goals, loss functions,and reward functions.•B. Shneiderman, “[Human Responsibility for Autonomous Agents,”](https://ieeexplore.ieee.org/abstract/document/4136860) _IEEE Intelligent Systems _22, no. 2, pp. 60–61, 2007.•A. Matthias, “[The Responsibility Gap: Ascribing Responsibility for the Actions of Learning Automata.”](https://link.springer.com/article/10.1007%2Fs10676-004-3422-1) _Ethics and Information Technology _6, no. 3, pp. 175–183, 2004.•A. Hevelke and J. Nida-Rümelin, “[Responsibility for Crashes of Autonomous Vehicles: An Ethical Analysis,”](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4430591/) _Science and Engineering Ethics _21, no. 3, pp. 619–630, 2015.•An example of good practice (in relation to Recommendation #3) can be found in [Sciencewise](http://www.sciencewise-erc.org.uk/)—the U.K. national center for public dialogue in policy-making involving science and technology issues.p.29-30","id":"recwlipa3l9qtydws","dom_id":"item_recwlipa3l9qtydws"},{"Sources":["recfYC5jjPmpLfSlM"],"Strategies":["recGS9OVQ7qAjvIYE","recZPU5yTmrjPXlF2"],"equivalentTo":["recSqx6wklVpDzx3s"],"title":"PROTECT the priorities of social values, justice, and the public interest","category":"Principles","name":"recZToVrPeFlFq0Aw","tags":[],"created_at":"2023-05-19T10:59:26.000Z","description":"“• Treat all individuals equally and protect social equity • Use digital technologies as an essential support for the protection of fair and equal treatment under the law • Prioritise social welfare, public interest, and the consideration of the social and ethical impacts of innovation in determining the legitimacy and desirability of AI technologies • Use AI to empower and to advance the interests and well-being of as many individuals as possible • Think big-picture about the wider impacts of the AI technologies you are conceiving and developing. Think about the ramifications of their effects and externalities for others around the globe, for future generations, and for the biosphere as a whole” (Leslie, 2019, p. 11)","id":"recztovrpeflfq0aw","dom_id":"item_recztovrpeflfq0aw"},{"Challenges":["recHHr97jsyNDnlsJ","recL5M2Hye2XHK7zg"],"Sources":["recfYC5jjPmpLfSlM"],"childOf":["recZToVrPeFlFq0Aw"],"title":"Use digital technologies as an essential support for the protection of fair and equal treatment under the law","category":"Principles","name":"recZbEXiEs1AlDdn3","tags":[],"created_at":"2023-05-19T10:59:55.000Z","id":"reczbexies1alddn3","dom_id":"item_reczbexies1alddn3"},{"Cases":["reciNqxyfUgE5XM7t","recmS3zSMbR3ofAR5"],"Sources":["rec9jnxuHOioQn4DC"],"Strategies":["rec35PeHdUmtalypk"],"title":"Diversity, non-discrimination, and fairness","category":"Principles","name":"recaMsksKInFYbnCL","tags":[],"created_at":"2023-06-08T06:31:50.000Z","description":"“Diversity, non-discrimination, and fairness including accessibility, universal design, the avoidance of unfair bias, and stakeholder participation, which allows use regardless of age, gender, abilities, or characteristics - with a particular focus for students with special needs.” (\\[European Commission, 2022, p. 18]\\(zotero://select/groups/4907410/items/3BCJVLT9)) (\\[pdf]\\(zotero://open-pdf/groups/4907410/items/6A6DKVJ2?page=18\u0026annotation=WFTMSW2T))","id":"recamskskinfybncl","dom_id":"item_recamskskinfybncl"},{"Sources":["recfYC5jjPmpLfSlM"],"childOf":["recZToVrPeFlFq0Aw"],"equivalentTo":["recNB5h9bK4gEE9uc"],"title":"Think big-picture about the wider impacts of the AI technologies you are conceiving and developing. Think about the ramifications of their effects and externalities for others around the globe, for future generations, and for the biosphere as a whole","category":"Principles","name":"recduyDUAsQdJ4EMy","tags":[],"created_at":"2023-05-19T11:00:14.000Z","id":"recduyduasqdj4emy","dom_id":"item_recduyduasqdj4emy"},{"Challenges":["rec1QZHHMARBQcZoo","reciNc7OeTUmnsLqg","recA7Kh502s4UKWGo","recGRpoF7DA23ODSj"],"Sources":["recpXl48pJdKDhc6f"],"Strategies":["recZI7HDWKBU5T22v"],"childOf":["recSwTdeEk6XbEgXU","rec9uueW1gq31CcBo"],"title":"Wellbeing","category":"Principles","name":"receFm7cGasHwpJZO","tags":[],"created_at":"2023-05-28T19:19:19.000Z","description":"**A/IS creators shall adopt increased human well-being as a primary success criterion for development.**## BackgroundFor A/IS technologies to demonstrably advance benefit for humanity, we need to be able to define and measure the benefit we wish to increase. But often the only indicators utilized in determining success for A/IS are avoiding negative unintended consequences and increasing productivity and economic growth for customers and society. Today, these are largely measured by gross domestic product (GDP), profit, or consumption levels.Well-being, for the purpose of _Ethically Aligned Design_, is based on the Organization for Economic Co-operation and Development’s (OECD) [”Guidelines on Measuring Subjective Well-being” p](http://www.oecd.org/statistics/oecd-guidelines-on-measuring-subjective-well-being-9789264191655-en.htm)erspective that, “Being able to measure people’s quality of life is fundamental when assessing the progress of societies.” There is now widespread acknowledgement that measuring subjective well-being is an essential part of measuring quality of life alongside other social and economic dimensions as identified within [Nassbaum-Sen’s capability approach ](https://link.springer.com/article/10.1007/s11205-005-6518-z)whereby well-being is objectively defined in terms of human capabilities necessary for functioning and flourishing.Since modern societies will be largely constituted of A/IS users, we believe these considerations to be relevant for A/IS creators. A/IS technologies can be narrowly conceived from an ethical standpoint. They can be legal, profitable, and safe in their usage, yet not positively contribute to human and environmental well-being. This means technologies created with the best intentions, but without considering well-being, can still have dramatic negative consequences on people’s mental health, emotions, sense of themselves, their autonomy, their ability to achieve their goals, and other dimensions of well-being.## RecommendationA/IS should prioritize human well-being as an outcome in all system designs, using the best available and widely accepted well-being metrics as their reference point.## Further Resources•IEEE P7010™, [Well-being Metric for Autonomous and Intelligent Systems](https://standards.ieee.org/project/7010.html).•[The Measurement of Economic Performance and Social Progress n](http://www.stat.si/doc/drzstat/Stiglitz%20report.pdf)ow commonly referred to as “The Stiglitz Report”, commissioned by the then President of the French Republic, 2009. From the report: “…the time is ripe for our measurement system to shift emphasis from measuring economic production to measuring people’s well-being … emphasizing well-being is important because there appears to be an increasing gap between the information contained in aggregate GDP data and what counts for common people’s well-being.”•[OECD Guidelines on Measuring Subjective Well-being,](https://www.oecd-ilibrary.org/economics/oecd-guidelines-on-measuring-subjective-well-being_9789264191655-en) 2013.•[OECD Better Life Index,](http://www.oecdbetterlifeindex.org/) 2017.•[World Happiness Reports](http://worldhappiness.report/), 2012 – 2018.•United Nations [Sustainable Development Goal](https://unstats.un.org/sdgs/) (SDG) [Indicators,](https://unstats.un.org/sdgs/indicators/indicators-list/) 2018.•[Beyond GDP,](http://ec.europa.eu/environment/beyond_gdp/index_en.html) European Commission, 2018.[ ](http://ec.europa.eu/environment/beyond_gdp/index_en.html)From the site: “The Beyond GDP initiative is about developing indicators that are as clear and appealing as GDP, but more inclusive of environmental and social aspects of progress.”•[Genuine Progress Indicator,](http://dnr.maryland.gov/mdgpi/Pages/default.aspx) State of Maryland (first developed by Redefining Progress), 2015.•The International Panel on Social Progress, [Social Justice, Well-Being and Economic Organization,](https://comment.ipsp.org/chapter/chapter-8-social-justice-well-being-and-economic-organization) 2018.•R. Veenhoven, World Database of Happiness, Erasmus University Rotterdam, The Netherlands, Accessed 2018 at: [http:// worlddatabaseofhappiness.eur.nl.](http://worlddatabaseofhappiness.eur.nl/)Royal Government of Bhutan, [The Report of the High-Level Meeting on Wellbeing and Happiness: Defining a New Economic Paradigm](https://sustainabledevelopment.un.org/content/documents/617BhutanReport_WEB_F.pdf), New York: The Permanent Mission of the Kingdom of Bhutan to the United Nations, 2012\"p.23-24","id":"recefm7cgashwpjzo","dom_id":"item_recefm7cgashwpjzo"},{"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recKDgUEDD2f1egyd","recsAAZKwCyesrHK6","recpvGxQzTNQS1smH","rechaQXedBh3OsMjZ","recTWhZ88TbQLcNaQ"],"childOf":["recOHnq45Fq7YWsRO"],"title":"that is designed to achieve its aims rigorously","category":"Principles","name":"recgDkzdE9dfpTxCK","tags":[],"created_at":"2023-05-18T14:28:03.000Z","id":"recgdkzde9dfptxck","dom_id":"item_recgdkzde9dfptxck"},{"Sources":["recfYC5jjPmpLfSlM"],"childOf":["recZToVrPeFlFq0Aw"],"equivalentTo":["recNB5h9bK4gEE9uc"],"title":"Prioritise social welfare, public interest, and the consideration of the social and ethical impacts of innovation in determining the legitimacy and desirability of AI technologies","category":"Principles","name":"recgEYKqzkCYqPmXW","tags":[],"created_at":"2023-05-19T10:59:56.000Z","id":"recgeykqzkcyqpmxw","dom_id":"item_recgeykqzkcyqpmxw"},{"Sources":["recfYC5jjPmpLfSlM"],"childOf":["rec7n2TGrH9RHYpQj"],"title":"Design and deploy AI systems to foster and to cultivate the welfare of all stakeholders whose interests are affected by their use","category":"Principles","name":"rechB1SCdOpa940OZ","tags":[],"created_at":"2023-05-19T10:57:48.000Z","id":"rechb1scdopa940oz","dom_id":"item_rechb1scdopa940oz"},{"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recpvGxQzTNQS1smH"],"childOf":["recOHnq45Fq7YWsRO"],"title":"and with suitable resources to conduct the work as proposed","category":"Principles","name":"recheWZC64aZRgmpo","tags":[],"created_at":"2023-05-18T14:28:06.000Z","id":"rechewzc64azrgmpo","dom_id":"item_rechewzc64azrgmpo"},{"Challenges":["rec1QZHHMARBQcZoo"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recpvGxQzTNQS1smH","recinajIdAe7hRxjN","recTWhZ88TbQLcNaQ"],"childOf":["recOHnq45Fq7YWsRO"],"title":"do research that is justifiable by potential benefit","category":"Principles","name":"recint2IxoR8aILCp","tags":[],"created_at":"2023-05-18T14:27:54.000Z","id":"recint2ixor8ailcp","dom_id":"item_recint2ixor8ailcp"},{"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recY9yr9vYcOAUSEA","recDOGXIsqtXrCuSz","recsonQhKLmX4D1ao"],"childOf":["recOHnq45Fq7YWsRO"],"title":"and that the work is carried out with suitable expertise","category":"Principles","name":"recjViPnz3atRIOpD","tags":[],"created_at":"2023-05-18T14:28:05.000Z","id":"recjvipnz3atriopd","dom_id":"item_recjvipnz3atriopd"},{"Sources":["recnCULdYQ36cpZR7"],"Strategies":["recHEUOqkzQTNsAMw","rec03QV2RUJkP3dfo","recffbHgJO0d179DG"],"title":"Respect for democracy, justice and the rule of law","category":"Principles","name":"reckb3cgfeDh1EeUP","tags":[],"created_at":"2023-05-25T18:04:04.000Z","description":"“All governmental power in constitutional democracies must be legally authorised and limited by law. AI systems should serve to maintain and foster democratic processes and respect the plurality of values and life choices of individuals. AI systems must not undermine democratic processes, human deliberation or democratic voting systems. AI systems must also embed a commitment to ensure that they do not operate in ways that undermine the foundational commitments upon which the rule of law is founded, mandatory laws and regulation, and to ensure due process and equality before the law.” (High-Level Expert Group on AI, 2019, p. 11)","id":"reckb3cgfedh1eeup","dom_id":"item_reckb3cgfedh1eeup"},{"Sources":["recnCULdYQ36cpZR7"],"Strategies":["rec4oGONFgYMu3JGf","recAMa23XxDLEzMn8","recBUdrJ8n1tIJLTz","rec95I69E1YcGg0Sa","recTjwhqfrJoaRxYo","recq6GeKNegFQdzS9","recaTTvWN1olqx608"],"equivalentTo":["recmzjcGKv3yNOxbl"],"title":"Prevention of harm","category":"Principles","name":"reclPiw2VvNOSTzv5","tags":[],"created_at":"2023-05-25T18:06:18.000Z","description":"“AI systems should neither cause nor exacerbate harm29 or otherwise adversely affect human beings.30 This entails the protection of human dignity as well as mental and physical integrity. AI systems and the environments in which they operate must be safe and secure. They must be technically robust and it should be ensured that they are not open to malicious use. Vulnerable persons should receive greater attention and be included in the development, deployment and use of AI systems. Particular attention must also be paid to situations where AI systems can cause or exacerbate adverse impacts due to asymmetries of power or information, such as between employers and employees, businesses and consumers or governments and citizens. Preventing harm also entails consideration of the natural environment and all living beings.” (High-Level Expert Group on AI, 2019, p. 12)","id":"reclpiw2vvnostzv5","dom_id":"item_reclpiw2vvnostzv5"},{"Sources":["recfYC5jjPmpLfSlM"],"Strategies":["rec0GhefNkhJqW0c2"],"childOf":["rectPrYetyr4YIuq8"],"title":"Safeguard the integrity of interpersonal dialogue, meaningful human connection, and social cohesion","category":"Principles","name":"reclur1ImQFlLyYof","tags":[],"created_at":"2023-05-19T09:41:35.000Z","id":"reclur1imqfllyyof","dom_id":"item_reclur1imqfllyyof"},{"PrincipleCollation":["rec3rTrXMiXaZjy7s","reci6dYShRhu4H0ad","recLwscIEEHBzI5Nq","recl9ptNqbtslS9s6","recgk6VOEM4y4w7gV","recEhiFWkyd6l5Kim","rec2LOHao3FmjpwIv","recdmyjbklrgjcfc3","recm9sVI6QmlMdHor","recRopSKW7iYgFHlz","recVOZptlmWy2vW4P","reco1j4ikYZLaYZi6","reck7CXWZJAjaLrDR"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["rec0GhefNkhJqW0c2","recGS9OVQ7qAjvIYE","recAnT7HDpWYvdJ1V","recnFHf9V2VtAVJMx","rec8dLtyDCwvjMWge","rechaQXedBh3OsMjZ","rec7daqDHSCuc70yS","recRTVqtvPcBS6zps","recTjwhqfrJoaRxYo","recq6GeKNegFQdzS9","rec0WScLo6sUnoOQN","recJMrMEZnz9rYlnD","reccAKmFQRH7IiuJi"],"equivalentTo":["rec7n2TGrH9RHYpQj","reclPiw2VvNOSTzv5"],"title":"Beneficence","category":"Principles","name":"recmzjcGKv3yNOxbl","tags":[],"created_at":"2023-05-18T14:25:01.000Z","description":"\"1.6 The likely benefit of the research must justify any risks of harm or discomfort to participants. The likely benefit may be to the participants, to the wider community, or to both.1.7 Researchers are responsible for:1. (a) designing the research to minimise the risks of harm or discomfort to participants2. (b) clarifying for participants the potential benefits and risks of the research3. (c) the welfare of the participants in the research context.1.8 Where there are no likely benefits to participants, the risk to participants should be lower than would be ethically acceptable where there are such likely benefits.1.9 Where the risks to participants are no longer justified by the potential benefits of the research, the research must be suspended to allow time to consider whether it should be discontinued or at least modified. This decision may require consultation between researchers, participants, the relevant ethical review body, and the institution. The review body must be notified promptly of such suspension, and of any decisions following it (see paragraphs 5.5.7 to 5.5.10).\" (NS 1.6-1.9)","id":"recmzjcgkv3ynoxbl","dom_id":"item_recmzjcgkv3ynoxbl"},{"Cases":["reciNqxyfUgE5XM7t","recOOmVQviRyJGvea"],"Sources":["rec9jnxuHOioQn4DC"],"Strategies":["recegTm800wJXGjO1"],"title":"Human agency and oversight","category":"Principles","name":"reco4DUa3rsjP0hyg","tags":[],"created_at":"2023-06-08T06:32:23.000Z","description":"“Human agency and oversight including fundamental rights, children’s rights, human agency, and human oversight.” (\\[European Commission, 2022, p. 18]\\(zotero://select/groups/4907410/items/3BCJVLT9)) (\\[pdf]\\(zotero://open-pdf/groups/4907410/items/6A6DKVJ2?page=18\u0026annotation=YBKGBYD4))","id":"reco4dua3rsjp0hyg","dom_id":"item_reco4dua3rsjp0hyg"},{"Sources":["recfYC5jjPmpLfSlM"],"childOf":["rectPrYetyr4YIuq8"],"title":"Encourage all voices to be heard and all opinions to be weighed seriously and sincerely throughout the production and use lifecycle","category":"Principles","name":"recoXK4tOGqoh2nE3","tags":[],"created_at":"2023-05-19T09:41:37.000Z","id":"recoxk4togqoh2ne3","dom_id":"item_recoxk4togqoh2ne3"},{"Cases":["recrVkbG0XGe2Ca0v","recAlOHJhEy5nDwA6"],"Sources":["rec9jnxuHOioQn4DC"],"Strategies":["rec7daqDHSCuc70yS"],"title":"Societal and environmental wellbeing","category":"Principles","name":"recqdGhz7l1cGnQeU","tags":[],"created_at":"2023-06-08T06:31:30.000Z","description":"“Societal and environmental wellbeing including sustainability and environmental friendliness, social impact, society, and democracy.” (European Commission, 2022, p. 18)","id":"recqdghz7l1cgnqeu","dom_id":"item_recqdghz7l1cgnqeu"},{"Challenges":["recYHYaeqVHR93Kuo","recO1L6GoFMkA6Lt4","recrHBtVR4EOXSJh2"],"PrincipleCollation":["rec3rTrXMiXaZjy7s"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recpUrzG3GpRiwqnz","recgWFCdfcVaeaPQO","reczG9x5YfScZJdCo","recazD3B5XpqgOCGV","rec09f7Nm4RTf6WjE"],"childOf":["recKdujFoPJr4ZAhZ"],"title":"Informed consent","category":"Principles","name":"recsvi4LnhEEPyQ1h","tags":[],"created_at":"2023-05-18T13:40:06.000Z","id":"recsvi4lnheepyq1h","dom_id":"item_recsvi4lnheepyq1h"},{"Sources":["recfYC5jjPmpLfSlM"],"equivalentTo":["recLHILkx2JDFsLbX"],"title":"CONNECT with each other sincerely, openly, and inclusively","category":"Principles","name":"rectPrYetyr4YIuq8","tags":[],"created_at":"2023-05-19T09:39:45.000Z","description":"“• Safeguard the integrity of interpersonal dialogue, meaningful human connection, and social cohesion • Prioritise diversity, participation, and inclusion at all points in the design, development, and deployment processes of AI innovation. • Encourage all voices to be heard and all opinions to be weighed seriously and sincerely throughout the production and use lifecycle • Use the advancement and proliferation of AI technologies to strengthen the developmentally essential relationship between interacting human beings. • Utilise AI innovations pro-socially so as to enable bonds of interpersonal solidarity to form and individuals to be socialised and recognised by each other • Use AI technologies to foster this capacity to connect so as to reinforce the edifice of trust, empathy, reciprocal responsibility, and mutual understanding upon which all ethically wellfounded social orders rest” (Leslie, 2019, p. 10)","id":"rectpryetyr4yiuq8","dom_id":"item_rectpryetyr4yiuq8"},{"Sources":["recpXl48pJdKDhc6f"],"equivalentTo":["recOHnq45Fq7YWsRO"],"title":"Pillar of the Ethically Aligned Design Conceptual Framework: Technical Dependability","category":"Principles","name":"recuQpwelm0FwdAib","tags":[],"created_at":"2023-06-03T18:27:42.000Z","description":"\"**Technical Dependability: **Ultimately, A/IS should deliver services that can be trusted.2 This trust means that A/IS will reliably, safely, and actively accomplish the objectives for which they were designed while advancing the human-driven values they were intended to reflect. Technologies should be monitored to ensure that their operation meets predetermined ethical objectives aligning with human values and respecting codified rights. In addition, validation and verification processes, including aspects of explainability, should be developed that could lead to better auditability and to certification of A/IS\" p.7","id":"recuqpwelm0fwdaib","dom_id":"item_recuqpwelm0fwdaib"},{"Sources":["recfYC5jjPmpLfSlM"],"childOf":["recImuZ3T4iDiNP2B"],"equivalentTo":["recU6u0AZbcNj1ik9"],"title":"Support their abilities to flourish, to fully develop themselves, and to pursue their passions and talents according to their own freely determined life plans","category":"Principles","name":"recv9z5lu6zZspHYA","tags":[],"created_at":"2023-05-19T09:39:38.000Z","id":"recv9z5lu6zzsphya","dom_id":"item_recv9z5lu6zzsphya"},{"Sources":["recfYC5jjPmpLfSlM"],"childOf":["rectPrYetyr4YIuq8"],"title":"Utilise AI innovations pro-socially so as to enable bonds of interpersonal solidarity to form and individuals to be socialised and recognised by each other","category":"Principles","name":"recvLGtdSLYOV3Azw","tags":[],"created_at":"2023-05-19T09:42:38.000Z","id":"recvlgtdslyov3azw","dom_id":"item_recvlgtdslyov3azw"},{"Cases":["reciNqxyfUgE5XM7t","recmS3zSMbR3ofAR5"],"Sources":["rec9jnxuHOioQn4DC"],"Strategies":["rechaQXedBh3OsMjZ"],"title":"Transparency","category":"Principles","name":"recwjv8IMAZFWfMSr","tags":[],"created_at":"2023-06-08T06:32:12.000Z","description":"“Transparency including traceability, explainability and communication.” (\\[European Commission, 2022, p. 18]\\(zotero://select/groups/4907410/items/3BCJVLT9)) (\\[pdf]\\(zotero://open-pdf/groups/4907410/items/6A6DKVJ2?page=18\u0026annotation=GKNH3GVZ))","id":"recwjv8imazfwfmsr","dom_id":"item_recwjv8imazfwfmsr"},{"Sources":["recpXl48pJdKDhc6f"],"childOf":["recSwTdeEk6XbEgXU","rec9uueW1gq31CcBo"],"title":"Respect for human rights","category":"Principles","name":"recxc8SQN09R985HI","tags":[],"created_at":"2023-05-28T19:19:19.000Z","description":"\"**A/IS shall be created and operated to respect, promote, and protect internationally recognized human rights.**## BackgroundHuman benefit is a crucial goal of A/IS, as is respect for human rights set out in works including, but not limited to: [The](http://www.un.org/en/universal-declaration-human-rights/) [Universal Declaration of Human Rights,](http://www.un.org/en/universal-declaration-human-rights/) the [International Covenant on Civil and Political Rights,](http://www.ohchr.org/en/professionalinterest/pages/ccpr.aspx) the [Convention on the Rights of the Child,](http://www.ohchr.org/en/professionalinterest/pages/crc.aspx) the [Convention on the Elimination of all forms of Discrimination against Women,](http://www.un.org/womenwatch/daw/cedaw/) the [Convention on the Rights of Persons with Disabilities,](https://www.un.org/development/desa/disabilities/convention-on-the-rights-of-persons-with-disabilities.html) and the [Geneva Conventions.](https://www.icrc.org/en/war-and-law/treaties-customary-law/geneva-conventions)Such rights need to be fully taken into consideration by individuals, companies, professional bodies, research institutions, and governments alike to reflect the principle thatA/IS should be designed and operated in a way that both respects and fulfills human rights, freedoms, human dignity, and cultural diversity.    While their interpretation may change over time, “human rights”, as defined by international law, provide a unilateral basis for creating any A/IS, as these systems affect humans, their emotions, data, or agency. While the direct coding of human rights in A/IS may be difficult or impossible based on contextual use, newer guidelines from The United Nations provide methods to pragmatically implement human rights ideals within business or corporate contexts that could be adapted for engineers and technologists. In this way, technologists can take into account human rights in the way A/IS are developed, operated, tested, and validated. In short, human rights should be part of the ethical risk assessment of A/IS.## RecommendationsTo best respect human rights, society must assure the safety and security of A/IS so that they are designed and operated in a way that benefits humans. Specifically:•Governance frameworks, including standards and regulatory bodies, should be established to oversee processes which ensure that the use of A/IS does not infringe upon human rights, freedoms, dignity, and privacy, and which ensure traceability. This will contribute to building public trust in A/IS.•A way to translate existing and forthcoming legal obligations into informed policy and technical considerations is needed. Such a method should allow for diverse cultural norms as well as differing legal and regulatory frameworks.•A/IS should always be subordinate to human judgment and control.•For the foreseeable future, A/IS should not be granted rights and privileges equal to human rights.## Further ResourcesThe following documents and organizations are provided both as references and examples of the types of work that can be emulated, adapted, and proliferated regarding ethical best practices around A/IS to best honor human rights:•[The Universal Declaration of Human Rights](http://www.ohchr.org/EN/UDHR/Pages/Language.aspx?LangID=eng), 1947.•N. Wiener, _The Human Use of Human Beings_, New York: Houghton Mifflin, 1954.•[The International Covenant on Civil and Political Rights](http://www.ohchr.org/EN/ProfessionalInterest/Pages/CCPR.aspx), 1966.•[The International Covenant on Economic, Social and Cultural Rights](http://www.ohchr.org/EN/ProfessionalInterest/Pages/CESCR.aspx), 1966.•[The International Convention on the Elimination of All Forms of Racial Discrimination](http://www.ohchr.org/EN/ProfessionalInterest/Pages/CERD.aspx), 1965.•T[he Convention on the Rights of the Child,](http://www.ohchr.org/en/professionalinterest/pages/crc.aspx) 1990.•T[he Convention on the Elimination of All Forms of Discrimination against Women](http://www.un.org/womenwatch/daw/cedaw/), 1979.•[The Convention on the Rights of Persons with Disabilities](https://www.un.org/development/desa/disabilities/convention-on-the-rights-of-persons-with-disabilities.html), 2006.•[The Geneva Conventions and Additional Protocols](https://www.icrc.org/eng/war-and-law/treaties-customary-law/geneva-conventions/overview-geneva-conventions.htm), 1949.•[IRTF’s Research into Human Rights Protocol Considerations,](https://tools.ietf.org/html/draft-irtf-hrpc-research) 2018.•[The UN Guiding Principles on Business and Human Rights, 2011.](http://www.ohchr.org/Documents/Publications/GuidingPrinciplesBusinessHR_EN.pdf)•British Standards Institute BS8611:2016, Robots and Robotic Devices. [Guide to the Ethical Design and Application of Robots and ](http://shop.bsigroup.com/ProductDetail?pid=000000000030320089)Robotic Systems\"p.21-22","id":"recxc8sqn09r985hi","dom_id":"item_recxc8sqn09r985hi"},{"Challenges":["recdmBNNa98cN8Sda"],"Sources":["recnCULdYQ36cpZR7"],"Strategies":["rec03QV2RUJkP3dfo","recdtW2BdWmY6WSP5","recEHwRaLD44KPk8v","reczFKqCos9f1opXO","recJyzFLE9ry4YEbb","rec81gtnlFS5W2BBF"],"childOf":["recOHnq45Fq7YWsRO"],"title":"Explicability","category":"Principles","name":"recxcFmvPG5wrCqpO","tags":[],"created_at":"2023-05-25T18:07:49.000Z","description":"“Explicability is crucial for building and maintaining users’ trust in AI systems. This means that processes need to be transparent, the capabilities and purpose of AI systems openly communicated, and decisions – to the extent possible – explainable to those directly and indirectly affected. Without such information, a decision cannot be duly contested. An explanation as to why a model has generated a particular output or decision (and what combination of input factors contributed to that) is not always possible. These cases are referred to as ‘black box’ algorithms and require special attention. In those circumstances, other explicability measures (e.g. traceability, auditability and transparent communication on system capabilities) may be required, provided that the system as a whole respects fundamental rights. The degree to which explicability is needed is highly dependent on the context and the severity of the consequences if that output is erroneous or otherwise inaccurate.33” (\\[High-Level Expert Group on AI, 2019, p. 13]\\(zotero://select/groups/4907410/items/XPCD8D3T)) (\\[pdf]\\(zotero://open-pdf/groups/4907410/items/CPFPH28M?page=15\u0026annotation=J8F447TV))","id":"recxcfmvpg5wrcqpo","dom_id":"item_recxcfmvpg5wrcqpo"},{"Sources":["recZRK4rseqW5VsRL"],"Strategies":["reczG9x5YfScZJdCo","recTWhZ88TbQLcNaQ"],"childOf":["recOHnq45Fq7YWsRO"],"title":"and grounded in literature and prior knowledge","category":"Principles","name":"recy4stJ6Y4e2Fezp","tags":[],"created_at":"2023-05-18T14:28:04.000Z","id":"recy4stj6y4e2fezp","dom_id":"item_recy4stj6y4e2fezp"},{"Sources":["recfYC5jjPmpLfSlM"],"Strategies":["recKWkdYO98PeEKDz"],"childOf":["rec7n2TGrH9RHYpQj"],"title":"Do no harm with these technologies and minimise the risks of their misuse or abuse","category":"Principles","name":"recy6hrMpKZ7TOn3Q","tags":[],"created_at":"2023-05-19T10:57:55.000Z","id":"recy6hrmpkz7ton3q","dom_id":"item_recy6hrmpkz7ton3q"},{"Sources":["recfYC5jjPmpLfSlM"],"childOf":["recZToVrPeFlFq0Aw"],"equivalentTo":["recNB5h9bK4gEE9uc"],"title":"Use AI to empower and to advance the interests and well-being of as many individuals as possible","category":"Principles","name":"recyigniIRfB6DPTu","tags":[],"created_at":"2023-05-19T11:00:08.000Z","id":"recyigniirfb6dptu","dom_id":"item_recyigniirfb6dptu"},{"Challenges":["recefglLZ3oJWw2SZ","recCpTJXuuowqsqQa","recHHr97jsyNDnlsJ","recKzhZVabDuYM6rG"],"PrincipleCollation":["rec5tUjckYAzHmc7e"],"Sources":["recZRK4rseqW5VsRL"],"Strategies":["recY9yr9vYcOAUSEA","recPynxbe7x5wOs5E","recmhwo8kmYkBZ7Sy","recgswAsiepwEclOd","recNzXJwCfbwBLmVU","recAhcg8OdusJvk43","recB7MF7TeUKH3chO","rec35PeHdUmtalypk","reciw4r1bRvx6A6Fq","rec7m69DQyCw3rlFg","recJyzFLE9ry4YEbb"],"childOf":["recLHILkx2JDFsLbX"],"title":"Protection of vulnerable persons","category":"Principles","name":"reczVPIH1y2OMpAJH","tags":[],"created_at":"2023-05-18T14:20:33.000Z","id":"reczvpih1y2ompajh","dom_id":"item_reczvpih1y2ompajh"},{"Cases":["recrVkbG0XGe2Ca0v","recmS3zSMbR3ofAR5","recSXcY4cnofb4zTP"],"Sources":["rec9jnxuHOioQn4DC"],"Strategies":["recRTVqtvPcBS6zps"],"title":"Accountability (Edu)","category":"Principles","name":"reczcRriFbQQpn8iX","tags":[],"created_at":"2023-06-08T06:34:38.000Z","description":"“Accountability including auditability, minimisation and reporting of negative impact, trade-offs, and redress. The considerations and requirements can help educators, school leaders and technology providers to adequately assess the impact, address the potential risks, and realise the benefits of an AI system deployed and used in education. As such they guide the development, deployment and use of trustworthy AI systems.” (European Commission, 2022, p. 19)","id":"reczcrrifbqqpn8ix","dom_id":"item_reczcrrifbqqpn8ix"},{"Principles":["recU6u0AZbcNj1ik9","recOHnq45Fq7YWsRO","recxcFmvPG5wrCqpO","reckb3cgfeDh1EeUP"],"Sources":["recnCULdYQ36cpZR7"],"title":"Overview of considerations in transparency","category":"Strategies","name":"rec03QV2RUJkP3dfo","tags":[],"created_at":"2023-05-28T19:04:35.000Z","description":"“Requirements of Trustworthy AI The principles outlined in Chapter I must be translated into concrete requirements to achieve Trustworthy AI. These requirements are applicable to different stakeholders partaking in AI systems’ life cycle: developers, deployers and end-users, as well as the broader society. By developers, we refer to those who research, design and/or develop AI systems. By deployers, we refer to public or private organisations that use AI systems within their business processes and to offer products and services to others. End-users are those engaging with the AI system, directly or indirectly. Finally, the broader society encompasses all others that are directly or indirectly affected by AI systems. Different groups of stakeholders have different roles to play in ensuring that the requirements are met: a. Developers should implement and apply the requirements to design and development processes; b. Deployers should ensure that the systems they use and the products and services they offer meet the requirements; c. End-users and the broader society should be informed about these requirements and able to request that they are upheld. The below list of requirements is non-exhaustive.35 It includes systemic, individual and societal aspects: 1 Human agency and oversight Including fundamental rights, human agency and human oversight 2 Technical robustness and safety Including resilience to attack and security, fall back plan and general safety, accuracy, reliability and reproducibility 3 Privacy and data governance Including respect for privacy, quality and integrity of data, and access to data 4 Transparency Including traceability, explainability and communication 5 Diversity, non-discrimination and fairness Including the avoidance of unfair bias, accessibility and universal design, and stakeholder participation 6 Societal and environmental wellbeing Including sustainability and environmental friendliness, social impact, society and democracy 7 Accountability Including auditability, minimisation and reporting of negative impact, trade-offs and redress.” (High-Level Expert Group on AI, 2019, p. 14)“While all requirements are of equal importance, context and potential tensions between them will need to be taken into account when applying them across different domains and industries. Implementation of these requirements should occur throughout an AI system’s entire life cycle and depends on the specific application. While most requirements apply to all AI systems, special attention is given to those directly or indirectly affecting individuals. Therefore, for some applications (for instance in industrial settings), they may be of lesser relevance. The above requirements include elements that are in some cases already reflected in existing laws. We reiterate that – in line with Trustworthy AI’s first component – it is the responsibility of AI practitioners to ensure that they comply with their legal obligations, both as regards horizontally applicable rules as well as domain-specific regulation” (High-Level Expert Group on AI, 2019, p. 15)The full document expands on each in detail.“To implement the above requirements, both technical and non-technical methods can be employed. These encompass all stages of an AI system’s life cycle. An evaluation of the methods employed to implement the requirements, as well as reporting and justifying51 changes to the implementation processes, should occur on an ongoing basis. AI systems are continuously evolving and acting in a dynamic environment.” (High-Level Expert Group on AI, 2019, p. 20) ","id":"rec03qv2rujkp3dfo","dom_id":"item_rec03qv2rujkp3dfo"},{"Principles":["recsvi4LnhEEPyQ1h","recU6u0AZbcNj1ik9"],"Sources":["rec6r8OkE2Q2EdiM3"],"title":"Questions to consider in key stages of AI and machine learning based research, regarding legitmacy and power in consent","category":"Strategies","name":"rec09f7Nm4RTf6WjE","tags":["reflection-questions","project-design"],"created_at":"2023-05-19T12:22:50.000Z","description":"“Conducting research, especially with complex and often opaque ML models, confers a degree of power in the researcher. The researcher decides which technical features will be used, and which data flows will be enacted. This paternalistic approach raises questions about the extent to which the stakeholders or data subjects are aware and in agreement with the experiments or data collections conducted in their social domains.26 Additional to questions from IRE 3.0, other questions are therefore relevant to highlight: - How does the researcher justify the foreseen intervention in a social, economic, or legal context by technical means? - Is the researcher able to understand and explain in an accessible manner how AI is to be used in the research?” - Is the researcher able to explain why the approach by technical means and the use of AI are better suited than any alternative methodology? - Have the persons who will be affected by the AI system requested the research? - If not, have they been informed and have they agreed? - If gaining the informed consent of all data subjects is infeasible, can the researcher obtain proxy consent from a representative or institutional ethics board on their behalf? - How has the balance between advantageous ends and individual freedom been struck? - Which values did the organization decide to promote, and how?\"(franzke et al., 2020, p. 37-38)","id":"rec09f7nm4rtf6wje","dom_id":"item_rec09f7nm4rtf6wje"},{"Principles":["recmzjcGKv3yNOxbl","recOHnq45Fq7YWsRO","rec6tz9Phzck0hvT8","reclur1ImQFlLyYof"],"Sources":["recfYC5jjPmpLfSlM"],"title":"Principle of Accountability, key considerations","category":"Strategies","name":"rec0GhefNkhJqW0c2","tags":["reflection-discussion"],"created_at":"2023-05-19T11:35:49.000Z","description":"\"When considering the role of accountability in the AI project delivery lifecycle, it is important first to make sure that you are taking a ‘best practices’ approach to data processing that is aligned with Principle 6 of the Data Ethics Framework. Beyond following this general guidance, however, you should pay special attention to the new and unique challenges posed to public sector accountability by the design and implementation of AI systems.Responsible AI project delivery requires that two related challenges to public sector accountability be confronted directly:1\\.Accountability gap: As mentioned above, automated decisions are not self-justifiable. Whereas human agents can be called to account for their judgements and decisions in instances where those judgments and decisions affect the interests of others, the statistical models and underlying hardware that compose AI systems are not responsible in the same morally relevant sense. This creates an accountability gap that must be addressed so that clear and imputable sources of human answerability can be attached to decisions assisted or produced by an AI system.2\\.Complexity of AI production processes: Establishing human answerability is not a simple matter when it comes to the design and deployment of AI systems. This is due to the complexity and multi-agent character of the development and use of these systems. Typically, AI project delivery workflows include department and delivery leads, technical experts, data procurement and preparation personnel, policy and domain experts, implementers, and others. Due to this production complexity, it may become difficult to answer the question of who among these parties involved in the production of AI systems should bear responsibility if these systems’ uses have negative consequences and impacts.Meeting the special requirements of accountability, which are born out of these two challenges, call for a sufficiently fine-grained concept of what would make an AI project properly accountable. This concept can be broken down into two subcomponents of accountability: answerability and auditability:•Answerability: The principle of accountability demands that the onus of justifying algorithmically supported decisions be placed on the shoulders of the human creators and users of those AI systems. This means that it is essential to establish a continuous chain of human responsibility across the whole AI project delivery workflow. Making sure that accountability is effective from end to end necessitates that no gaps be permitted in the answerability of responsible human authorities from first steps of the design of an AI system to its algorithmically steered outcomes.Answerability also demands that explanations and justifications of both the content of algorithmically supported decisions and the processes behind their production be offered by competent human authorities in plain, understandable, and coherent language. These explanations and justifications should be based upon sincere, consistent, sound, and impartial reasons that are accessible to non-technical hearers.•Auditability: Whereas the notion of answerability responds to the question of who is accountable for an automation supported outcome, the notion of auditability answers the question of how the designers and implementers of AI systems are to be held accountable. This aspect of accountability has to do with demonstrating both the responsibility of design and use practices and the justifiability of outcomes.Your project team must ensure that every step of the process of designing and implementing your AI project is accessible for audit, oversight, and review. Successful audit requires builders and implementers of algorithmic systems to keep records and to make accessible information that enables monitoring of the soundness and diligence of the innovation processes that produced the AI system.Auditability also requires that your project team keep records and make accessible information that enables monitoring of data provenance and analysis from the stages of collection, pre-processing, and modelling to training, testing, and deploying. This is the purpose of the previously mentioned Dataset Factsheet.Moreover, it requires your team to enable peers and overseers to probe and to critically review the dynamic operation of the system in order to ensure that the procedures and operations which are producing the model’s behaviour are safe, ethical, and fair. Practically transparent algorithmic models must be built for auditability, reproducible, and equipped for end-to-end recording and monitoring of their data processing. The deliberate incorporation of both of these elements of accountability (answerability and auditability) into the AI project lifecycle may be called Accountability-by-Design:Accountability deserves consideration across the entire design and implementation workflowAs a best practice, you should actively consider the different demands that accountability by design places on you before and after the roll out of your AI project. We will refer to the process of ensuring accountability during the design and development stages of your AI project as ‘anticipatory accountability.’ This is because you are anticipating your AI project’s accountability needs prior to it being completed. Following a similar logic, we will refer to the process of addressing accountability after the start of the deployment of your AI project as ‘remedial accountability.’ This is because after the initial implementation of your system, you are remedying any of the issues that may be raised by its effects and potential externalities. These two subtypes of accountability are sometimes referred to as ex-ante (or before-the-event) accountability and ex-post (after-the-event) accountability respectively.•Anticipatory Accountability: Treating accountability as an anticipatory principle entails that you take as of primary importance the decisions made and actions taken by your project delivery team prior to the outcome of an algorithmically supported decision process.This kind of ex ante accountability should be prioritised over remedial accountability, which focuses instead on the corrective or justificatory measures that can be taken after that automation supported process had been completed.By ensuring the AI project delivery processes are accountable prior to the actual application of the system in the world, you will bolster the soundness of design and implementation processes and thereby more effectively pre-empt possible harms to individual wellbeing and public welfare.Likewise, by establishing strong regimes of anticipatory accountability and by making the design and delivery process as open and publicly accessible as possible, you will put affected stakeholders in a position to make better informed and more knowledgeable decisions about their involvement with these systems in advance of potentially harmful impacts. In doing so, you will also strengthen the public narrative and help to safeguard the project from reputational harm.•Remedial Accountability: While remedial accountability should be seen, along these lines, as a necessary fallback rather than as a first resort for imputing responsibility in the design and deployment of AI systems, strong regimes of remedial accountability are no less important in providing necessary justifications for the bearing these systems have on the lives of affected stakeholders.Putting in place comprehensive auditability regimes as part of your accountability framework and establishing transparent design and use practices, which are methodically logged throughout the AI project delivery lifecycle, are essential components for this sort of remedial accountability.One aspect of remedial accountability that you must pay close attention to is the need to provide explanations to affected stakeholders for algorithmically supported decisions. This aspect of accountable and transparent design and use practices will be called explicability, which literally means the ability to make explicit the meaning of the algorithmic model’s result.Offering explanations for the results of algorithmically supported decision-making involves furnishing decision subjects and other interested parties with an understandable account of the rationale behind the specific outcome of interest. It also involves furnishing the decision subject and other interested parties with an explanation of the ethical permissibility, the fairness, and the safety of the use of the AI system. These tasks of content clarification and practical justification will be explored in more detail below as part of the section on transparency.\"(Leslie, 2019, p.22-23)","id":"rec0ghefnkhjqw0c2","dom_id":"item_rec0ghefnkhjqw0c2"},{"Challenges":["recvWM2glArsVhaye"],"Principles":["recmzjcGKv3yNOxbl"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Sustainable and environmentally friendly AI","category":"Strategies","name":"rec0WScLo6sUnoOQN","tags":["governance-question"],"created_at":"2023-05-28T19:44:36.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Societal and environmental well-being Sustainable and environmentally friendly AI:\u003cU+F0FC\u003e Did you establish mechanisms to measure the environmental impact of the AI system’s development, deployment and use (for example the type of energy used by the data centres)? \u003cU+F0FC\u003e Did you ensure measures to reduce the environmental impact of your AI system’s life cycle?” (High-Level Expert Group on AI, 2019, p. 30)","id":"rec0wsclo6sunooqn","dom_id":"item_rec0wsclo6sunooqn"},{"Challenges":["reci0hfcAOIxLVNhB"],"Sources":["recpXl48pJdKDhc6f"],"title":"Trust identity verification services to validate and protect identity","category":"Strategies","name":"rec1Ha94n5xbeqcOY","tags":[],"created_at":"2023-06-05T09:39:00.000Z","description":"\"**Issue: **How can we increase agency by providing individuals access to services allowing them to create a trusted identity to control the safe, specific, and finite exchange of their data?## RecommendationIndividuals should have access to trusted identity verification services to validate, prove, and support the context-specific use of their identity.## Further Resources•Sovrin Foundation, [The Inevitable Rise of SelfSovereign Identity,](https://sovrin.org/wp-content/uploads/2017/06/The-Inevitable-Rise-of-Self-Sovereign-Identity.pdf) Sept. 29, 2016.•T. Ruff, “[Three Models of Digital Identity Relationships](https://medium.com/evernym/the-three-models-of-digital-identity-relationships-ca0727cb5186),” Evernym, Apr. 24, 2018.•C. Pettey, [The Beginner’s Guide to Decentralized Identity.](https://www.gartner.com/smarterwithgartner/the-beginners-guide-to-decentralized-identity/) Gartner, 2018.•C. Allen, [The Path to Self-Sovereign Identity](https://github.com/ChristopherA/self-sovereign-identity/blob/master/ThePathToSelf-SovereignIdentity.md). GitHub, 2017.\"p.112-113 IEEE report","id":"rec1ha94n5xbeqcoy","dom_id":"item_rec1ha94n5xbeqcoy"},{"Challenges":["recJfAMUB8HdjVQYD"],"Sources":["recpXl48pJdKDhc6f"],"title":"Educate the learners in formal education, the public, professionals, and policy makers for designing, and working alongside, AI to advance the SDGs.","category":"Strategies","name":"rec2679E9TXh1DCL8","tags":["education"],"created_at":"2023-06-05T12:00:13.000Z","description":"## \"RecommendationsEducation with respect to A/IS must be targeted to three sets of students: the general public, present and future professionals in A/IS, and present and future policy makers. To prepare the future workforce to develop culturally appropriate A/IS, to work productively and ethically alongside such technologies, and to advance the UN SDGs, the curricula in HIC and LMIC universities and professional schools require innovation. Equally importantly, preuniversity education systems, starting with early childhood education, need to be reformed to prepare society for the risks and opportunities of the A/IS age, rather than the current system which prepares society for work in an industrial age that ended with the 20th century. Specific recommendations include:•Preparing future managers, lawyers, engineers, civil servants, and entrepreneurs to work productively and ethically as global citizens alongside A/IS, through reform of undergraduate and graduate curricula as well as of preschool, primary, and secondary school curricula. This will require:•Fomenting interaction between universities and other actors such as companies, governments, NGOs, etc., with respect to A/IS research through definition of research priorities and joint projects, subcontracts to universities, participation in observatories, and co-creation of curricula, cooperative teaching, internships/service learning, and conferences/seminars/courses.•Establishing and supporting more multidisciplinary degrees that includeA/IS, and adapting university curricula to provide a broad, integrated perspective which allows students to understand the impact of A/IS in the global, economic, environmental, and sociocultural domains and trains them as future policy makers in A/IS fields.•Integrating the teaching of ethics andA/IS across the education spectrum, from preschool to postgraduate curricula, instead of relegating ethics to a standalone module with little direct practical application.•Promoting service learning opportunities that allow A/IS undergraduate and graduate students to apply their knowledge to meet the needs of a community.•Creating international exchange programs, through both private and public institutions, which expose students to different cultural contexts for A/IS applications in both HIC and LMIC.•Creating experimental curricula to prepare people for information-based work in the 21st century, from preschool through postgraduate education.•Taking into account transversal competencies students need to acquire to become ethical global citizens, i.e., critical thinking, empathy, sociocultural awareness, flexibility, and deontological reasoningin the planning and assessment ofA/IS curricula.•Training teachers in teaching methodologies suited to addressing challenges imposed in the age of A/IS.•Stimulating STEAM courses in preuniversity education.•Encouraging high-quality HIC-LMIC collaborative A/IS research in both private and public universities.•Conducting research to support innovation in education and business for the A/IS world, which could include:•Researching the impact of A/IS on the governance and macro/micro strategies of companies and organizations, together with those companies, in an interdisciplinary manner which harnesses expertise of both social scientists and technology experts.•Researching the impact of A/IS on the business model for the development of new products and services through the collaborative efforts of management, operations, and the technical research and development function.•Researching how empathy can be taught and integrated into curricula, starting at the preschool level.•Researching how schools and education systems in low-income settings of both HIC and LMIC can leverage their lessentrenched interests to leapfrog into a 21st century-ready education system.•Establishing ethics observatories in universities with the purpose of fostering an informed public opinion capableof participating in policy decisionsregarding the ethics and social impactof A/IS applications.•Creating professional continuing education and employment opportunities in A/IS for current professionals, including through online and executive education courses.•Creating educative mass media campaigns to elevate society’s ongoing baseline level of understanding of A/IS systems, including what it is, if and how it can be trusted in various contexts, and what are its limitations.## Further resources•ABET Computing and Engineering Accreditation Criteria 2018. Available at: [http://www.abet.org/accreditation/ accreditation-criteria/](http://www.abet.org/accreditation/accreditation-criteria/)•ABET, 2017 ABET Impact Report, Working Together for a Sustainable Future_, _2017.•emlyon business school, Artificial Intelligence in Management (AIM) Institute [http://aim. em-lyon.com](http://aim.em-lyon.com/)UNESCO,_ The UN Decade of Education for Sustainable Development, Shaping the Education of Tomorrow_. UNESCO 2012.\"p.153-156","id":"rec2679e9txh1dcl8","dom_id":"item_rec2679e9txh1dcl8"},{"Cases":["reciNqxyfUgE5XM7t"],"Principles":["reczVPIH1y2OMpAJH","recaMsksKInFYbnCL"],"Sources":["rec9jnxuHOioQn4DC"],"title":"Guiding questions for educators regarding Diversity, non-Discrimination, and Fairness of AI in Education","category":"Strategies","name":"rec35PeHdUmtalypk","tags":["reflection-questions"],"created_at":"2023-05-19T13:36:43.000Z","description":"“• Is the system accessible by everyone in the same way without any barriers? • Does the system provide appropriate interaction modes for learners with disabilities or special education needs? Is the AI system designed to treat learners respectfully adapting to their individual needs? • Is the user interface appropriate and accessible for the age level of the learners? Has the usability and user-experience been tested for the target age group? • Are there procedures in place to ensure that AI use will not lead to discrimination or unfair behaviour for all users? • Does the AI system documentation or its training process provide insight into potential bias in the data? • Are procedures in place to detect and deal with bias or perceived inequalities that may arise?” (European Commission, 2022, p. 20)","id":"rec35pehdumtalypk","dom_id":"item_rec35pehdumtalypk"},{"Principles":["recU6u0AZbcNj1ik9"],"Sources":["rec6r8OkE2Q2EdiM3"],"title":"Questions to consider in key stages of AI and machine learning based research, regarding sociotechnical context","category":"Strategies","name":"rec3q0xKZABgZf9Dg","tags":["reflection-questions","project-inception"],"created_at":"2023-05-19T12:17:03.000Z","description":"“The first step is to create an understanding of the AI, its operation, and social impact based on relevant contextual factors. The researcher must scrutinize the social, political, and economic context within which a technology operates, as much as the technology itself. This is particularly pertinent when dealing with AI as the learning models may show patterns that are unexpected. AI can also amplify already existing social hierarchies. The influence of stakeholders (e.g. developers in different communities and organizations inside and outside academia) may be strong when for instance cleaning data, using pretrained models or changing the models. - How would you characterize the social context within which or about which the research is conducted? - Who are the data subjects and affected stakeholders involved in this project (directly or indirectly)? - How would the researcher characterize the norms (e.g. privacy, social hierarchy) and sensitivities in this social context?” - How have these norms and sensitivities influenced the application of AI in gathering data for this research project? - How will the implementation of the AI system or use of the model affect the norms and sensitivities?(franzke et al., 2020, p. 36-37)","id":"rec3q0xkzabgzf9dg","dom_id":"item_rec3q0xkzabgzf9dg"},{"Principles":["rec42P8U9usfYCtv9","reclPiw2VvNOSTzv5"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Resilience to attack and security","category":"Strategies","name":"rec4oGONFgYMu3JGf","tags":["governance-question"],"created_at":"2023-05-28T19:19:25.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Technical robustness and safety Resilience to attack and security: \u003cU+F0FC\u003e Did you assess potential forms of attacks to which the AI system could be vulnerable? \u003cU+F0A7\u003e Did you consider different types and natures of vulnerabilities, such as data pollution, physical infrastructure, cyber-attacks? \u003cU+F0FC\u003e Did you put measures or systems in place to ensure the integrity and resilience of the AI system against potential attacks? \u003cU+F0FC\u003e Did you verify how your system behaves in unexpected situations and environments? \u003cU+F0FC\u003e Did you consider to what degree your system could be dual-use? If so, did you take suitable preventative measures against this case (including for instance not publishing the research or deploying the system)?” (High-Level Expert Group on AI, 2019, p. 27)","id":"rec4ogonfgymu3jgf","dom_id":"item_rec4ogonfgymu3jgf"},{"Principles":["recMGB4iC5oaCtr5x","rec42P8U9usfYCtv9","rec6O9e1nYBJtQUTj"],"Sources":["recQzldmBLByP78Uu"],"title":"Questions to consider in SoTL research method selection","category":"Strategies","name":"rec55h8FhGKyGPNgw","tags":["reflection-questions","education-research"],"created_at":"2023-05-19T12:58:52.000Z","description":"Methods:- What methods will you use in your investigation? - What type of data will you gather? Will this include data that goes beyond normal classroom activities and assessments? How much class time will additional data collection activities take? - How can your investigation be made educationally valuable for students? Might students be involved, for instance, by gathering and analyzing data?-  Will your data collection choices (e.g., video recording, use of personal writing, use of data from whole-class discussion) affect your ability to protect students’ privacy?(Fedoruk, 2017, p. 2)","id":"rec55h8fhgkygpngw","dom_id":"item_rec55h8fhgkygpngw"},{"Challenges":["rec2ULcHUjSbbSnHL"],"Sources":["recpXl48pJdKDhc6f"],"title":"Monitor impact of affective AI in care on human-human relationships and implement safeguards","category":"Strategies","name":"rec5CFheImo8onTcd","tags":[],"created_at":"2023-06-05T11:35:18.000Z","description":"## \"RecommendationsAs this technology develops, it is important to monitor research into the development of intimate relationships between A/IS and humans. Research should emphasize any technical and normative developments that reflect use ofA/IS in positive and therapeutic ways while also creating appropriate safeguards to mitigate against uses that contribute to problematic individual or social relationships:1\\.Intimate systems must not be designed or deployed in ways that contribute to stereotypes, gender or racial inequality,or the exacerbation of human misery.2\\.Intimate systems must not be designed to explicitly engage in the psychological manipulation of the users of these systems unless the user is made aware they are being manipulated and consents to this behavior. Any manipulation should be governedthrough an opt-in system.3\\.Caring A/IS should be designed to avoid contributing to user isolation from society.4\\.Designers of affective robotics must publicly acknowledge, for example, within a notice associated with the product, that these systems can have side effects, such as interfering with the relationship dynamics between human partners, causing attachments between the user and the A/IS that are distinct from human partnership.5\\.Commercially marketed A/IS for caring applications should not be presented to be a person in a legal sense, nor marketed as a person. Rather its artifactual, that is, authored, designed, and built deliberately, nature should always be made as transparent as possible, at least at point of sale and in available documentation, as noted in Section 4, Systems Supporting Human Potential.6.Existing laws regarding personal imagery need to be reconsidered in light of caring A/IS.In addition to other ethical considerations, it will also be necessary to establish conformance with local laws and mores in the context of caring A/IS systems.## Further Resources•M. Boden, J. Bryson, D. Caldwell, K. Dautenhahn, L. Edwards, S. Kember, P.Newman, V. Parry, G. Pegman, T. Rodden and T. Sorrell, Principles of robotics: regulating robots in the real world. Connection Science, vol. 29, no. 2, pp. 124-129, April 2017.•J. J. Bryson, M. E. Diamantis, and T. D. Grant, “Of, For, and By the People: The Legal Lacuna of Synthetic Persons.” _Artificial Intelligence \u0026 Law_, vol. 25, no. 3, pp. 273–291, Sept. 2017.•M. Scheutz, “The Inherent Dangers ofUnidirectional Emotional Bonds between Humans and Social Robots,” in _Robot Ethics: The Ethical and Social Implications of Robotics,_ P. Lin, K. Abney, and G. Bekey, Eds., pp. 205. Cambridge, MA: MIT Press, 2011.","id":"rec5cfheimo8ontcd","dom_id":"item_rec5cfheimo8ontcd"},{"Cases":["recrVkbG0XGe2Ca0v","recAlOHJhEy5nDwA6"],"Principles":["recmzjcGKv3yNOxbl","recqdGhz7l1cGnQeU"],"Sources":["rec9jnxuHOioQn4DC"],"title":"Guiding questions for educators regarding Societal and Environmental Wellbeing of AI in Education","category":"Strategies","name":"rec7daqDHSCuc70yS","tags":["reflection-questions"],"created_at":"2023-05-19T13:36:44.000Z","description":"“• How does the AI system affect the social and emotional wellbeing of learners and teachers? • Does the AI system clearly signal that its social interaction is simulated and that it has no capacities of feeling or empathy? • Are students or their parents involved in the decision to use the AI system and support it? • Is data used to support teachers and school leaders to evaluate student wellbeing and if so, how is this being monitored? • Does use of the system create any harm or fear for individuals or for society?” (European Commission, 2022, p. 20)","id":"rec7daqdhscuc70ys","dom_id":"item_rec7daqdhscuc70ys"},{"Principles":["recPg7Ov0priGGtLm","recOHnq45Fq7YWsRO","reczVPIH1y2OMpAJH"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Access to data","category":"Strategies","name":"rec7m69DQyCw3rlFg","tags":["governance-question"],"created_at":"2023-05-28T19:32:51.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Privacy and data governance Respect for privacy and data Protection: “Access to data:\u003cU+F0FC\u003e What protocols, processes and procedures did you follow to manage and ensure proper data governance? \u003cU+F0A7\u003e Did you assess who can access users’ data, and under what circumstances? \u003cU+F0A7\u003e Did you ensure that these persons are qualified and required to access the data, and that they have the necessary competences to understand the details of data protection policy? \u003cU+F0A7\u003e Did you ensure an oversight mechanism to log when, where, how, by whom and for what purpose data was accessed?” (High-Level Expert Group on AI, 2019, p. 28)","id":"rec7m69dqycw3rlfg","dom_id":"item_rec7m69dqycw3rlfg"},{"Cases":["recmS3zSMbR3ofAR5"],"Challenges":["recdmBNNa98cN8Sda","rececsX8igwNqhhkC","rec9Cuz3HWc9lWW23","recOpn4I30te1qiKl"],"Principles":["recOHnq45Fq7YWsRO","recQEiU22Qy1E0YuA","recxcFmvPG5wrCqpO"],"Sources":["rec6r8OkE2Q2EdiM3","recpXl48pJdKDhc6f","recnCULdYQ36cpZR7"],"title":"Questions to consider in key stages of AI and machine learning based research, regarding transparency and explainability","category":"Strategies","name":"rec81gtnlFS5W2BBF","tags":["reflection-questions","project-dissemination","governance-question"],"created_at":"2023-05-19T12:42:14.000Z","description":"The scientific method requires a high degree of transparency and explainability of how research findings were derived. This conflicts to some extent with the complex nature of AI technologies (Ananny \u0026 Crawford, 2018; Calo, 2017; Danaher, 2016; Wachter, Mittelstadt \u0026 Floridi, 2017; Weller, 2017). Indeed, it can be too complex for a researcher to precisely state how research data was created given the way a neural network with potentially hidden layers operates. However, the concepts of transparency and explainability do not mean strictly understanding the highly specialized technical code and data of the trained neural networks. For example, a richer notion of transparency asks researchers to explain the ends, means, and thought processes that went into developing the code, the model, and how the resulting research data was shaped. Similarly, explainability does not need to be exact, but can learn from interpretations in philosophy, cognitive psychology or cognitive science, and social psychology (Miller, 2017). \u003cU+25CF\u003eCan the researcher give an account of how the model operates and how research data was generated? \u003cU+25CF\u003eHas the researcher explained how the model works to an institutional ethics board, and have they understood the reasons and methods of data processing? \u003cU+25CF\u003eWhat roles have the developers and researchers played and what choices have they made in constructing the model, choosing and cleaning the training data and how has this affected the results and prediction? \u003cU+25CF\u003eWhat kind of negotiations have taken place in the decision-making around model selection, adjustments and data modelling in the research process that can affect the result and prediction? (franzke, 2020, p.45)### IEEE Recommendation\"When systems are built that could impact the safety or well-being of humans, it is not enough to just presume that a system works. Engineers must acknowledge and assess the ethical risks involved with black box software and implement mitigation strategies.Technologists should be able to characterize what their algorithms or systems are going to do via documentation, audits, and transparent and traceable standards. To the degree possible, these characterizations should be predictive, but given the nature of A/IS, they might need to be more retrospective and mitigation-oriented. As such, it is also important to ensure access to remedy adverse impacts.Technologists and corporations must do their ethical due diligence before deploying A/IS technology. Standards for what constitutes ethical due diligence would ideally be generated by an international body such as IEEE or ISO, and barring that, each corporation should work to generate a set of ethical standards by which their processes are evaluated and modified. Similar to a flight data recorder in the field of aviation, algorithmic traceability can provide insights on what computations led to questionable or dangerous behaviors. Even where such processes remain somewhat opaque, technologists should seek indirect means of validating results and detecting harms.## Further resources•M. Ananny and K. Crawford, “[Seeing without Knowing: Limitations of the Transparency Ideal and Its Application to Algorithmic Accountability](http://journals.sagepub.com/doi/abs/10.1177/1461444816676645),” _New Media \u0026 Society_, vol. 20, no. 3, pp. 973-989, Dec. 13, 2016.•D. Reisman, J. Schultz, K. Crawford, and M. Whittaker, “Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability,” AI NOW 2018. [Online]. Available: [https://ainowinstitute.org/ aiareport2018.pdf](https://ainowinstitute.org/aiareport2018.pdf).[Accessed October 28, 2018].•J. A. Kroll “[The Fallacy of Inscrutability](http://rsta.royalsocietypublishing.org/content/376/2133/20180084).” _Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences_, C. Cath, S. Wachter, B. Mittelstadt and L. Floridi, Eds., October 15, 2018 DOI: 10.1098/rsta.2018.0084.\"p.139And recommendation regarding documentation:\"Engineers should be required to thoroughly document the end product and related data flows, performance, limitations, and risks ofA/IS. Behaviors and practices that have been prominent in the engineering processes should also be explicitly presented, as well as empirical evidence of compliance and methodology used, such as training data used in predictive systems, algorithms and components used, and results of behavior monitoring. Criteria for such documentation could be: auditability, accessibility, meaningfulness, and readability.Companies should make their systems auditable and should explore novel methods for external and internal auditing.## Further reading•S. Wachter, B. Mittelstadt, and L. Floridi. “[Transparent, Explainable, and Accountable AI for Robotics](http://robotics.sciencemag.org/content/2/6/eaan6080).” _[Science Robotics, v](http://robotics.sciencemag.org/content/2/6/eaan6080)ol. _2, no. 6, May 31, 2017. [Online]. Available: DOI: 10.1126/scirobotics.aan6080. [Accessed Nov.•S. Barocas, and A. D. Selbst, “[Big Data’s Disparate Impact.](http://www.californialawreview.org/wp-content/uploads/2016/06/2Barocas-Selbst.pdf)” _California Law Review_ 104, 671-732, 2016.•J. A. Kroll, J. Huey, S. Barocas, E. W. Felten, J. R. Reidenberg, D. G. Robinson, and H. Yu. “[Accountable Algorithms.](https://www.pennlawreview.com/print/165-U-Pa-L-Rev-633.pdf)” _University of Pennsylvania Law Review _165, no. 1, 633– 705, 2017.•J. M. Balkin, “[Free Speech in the Algorithmic Society: Big Data, Private Governance, and New School Speech Regulation.](https://ssrn.com/abstract%3D3038939)” _UC Davis Law Review_, 2017.\"p.135## High-level expert group“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION)“Transparency“Explainability:\u003cU+F0FC\u003e Did you assess:\u003cU+F0A7\u003e to what extent the decisions and hence the outcome made by the AI system can be understood?\u003cU+F0A7\u003e to what degree the system’s decision influences the organisation’s decision-making processes?\u003cU+F0A7\u003e why this particular system was deployed in this specific area?\u003cU+F0A7\u003e what the system’s business model is (for example, how does it create value for the organisation)?\u003cU+F0FC\u003e Did you ensure an explanation as to why the system took a certain choice resulting in a certain outcome that all users can understand?\u003cU+F0FC\u003e Did you design the AI system with interpretability in mind from the start?\u003cU+F0A7\u003e Did you research and try to use the simplest and most interpretable model possible for the application in question?\u003cU+F0A7\u003e Did you assess whether you can analyse your training and testing data? Can you change and update this over time?\u003cU+F0A7\u003e Did you assess whether you can examine interpretability after the model’s training and development, or whether you have access to the internal workflow of the model?” (High-Level Expert Group on AI, 2019, p. 29)","id":"rec81gtnlfs5w2bbf","dom_id":"item_rec81gtnlfs5w2bbf"},{"Principles":["recOHnq45Fq7YWsRO"],"Sources":["rec6r8OkE2Q2EdiM3"],"title":"Questions to consider in key stages of AI and machine learning based research, regarding corporate-academic research interaction","category":"Strategies","name":"rec8cNT8sPSFtMSAc","tags":["reflection-questions"],"created_at":"2023-05-19T12:49:28.000Z","description":"\"[Questions to] help researchers in identifying the ethical issues raised in the cooperation between academy and corporations or while working on corporate data:•Who are the stakeholders involved? •Who is the subject that is providing financial support?•What are the goals of the research? Are the researchers answering only to the financer’s needs and/or do they have also their own research questions to pursue? •Which is the relationship between the funder(s) and the researchers? And between the different stakeholders involved? Is it a relationship on equal terms or are there some power imbalances? •Who decides the methodological approach(es)? •What is / are the methodological approach(es)? •Who is the owner of the data obtained during the research process and where do the data come from (i.e. originally produced/retrieved for the research and/or already extant corporate data provided by the funder)? •What are the forms of dissemination of the research (i.e. academic, corporate, educational)?•Are there risks for the researchers or for the academic institution in carrying on such research (i.e. reputation, data management, ethics)?•Do the corporate data involve any human subject data? Where are the data stored? Which are the terms of use of the platform studied? (franzke, 2020, p.59-60)","id":"rec8cnt8spsftmsac","dom_id":"item_rec8cnt8spsftmsac"},{"Principles":["recOHnq45Fq7YWsRO","recmzjcGKv3yNOxbl"],"Sources":["rec6r8OkE2Q2EdiM3"],"title":"Questions to consider in key stages of AI and machine learning based research, regarding downstream responsibility","category":"Strategies","name":"rec8dLtyDCwvjMWge","tags":["reflection-questions","project-dissemination","internet-research"],"created_at":"2023-05-19T12:42:55.000Z","description":"Models can be used in a variety of ways, or they may influence others to create similar models for other ends. Research ethics frameworks, however, typically require the review process to limit itself to the immediate impact on research stakeholders and not necessarily assess the potential long-term impacts of the research outputs (Zevenbergen, Brown, Wright, \u0026 Erdos, 2013).This may be problematic for omni-use technologies such as AI models. Innovations in AI technologies and their inferences on social and human dynamics may be used for a multitude of purposes for instance tailoring microtargeted communication and thus potentially undermining democracy (e.g. issues of fair election \u0026 voting discrimination). Models or datasets that were designed to produce positive social ends can be used towards malicious and destructive ends (e.g. facial recognition to clamp down on political gatherings/dissent). Even if the research aims are beneficial for a wide group of stakeholders, research methods and models may need to be published along with the research outcomes and thus set a standard or precedent and initiate function creep and unintended consequences.Once an AI system has left the hands of the original researchers, they may not have any control over how their models are used by others. The same is true for the generated research data: once it has been freely published, it will be difficult to contain its further uses.Legal and constitutional checks and balances on the exertion of power or the use of data may differ around the world (Knapp \u0026 VandeCreek, 2007). While it is beyond the scope of an ethics review to assess the political governance in countries around the world, it is useful for researchers to be mindful that their data and models may contain personal and sensitive data that could be used directly or indirectly against individuals in other countries or political systems.Researchers should thus engage actively with the reality that their methods and models may be misused by others, and find ways to mitigate risks and harms. It is ultimately the responsibility of researchers – in dialogue with ethics boards and other stakeholders in a specific project – to agree on the limitations based on a thorough understanding of the project weighed heavily against the knowledge production it produces. \u003cU+25CF\u003eWhat could be the downstream consequence for data subjects for erroneous identifications, labelling, or categorization?\u003cU+25CF\u003eTo what extent is the researcher sensitive to the local norms, values, and legal mechanisms that could adversely affect the subjects of their research? \u003cU+25CF\u003eTo what extent can the researcher foresee how the data created through research project inferences may be used in further, third-party systems that make decisions about people?\u003cU+25CF\u003eIs it foreseeable that the methodologies, actions, and resulting knowledge may be used for malicious ends in another context than research and to what extent can this be mitigated?\u003cU+25CF\u003eWhich actors will likely be interested to use the methodology for malevolent purposes, and how?\u003cU+25CF\u003eCan the inferred data be directly useful to authoritarian governments who may target or justify crackdowns on minorities or special interest groups based on (potentially erroneously inferred) collected data? Can this be mitigated without destroying the findings for the specific research project? \u003cU+25CF\u003eIs it possible to contain the potential malevolent future uses by design? \u003cU+25CF\u003eUp to which hypothetical data reuse moment is it appropriate to hold the researcher responsible? (franazke, 2020, p45-46)","id":"rec8dltydcwvjmwge","dom_id":"item_rec8dltydcwvjmwge"},{"Principles":["rec42P8U9usfYCtv9","reclPiw2VvNOSTzv5","recOHnq45Fq7YWsRO"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Reliability and reproducibility","category":"Strategies","name":"rec95I69E1YcGg0Sa","tags":["governance-question"],"created_at":"2023-05-28T19:29:14.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) Technical robustness and safety“Reliability and reproducibility“\u003cU+F0FC\u003e Did you put in place a strategy to monitor and test if the AI system is meeting the goals, purposes and intended applications? \u003cU+F0A7\u003e Did you test whether specific contexts or particular conditions need to be taken into account to ensure reproducibility? \u003cU+F0A7\u003e Did you put in place verification methods to measure and ensure different aspects of the system's reliability and reproducibility? \u003cU+F0A7\u003e Did you put in place processes to describe when an AI system fails in certain types of settings? \u003cU+F0A7\u003e Did you clearly document and operationalise these processes for the testing and verification of the reliability of AI systems? \u003cU+F0A7\u003e Did you establish mechanisms of communication to assure (end-)users of the system’s reliability?” (High-Level Expert Group on AI, 2019, p. 28)","id":"rec95i69e1ycgg0sa","dom_id":"item_rec95i69e1ycgg0sa"},{"Challenges":["recZ43i8Cnhohwb2x","recOpn4I30te1qiKl"],"Sources":["recpXl48pJdKDhc6f"],"title":"Company culture, values, codes, and power structures should enable speaking to ethical concerns","category":"Strategies","name":"rec9SSyrfFkmuJkCe","tags":[],"created_at":"2023-06-05T10:31:54.000Z","description":"## \"RecommendationsEmployees should be empowered and encouraged to raise ethical concerns inday-to-day professional practice.To be effective in ensuring adoption of ethical considerations during product development or internal implementation of A/IS, organizations should create a company culture and set of norms that encourage incorporating ethical considerations in the design and implementation processes.New categories of considerations around these issues need to be accommodated, along with updated Codes of Conduct, company value-statements, and other management principles so individuals are empowered to share their insights and concerns in an atmosphere of trust. Additionally, bottom-up approaches like company “town hall meetings” should be explored that reward, rather than punish, those who bring up ethical concerns.## Further Resources•[The British Computer Society (BCS),](http://www.bcs.org/category/6030) Codeof Conduct, 2019.•C. Cath, and L. Floridi, “[The Design of the Internet’s Architecture by the Internet Engineering Task Force (IETF) and Human Rights](http://link.springer.com/article/10.1007%2Fs11948-016-9793-y),” _Science and Engineering Ethics, _vol._ _23, no. 2, pp. 449–468, Apr. 2017.\"p.129","id":"rec9ssyrffkmujkce","dom_id":"item_rec9ssyrffkmujkce"},{"Challenges":["rec2ajglpzbFYRivi","reciH69D8oIGI1p93","recrVvneRZaEutaGw"],"Sources":["recpXl48pJdKDhc6f"],"title":"Systematic analysis of consequences, transparency, user agency, and safeguards are central to implementation of AI nudges","category":"Strategies","name":"rec9TbOu2ZXUZG4A5","tags":[],"created_at":"2023-06-05T11:36:59.000Z","description":"## \"Recommendations1\\.Systematic analyses are needed that examine the ethics and behavioral consequences of designing affective systems to nudge human beings prior to deployment.2\\.The user should be empowered, through an explicit opt-in system and readily available, comprehensible information, to recognize different types of A/IS nudges, regardless of whether they seek to promote beneficial social manipulation or to enhance consumer acceptance of commercial goals. The user should be able to access and check facts behind the nudges and then make a conscious decision to accept or reject a nudge. Nudging systems must be transparent, with a clear chain of accountability that includes human agents: data logging is required so users can know how, why, and by whom they were nudged.3\\.A/IS nudging must not become coercive and should always have an opt-in system policy with explicit consent.4\\.Additional protections against unwanted nudging must be put in place for vulnerable populations, such as children, or when informed consent cannot be obtained. Protections against unwanted nudging should be encouraged when nudges alter long-term behavior or when consent alone may not bea sufficient safeguard against coercionor exploitation.5\\.Data gathered which could reveal an individual or groups’ susceptibility to a nudge or their emotional reaction to a nudge should not be collected or distributed without opt-in consent, and should only be retained transparently, with access restrictions in compliance with the highest requirements of data privacy and law.## Further Resources•R. Thaler, and C. R. Sunstein, _Nudge: Improving Decision about Health, Wealth and Happiness_, New Haven, CT: Yale University Press, 2008.•L. Bovens, “The Ethics of Nudge,” in _Preference change: Approaches from Philosophy, Economics and Psychology_, T. Grüne-Yanoff and S. O. Hansson, Eds., Berlin, Germany: Springer, 2008 pp. 207–219.•S. D. Hunt and S. Vitell. \"A General Theory of Marketing Ethics.\" Journal of Macromarketing, vol.6, no. 1, pp. 5-16, June 1986.•A. McStay, [Empathic Media and Advertising: Industry, Policy, Legal and Citizen Perspectives (the Case for Intimacy)](http://journals.sagepub.com/doi/pdf/10.1177/2053951716666868), Big Data \u0026 Society, pp. 1-11, December 2016.•J. de Quintana Medina and P. Hermida Justo, “Not All Nudges Are Automatic: Freedom of Choice and Informative Nudges.” Working paper presented to the European Consortium for Political Research, Joint Session of Workshops, 2016 Behavioral Change and Public Policy, Pisa, Italy, 2016.•M. D. White, _[The Manipulation of Choice. Ethics and Libertarian Paternalism. ](http://www.palgraveconnect.com/doifinder/10.1057/9781137313577)_New York: Palgrave Macmillan, 2013•C.R. Sunstein, The Ethics of Influence: Government in the Age of Behavioral Science. New York: Cambridge, 2016•M. Scheutz, “[The Affect Dilemma for Artificial Agents: Should We Develop Affective Artificial Agents? ](http://ieeexplore.ieee.org/document/6296668/)” _IEEE Transactions on Affective Computing, _vol._ _3, no. 4,pp. 424–433,Sept. 2012.•A. Grinbaum, R. Chatila, L. Devillers, J.G. Ganascia, C. Tessier and M. Dauchet. “[Ethics in Robotics Research: CERNA Recommendations,](http://ieeexplore.ieee.org/document/7822928/)” _IEEE Robotics and Automation Magazine, _vol._ _24, no. 3,pp. 139–145, Sept. 2017.“Designing Moral Technologies: Theoretical, Practical, and Ethical Issues” Conference July 10–15, 2016, Monte Verità, Switzerland\"p.96-97## \"Recommendations1\\.As more and more computing devices subtly and overtly influence human behavior, it is important to draw attention to whether it is ethically appropriate to pursue this type of design pathway in the context of governmental actions.2\\.There needs to be transparency regarding who the intended beneficiaries are, and whether any form of deception or manipulation is going to be used to accomplish the intended goal.## Further Resources•J. Borenstein and R. Arkin, “[Robotic Nudges: Robotic Nudges: The Ethics of Engineering a More Socially Just Human Being Just Human Being.](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)” _Science and Engineering Ethics, _vol._ _22, no. 1,pp. 31–46, Feb. 2016.•J. Borenstein and R. Arkin. “[Nudging for Good: Robots and the Ethical Appropriateness of Nurturing Empathy and Charitable Behavior ](https://link.springer.com/article/10.1007/s00146-016-0684-1?no-access=true).” _AI and Society, _vol._ _32, no. 4, pp. 499–507, Nov. 2016.\"p.97-98\"1.Consideration should be given to the development of a system of technical licensing (“permits”) or other certification from governments or non-governmental organizations (NGOs) that can aid users to understand the nudges from A/IS in their lives.2\\.User autonomy is a key and essential consideration that must be taken into account when addressing whether affective systems should be permitted to nudge human beings.3\\.Design features of an affective system that nudges human beings should include the ability to accurately distinguish between users, including detecting characteristics such as whether the user is an adult or a child.4\\.Affective systems with nudging strategies should incorporate a design system of evaluation, monitoring, and control for unintended consequences.## Further Resources•J. Borenstein and R. Arkin, “[Robotic Nudges: ](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)[Robotic Nudges: The Ethics of Engineering a ](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)[More Socially Just Human Being Just Human Being.](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)” _Science and Engineering Ethics, _vol._ _22, no. 1, pp. 31–46, 2016.•R. C. Arkin, M. Fujita, T. Takagi, and R. Hasegawa, “[An Ethological and Emotional Basis for Human- Robot Interaction.](http://www.sciencedirect.com/science/article/pii/S0921889002003755)” _Robotics and Autonomous Systems, _vol._ _42, no. 3–4 pp.191–201, March 2003.•S. Omohundro “[Autonomous Technology and the Greater Human Good.](http://www.tandfonline.com/doi/abs/10.1080/0952813X.2014.895111?journalCode=teta20)” _Journal of Experimental and Theoretical Artificial Intelligence, vol. _26, no. 3, pp. 303–315, 2014.\"p.102\"Deception is commonplace in everyday human-human interaction. According to Kantian ethics, it is never ethically appropriate to lie, while utilitarian frameworks indicate that it can be acceptable when deception increases overall happiness. Given the diversity of views on ethics and the appropriateness of deception, should affective systems be designed to deceive? Does the non-consensual nature of deception restrict the use of A/IS in contexts in which deception may be required?It is necessary to develop recommendations regarding the acceptability of deception performed by A/IS, specifically with respect to when and under which circumstances, if any,it is appropriate.1\\.In general, deception may be acceptable in an affective agent when it is used for the benefit of the person being deceived, not for the agent itself. For example, deception might be necessary in search and rescue operations or for elder- or child-care.2\\.For deception to be used under any circumstance, a logical and reasonable justification must be provided by the designer, and this rationale should be certified by an external authority, such as a licensing bodyor regulatory agency.## **Further resources**•R. C. Arkin, “Robots That Need to Mislead: Biologically-inspired Machine Deception.” _IEEE Intelligent Systems _27, no. 6, pp. 60–75, 2012.•J. Shim and R. C. Arkin, “Other-Oriented Robot Deception: How Can a Robot’s Deceptive Feedback Help Humans in HRI?” _Eighth International Conference on Social Robotics (ICSR 2016)_, Kansas, MO., November 2016.•J. Shim and R. C. Arkin, “The Benefits of Robot Deception in Search and Rescue: Computational Approach for Deceptive Action Selection via Case-based Reasoning.” _2015 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR 2015)_, West Lafayette, IN, October 2015.•J. Shim and R. C. Arkin, “A Taxonomy of Robot Deception and its Benefits in HRI.” _Proceedings of IEEE Systems, Man and Cybernetics Conference, _Manchester England, October 2013.\"p.103","id":"rec9tbou2zxuzg4a5","dom_id":"item_rec9tbou2zxuzg4a5"},{"Challenges":["recL5sNl6bi1vrIzj"],"Sources":["recpXl48pJdKDhc6f"],"title":"Design recommendations for affective systems","category":"Strategies","name":"rec9UfowPkXtPUBC9","tags":[],"created_at":"2023-06-05T11:29:43.000Z","description":"## \"Recommendations1\\.A well-designed affective system will have a set of essential norms, specific to its intended cultural context of use, in its knowledge base. Research has shown that A/IS technologies can use at least five types of cues to simulate social interactions.2\\.These include: physical cues such as simulated facial expressions, psychological cues such as 1simulated humor or other emotions, use of language, use of social dynamics like taking turns, and through social roles such as acting as a tutor or medical advisor. Further examples are listed below:a.Well-designed affective systems will use language with affective content carefully and within the contemporaneous expectations of the culture. An example is small talk. Although small talk is useful for establishing a friendly rapport in many communities, some communities see people that use small talk as insincere and hypocritical. Other cultures may consider people that do not use small talk as unfriendly, uncooperative, rude, arrogant, or ignorant. Additionally, speaking with proper vocabulary, grammar, and sentence structure may contrast with the typical informal interactions between individuals. For example, the latest trend, TV show, or other media may significantly influence what is viewed as appropriate vocabulary and interaction style.b.Well-designed affective systems will recognize that the amount of personal space (proxemics) given by individuals in an important part of culturally specific human interaction. People from varying cultures maintain, often unknowingly, different spatial distances between themselves to establish smooth communication. Crossing these limits may require explicit or implicit consent, which A/IS must learn to negotiate to avoid transmitting unintended messages.c.Eye contact is an essential component for culturally sensitive social interaction. For some interactions, direct eye contact is needed but for others it is not essential and may even generate misunderstandings. It is important that A/IS be equipped to recognize the role of eye contact in the development of emotional interaction.d.Hand gestures and other non-verbal communication are very important for social interaction. Communicative gestures are culturally specific and thus should be used with caution in cross-cultural situations. The specificity of physical communication techniques must be acknowledged in the design of functional affective systems. For instance, although a “thumbs-up” sign is commonly used to indicate approval, in some countries this gesture can be considered an insult.e.Humans use facial expressions to detect emotions and facilitate communication. Facial expressions may not be universal across cultures, however, and A/IS trained with a dataset from one culture may not be readily usable in another culture. Well-developed A/IS will be able to recognize, analyze, and even display facial expressions essential for culturally specific social interaction.3\\.Engineers should consider the need for cross-cultural use of affective systems.Well-designed systems will have options innate to facilitate flexibility in cultural programming. Mechanisms to enable and disable culturally specific “add-ons” should be considered an essential part of A/IS development.## Further Resources•G. Cotton, “[Gestures to Avoid in Cross-Cultural Business: In Other Words, ‘Keep Your Fingers to Yourself!’](http://www.huffingtonpost.com/gayle-cotton/cross-cultural-gestures_b_3437653.html)” _Huffington Post, _June 13, 2013.•“[Paralanguage Across Cultures,](https://cultureplusconsulting.com/2015/04/16/paralanguage-across-cultures/)” Sydney, Australia: Culture Plus Consulting, 2016.•G. Cotton, _[Say Anything to Anyone, Say Anything to Anyone, Anywhere: 5 Keys to Successful Cross-Cultural Communication](http://www.wiley.com/WileyCDA/WileyTitle/productCd-111842042X.html)_[.](http://www.wiley.com/WileyCDA/WileyTitle/productCd-111842042X.html) Hoboken, NJ: Wiley, 2013.•D. Elmer, _[Cross-Cultural Connections: ](https://www.ivpress.com/cross-cultural-connections)__[Stepping Out and Fitting In Around the World](https://www.ivpress.com/cross-cultural-connections)_[.](https://www.ivpress.com/cross-cultural-connections) Westmont, IL: InterVarsity Press, 2002.•B. J. Fogg, [Persuasive Technology.](https://dl.acm.org/citation.cfm?id=763957) _Ubiquity_, December 2, 2002.•A. McStay, Emotional AI: The Rise of Empathic Media. London: Sage, 2018.M. Price, “[Facial Expressions—Including Fear— May Not Be as Universal as We Thought.](http://www.sciencemag.org/news/2016/10/facial-expressions-including-fear-may-not-be-universal-we-thought)” _Science, _October 17, 2016.\"p.90-91","id":"rec9ufowpkxtpubc9","dom_id":"item_rec9ufowpkxtpubc9"},{"Principles":["rec42P8U9usfYCtv9","reclPiw2VvNOSTzv5"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Fallback plan and general safety","category":"Strategies","name":"recAMa23XxDLEzMn8","tags":["governance-question"],"created_at":"2023-05-28T19:21:35.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) Technical robustness and safety“Fallback plan and general safety: \u003cU+F0FC\u003e Did you ensure that your system has a sufficient fallback plan if it encounters adversarial attacks or other unexpected situations (for example technical switching procedures or asking for a human operator before proceeding)? \u003cU+F0FC\u003e Did you consider the level of risk raised by the AI system in this specific use case? \u003cU+F0A7\u003e Did you put any process in place to measure and assess risks and safety? \u003cU+F0A7\u003e Did you provide the necessary information in case of a risk for human physical integrity? \u003cU+F0A7\u003e Did you consider an insurance policy to deal with potential damage from the AI system? \u003cU+F0A7\u003e Did you identify potential safety risks of (other) foreseeable uses of the technology, including accidental or malicious misuse? Is there a plan to mitigate or manage these risks? \u003cU+F0FC\u003e Did you assess whether there is a probable chance that the AI system may cause damage or harm to users or third parties? Did you assess the likelihood, potential damage, impacted audience and severity? \u003cU+F0A7\u003e Did you consider the liability and consumer protection rules, and take them into account? \u003cU+F0A7\u003e Did you consider the potential impact or safety risk to the environment or to animals? \u003cU+F0A7\u003e Did your risk analysis include whether security or network problems such as cybersecurity hazards could pose safety risks or damage due to unintentional behaviour of the AI system? \u003cU+F0FC\u003e Did you estimate the likely impact of a failure of your AI system when it provides wrong results, becomes unavailable, or provides societally unacceptable results (for example discrimination)? \u003cU+F0A7\u003e Did you define thresholds and did you put governance procedures in place to trigger alternative/fallback plans? \u003cU+F0A7\u003e Did you define and test fallback plans?” (High-Level Expert Group on AI, 2019, p. 27)","id":"recama23xxdlezmn8","dom_id":"item_recama23xxdlezmn8"},{"Principles":["reczVPIH1y2OMpAJH","rec42P8U9usfYCtv9","rec7n2TGrH9RHYpQj"],"Sources":["recfYC5jjPmpLfSlM"],"title":"Principle of Discriminatory non-harm for fairness, key considerations for implementation fairness","category":"Strategies","name":"recAhcg8OdusJvk43","tags":["reflection-discussion"],"created_at":"2023-05-19T11:13:37.000Z","description":"\"Implementation fairness When your project team is approaching the beta stage, you should begin to build out your plan for implementation training and support. This plan should include adequate preparation for the responsible and unbiased deployment of the AI system by its on-the-ground users. Automated decision-support systems present novel risks of bias and misapplication at the point of delivery, so special attention should be paid to preventing harmful or discriminatory outcomes at this critical juncture of the AI project lifecycle. In order to design an optimal regime of implementer training and support, you should pay special attention to the unique pitfalls of bias-in-use to which the deployment of AI technologies give rise. These can be loosely classified as decision-automation bias (more commonly just ‘automation bias’) and automation-distrust bias: - Decision-Automation Bias/The Technological Halo Effect: Users of automated decision-support systems may tend to become hampered in their critical judgment, rational agency, and situational awareness as a result of their faith in the perceived objectivity, neutrality, certainty, or superiority of the AI system. This may lead to over-reliance or errors of omission, where implementers lose the capacity to identify and respond to the faults, errors, or deficiencies, which might arise over the course of the use of an automated system, because they become complacent and overly deferent to its directions and cues. Decision-automation bias may also lead to over-compliance or errors of commission where implementers defer to the perceived infallibility of the system and thereby become unable to detect problems emerging from its use for reason of a failure to hold the results against available information. Both over-reliance and over-compliance may lead to what is known as out-of-loop syndrome where the degradation of the role of human reason and the deskilling of critical thinking hampers the user’s ability to complete the tasks that have been automated. This condition may bring about a loss of the ability to respond to system failure and may lead both to safety hazards and to dangers of discriminatory harm. To combat risks of decision-automation bias, you should operationalise strong regimes of accountability at the site of user deployment to steer human decision-agents to act on the basis of good reasons, solid inferences, and critical judgment. - Automation-Distrust Bias: At the other extreme, users of an automated decision-support system may tend to disregard its salient contributions to evidence-based reasoning either as a result of their distrust or skepticism about AI technologies in general or as a result of their over-prioritisation of the importance of prudence, common sense, and human expertise. An aversion to the non-human and amoral character of automated systems may also influence decision subjects’ hesitation to consult these technologies in high impact contexts such as healthcare, transportation, and law. In order to secure and safeguard fair implementation of AI systems by users well-trained to utilise the algorithmic outputs as tools for making evidence-based judgements, you should consider the following measures: - Training of implementers should include the conveyance of basic knowledge about the statistical and probabilistic character of machine learning and about the limitations of AI and automated decision-support technologies. This training should avoid any anthropomorphic (or human-like) portrayals of AI systems and should encourage users to view the benefits and risks of deploying these systems in terms of their role in assisting human judgment rather than replacing it. - Forethought should be given in the design of the user-system interface about human factors and about possibilities for implementation biases. The systems should be designed into processes that encourage active user judgment and situational awareness. The interface between the user and the system should be designed to make clear and accessible to the user the system’s rationale, compliance to fairness standards, and confidence level. Ideally this should happen in a ‘runtime’ manner. • Training of implementers should include a pre-emptive exploration of the cognitive and judgmental biases that may occur across deployment contexts. This should be done in a use case based manner that highlights the particular misjudgements that may occur when people weigh statistical evidence. Examples of the latter may include overconfidence in prediction based on the historical consistency of data, illusions that any clustering of data points necessarily indicates significant insights, and discounting of societal patterns that exist beyond the statistical results.Putting the principle of discriminatory non-harm into action When you are considering how to put the principle of discriminatory non-harm into action, you should come together with all the managers on the project team to map out team member involvement at each stage of the AI project pipeline from alpha through beta. Considering fairness aware design and implementation from a workflow perspective will allow you, as a team, to concretise and make explicit end-to-end paths of accountability in a clear and peer-reviewable manner. This is essential for establishing a robust accountability framework.Considering fairness aware design and implementation from such a workflow perspective will also assist you in pinpointing risks of bias or downstream discrimination and streamlining possible solutions in a proactive, pre-emptive, and anticipatory way. At each stage of the AI project pipeline (i.e. at each column of the above table), you and the relevant members of your team should carry out a collaborative self-assessment with regard to the applicable dimension of fairness. This is a three-step process:1. Step 1: Identify the fairness and bias mitigation dimensions that apply to the specific stage under consideration (for example, at the data pre-processing stage, dimensions of data fairness, design fairness, and outcome fairness may be at issue). 2. Step 2: Scrutinise how your particular AI project might pose risks or have unintended vulnerabilities in each of these areas. 3. Step 3: Take action to correct any existing problems that have been identified, strengthen areas of weakness that have possible discriminatory consequences, and take proactive bias-prevention measures in areas that have been identified to pose potential future risks.(Leslie, 2019, p. 20-23)","id":"recahcg8odusjvk43","dom_id":"item_recahcg8odusjvk43"},{"Principles":["recOHnq45Fq7YWsRO","recmzjcGKv3yNOxbl"],"Sources":["rec6r8OkE2Q2EdiM3"],"title":"Questions to consider in key stages of AI and machine learning based research, regarding aims and risks","category":"Strategies","name":"recAnT7HDpWYvdJ1V","tags":["reflection-questions","project-design"],"created_at":"2023-05-19T12:20:36.000Z","description":"“The aim, purpose, and perceived benefits of a project need to be clearly stated as a crucial first step of an ethical analysis. Emerging risks will be judged against these in a balancing exercise. Some AI scholars would claim that stating a too narrow aim would prevent the model from performing properly as the strength of AI lies in processing large amounts of data inductively (Alpaydin, 2016; Shirky, 2005). Other critical algorithmic researchers would claim that we need to disclose potential power structures in these loose spaces of operation (Bechmann \u0026 Bowker, 2019; boyd \u0026 Crawford, 2012; Crawford \u0026 Calo, 2016; Sandvig, Hamilton, Karahalios \u0026 Langbort, 2016). - Can the researcher articulate what their work aims to uncover?- How will this research contribute to the state of the art in understanding Internetrelated phenomena? - How will the research benefit society and specific stakeholders? - Will the research aims create potential risks of harm for the individuals and groups involved directly or indirectly? - If a generic aim is used how may the researchers/developers influence the field and subjects directly and indirectly involved?” (franzke et al., 2020, p. 37)","id":"recant7hdpwyvdj1v","dom_id":"item_recant7hdpwyvdj1v"},{"Principles":["rec42P8U9usfYCtv9","reczVPIH1y2OMpAJH"],"Sources":["rec6r8OkE2Q2EdiM3"],"title":"Questions to consider in key stages of AI and machine learning based research, regarding data collection","category":"Strategies","name":"recB7MF7TeUKH3chO","tags":["reflection-questions","project-design"],"created_at":"2023-05-19T12:24:51.000Z","description":"“Data collection for the model training and the research can be done either by using open data repositories or by directly recruiting participants to make a new dataset. The use of existing datasets raises issues around its intended openness, consent for reuse, and the change of context for which the data is used (Nissenbaum, 2001, 2009). Collecting new data raises issues around meaningful informed consent, whether the subjects are aware of what their data and the resulting research outputs will be used for, how this will affect them and others, and the representation of humans by a necessarily more limited model. More general questions arise about privacy as a concept to allow data subjects self-determination and control over how data about them is used. Further, respect for autonomy ensures an individual’s ability to make decisions for themselves, and to act upon them. Modern digital data collection (e.g. Application Programming Interfaces) and processing techniques have put the various concepts of privacy and autonomy under significant strain. It is therefore important for researchers to be mindful of ways to minimize the risk to research subjects’ and any violations of privacy and autonomy by third parties. Further, applying technological solutions such as encryption are often mistakenly classed as efforts to improve privacy, while they instead provide more security. Similarly, not disclosing information is called confidentiality, not necessarily privacy. **General Data Collection: **- Are the identified data points necessary, relevant and not excessive in relation to the research aim? - To what extent will data in the database identify individuals directly, or indirectly through inference? - Do the datasets contain classifiers that are particularly sensitive or even protected classes? If so, what purpose do they serve? Can data points be used as proxies to reconstruct sensitive and protected classes? Is it possible to prevent the re-construction of sensitive and protected classes? - How does the researcher protect the privacy of its users beyond security measures? For example, is data deleted after a certain amount of time? Is data that is not used for the purpose of the model deleted upon its inadvertent collection? Existing Datasets: - Is the existing dataset explicitly open for public or research, or was this dataset found without its reuse permissions being specified? - Is the use of the existing dataset restricted by legal or other means? - Could the data subjects (whether anonymized or not) in the existing dataset conceivably object to the new use of their data? Does the initial consent (informed or proxy) cover the intended re-use of the dataset? - What are the limitations in the knowledge derived from the data in modeling individual and collective behaviour in its totality? How does this limit the generalizability of the findings of the study or the applicability of the precision and/or predictors found? **New Data Collection: **- Have data subjects consented to the collection of their data with a full understanding of what is being collected, for which purposes, and with an understanding of how the data will be used by the researcher? If not, have the collection processes gone through an ethical review board? And/or how has the research team reflected on how to otherwise gain proxy consent and the potential consequences of the proxy status? - How could potential risks of harm be communicated to the research participants before entering the study? - To what extent can researchers confirm whether people understand the consequences of derivative uses of their data in AI and ML, knowing from existing literature that the concept of ‘informed’ consent may not be meaningful for the data subjects? - Has the organization decided how the privacy of data subjects is safeguarded? - Does the system collect more information than it needs - Are data subjects empowered to decide which data is collected and which inferences are made about them? - Can the data subject have access to their data? Can they choose to withdraw their data from the system?”(franzke et al., 2020, p. 38-40)","id":"recb7mf7teukh3cho","dom_id":"item_recb7mf7teukh3cho"},{"Principles":["rec42P8U9usfYCtv9","reclPiw2VvNOSTzv5","recOHnq45Fq7YWsRO"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Accuracy","category":"Strategies","name":"recBUdrJ8n1tIJLTz","tags":["governance-question"],"created_at":"2023-05-28T19:28:01.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) Technical robustness and safety“Accuracy \u003cU+F0FC\u003e Did you assess what level and definition of accuracy would be required in the context of the AI system and use case? \u003cU+F0A7\u003e Did you assess how accuracy is measured and assured? \u003cU+F0A7\u003e Did you put in place measures to ensure that the data used is comprehensive and up to date? \u003cU+F0A7\u003e Did you put in place measures in place to assess whether there is a need for additional data, for example to improve accuracy or to eliminate bias? \u003cU+F0FC\u003e Did you verify what harm would be caused if the AI system makes inaccurate predictions? \u003cU+F0FC\u003e Did you put in place ways to measure whether your system is making an unacceptable amount of inaccurate predictions? \u003cU+F0FC\u003e Did you put in place a series of steps to increase the system's accuracy?” (High-Level Expert Group on AI, 2019, p. 27)","id":"recbudrj8n1tijltz","dom_id":"item_recbudrj8n1tijltz"},{"Challenges":["rec9qgAZS5U8GL137"],"Sources":["recpXl48pJdKDhc6f"],"title":"Establish corporate ethics review boards","category":"Strategies","name":"recCN9eqkgKT3KjCB","tags":[],"created_at":"2023-06-05T10:38:42.000Z","description":"## RecommendationsOrganizations should clarify the relationship between professional ethics and appliedA/IS ethics by helping or enabling designers, engineers, and other company representatives to discern the differences between these kinds of ethics and where they complement each other.Corporate ethical review boards, or comparable mechanisms, should be formed to addressethical and behavioral concerns in relation toA/IS design, development and deployment. Such boards should seek an appropriately diverse composition and use relevant criteria, including both research ethics and product ethics, at the appropriate levels of advancement of research and development. These boards should examine justifications of research or industrial projects.## Further Resources• HH van der Kloot Meijberg and RHJ ter Meulen, “[Developing Standards for Institutional](https://jme.bmj.com/content/27/suppl_1/i36.full) [Ethics Committees: Lessons from the Netherlands,](https://jme.bmj.com/content/27/suppl_1/i36.full)” _Journal of Medical Ethics_ 27 i36-i40, 2001.\"p.130","id":"reccn9eqkgkt3kjcb","dom_id":"item_reccn9eqkgkt3kjcb"},{"Principles":["recjViPnz3atRIOpD"],"Sources":["recQzldmBLByP78Uu"],"title":"Questions to consider in SoTL research professional learning","category":"Strategies","name":"recDOGXIsqtXrCuSz","tags":["reflection-questions","education-research"],"created_at":"2023-05-19T13:04:12.000Z","description":"• Whom can you talk to about the above questions? How can you create occasions for discussion and reflection about them with colleagues? • What are you learning from your project that can inform future practice related to ethical issues in the scholarship of teaching and learning?(Fedoruk, 2017, p. 3)","id":"recdogxisqtxrcusz","dom_id":"item_recdogxisqtxrcusz"},{"Principles":["recxcFmvPG5wrCqpO","recQEiU22Qy1E0YuA","recU6u0AZbcNj1ik9","recGb4WZzr34RXKr0"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Communication","category":"Strategies","name":"recEHwRaLD44KPk8v","tags":["governance-question"],"created_at":"2023-05-28T19:35:32.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Communication: \u003cU+F0FC\u003e Did you communicate to (end-)users – through a disclaimer or any other means – that they are interacting with an AI system and not with another human? Did you label your AI system as such? \u003cU+F0FC\u003e Did you establish mechanisms to inform (end-)users on the reasons and criteria behind the AI system’s outcomes? \u003cU+F0A7\u003e Did you communicate this clearly and intelligibly to the intended audience? \u003cU+F0A7\u003e Did you establish processes that consider users’ feedback and use this to adapt the system? \u003cU+F0A7\u003e Did you communicate around potential or perceived risks, such as bias? \u003cU+F0A7\u003e Depending on the use case, did you consider communication and transparency towards other audiences, third parties or the general public? \u003cU+F0FC\u003e Did you clarify the purpose of the AI system and who or what may benefit from the product/service? \u003cU+F0A7\u003e Did you specify usage scenarios for the product and clearly communicate these to ensure that it is understandable and appropriate for the intended audience? \u003cU+F0A7\u003e Depending on the use case, did you think about human psychology and potential limitations, such as risk of confusion, confirmation bias or cognitive fatigue? \u003cU+F0FC\u003e Did you clearly communicate characteristics, limitations and potential shortcomings of the AI system? \u003cU+F0A7\u003e In case of the system's development: to whoever is deploying it into a product or service? \u003cU+F0A7\u003e In case of the system's deployment: to the (end-)user or consumer?” (High-Level Expert Group on AI, 2019, p. 29)","id":"recehwrald44kpk8v","dom_id":"item_recehwrald44kpk8v"},{"Principles":["recKdujFoPJr4ZAhZ","recQ9DIFEsOEkCx3O"],"Sources":["recQzldmBLByP78Uu"],"title":"Key discussion points for planning SoTL research regarding power relationships","category":"Strategies","name":"recELo3u36oZuujxN","tags":["reflection-questions","education-research"],"created_at":"2023-05-19T13:11:32.000Z","description":"## Conflicts of Interestand Power Relationships“**Key Principle:** When you are acting as both instructor and researcher, mitigate undue influence, coercion, or power imbalances by basing decisions first and foremost on your role as an Instructor (which sometimes may be at odds with your goals as a researcher) and by being sensitive to the inherent power differential between instructor and student. **Strategies for Ethical Practice **- As you begin to design your study, describe your research plans to colleagues and former students, and invite them to help you identify blind spots you might have in terms of influence, coercion, or power imbalances, so that you can address them in your planning. For a list of discussion questions to facilitate the identification of blind spots, see Table 2 – Questions to Consider When Planning SoTL Research on page 16 of this Guide. - Provide student participants with information about contacting the university’s REB with ethical questions or concerns. **Definitions:**- Third party Someone who does not have grading authority or perceived power over potential participants who can act as an intermediary or buffer between you and the students. This person will be the only one who knows which students are participating in the research (you will not), and students will be informed of this person’s role before deciding whether to participate. With your direction, the third party may do any or all the following: introduce/present the study to potential participants, collect and store consent forms, field participants’ questions and/ or de-identify then direct their questions your way and convey your response(s) to them, provide prospective participants with updates/ongoing information about the study, conduct interviews and/or lead focus groups with student participants, etc. - Identifying information Information that can or may reveal which students are participating in the research. This information might include first and/ or last names, student ID numbers, physical descriptions, or other references to or about people that might reasonably identify them as research participants.” (Fedoruk, 2017, p. 5)“If possible: - Use a third party to assist with participant recruitment, information provision, and data generation and analysis. This approach protects participating and non-participating students’ identities by ensuring that they can become informed about the study, raise questions, participate in, and/or withdraw from the study without revealing their identities as research participants (or not) to you. - Collect data (e.g., conduct interviews or focus groups, distribute surveys) after final grades have been submitted and released to the students, and after the appeal deadline has passed. - Analyze student work after identifying information has been removed. - Conduct the research using students in a school or classroom other than your own.”(Fedoruk, 2017, p. 6)## **Consent process**### Key PrincipleEnsure that each student’s decision to participate in your research (or not) is voluntary, and that their privacy is protected when offering or declining consent.### Strategies for Ethical Practice•Use a third party to facilitate consent/withdrawal processes to protect students’ privacy.•Clearly communicate to students that there are no repercussions for their refusal to consent.•When conducting surveys, use web-based survey tools (e.g., Qualtrics, etc.) that allow students to participate anonymously. Anonymous online participation eliminates personal identifiers and peer pressure, and allows students who are not interested in participating to decline privately.•When collecting consent forms from student participants in class, design the forms so that all students must sign and hand in the paper form in order to prevent knowledge of who is and is not participating (e.g., explain that everyone signs the consent form, but those who do NOT want to participate can then draw two lines through their signatures).•When offering incentives, keep them to a minimum to avoid undue influence (e.g., $25 bookstore gift card, a draw for a $50 gift card), and provide students with clear timelines during which they may opt-in or out of participation in the study. If the incentive includes a small percentage of their grade (1 to 5%), give students not participating in the study an opportunity to earn the same incentive through an alternate option, such as an additional assignment equivalent in time and effort.If you are collecting and analyzing your own data, where applicable, inform students:•About where or to whom they might direct questions about the study, before, during, and after the study.•That you will not know who has agreed to participate until students’ grades are submitted and released, and the appeal deadline has passed.•That you will not analyze data until after grades are submitted and released to students and the appeal deadline has passed.(Fedoruk, 2017, p. 7)### Key PrincipleEnsure that students’ decisions to participate in your research (or not) are informed by telling them about the purpose, benefits, risks, and consequences of your research before asking for their consent.### Strategies for Ethical Practice•Describe and discuss (or have a third party describe and discuss) the research with students before seeking their consent to participate.•Include clear and transparent descriptions of the project on consent forms (even if the project has already been described in detail to participants).•Commit to students at the onset that results will be shared with them upon completion.•When conducting focus groups, ensure that the consent process asks that each member of the focus group respect the confidentiality of other members, but that you cannot guarantee confidentiality. Because of this, in REB applications, it may be advisable to attend to why group data (rather than individual interview data) is preferable in your research design.When video or audio recording, because this method of collecting data can inadvertently capture material produced by students who have not consented to participate in the research process, it is advisable that researchers articulate to their institutional REBs why this data collection method (as opposed to others) is essential to the research design. If using video, give consenting students the option to choose whether the research team will only view their presence on the video (a), or (b) may be viewed by the research team and shared during the dissemination of research findings, and indicating their choice on the informed consent form.When possible, include a brief explanation of the research on your course outline or syllabus (if advisable within your campus context). For example: “Please be advised that within this course, you will have the opportunity to volunteer as a research participant in a study that examines the reading comprehension of second year, undergraduate students as they progress through Language and Literature 201. Details will be provided at the start of the course.### Key PrincipleMake sure students have the autonomy to freely and privately choose to participate, refuse to participate,or withdraw from participation at any time during or after the research (provided that it has not already been disseminated).### Strategies for Ethical Practice•Provide students with the option to withdraw from the research simply (e.g., by sending an email) and at any time prior to dissemination. Indicate what will happen to their data after they have withdrawn from the research (e.g., wherever possible, it will be extracted and destroyed).In cases where the research timeline needs to be extended, whenever possible, seek students’ consent regarding these extensions. ## **Fairness and Equity inResearch Participation **###   Key PrincipleAs much as possible, within the research project’s goals, be inclusive, fair, and equitable whenselecting participants.### Strategies for Ethical Practice•Have a clear rationale for participant inclusion and exclusion criteria connected to your project’s goals and specific research question.For instance, if you are not including seniors, or men, or non-majors, or non-native English speakers, explain how this exclusion is relevant to your specific project (e.g., “Because this research is specifically focused on the female experience of power relationships, and because we are linkingto gender theory, only women-identified people will be included for participation”).•Consider your assumptions about potential participants in your study. For example, do not assume that students with physical disabilities should be excluded from a study that uses physical activity games to assess comprehension of biomechanics principles. Invite colleagues and/or former students to help you identify assumptions that you might be making about participants, to ensure that your inclusion/ exclusion criteria do not suffer from blind spots.•If there is a language barrier between you (or your third party) and participant(s), involve an intermediary who is competent in both languages to assist with communication between you (or your third party) and participant(s).### Key PrincipleEnsure that the benefits of participating in your study are equitably distributed among participants.### Strategies for Ethical Practice•Discuss potential research benefits with students at the onset of the study.•Ensure an “equitable distribution of research benefits” (TCPS2, 2014, p. 55) by avoiding circumstances in which the conditions of some participants are significantly more beneficial than the conditions for other participants or nonparticipants. If you are using a comparison group or a differential experience for non-participants, you should closely monitor the impact of an intervention to guard against one group (e.g., research participants, or non-participants) experiencing significantly more benefits over the other group(s). You are responsible for gauging if the discrepancy between groups becomes unethical and could have negative implications for the other group(s), in which case contingencies and modifications to a study may be needed.•For instance, an instructor teaching two sections of the same course might use a “flipped classroom” model for one section and leave the other unchanged (as a control group). If students in the flipped classroom are showing significant gains, the students in the control group may be disadvantaged, and the instructor may decide to flip both sections to mitigate an unethical disparity between groups. This type of contingency should be included in the research design, and student success should be prioritized.## Research Results### Key PrincipleUpon completing the study, make the results available, accessible, and understandable to all participants.### Strategies for Ethical Practice•Inform students that you will share the outcomes of your research and in what format (e.g., a one-page brief, a paper, etc.) as soon as they become available. Invite students to provide contact information during the consent process to indicate how to reach them with research outcomes (e.g., an email address to which the outcomes can be sent).When the outcomes are available, provide participants with the citation of the journal in which it is published or copies of the publication and a written summary of results written in clear, understandable language.## **Privacy and Confidentiality**### Key PrincipleProtect the participants’ information and the integrity of the research project.### Strategies for Ethical Practice•Discuss the practical implications of confidentiality with all members of your research team, and where relevant, have all members sign a confidentiality agreement.•Do not share any specific identifying information about the data collected with anyone other than your research team.•If information sharing with government agencies, community research partners, research sponsors, or regulatory agencies may occur during the study, describe and include this possibility as part of the information provided to students before they decide whether to participate.•If confidentiality is unexpectedly breached, let participants know immediately and advise them of the steps you have taken to address the situation and prevent further breaches.If possible:•De-identifyStudent data, or have a third party de-identify the data for you.•When groups are small (e.g., fewer than 10 or 15 members of a particular type), aggregate or combine data and remove identifying information to diminish the possibility that the responses of specific identifiable groups will be deduced.In an online presentation about REBs, Babcock and Henry (2014) reproduce the Risk Matrix below. The matrix demonstrates a two-fold relationship between disclosure and harm reduction. Figure 2 demonstrates that privacy protection and ethics are warranted in cases where the data consists of identifiable, confidential information, where therisk of disclosure and harm are most pronounced. The light orange cells in the table indicate thatrisk of disclosure and harm still exists in cases in which the data has been de-identified (here,referred to as anonymized), thus requiringprivacy protection and REB review.### Key PrincipleDuring data collection and analysis, useappropriate safeguards and security measuresto protect participant information and data.### Strategies for Ethical Practice•Use encryption software and/or passwordprotected digital documents, folders, and/ or systems to limit access to data and protect participant confidentiality.•Store all hard copies of participant-identifying data, including signed consent forms, in a locked cabinet with a key and protect that key.•Keep an up-to-date list of all persons with access to participant information, ensuring they have signed a non-disclosure or confidentiality agreement (e.g., You can find an example of such an agreement at the University of Calgary CFREB website: https://ucalgary.ca/research/ researchers/ethics-compliance/cfreb).•If appropriate, destroy all identifying participant information and identifying data upon completion of the research project (e.g., by shredding hardcopy materials and/or by reformatting/wiping digital storage devices). It may be inappropriate to destroy data when, for example, the data consists of course material (e.g., assignments, papers, exams) or evaluative material (e.g., course and/or teaching evaluations) that may be otherwise used and retained for other purposes (e.g., course redesign).Babcock and Henry (2014) also outline a “data hierarchy” that helps researchers attend to the disclosure risks that can arise concerning confidentiality and human participants. This fourtiered represents risk as least to most pronounced by information type. Specifically, the anonymous information is represented as lowest in risk in terms of disclosing confidentiality, and identifiable information is positioned as the kind of information with the highest risk of disclosure.## Overarching questions  **Conflicts of Interest and Power ****Relationships** Mitigate undue influence, coercion, or power imbalances by: a. basing decisions first and foremost on an instructor’s goals (which sometimes may be at odds with research goals), and b. being sensitive to the inherent power differential between instructor and student. •“Could any part of the research design interfere with the effectiveness and/or credibility an instructor and/or with students’ interests and/or ability to learn?”•“Are there ways in which participating in this research - or not - might be something that students feel like they had to do? If so, why?”•“Could a third-party help with the consent and data collection process to mitigate powerdifferentials?”   **Consent Processes** Ensure that students’ decisions to participate in the research (or not) is informed and voluntary by: a. telling them about the purpose, benefits, risks, and consequences of the research before asking for their consent, and b. making sure they have the autonomy to freely and privately choose to participate, refuse to participate, or withdraw from participation at any time during the research. •“What else would you want to know before making a decision about participating in this research?”•“In what ways might students feel compelled to participate or compromised in their ability to withdraw from the study without consequence?”    **Fairness and Equity ****in Research ****Participation ** Within the research project’s goals, be inclusive, fair, and equitable when selecting participants by:a. recognizing and respecting the vulnerability of individuals or groups, and b. making the results available, accessible, and understandable toall participants upon completionof the study. •• “Are there any individuals or groups that this research might directly or indirectly exclude?”“How can I be sure that the results of this study can be accessible toall participants?”   **Privacy and Confidentiality** Protect the participants’ information and the integrity of the research project by: a. meeting confidentiality obligations in the research,b. implementing appropriate institutional safeguards and security measures to protect participant information and data, and c. if the research involves identifiable secondary use of data (e.g., former students’ work or other identifiable materials collected before seeking REB approval), seeking students’ informed consent and applying the above principles of ethical practice to this secondary use of data.-  “Are there ways in which this research design might, in any way, compromise participants’ confidentiality?”- “Are there adequate safeguardsto protect participants’ information and data?”- “Have I obtained informed consent from all students whose data I am using in this study, regardless of when said data was collected?” All drawn from Fedoruk (2017)","id":"recelo3u36ozuujxn","dom_id":"item_recelo3u36ozuujxn"},{"Challenges":["recAoyMGCCLhaSqEb"],"Principles":["recPg7Ov0priGGtLm"],"Sources":["recpXl48pJdKDhc6f"],"title":"AI assistants should be created to help users understand and manage how their data is being used","category":"Strategies","name":"recG86YsfXnKY9kNc","tags":[],"created_at":"2023-06-05T09:36:00.000Z","description":"\"Algorithmic agents should be developed for individuals to curate and share their personal data. Specifically:•For purposes of privacy, a person must be able to set up complex permissions that reflect a variety of wishes.•The agent should help a person foresee and mitigate potential ethical implications of specific machine learning data exchanges.•A user should be able to override his/her personal agents should he/she decide that the service offered is worth the conditions imposed.•An agent should enable machine-to-machine processing of information to compare, recommend, and assess offers and services.•Institutional systems should ensure support for and respect the ability of individuals to bring their own agent to the relationship without constraints that would make some guardians inherently incompatible or subject to censorship.•Vulnerable parts of the population will need protection in the process of granting access.\"p.111 IEEE report","id":"recg86ysfxnky9knc","dom_id":"item_recg86ysfxnky9knc"},{"Challenges":["recvWM2glArsVhaye"],"Principles":["recZToVrPeFlFq0Aw","recmzjcGKv3yNOxbl","recOHnq45Fq7YWsRO"],"Sources":["recfYC5jjPmpLfSlM"],"title":"Principle of Sustainability, key considerations","category":"Strategies","name":"recGS9OVQ7qAjvIYE","tags":["reflection-discussion","impact-assessment"],"created_at":"2023-05-19T11:43:19.000Z","description":"\"SustainabilityDesigners and users of AI systems should remain aware that these technologies may have transformative and long-term effects on individuals and society. In order to ensure that the deployment of your AI system remains sustainable and supports the sustainability of the communities it will affect, you and your team should proceed with a continuous sensitivity to the real-world impacts that your system will have.Stakeholder Impact AssessmentYou and your project team should come together to evaluate the social impact and sustainability of your AI project through a Stakeholder Impact Assessment (SIA), whether the AI project is being used to deliver a public service or in a back-office administrative capacity. When we refer to ‘stakeholders’ we are referring primarily to affected individual persons, but the term may also extend to groups and organisations in the sense that individual members of these collectives may also be impacted as such by the design and deployment of AI systems. Due consideration to stakeholders should be given at both of these levels.The purpose of carrying out an SIA is multidimensional. SIAs can serve several purposes, some of which include:(1)Help to build public confidence that the design and deployment of the AI system by the public sector agency has been done responsibly(2)Facilitate and strengthen your accountability framework(3)Bring to light unseen risks that threaten to affect individuals and the public good (4)Underwrite well-informed decision-making and transparent innovation practices(5)Demonstrate forethought and due diligence not only within your organisation but also to the wider publicYour team should convene to evaluate the social impact and sustainability of your AI project through the SIA at three critical points in the project delivery lifecycle:1\\.Alpha Phase (Problem Formulation): Carry out an initial Stakeholder Impact Assessment (SIA) to determine the ethical permissibility of the project. Refer to the SUM Values as a starting point for the considerations of the possible effects of your project on individual wellbeing and public welfare. In cases where you conclude that your AI project will have significant ethical and social impacts, you should open your initial SIA to the public so that their views can be properly considered. This will bolster the inclusion of a diversity of voices and opinions into the design and development process through the participation of a more representative range of stakeholders. You should also consider consulting with internal organisational stakeholders, whose input will likewise strengthen the openness, inclusivity, and diversity of your project.2\\.From Alpha to Beta (Pre-Implementation): Once your model has been trained, tested, and validated, you and your team should revisit your initial SIA to confirm that the AI system to be implemented is still in line with the evaluations and conclusions of your original assessment. This check-in should be logged on the pre-implementation section of the SIA with any applicable changes added and discussed. Before the launch of the system, this SIA should be made publicly available. At this point you must also set a timeframe for re- assessment once the system is in operation as well as a public consultation which predates and provides input for that re-assessment. Timeframes for these re-assessments should be decided by your team on a case-by-case basis but should be proportional to the scale of the potential impact of the system on the individuals and communities it will affect.3\\.Beta Phase (Re-Assessment): After your AI system has gone live, your team should intermittently revisit and re-evaluate your SIA. These check-ins should be logged on the re- assessment section of the SIA with any applicable changes added and discussed. Re- assessment should focus both on evaluating the existing SIA against real world impacts and on considering how to mitigate the unintended consequences that may have ensued in the wake of the deployment of the system. Further public consultation for input at the beta stage should be undertaken before the re-assessment so that stakeholder input can be included in re-assessment deliberations.You should keep in mind that, in its specific focus on social and ethical sustainability, your Stakeholder Impact Assessment constitutes just one part of the governance platform for your AI project and should be a complement to your accountability framework and other auditing and activity-monitoring documentation.Your SIA should be broken down into four sections of questions and responses. In the 1st section, there should be general questions about the possible big-picture social and ethical impacts of the use of the AI system you plan to build. In the 2nd section, your team should collaboratively formulate relevant sector-specific and use case-specific questions about the impact of the AI system on affected stakeholders. The 3rd section should provide answers to the additional questions relevant to pre-implementation evaluation. The 4th section should provide the opportunity for members of your team to reassess the system in light of its real-world impacts, public input, and possible unintended consequences.## Stakeholder impact assessment**Stakeholder Impact Assessment for (Project Name)**   **1. Alpha Phase (Problem Formulation) General Questions**- Completed on this Date:1. Identifying Affected Stakeholders: Who are the stakeholders that this AI project is most likely to affect? What groups of these stakeholders are most vulnerable? How might the project negatively impact them?2. Goal-Setting and Objective-Mapping: How are you defining the outcome (the target variable) that the system is optimising for? Is this a fair, reasonable, and widely acceptable definition?3. Does the target variable (or its measurable proxy) reflect a reasonable and justifiable translation of the project’s objective into the statistical frame? Is this translation justifiable given the general purpose of the project and the potential impacts that the outcomes of its implementation will have on the communities involved?4. Possible Impacts on the Individual: How might the implementation of your AI system impact the abilities of affected stakeholders to make free, independent, and well-informed decisions about their lives? How might it enhance or diminish their autonomy? How might it affect their capacities to flourish and to fully develop themselves? How might it do harm to their physical or mental integrity? Have risks to individual health and safety been adequately considered and addressed? How might it infringe on their privacy rights, both on the data processing end of designing the system and on the implementation end of deploying it?5. Possible Impacts on Society and Interpersonal Relationships How might the implementation of your AI system adversely affect each stakeholder’s fair and equal treatment under the law? Are there any aspects of the project that expose vulnerable communities to possible discriminatory harm? How might the use of your system affect the integrity of interpersonal dialogue, meaningful human connection, and social cohesion? Have the values of civic participation, inclusion, and diversity been adequately considered in articulating the purpose and setting the goals of the project? If not, how might these values be incorporated into your project design? Does the project aim to advance the interests and well-being of as many affected individuals as possible? Might any disparate socioeconomic impacts result from its deployment? Have you sufficiently considered the wider impacts of the system on future generations and on the planet as a whole?**2. Alpha Phase (Problem Formulation) Sector-Specific and Use Case-Specific Questions**Completed on this Date: In this section you should consider the sector-specific and use case-specific issues surrounding the social and ethical impacts of your AI project on affected stakeholders. Compile a list of the questions and concerns you anticipate. State how your team is attempting to address these questions and concerns.   **3. From Alpha to Beta (Pre-Implementation)**Completed on this Date:After reviewing the results of your initial SIA, answer the following questions:- Are the trained model’s actual objective, design, and testing results still in line with the evaluations and conclusions contained in your original assessment? If not, how does your assessment now differ?- Have any other areas of concern arisen with regard to possibly harmful social or ethical impacts as you have moved from the alpha to the beta phase?- You must also set a reasonable timeframe for public consultation and beta phase re-assessment: Dates of Public Consultation on Beta-Phase Impacts: Date of Planned Beta Phase Re-Assessment:   **4. Beta Phase (Re-Assessment)**Completed on this Date:Once you have reviewed the most recent version of your SIA and the results of the public consultation, answer the following questions:- How does the content of the existing SIA compare with the real-world impacts of the AI system as measured by available evidence of performance, monitoring data, and input from implementers and the public?- What steps can be taken to rectify any problems or issues that have emerged?- Have any unintended harmful consequences ensued in the wake of the deployment of the system? If so, how might these negative impacts be mitigated and redressed?- Have the maintenance processes for your AI model adequately taken into account the possibility of distributional shifts in the underlying population? Has the model been properly retuned and retrained to accommodate changes in the environment?Dates of Public Consultation on Beta-Phase Impacts: Date of Next Planned Beta Phase Re-Assessment:\"(Leslie, 2019, p.23-24)","id":"recgs9ovq7qajviye","dom_id":"item_recgs9ovq7qajviye"},{"Principles":["reckb3cgfeDh1EeUP"],"Sources":["recnCULdYQ36cpZR7"],"title":"Consider tensions and absolute rights","category":"Strategies","name":"recHEUOqkzQTNsAMw","tags":[],"created_at":"2023-05-28T18:58:31.000Z","description":"“Tensions may arise between the above principles, for which there is no fixed solution. In line with the EU fundamental commitment to democratic engagement, due process and open political participation, methods of accountable deliberation to deal with such tensions should be established. For instance, in various application domains, the principle of prevention of harm and the principle of human autonomy may be in conflict. Consider as an example the use of AI systems for ‘predictive policing’, which may help to reduce crime, but in ways that entail surveillance activities that impinge on individual liberty and privacy. Furthermore, AI systems’ overall benefits should substantially exceed the foreseeable individual risks. While the above principles certainly offer guidance towards solutions, they remain abstract ethical prescriptions. AI practitioners can hence not be expected to find the right solution based on the principles above, yet they should approach ethical dilemmas and trade-offs via reasoned, evidence-based reflection rather than intuition or random discretion. There may be situations, however, where no ethically acceptable trade-offs can be identified. Certain fundamental rights and correlated principles are absolute and cannot be subject to a balancing exercise (e.g. human dignity).” (\\[High-Level Expert Group on AI, 2019, p. 13]\\(zotero://select/groups/4907410/items/XPCD8D3T)) (\\[pdf]\\(zotero://open-pdf/groups/4907410/items/CPFPH28M?page=15\u0026annotation=5CFBE37J))","id":"recheuoqkzqtnsamw","dom_id":"item_recheuoqkzqtnsamw"},{"Challenges":["recIUf0R50ogibwe6"],"Sources":["recpXl48pJdKDhc6f"],"title":"Affective systems should be configurable to cultural context","category":"Strategies","name":"recHsUV9w4HKBA1w1","tags":[],"created_at":"2023-06-05T11:34:16.000Z","description":"## \"RecommendationsAssuming that well-designed affective systems have a minimum subset of configurable norms incorporated in their knowledge base:1\\.Affective systems should have capabilities to identify differences between the values they are designed with and the differing values of those with whom the systems are interacting.2\\.Where appropriate, affective systems will adapt accordingly over time to better fit the norms of their users. As societal values change, there needs to be a means to detect and accommodate such cultural change in affective systems.3\\.Those actions undertaken by an affective system that are most likely to generate an emotional response should be designed to be easily changed in appropriate ways by the user without being easily hacked by actors with malicious intentions. Similar to how software today externalizes the language and vocabulary to be easily changeable based on location, affective systems should externalize someof the core aspects of their actions.## Further Resources- J. Bielby, “Comparative Philosophies in Intercultural Information Ethics.” _Confluence: Online Journal of World Philosophies _2, no. 1, pp. 233–253, 2015.- M. Velasquez, C. Andre, T. Shanks, and M. J. Meyer. “[Ethical Relativism.](https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/ethical-relativism/)” Markkula Center for Applied Ethics, Santa Clara, CA: Santa Clara University, August 1, 1992.- Culture reflects the moral values and ethical norms governing how people should behave and interact with others. “[Ethics, an Overview](https://courses.lumenlearning.com/boundless-management/chapter/ethics-an-overview/).” Boundless Management.- T. Donaldson, “[Values in Tension: Ethics Away from Home Away from Home](https://hbr.org/1996/09/values-in-tension-ethics-away-from-home).” _Harvard Business Review. _September– October 1996.","id":"rechsuv9w4hkba1w1","dom_id":"item_rechsuv9w4hkba1w1"},{"Principles":["recKdujFoPJr4ZAhZ","recQ9DIFEsOEkCx3O"],"Sources":["recQzldmBLByP78Uu"],"title":"Key discussion points for planning SoTL research regarding secondary analysis of data","category":"Strategies","name":"recI5BV6v0BCK03VN","tags":["reflection-questions","education-research"],"created_at":"2023-05-19T13:25:25.000Z","description":"Including the principles (which are drawn from the Canadian model)## TCPS2 | “Reasons to Conduct Secondary Analyses of Data”“Reasons to conduct secondary analyses of data include: avoidance of duplication in primary collection and the associated reduction of burdens on participants; corroboration or criticism of the conclusions of the original project; comparison of change in a research sample over time; applicationof new tests of hypotheses that were not availableat the time of original data collection; and confirmation that the data are authentic. Privacy concerns and questions about the need to seek consent arrive, however, when information provided for secondary use in research can be linked to individuals, and when the possibility exists that individuals can be identified in published reports,or through data linkage” (TCPS2, Chapter 5, D. Consent and Secondary Use of Identifiable Information for Research Purposes).## TCPS2 | Article 5.5A“If a researcher satisfies all the conditions in Article 5.5A (a) to (f), the REB may approve the research without requiring consent from the individuals to whom the information relates.**a.**identifiable information is essential to theresearch;**b.**the use of identifiable information without the participants’ consent is unlikely to adverselyaffect the welfare of individuals to whom the information relates;**c.**the researchers will take appropriate measuresto protect the privacy of individuals, and to safeguard the identifiable information;**d.**the researchers will comply with any knownpreferences previously expressed by individuals about any use of their information;**e.**it is impossible or impracticable to seek consentfrom individuals to whom the information relates; and**f.**the researchers have obtained any other necessary permission for secondary use of information for research purposes” (TCPS2, Chapter 5, D. Consent and Secondary Use of Identifiable Information for Research Purposes).### Secondary analyses of data Secondary analyses of data, also referred to herein as secondary use of data consists of information originally collected for other purposes. Such information might consist of student work, information obtained for program evaluation, school records, or other identifiable materials collected for educational or administrative purposes.****## TCPS2 | Article 5.5B“Researchers shall seek REB review, but are not required to seek participant consent, for research that relies exclusively on the secondary use of non-identifiable information” (TCPS2, Chapter 5, D. Consent and Secondary Use of Identifiable Information for Research Purposes).### Key PrincipleApply the above principles of privacy, and seekREB review even if your research involves data initially collected for other reasons (e.g.,“secondary use of data”).### Strategies for Ethical Practice•When possible, use anonymous data.•If generating anonymous data conflicts with your research question and design, when possible, use data that has been de-identified.•Although seeking participant consent is not required for non-identifiable data (Article 5.5B above), it is still good practice to seek students’ consent to use their data again.•If the data you want to use is identifiable andthe REB requires that you seek students’consent anyway, apply the principles and strategies in “Consent Processes,” startingon page 6 of this Guide.•If you are emailing former students to seek consent to use their previously generated information as data or for additional information that may serve as data, be sensitive to general overuse of email and full inboxes.•Use a third party to collect consent for research participants who are not your current students. Although this is not required, using a third party is a good practice if these students want to enroll in a future course you teach or ask you to serve on an advisory committee or write a reference letter for them, etc.•If you are contacting former students to seek their consent to use their previously generated information as data or for additional information that may serve as data (e.g., “secondary use of data”), be prepared to explain to the REB:**› **why you want to contact these former students,**› **how the potential benefits of this follow-up or additional data outweigh any drawbacks of contacting them,**› **who will be contacting the individuals and the nature of their relationship with those students (e.g., a third party), and** **how they will be contacted (Article 5.6). All drawn from Fedoruk (2017)","id":"reci5bv6v0bck03vn","dom_id":"item_reci5bv6v0bck03vn"},{"Challenges":["recELObWGfkhXzFG2"],"Sources":["recpXl48pJdKDhc6f"],"title":"Identify priorities and develop standards for the governance of AI research","category":"Strategies","name":"recI7WdfNhrQxRt7F","tags":[],"created_at":"2023-06-05T10:23:45.000Z","description":"## RecommendationsThe IEEE and other standards-setting bodies should draw upon existing standards, empirical research, and expertise to identify prioritiesand develop standards for the governance ofA/IS research and partner with relevant national agencies, and international organizations,when possible.## Further Resources•S. R. Jordan, “The Innovation Imperative.” _Public Management Review _16, no. 1,pp. 67–89, 2014.•B. Schneiderman, “[The Dangers of Faulty, Biased, or Malicious Algorithms Requires Independent Oversight.](http://www.pnas.org/content/113/48/13538.long)” _Proceedings of the National Academy of Sciences of the United States of America _113, no. 48, 13538–13540, 2016.•J. Metcalf and K. Crawford, “[Where are Human Subjects in Big Data Research? The Emerging Ethics Divide](http://papers.ssrn.com/abstract%3D2779647).” _Big Data \u0026 Society,_ May 14, 2016._ _[Online]. Available: SSRN: [https://ssrn. com/abstract=2779647](https://ssrn.com/abstract=2779647). [Accessed Nov. 1, 2018].•R. Calo, “[Consumer Subject Review Boards: A Thought Experiment](https://www.stanfordlawreview.org/online/privacy-and-big-data-consumer-subject-review-boards/).” _Stanford Law Review Online _66 97, Sept. 2013.","id":"reci7wdfnhrqxrt7f","dom_id":"item_reci7wdfnhrqxrt7f"},{"Principles":["recmzjcGKv3yNOxbl"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Social impact","category":"Strategies","name":"recJMrMEZnz9rYlnD","tags":["governance-question"],"created_at":"2023-05-28T19:45:48.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Societal and environmental well-being “Social impact: \u003cU+F0FC\u003e In case the AI system interacts directly with humans:\u003cU+F0A7\u003e Did you assess whether the AI system encourages humans to develop attachment and empathy towards the system? \u003cU+F0A7\u003e Did you ensure that the AI system clearly signals that its social interaction is simulated and that it” (High-Level Expert Group on AI, 2019, p. 30)","id":"recjmrmeznz9rylnd","dom_id":"item_recjmrmeznz9rylnd"},{"Principles":["rec42P8U9usfYCtv9","recSqx6wklVpDzx3s","recPg7Ov0priGGtLm"],"Sources":["recQiVQ7CTC72xp6O"],"title":"Questions to consider regarding re-identification and issues of justice in IP","category":"Strategies","name":"recJQF6QfQGlcSzDm","tags":["reflection-questions"],"created_at":"2023-05-19T08:22:58.000Z","description":"“How are findings presented? § What immediate or future risk might occur by using exact-quoted material in published reports? (For example, while a participant might not think his or her information is sensitive now, this might change in five years. What protections might be put in place to anticipate changing perceptions?22) § Are individuals adequately protected in pre-publication reports, such as workshops, conferences, or informal meetings?23 § Could materials be restricted because of copyright? (For example, many countries have strong restrictions on using screenshots or images taken from the web without permission. Certain sites have restrictions in their terms of service. Whereas there may be allowances for the scholarly use of copyrighted materials without permission, such as the U.S. doctrine of fair use, this is not a guarantee of protection against copyright infringement.)” (Markham and Buchanan, 2012, p. 10)","id":"recjqf6qfqglcszdm","dom_id":"item_recjqf6qfqglcszdm"},{"Challenges":["recX6r1O4jcsp0nIM"],"Principles":["recKdujFoPJr4ZAhZ","recxcFmvPG5wrCqpO","rec42P8U9usfYCtv9","reczVPIH1y2OMpAJH"],"Sources":["recnCULdYQ36cpZR7","recpXl48pJdKDhc6f"],"title":"Considerations in assessing trustworthy AI - Minimising and reporting negative impacts","category":"Strategies","name":"recJyzFLE9ry4YEbb","tags":["governance-question"],"created_at":"2023-05-28T19:48:37.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Accountability “Minimising and reporting negative Impact: \u003cU+F0FC\u003e Did you carry out a risk or impact assessment of the AI system, which takes into account different stakeholders that are (in)directly affected? \u003cU+F0FC\u003e Did you provide training and education to help developing accountability practices? \u003cU+F0A7\u003e Which workers or branches of the team are involved? Does it go beyond the development phase?\u003cU+F0A7\u003e Do these trainings also teach the potential legal framework applicable to the AI system?\u003cU+F0A7\u003e Did you consider establishing an ‘ethical AI review board’ or a similar mechanism to discuss overall accountability and ethics practices, including potentially unclear grey areas? \u003cU+F0FC\u003e Did you foresee any kind of external guidance or put in place auditing processes to oversee ethics and accountability, in addition to internal initiatives? \u003cU+F0FC\u003e Did you establish processes for third parties (e.g. suppliers, consumers, distributors/vendors) or workers to report potential vulnerabilities, risks or biases in the AI system?” (High-Level Expert Group on AI, 2019, p. 31)## IEEE recommendations**Issue:** Oversight for algorithms## BackgroundThe algorithms behind A/IS are not subject to consistent oversight. This lack of assessment causes concern because end users have no account of how a certain algorithm or system came to its conclusions. These recommendations are similar to those made in the “General Principles” and “Embedding Values into Autonomous and Intelligent Systems” chapters of _Ethically Aligned Design_, but here the recommendations are used as they apply to the narrow scope of this chapter .## RecommendationsAccountability: As touched on in the General Principles chapter of _Ethically Aligned Design_, algorithmic transparency is an issue of concern. It is understood that specifics relating to algorithms or systems contain intellectual property that cannot, or will not, be released to the general public. Nonetheless, standards providing oversight of the manufacturing process of A/IS technologies need to be created to avoid harm and negative consequences. We can look to other technical domains, such as biomedical, civil, and aerospace engineering, where commercial protections for proprietary technology are routinely and effectively balanced with the need for appropriate oversight standards and mechanisms to safeguard the public.Human rights and algorithmic impact assessments should be explored as a meaningful way to improve the accountability of A/IS.These need to be paired with public consultations, and the final impactassessments must be made public.## Further Resources•F. Pasquale, The Black Box Society: The Secret Algorithms That Control Money and Information. Cambridge, MA: Harvard University Press, 2016.•R. Calo, “Artificial Intelligence Policy: A Primer and Roadmap,” _UC Davis Law Review,_ 52: pp. 399–435, 2017.•ARTICLE 19. “Privacy and Freedom of Expression in the Age of Artificial Intelligence,” Privacy International, April 2018. [Online]. Available: [https://www.article19.org/wpcontent/uploads/2018/04/Privacy-andFreedom-of-Expression-In-the-Age-of-ArtificialIntelligence-1.pdf.](https://www.article19.org/wp-content/uploads/2018/04/Privacy-and-Freedom-of-Expression-In-the-Age-of-Artificial-Intelligence-1.pdf) [Accessed October 28, 2018].p.132-133","id":"recjyzfle9ry4yebb","dom_id":"item_recjyzfle9ry4yebb"},{"Principles":["rec6O9e1nYBJtQUTj","recMGB4iC5oaCtr5x","recgDkzdE9dfpTxCK"],"Sources":["recQiVQ7CTC72xp6O"],"title":"Questions to consider in assessing benefits","category":"Strategies","name":"recKDgUEDD2f1egyd","tags":["reflection-questions"],"created_at":"2023-05-19T08:27:06.000Z","description":"“What are potential benefits associated with this study? § Who benefits from the study - do the potential participants? If not, what greater benefit justifies the potential risks? § Is the research aiming at a good or desirable goal? § Can we be sure the data collected from online sites, fora, communities, is “legitimate” and “valuable”?27” (Markham and Buchanan, 2012, p. 11)","id":"reckdguedd2f1egyd","dom_id":"item_reckdguedd2f1egyd"},{"Principles":["recOHnq45Fq7YWsRO","recy6hrMpKZ7TOn3Q"],"Sources":["recfYC5jjPmpLfSlM","recpXl48pJdKDhc6f"],"title":"Principle of Safety, key considerations","category":"Strategies","name":"recKWkdYO98PeEKDz","tags":["reflection-discussion"],"created_at":"2023-05-19T11:51:22.000Z","description":"\"Beyond safeguarding the sustainability of your AI project as it relates to its social impacts on individual wellbeing and public welfare, your project team must also confront the related challenge of technical sustainability or safety. A technically sustainable AI system is safe, accurate, reliable, secure, and robust. Securing these goals, however, is a difficult and unremitting task.Because AI systems operate in a world filled with uncertainty, volatility, and flux, the challenge of building safe and reliable AI can be especially daunting. This job, however, must be met head-on. Only by making the goal of producing safe and reliable AI technologies central to your project, will you be able to mitigate risks of your system failing at scale when faced with real-world unknowns and unforeseen events. The issue of AI safety is of paramount importance, because these potential failures may both produce harmful outcomes and undermine public trust.In order to safeguard that your AI system functions safely, you must prioritise the technical objectives of accuracy, reliability, security, and robustness. This requires that your technical team put careful forethought into how to construct a system that accurately and dependably operates in accordance with its designers’ expectations even when confronted with unexpected changes, anomalies, and perturbations. Building an AI system that meets these safety goals also requires rigorous testing, validation, and re-assessment as well as the integration of adequate mechanisms of oversight and control into its real-world operation._Accuracy, reliability, security, and robustness___It is important that you gain a strong working knowledge of each of the safety relevant operational objectives (accuracy, reliability, security, and robustness):·Accuracy and Performance Metrics: In machine learning, the accuracy of a model is the proportion of examples for which it generates a correct output. This performance measure is also sometimes characterised conversely as an error rate or the fraction of cases for which the model produces an incorrect output. Keep in mind that, in some instances, the choice of an acceptable error rate or accuracy level can be adjusted in accordance with the use case specific needs of the application. In other instances, it may be largely set by a domain established benchmark. As a performance metric, accuracy should be a central component to establishing and nuancing your team’s approach to safe AI. That said, specifying a reasonable performance level for your system may also often require you to refine or exchange your measure of accuracy. For instance, if certain errors are more significant or costly than others, a metric for total cost can be integrated into your model so that the cost of one class of errors can be weighed against that of another. Likewise, if the precision and sensitivity of the system in detecting uncommon events is a priority (say, in instances of the medical diagnosis of rare cases of a disease), you can use the technique of precision and recall. This method of addressing imbalanced classification would allow you to weigh the proportion of the system’s correct detections—both of frequent and of rare outcomes—against the proportion of actual detections of the rare event (i.e. the ratio of the true detections of the rare outcome to the sum of the true detections of that outcome and the missed detections or false negatives for that outcome).In general, measuring accuracy in the face of uncertainty is a challenge that must be given significant thought. The confidence level of your AI system will depend heavily on problems inherent in attempts to model a chaotic and changing reality. Concerns about accuracy must cope with issues of unavoidable noise present in the data sample, architectural uncertainties generated by the possibility that a given model is missing relevant features of the underlying distribution, and inevitable changes in input data over time.·Reliability: The objective of reliability is that an AI system behaves exactly as its designers intended and anticipated. A reliable system adheres to the specifications it was programmed to carry out. Reliability is therefore a measure of consistency and can establish confidence in the safety of a system based upon the dependability with which it operationally conforms to its intended functionality.·Security: The goal of security encompasses the protection of several operational dimensions of an AI system when confronted with possible adversarial attack. A secure system is capable of maintaining the integrity of the information that constitutes it. This includes protecting its architecture from the unauthorised modification or damage of any of its component parts. A secure system also remains continuously functional and accessible to its authorised users and keeps confidential and private information secure even under hostile or adversarial conditions.·Robustness: The objective of robustness can be thought of as the goal that an AI system functions reliably and accurately under harsh conditions. These conditions may include adversarial intervention, implementer error, or skewed goal-execution by an automated learner (in reinforcement learning applications). The measure of robustness is therefore the strength of a system’s integrity and the soundness of its operation in response to difficult conditions, adversarial attacks, perturbations, data poisoning, and undesirable reinforcement learning behaviour._Risks posed to accuracy and reliability:_ Concept Drift: Once trained, most machine learning systems operate on static models of the world that have been built from historical data which have become fixed in the systems’ parameters. This freezing of the model before it is released ‘into the wild’ makes its accuracy and reliability especially vulnerable to changes in the underlying distribution of data. When the historical data that have crystallised into the trained model’s architecture cease to reflect the population concerned, the model’s mapping function will no longer be able to accurately and reliably transform its inputs into its target output values. These systems can quickly become prone to error in unexpected and harmful ways.There has been much valuable research done on methods of detecting and mitigating concept and distribution drift, and you should consult with your technical team to ensure that its members have familiarised themselves with this research and have sufficient knowledge of the available ways to confront the issue. In all cases, you should remain vigilant to the potentially rapid concept drifts that may occur in the complex, dynamic, and evolving environments in which your AI project will intervene. Remaining aware of these transformations in the data is crucial for safe AI, and your team should actively formulate an action plan to anticipate and to mitigate their impacts on the performance of your system.Brittleness: Another possible challenge to the accuracy and reliability of machine learning systems arises from the inherent imitations of the systems themselves. Many of the high- performing machine learning models such as deep neural nets (DNN) rely on massive amounts of data and brute force repetition of training examples to tune the thousands, millions, or even billions of parameters, which collectively generate their outputs.However, when they are actually running in an unpredictable world, these systems may have difficulty processing unfamiliar events and scenarios. They may make unexpected and serious mistakes, because they have neither the capacity to contextualise the problems they are programmed to solve nor the common-sense ability to determine the relevance of new‘unknowns’. Moreover, these mistakes may remain unexplainable given the high- dimensionality and computational complexity of their mathematical structures. This fragility or brittleness can have especially significant consequences in safety-critical applications like fully automated transportation and medical decision support systems where undetectable changes in inputs may lead to significant failures. While progress is being made in finding ways to make these models more robust, it is crucial to consider safety first when weighing up their viability._Risks posed to security and robustness___Adversarial Attack: Adversarial attacks on machine learning models maliciously modify input data—often in imperceptible ways—to induce them into misclassification or incorrect prediction. For instance, by undetectably altering a few pixels on a picture, an adversarial attacker can mislead a model into generating an incorrect output (like identifying a panda as a gibbon or a ‘stop’ sign as a ‘speed limit’ sign) with an extremely high confidence. While a good amount of attention has been paid to the risks that adversarial attacks pose in deep learning applications like computer vision, these kinds of perturbations are also effective across a vast range of machine learning techniques and uses such as spam filtering and malware detection. These vulnerabilities of AI systems to adversarial examples have serious consequences for AI safety. The existence of cases where subtle but targeted perturbations cause models to be misled into gross miscalculation and incorrect decisions have potentially serious safety implication for the adoption of critical systems like applications in autonomous transportation, medical imaging, and security and surveillance. In response to concerns about the threats posed to a safe and trusted environment for AI technologies by adversarial attacks a field called adversarial machine learning has emerged over the past several years.Work in this area focuses on securing systems from disruptive perturbations at all points of vulnerability across the AI pipeline.One of the major safety strategies that has arisen from this research is an approach called model hardening, which has advanced techniques that combat adversarial attacks by strengthening the architectural components of the systems. Model hardening techniques may include adversarial training, where training data is methodically enlarged to include adversarial examples. Other model hardening methods involve architectural modification, regularisation, and data pre-processing manipulation. A second notable safety strategy is run- time detection, where the system is augmented with a discovery apparatus that can identify and trace in real-time the existence of adversarial examples. You should consult with members of your technical team to ensure that the risks of adversarial attack have been taken into account and mitigated throughout the AI lifecycle. A valuable collection of resources to combat adversarial attack can be found at [https://github.com/IBM/adversarial-](https://github.com/IBM/adversarial-robustness-toolbox) [robustness-toolbox](https://github.com/IBM/adversarial-robustness-toolbox) .Data Poisoning: A different but related type of adversarial attack is called data poisoning. This threat to safe and reliable AI involves a malicious compromise of data sources at the point of collection and pre-processing. Data poisoning occurs when an adversary modifies or manipulates part of the dataset upon which a model will be trained, validated, and tested. By altering a selected subset of training inputs, a poisoning attack can induce a trained AI system into curated misclassification, systemic malfunction, and poor performance. An especially concerning dimension of targeted data poisoning is that an adversary may introduce a‘backdoor’ into the infected model whereby the trained system functions normally until itprocesses maliciously selected inputs that trigger error or failure.In order to combat data poisoning, your technical team should become familiar with the state of the art in filtering and detecting poisoned data. However, such technical solutions are not enough. Data poisoning is possible because data collection and procurement often involves potentially unreliable or questionable sources. When data originates in uncontrollable environments like the internet, social media, or the Internet of Things, many opportunities present themselves to ill-intentioned attackers, who aim to manipulate training examples. Likewise, in third- party data curation processes (such as ‘crowdsourced’ labelling, annotation, and content identification), attackers may simply handcraft malicious inputs.Your project team should focus on the best practices of responsible data management, so that they are able to tend to data quality as an end-to-end priority.·Misdirected Reinforcement Learning Behaviour: A different set of safety risks emerges from the approach to machine learning called reinforcement learning (RL). In the more widely applied methods of supervised learning that have largely been the focus of this guide, a model transforms inputs into outputs according to a fixed mapping function that has resulted from its passively received training. In RL, by contrast, the learner system actively solves problems by engaging with its environment through trial and error. This exploration and‘problem-solving’ behaviour is determined by the objective of maximising a reward function that is defined by its designers.This flexibility in the model, however, comes at the price of potential safety risks. An RL system, which is operating in the real-world without sufficient controls, may determine a reward-optimising course of action that is optimal for achieving its desired objective but harmful to people. Because these models lack context-awareness, common sense, empathy, and understanding, they are unable to identify, on their own, scenarios that may have damaging consequences but that were not anticipated and constrained by their programmers. This is a difficult problem, because the unbounded complexity of the world makes anticipating all of its pitfalls and detrimental variables veritably impossible.Existing strategies to mitigate such risks of misdirected reinforcement learning behaviour include:oRunning extensive simulations during the testing stage, so that appropriate measures of constraint can be programmed into the systemoContinuous inspection and monitoring of the system, so that its behaviour can be better predicted and understoodoFinding ways to make the system more interpretable so that its decisions can be better assessedoHard-wiring mechanisms into the system that enable human override and system shut-downEnd-to-End AI SafetyThe safety risks you face in your AI project will depend, among other factors, on the sort of algorithm(s) and machine learning techniques you are using, the type of applications in which those techniques are going to be deployed, the provenance of your data, the way you are specifying your objective, and the problem domain in which that specification applies. As a best practice, regardless of this variability of techniques and circumstances, safety considerations of accuracy, reliability, security, and robustness should be in operation at every stage of your AI project lifecycle.This should involve both rigorous protocols of testing, validating, verifying, and monitoring the safety of the system and the performance of AI safety self-assessments by relevant members of your team at each stage of the workflow. Such self-assessments should evaluate how your team’s design and implementation practices line up with the safety objectives of accuracy, reliability, security, and robustness. Your AI safety self-assessments should be logged across the workflow on a single document in a running fashion that allows review and re-assessment.\"(Leslie, 2019, p26-30)### IEEE Report\"Operational failures and, in particular, violations of a system’s embedded community norms, are unavoidable, both during system testing and during deployment. Not only are implementations never perfect, but A/IS with embedded norms will update or expand their norms over time (see Section 1, Issue 2) and interactions in the social world are particularly complex and uncertain. Thus, prevention and mitigation strategies must be adopted, and we sample four possible ones.First, anticipating the process of evaluation during the implementation phase requires defining criteria and metrics for such evaluation, which in turn better allows the detection and mitigation of failures. Metrics will include:•Technical variables, such as traceability and verifiability,•User-level variables such as reliability, understandable explanations, and responsiveness to feedback, and•Community-level variables such as justified trust (see Issue 2) and the collective belief that A/IS are generally creating social benefits rather than, for example, technological unemployment.Second, a systematic risk analysis and management approach can be useful (Oetzel and Spiekermann 201443) for an application to privacy norms. This approach tries to anticipate potential points of failure, e.g., norm violations, and, where possible, develops some ways to reduce or remove the effects of failures. Successful behavior, and occasional failures, can then iteratively improve predictions andmitigation attempts.Third, because not all risks and failures are predictable (Brundage et al 201844; Vanderelst and Winfield 201845), especially in complex human-machine interactions in social contexts, additional mitigation mechanisms must be made available. Designers are strongly encouraged to augment the architectures of their systems with components that handle unanticipated norm violations with a fail-safe, such as the symbolic “gateway” agents discussed in Section 2, Issue 1. Designers should identify a number of strict laws, that is, task- and community-specific norms that should never be violated, and the failsafe components should continuously monitor operations against possible violations of these laws. In case of violations, the higher-order gateway agent should take appropriate actions, such as safely disabling the system’s operation, or greatly limiting its scope of operation, untilthe source of failure is identified. The failsafe components need to be understandable, extremely reliable, and protected against security breaches, which can be achieved, for example, by validating them carefully and not letting them adapt their parameters during execution.Fourth, once failures have occurred, responsible entities, e.g., corporate, government, science, and engineering, shall create a publicly accessible database with undesired outcomes caused by specific A/IS systems. The database would include descriptions of the problem, background information on how the problem was detected, which context it occurred in, and how it was addressed.In summary, we offer the following recommendation.## RecommendationBecause designers and developers cannot anticipate all possible operating conditions and potential failures of A/IS, multiple strategies to mitigate the chance and magnitude of harmmust be in place.### Further Resources•M. Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garfunkel, A. Dafoe, P. Scharre, T. Zeitzo, et al. \" “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation,” CoRR abs/1802.07228 [cs.AI]. 2018. \u003chttps://arxiv.org/abs/1802.07228\u003e•M. C. Oetzel and S. Spiekermann, “A Systematic Methodology for Privacy Impact Assessments: A Design Science Approach.” _European Journal of Information Systems_, vol._ _23, pp. 126–150, 2014. [https://link.springer. com/article/10.1057/ejis.2013.18](https://link.springer.com/article/10.1057/ejis.2013.18)•D. Vanderelst and A.F. Winfield, 2018 “The Dark Side of Ethical Robots,” In Proc. The First AAAI/ACM Conf. on Artificial Intelligence, Ethics and Society, New Orleans, LA, Feb. 1 -3, 2018.\"p.180-181","id":"reckwkdyo98peekdz","dom_id":"item_reckwkdyo98peekdz"},{"Challenges":["recmRF2P1OOASMfNF"],"Sources":["recpXl48pJdKDhc6f"],"title":"Consider developing strategies for specific context of humanitarian action","category":"Strategies","name":"recMgYQ7EqHAyDfi1","tags":[],"created_at":"2023-06-05T12:03:31.000Z","description":"## **\"**RecommendationsThe potential for A/IS to contribute to humanitarian action to save and improve lives should be prioritized for research and development, including by organizing global research challenges, while also building in safeguards to protect the creation, collection, processing, sharing, use, and disposal of information, including data from and about individuals and populations. Specific recommendations include:•Promoting awareness of the vulnerable condition of certain communities around the globe and the need to develop and use A/IS applications for humanitarian purposes.•Elaborating competitions and challenges in high impact conferences and university hackathons to engage both technical and nontechnical communities in the development of A/IS for humanitarian purposes and to address social issues.•Support civil society groups who organize themselves for the purpose of A/IS research and advocacy to develop applications to benefit humanitarian causes.39•Developing and applying ethical standards for the collection, use, sharing, and disposal of data in fragile settings.•Following privacy protection frameworks for pressing humanitarian situations that ensure the most vulnerable are protected.40•Setting up clear ethical frameworks for exceptional use of A/IS technologies in lifesaving humanitarian situations, comparedto \"normal\" situations.41•Stimulating the development of low-costand open source solutions based on A/ISto address specific humanitarian problems.•Training A/IS experts in humanitarian action and norms, and humanitarian practitionersto catalyze collaboration in designing,piloting, developing, and implementingA/IS technologies for humanitarian purposes. Forging public-private A/IS participant alliances that develop crisis scenarios in advance.•Working on cultural and contextual acceptance of any A/IS introduced during emergencies.•Documenting and developing quantifiable metrics for evaluating the outcomes of humanitarian digital projects, and educating the humanitarian ecosystem on the same.## Further resources•E. Prestes et al., \"The 2016 Humanitarian Robotics and Automation Technology Challenge [Competitions],\" in _IEEE Robotics \u0026 Automation Magazine_, vol. 23, no. 3, pp. 23-24, Sept. 2016. [http://ieeexplore.ieee.org/stamp/ stamp.jsp?tp=\u0026arnumber=7565695\u0026isnumber=7565655](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\u0026arnumber=7565695\u0026isnumber=7565655)•L. Marques et al., \"Automation of humanitarian demining: The 2016 Humanitarian Robotics and Automation Technology Challenge,\" _2016 International Conference on Robotics and Automation for Humanitarian Applications (RAHA)_, Kollam, 2016, pp. 1-7. \u003chttp://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\u0026arnumber=7931893\u0026isnumber=7931858\u003e•CYBATHLON 2020 Preliminary Race Task Descriptions [http://www.cybathlon.ethz. ch/cybathlon-2020/preliminary-race-taskdescriptions.html](http://www.cybathlon.ethz.ch/cybathlon-2020/preliminary-race-task-descriptions.html)•CYBATHLON Scientific Publications\u003chttp://www.cybathlon.ethz.ch/\u003e•Immigration Policy Lab (IPL), “Harnessing Big Data to Improve Refugee Resettlement” [https://immigrationlab.org/project/harnessingbig-data-to-improve-refugee-resettlement/](https://immigrationlab.org/project/harnessing-big-data-to-improve-refugee-resettlement/)•Harvard Humanitarian Initiative, _The Signal Code_, [https://signalcode.org](https://signalcode.org/)•J.A. Quinn, et al., “Humanitarian applications of machine learning with remote-sensing data: review and case study in refugee settlement mapping” Philosophical Transactions of the Royal Society A, 376 20170363; DOI:10.1098/rsta.2017.0363. Aug. 6, 2018.•Humanitarian Innovation Guide: [https:// higuide.elrha.org/,](https://higuide.elrha.org/) 2019.•P. Meier, [Digital Humanitarians: How Big Data is Changing the Face of Humanitarian Response](http://cds.cern.ch/record/2123110). Florida: CRC Press, 2015.•“Technology for human rights: UN Human Rights Office announces landmark partnership with Microsoft” [https://www.ohchr.org/ EN/NewsEvents/Pages/DisplayNews. aspx?NewsID=21620\u0026LangID=E](https://www.ohchr.org/EN/NewsEvents/Pages/DisplayNews.aspx?NewsID=21620\u0026LangID=E)•M. Luengo-Oroz, “10 big data science challenges facing humanitarian organizations,” UNHCR, Nov. 22, 2016. [http://www. unhcr.org/innovation/10-big-data-sciencechallenges-facing-humanitarian-organizations/](http://www.unhcr.org/innovation/10-big-data-science-challenges-facing-humanitarian-organizations/)•Optic Technologies, Press Release, Vatican Hack 2018—Results, 18 March 2018, which announced winning AI applications to benefit migrants and refugees as well as social inclusion and interfaith dialogue,[http://optictechnology.org/index.php/en/news-en/151-vhack-2018winners-en ](http://optictechnology.org/index.php/en/news-en/151-vhack-2018winners-en)\"p157-159","id":"recmgyq7eqhaydfi1","dom_id":"item_recmgyq7eqhaydfi1"},{"Principles":["recKdujFoPJr4ZAhZ"],"Sources":["recQiVQ7CTC72xp6O"],"title":"Questions to consider regarding access to a context (and data) and the perceptions and autonomy of those related to that context regarding access to it","category":"Strategies","name":"recN2Lw4yXDBWLSYv","tags":["reflection-questions"],"created_at":"2023-05-18T13:54:19.000Z","description":"“How is the context (venue/participants/data) being accessed? § How are participants / authors situated in the context? § How are participants/authors approached by the researcher? § How is the researcher situated in the context?15 § If access to an online context is publicly available, do members/participants/authors perceive the context to be public? What considerations might be necessary to accommodate ‘perceived privacy’16 or the notion that individuals might care more about the appropriate flow of information as defining it as public or private17?” (Markham and Buchanan, 2012, p. 8-9)","id":"recn2lw4yxdbwlsyv","dom_id":"item_recn2lw4yxdbwlsyv"},{"Principles":["recKdujFoPJr4ZAhZ","recQEiU22Qy1E0YuA","rec6tz9Phzck0hvT8"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Human oversight","category":"Strategies","name":"recNHvwgyFPXERuJ8","tags":["governance-question"],"created_at":"2023-05-28T19:21:28.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) 1\\. Human agency and oversight Fundamental rights: \u003cU+F0FC\u003e Did you carry out a fundamental rights impact assessment where there could be a negative impact on fundamental rights? Did you identify and document potential trade-offs made between the different principles and rights? \u003cU+F0FC\u003e Does the AI system interact with decisions by human (end) users (e.g. recommended actions or decisions to take, presenting of options)? \u003cU+F0A7\u003e Could the AI system affect human autonomy by interfering with the (end) user’s decision-making process in an unintended way? \u003cU+F0A7\u003e Did you consider whether the AI system should communicate to (end) users that a decision, content, advice or outcome is the result of an algorithmic decision? \u003cU+F0A7\u003e In case of a chat bot or other conversational system, are the human end users made aware that they are interacting with a non-human agent?” (High-Level Expert Group on AI, 2019, p. 26)“\u003cU+F0FC\u003e Is there is a self-learning or autonomous AI system or use case? If so, did you put in place more specific mechanisms of control and oversight? \u003cU+F0A7\u003e Which detection and response mechanisms did you establish to assess whether something could go wrong” (High-Level Expert Group on AI, 2019, p. 26)“\u003cU+F0A7\u003e Did you ensure a stop button or procedure to safely abort an operation where needed? Does this procedure abort the process entirely, in part, or delegate control to a human?” (High-Level Expert Group on AI, 2019, p. 27)","id":"recnhvwgyfpxeruj8","dom_id":"item_recnhvwgyfpxeruj8"},{"Principles":["reczVPIH1y2OMpAJH","rec42P8U9usfYCtv9","rec7n2TGrH9RHYpQj"],"Sources":["recfYC5jjPmpLfSlM"],"title":"Principle of Discriminatory non-harm for fairness, key considerations for outcome fairness","category":"Strategies","name":"recNzXJwCfbwBLmVU","tags":["reflection-discussion"],"created_at":"2023-05-19T11:13:24.000Z","description":"“As part of this minimum safeguarding of discriminatory non-harm, forethought and well-informed consideration must be put into how you are going to define and measure the fairness of the impacts and outcomes of the AI system you are developing.There is a great diversity of beliefs in the area of outcome fairness as to how to properly classify what makes the consequences of an algorithmically supported decision equitable, fair, and allocatively just. Different approaches—detailed below—stress different principles: some focus on demographic parity, some on individual fairness, others on error rates equitably distributed across subpopulations.Your determination of outcome fairness should heavily depend both on the specific use case for which the fairness of outcome is being considered and the technical feasibility of incorporating your chosen criteria into the construction of the AI system. (Note that different fairness-aware methods involve different types of technical interventions at the pre-processing, modelling, or postprocessing stages of production). Again, this means that determining your fairness definition should be a cooperative and multidisciplinary effort across the project team.You will find below a summary table of some of the main definitions of outcome fairness that have been integrated by researchers into formal models as well as a list of current articles and technical resources, which should be consulted to orient your team to the relevant knowledge base. (Note that this is a rapidly developing field, so your technical team should keep updated about further advances.) The first four fairness types fall under the category of group fairness and allow for comparative criteria of non-discrimination to be considered in model construction and evaluation. The final two fairness types focus instead on cases of individual fairness, where context-specific issues of effective bias are considered and assessed at the level of the individual agent.Take note, though, that these technical approaches have limited scope in terms of the bigger picture issues of algorithmic fairness that we have already stressed. Many of the formal approaches work only in use cases that have distributive or allocative consequences. In order to carry out group comparisons, these approaches require access to data about sensitive/protected attributes (that may often be unavailable or unreliable) as well as accurate demographic information about the underlying population distribution. Furthermore, there are unavoidable trade-offs and inconsistences between these technical definitions that must be weighed in determining which of them are best fit for your use case. Consult those on your project team with the technical expertise to consider the use case appropriateness of a desired formal approach.” (Leslie, 2019, p. 18)Some Formalisable Definitions of Outcome Fairness:- **Demographic/ Statistical Parity (group fairness): **An outcome is fair if each group in the selected set receives benefit in equal or similar proportions, i.e. if there is no correlation between a sensitive or protected attribute and the allocative result. This approach is intended to prevent disparate impact, which occurs when the outcome of an algorithmic process disproportionately harms members of disadvantaged or protected groups.- **True positive rate (group fairness): **An outcome is fair if the ‘true positive’ rates of an algorithmic prediction or classification are equal across groups. This approach is intended to align the goals of bias mitigation and accuracy by ensuring that the accuracy of the model is equivalent between relevant population subgroups. This method is also referred to as ‘equal opportunity’ fairness because it aims to secure equalised odds of an advantageous outcome for qualified individuals in a given population regardless of the protected or disadvantaged groups of which they are members- **False positive rate parity (group fairness): **An outcome is fair if it does not disparately mistreat people belonging to a given social group by misclassifying them at a higher rate than the members of a second social group, for this would place the members of the first group at an unfair disadvantage. This approach is motivated by the position that sensitive groups and advantaged groups should have similar error rates in outcomes of algorithmic decisions.- **Positive predictive value parity (group fairness): **An outcome is fair if the rates of positive predictive value (the fraction of correctly predicted positive cases out of all predicted positive cases) are equal across sensitive and advantaged groups. Outcome fairness is defined here in terms of a parity of precision, where the probability of members from different groups actually having the quality they are predicted to have is the same across groups.- **Individual fairness (individual fairness): **An outcome is fair if it treats individuals with similar relevant qualifications similarly. This approach relies on the establishment of a similarity metric that shows the degree to which pairs of individuals are alike with regard to a specific task.- **Counterfactual fairness (individual fairness): **An outcome is fair if an automated decision made about an individual belonging to a sensitive group would have been the same were that individual a member of a different group in a closest possible alternative (or counterfactual) world. Like the individual fairness approach, this method of defining fairness focuses on the specific circumstances of an affected decision subject, but, by using the tools of contrastive explanation, it moves beyond individual fairness insofar as it brings out the causal influences behind the algorithmic output. It also presents the possibility of offering the subject of an automated decision knowledge of what factors, if changed, could have influenced a different outcome. This could provide them with actionable recourse to change an unfavourable decision.(Leslie, 2019, p. 19)\"Once you and your project team have thoroughly considered the use case appropriateness as well as technical feasibility of the formal models of fairness most relevant for your system and have incorporated the model into your application, you should prepare a Fairness Position Statement (FPS) in which the fairness criteria being employed in the AI system is made explicit and explained in plain and non-technical language. This FPS should then be made publicly available for review by all affected stakeholders.\" (Leslie, 2019, p.20)","id":"recnzxjwcfbwblmvu","dom_id":"item_recnzxjwcfbwblmvu"},{"Cases":["recrVkbG0XGe2Ca0v","recSXcY4cnofb4zTP","recAlOHJhEy5nDwA6"],"Principles":["rec42P8U9usfYCtv9","recKdujFoPJr4ZAhZ","recUYS0TFpk2MhVD7"],"Sources":["rec9jnxuHOioQn4DC"],"title":"Guiding questions for educators regarding Privacy and Data Governance of AI in Education","category":"Strategies","name":"recOo7Pmo4FBYcu7P","tags":["reflection-questions"],"created_at":"2023-05-19T13:37:43.000Z","description":"“• Are there mechanisms to ensure that sensitive data is kept anonymous? Are there procedures in place to limit access to the data only to those who need it? • Is access to learner data protected and stored in a secure location and used only for the purposes for which the data was collected? • Is there a mechanism to allow teachers and school leaders to flag issues related to privacy or data protection? • Are learners and teachers informed about what happens with their data, how it is used and for what purposes? • Is it possible to customise the privacy and data settings? • Does the AI system comply with General Data Protection Regulation?” (European Commission, 2022, p. 21)","id":"recoo7pmo4fbycu7p","dom_id":"item_recoo7pmo4fbycu7p"},{"Principles":["reczVPIH1y2OMpAJH"],"Sources":["recQiVQ7CTC72xp6O"],"title":"Questions to consider regarding protection of vulnerable populations","category":"Strategies","name":"recPynxbe7x5wOs5E","tags":["reflection-questions"],"created_at":"2023-05-19T08:28:55.000Z","description":"“What particular issues might arise around the issue of minors or vulnerable persons? § Are minors being excluded from the study because of the difficulties of getting ethical permission to study them? § In situations where identity, age, and ability of the participant is unknown or hidden, and harm cannot be determined as an a priori category based on known vulnerability of participant, how will harm be considered as an ethical concern and operationalized in the study? § How are minors identified as ‘minors’ in contexts where demographic information is not required? What harm might result from asking (or not asking) for participants to reveal their age? § How will parental or guardian consent be obtained in addition to assent where required by research regulations? What risks might arise in this particular consent process? (for any or all parties, including the minor, the parents, and the researcher)?” (Markham and Buchanan, 2012, p. 11)","id":"recpynxbe7x5wos5e","dom_id":"item_recpynxbe7x5wos5e"},{"Challenges":["recnYgPiGULdHLunC"],"Principles":["recU6u0AZbcNj1ik9","rec6tz9Phzck0hvT8","recB9JaNSRmLbD8eE"],"Sources":["recnCULdYQ36cpZR7","recpXl48pJdKDhc6f"],"title":"Considerations in assessing trustworthy AI - Stakeholder participation","category":"Strategies","name":"recQPwyiQbPcN0G47","tags":["governance-question"],"created_at":"2023-05-28T19:39:27.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Diversity, non-discrimination and fairness “Stakeholder participation: \u003cU+F0FC\u003e Did you consider a mechanism to include the participation of different stakeholders in the AI system’s development and use? \u003cU+F0FC\u003e Did you pave the way for the introduction of the AI system in your organisation by informing and involving impacted workers and their representatives in advance?” (High-Level Expert Group on AI, 2019, p. 30)### IEEE recommendation\"To ensure representation of stakeholders, organizations should enact a planned and controlled set of activities to account for the interests of the full range of stakeholders or practitioners who will be working alongsideA/IS and incorporating their insights to build upon, rather than circumvent or ignore, thesocial and practical wisdom of involved practitioners and other stakeholders.## Further Resources•C. Schroeter, et al., “[Realization and User Evaluation of a Companion Robot for People with Mild Cognitive Impairments](http://www.tu-ilmenau.de/fileadmin/media/neurob/publications/conferences_int/2013/Schroeter-ICRA-2013-fin.pdf),” _Proceedings of IEEE International Conference on Robotics and Automation (ICRA 2013)_, Karlsruhe, Germany 2013. pp. 1145–1151.•T. L. [Chen, et al. ](http://ieeexplore.ieee.org/abstract/document/6476704/)“[Robots for Humanity: Using Assistive Robotics to Empower People with Disabilities](http://ieeexplore.ieee.org/document/6476704/),” _IEEE Robotics and Automation Magazine, _vol. 20, no. 1, pp. 30–39, 2013.R. Hartson, and P. S. Pyla. _The UX Book: Process and Guidelines for Ensuring a Quality User Experience_. Waltham, MA: Elsevier, 2012\"p.130-131","id":"recqpwyiqbpcn0g47","dom_id":"item_recqpwyiqbpcn0g47"},{"Cases":["recrVkbG0XGe2Ca0v","recmS3zSMbR3ofAR5","recSXcY4cnofb4zTP"],"Principles":["recKdujFoPJr4ZAhZ","recmzjcGKv3yNOxbl","recOHnq45Fq7YWsRO","reczcRriFbQQpn8iX"],"Sources":["rec9jnxuHOioQn4DC"],"title":"Guiding questions for educators regarding Accountability of AI in Education","category":"Strategies","name":"recRTVqtvPcBS6zps","tags":["reflection-questions"],"created_at":"2023-05-19T13:37:49.000Z","description":"“• Who is responsible for the ongoing monitoring of results produced by the AI system and how the results are being used to enhance teaching, learning and assessment? • How is the effectiveness and impact of the AI system being evaluated and how does this evaluation consider key values of education? • Who is responsible and accountable for final decisions made regarding the procurement and implementation of the AI system? • Is there a Service Level Agreement in place, clearly outlining the support and maintenance services and steps to be taken to address reported problems?” (European Commission, 2022, p. 21)","id":"recrtvqtvpcbs6zps","dom_id":"item_recrtvqtvpcbs6zps"},{"Cases":["recOOmVQviRyJGvea","recAlOHJhEy5nDwA6"],"Principles":["recgDkzdE9dfpTxCK","recQ9DIFEsOEkCx3O","recy4stJ6Y4e2Fezp","recint2IxoR8aILCp","recWLcMWDPE9Fd1pE"],"Sources":["rec9jnxuHOioQn4DC"],"title":"Guiding questions for educators regarding Technical Robustness and Safety of AI in Education","category":"Strategies","name":"recTWhZ88TbQLcNaQ","tags":["reflection-questions"],"created_at":"2023-05-19T13:37:48.000Z","description":"• Is there sufficient security in place to protect against data breaches? • Is there a strategy to monitor and test if the AI system is meeting the goals, purposes and intended applications? • Are the appropriate oversight mechanisms in place for data collection, storage, processing, minimisation and use? • Is information available to assure learners and parents of the system’s technical robustness and safety?","id":"rectwhz88tbqlcnaq","dom_id":"item_rectwhz88tbqlcnaq"},{"Challenges":["recHHr97jsyNDnlsJ"],"Principles":["recmzjcGKv3yNOxbl","recScYLR2TNiv7iKf","reclPiw2VvNOSTzv5","rec42P8U9usfYCtv9"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Unfair bias avoidance","category":"Strategies","name":"recTjwhqfrJoaRxYo","tags":["governance-question"],"created_at":"2023-05-28T19:39:18.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Diversity, non-discrimination and fairness Unfair bias avoidance: \u003cU+F0FC\u003e Did you establish a strategy or a set of procedures to avoid creating or reinforcing unfair bias in the AI system, both regarding the use of input data as well as for the algorithm design? \u003cU+F0A7\u003e Did you assess and acknowledge the possible limitations stemming from the composition of the used data sets? \u003cU+F0A7\u003e Did you consider diversity and representativeness of users in the data? Did you test for specific populations or problematic use cases? \u003cU+F0A7\u003e Did you research and use available technical tools to improve your understanding of the data, model and performance” (High-Level Expert Group on AI, 2019, p. 29)“30 \u003cU+F0A7\u003e Did you put in place processes to test and monitor for potential biases during the development, deployment and use phase of the system? \u003cU+F0FC\u003e Depending on the use case, did you ensure a mechanism that allows others to flag issues related to bias, discrimination or poor performance of the AI system? \u003cU+F0A7\u003e Did you establish clear steps and ways of communicating on how and to whom such issues can be raised? \u003cU+F0A7\u003e Did you consider others, potentially indirectly affected by the AI system, in addition to the (end)users? \u003cU+F0FC\u003e Did you assess whether there is any possible decision variability that can occur under the same conditions? \u003cU+F0A7\u003e If so, did you consider what the possible causes of this could be? \u003cU+F0A7\u003e In case of variability, did you establish a measurement or assessment mechanism of the potential impact of such variability on fundamental rights? \u003cU+F0FC\u003e Did you ensure an adequate working definition of “fairness” that you apply in designing AI systems? \u003cU+F0A7\u003e Is your definition commonly used? Did you consider other definitions before choosing this one? \u003cU+F0A7\u003e Did you ensure a quantitative analysis or metrics to measure and test the applied definition of fairness? \u003cU+F0A7\u003e Did you establish mechanisms to ensure fairness in your AI systems? Did you consider other potential mechanisms?” (High-Level Expert Group on AI, 2019, p. 30)","id":"rectjwhqfrjoarxyo","dom_id":"item_rectjwhqfrjoarxyo"},{"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Fundamental rights","category":"Strategies","name":"recTr6cDE4OBKV0wv","tags":["governance-question"],"created_at":"2023-05-28T19:10:10.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) 1\\. Human agency and oversight Fundamental rights: \u003cU+F0FC\u003e Did you carry out a fundamental rights impact assessment where there could be a negative impact on fundamental rights? Did you identify and document potential trade-offs made between the different principles and rights? \u003cU+F0FC\u003e Does the AI system interact with decisions by human (end) users (e.g. recommended actions or decisions to take, presenting of options)? \u003cU+F0A7\u003e Could the AI system affect human autonomy by interfering with the (end) user’s decision-making process in an unintended way? \u003cU+F0A7\u003e Did you consider whether the AI system should communicate to (end) users that a decision, content, advice or outcome is the result of an algorithmic decision? \u003cU+F0A7\u003e In case of a chat bot or other conversational system, are the human end users made aware that they are interacting with a non-human agent?” (High-Level Expert Group on AI, 2019, p. 26)","id":"rectr6cde4obkv0wv","dom_id":"item_rectr6cde4obkv0wv"},{"Challenges":["recaBNAcact8Bz6dg"],"Sources":["recpXl48pJdKDhc6f"],"title":"Roles regarding ethics should have leadership capacity, to foster ethical culture across the organisation","category":"Strategies","name":"recV8WjuiXlfbVN5d","tags":[],"created_at":"2023-06-05T10:29:17.000Z","description":"## RecommendationsCompanies should create roles for senior-level marketers, engineers, and lawyers who can collectively and pragmatically implement ethically aligned design. There is also a need for more in-house ethicists, or positions that fulfill similar roles. One potential way to ensure values are on the agenda in A/IS development is to have a Chief Values Officer (CVO), a role first suggested by Kay Firth-Butterfield, see “Further Resources”. However, ethical responsibility should not be delegated solely to CVOs. They can support the creation of ethical knowledge in companies, but in the end, all members of an organization will need to act responsibly throughout the design process.Companies need to ensure that their understanding of values-based system innovation is based on _de jure _and _de facto _international human rights standards.•K. Firth-Butterfield, “[How IEEE Aims to Instill Ethics in Artificial Intelligence Design,](http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design)” The Institute. Jan. 19, 2017. [Online]. Available: [http://theinstitute.ieee.org/ieee-roundup/ blogs/blog/how-ieee-aims-to-instill-ethicsin-artificial-intelligence-design](http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design). [Accessed October 28, 2018]. •United Nations, [Guiding Principles on Business and Human Rights: Implementing the United Nations “Protect, Respect and Remedy” Framework,](http://www.ohchr.org/Documents/Publications/GuidingPrinciplesBusinessHR_EN.pdf) New York and Geneva: UN, 2011.•Institute for Human Rights and Business(IHRB), and Shift, ICT [Sector Guide on ](https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf)[Implementing the UN Guiding Principles on Business and Human Rights,](https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf) 2013.•C. Cath, and L. Floridi, “[The Design of the Internet’s Architecture by the Internet ](http://europepmc.org/abstract/med/27255607)[Engineering Task Force (IETF) and Human Rights](http://europepmc.org/abstract/med/27255607).” _Science and Engineering Ethics, _vol._ _23, no. 2, pp. 449–468, Apr. 2017.\"p.128","id":"recv8wjuixlfbvn5d","dom_id":"item_recv8wjuixlfbvn5d"},{"Principles":["recKdujFoPJr4ZAhZ"],"Sources":["recY4zreDoWrsbsv7"],"title":"Foundational issue: Promotion of Transparency","category":"Strategies","name":"recWm6C6wOuVr6UCX","tags":[],"created_at":"2023-05-29T07:49:51.000Z","description":"“AI models in edtech will be approximations of reality and, thus, constituents can always ask these questions: How precise are the AI models? Do they accurately capture what is most important? How well do the recommendations made by an AI model fit educational goals? What are the broader implications of using AI models at scale in educational processes? Building on what was heard from constituents, the sections of this report develop the theme of evaluating the quality of AI systems and tools using multiple dimensions as follows: \u003cU+25CF\u003e About AI: AI systems and tools must respect data privacy and security. Humans must be in the loop. \u003cU+25CF\u003e Learning: AI systems and tools must align to our collective vision for high-quality learning, including equity. \u003cU+25CF\u003e Teaching: AI systems and tools must be inspectable, explainable, and provide human alternatives to AI-based suggestions; educators will need support to exercise professional judgment and override AI models, when necessary.” (Cardona et al., 2023, p. 9)“\u003cU+25CF\u003e Formative Assessment: AI systems and tools must minimize bias, promote fairness, and avoid additional testing time and burden for students and teachers. \u003cU+25CF\u003e Research and Development: AI systems and tools must account for the context of teaching and learning and must work well in educational practice, given variability in students, teachers, and settings. \u003cU+25CF\u003e Recommendations: Use of AI systems and tools must be safe and effective for students. They must include algorithmic discrimination protections, protect data privacy, provide notice and explanation, and provide a recourse to humans when problems arise. The people most affected by the use of AI in education must be part of the development of the AI model, system, or tool, even if this slows the pace of adoption.” (Cardona et al., 2023, p. 10)","id":"recwm6c6wouvr6ucx","dom_id":"item_recwm6c6wouvr6ucx"},{"Principles":["recKdujFoPJr4ZAhZ"],"Sources":["recY4zreDoWrsbsv7"],"title":"Always Center Educators in Instructional Loops","category":"Strategies","name":"recX4wcNFSw9YljDD","tags":[],"created_at":"2023-05-29T08:05:27.000Z","description":"“To succeed with AI as an enhancement to learning and teaching, we need to always center educators (ACE). Practically speaking, practicing “ACE in AI” means keeping a humanistic view of teaching front and center. ACE is not just about making teachers’ jobs easier but also making it possible to do what most teachers want to do. That includes, for example, understanding their students more deeply and having more time to respond in creative ways to teachable moments.To bring more precision to how and where we should center educators, we return to our advocacy for human in the loop AI and ask, what are the loops in which teachers should be centered? Figure 5 suggests three key loops (inspired by research on adaptivity loops34):” (Cardona et al., 2023, p. 25)“1. The loop in which teachers make moment-to-moment decisions as they do the immediate work of teaching. 2\\. The loop in which teachers prepare for, plan, and reflect on teaching, which includes professional development. 3\\. The loop in which teachers participate in decisions about the design of AI-enabled technologies, participate in selecting the technologies, and shape the evaluation of technologies—thus setting a context for not only their own classroom but those of fellow teachers as well.” (Cardona et al., 2023, p. 26)","id":"recx4wcnfsw9yljdd","dom_id":"item_recx4wcnfsw9yljdd"},{"Principles":["rec42P8U9usfYCtv9","reczVPIH1y2OMpAJH","recU6u0AZbcNj1ik9","recjViPnz3atRIOpD","recPg7Ov0priGGtLm"],"Sources":["recQiVQ7CTC72xp6O"],"title":"Questions to consider in assessing risk of harms","category":"Strategies","name":"recY9yr9vYcOAUSEA","tags":["reflection-questions"],"created_at":"2023-05-19T08:25:30.000Z","description":"“What are the potential harms or risks associated with this study? § What is the potential harm or risk for individuals, for online communities, for researchers, for research? § Are risks being assessed throughout the study as well as in advance of the study? (Harm is only certain after it occurs. Thus, a priori assessments of risk might be useful but inadequate24). § How are the concepts of ‘vulnerability’ and ‘harm’ being defined and operationalized in the study? How are risks to the community/author/participant being assessed? § How is vulnerability determined in contexts where this categorization may not be apparent? § Would a mismatch between researcher and community/participant/author definitions of ‘harm’ or ‘vulnerability’ create an ethical dilemma? If so, how would this be addressed? § What harms--to life, to career, to reputation--may occur from the research? (e.g., would the research “out” an LGBTQ individual who is not publicly out and perhaps cause them to lose their jobs? Would the research cause someone to face criminal or civil penalties?) § What possible privacy-related harms may occur? For example, might online groups disband or individuals cease to use an online support group or withdraw from blogging activities because of the presence of researchers;25 Might individuals be upset that their perceived privacy has been violated;26 might individuals object to having their writing or speech anonymised, preferring to remain known and public in any published results? § Who or what else could cause harm to the author/participant beyond the researcher? “§ Are we acting in ways that minimizes risk? § Does our research adequately protect the researcher as well as the community/author/participant?” (Markham and Buchanan, 2012, p. 10-11)","id":"recy9yr9vycoausea","dom_id":"item_recy9yr9vycoausea"},{"Challenges":["rec1QZHHMARBQcZoo","recOpn4I30te1qiKl","reciNc7OeTUmnsLqg","recA7Kh502s4UKWGo","recGRpoF7DA23ODSj","rec5nbWC0ZbVwnmxJ","recaFyWRROaJ1ZFyY"],"Principles":["receFm7cGasHwpJZO"],"Sources":["recpXl48pJdKDhc6f"],"title":"Wellbeing measures and promotion should be part of AI evaluation","category":"Strategies","name":"recZI7HDWKBU5T22v","tags":[],"created_at":"2023-06-05T11:22:05.000Z","description":"## \"RecommendationsA/IS creators should work to better understand and apply well-being metrics in the algorithmic age. Specifically:•A/IS creators should work directly with experts, researchers, and practitioners in wellbeing concepts and metrics to identify existing metrics and combinations of indicators that would bring support a “triple bottom line”, i.e., accounting for economic, social, and environmental impacts, approach to wellbeing. However, well-being metrics should only be used with consent, respect for privacy, and with strict standards for collection and use of these data.•For A/IS to promote human well-being, the well-being metrics should be chosen in collaboration with the populations most affected by those systems—the A/IS stakeholders—including both the intended end-users or beneficiaries and those groups whose lives might be unintentionally transformed by them. This selection process should be iterative and through a learningand continually improving process. In addition, “metrics of well-being” should be treated as vehicles for learning and potential mid- course corrections. The effects of A/IS on human well-being should be monitored continuously throughout their life cycles, byA/IS creators and stakeholders, and both A/IS creators and stakeholders should be prepared to significantly modify, or even roll back, technology that is shown to reduce well-being, as defined by affected populations.•A/IS creators in the business or academic, engineering, or policy arenas are advised to review the additional resources on standards development models and frameworks at the end of this chapter to familiarize themselves with existing indicators relevant to their work.## Further Resources•PricewaterhouseCoopers (PwC). [Managing and Measuring Total Impact: A New Language for Business Decisions](https://www.pwc.com/gx/en/services/sustainability/total-impact-measurement-management/measuring-and-managing-total-impact-a-new-language-for-business-decisions.html), 2017.•World Economic Forum. [The Inclusive Growth and Development Report 2017](https://www.weforum.org/reports/the-inclusive-growth-and-development-report-2017), Geneva, Switzerland: World Economic Forum, January 16, 2017.•[OECD Guidelines on Measuring Subjective Well-being,](http://www.oecd.org/statistics/oecd-guidelines-on-measuring-subjective-well-being-9789264191655-en.htm) 2013.•National Research Council. [Subjective WellBeing: Measuring Happiness, Suffering, and Other Dimensions of Experience. D](https://www.nap.edu/catalog/18548/subjective-well-being-measuring-happiness-suffering-and-other-dimensions-of)C: The National Academies Press, 2013.\"p73-74\"Create technical standards for representing goals, metrics, and evaluation guidelines for well-being metrics and their precursors and components within A/IS that include:•[O](https://en.wikipedia.org/wiki/Ontology_(information_science))ntologies for representing technological requirements.•A testing framework for validating adherence to well-being metrics and ethical principles such as [IEEE P7010™ Standards Project for Wellbeing Metric for Autonomous and Intelligent Systems](https://standards.ieee.org/project/7010.html).above as well as others as a basis for a wellbeing metrics standard for A/IS creators. _(See page 191, [Additional Resources: Additional Resources: Standards Development Models and Frameworks)](https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/ead1e_standards_development_models_frameworks.pdf)_•The development of a well-being metrics standard for A/IS that encompasses an understanding of well-being as holistic and interlinked to social, economic, and ecological systems.\"p.79-80## \"RecommendationAppoint a lead team or person, “leads”, to facilitate stakeholder engagement and to serve as a resource for A/IS creators who use stakeholderbased processes to establish well-being indicators. Specifically:•Leads should solicit and collect lessons learned from specific applications of stakeholder engagement and deliberation in order to continually refine its guidance.•When determining well-being indicators, the leads should enlist the help of experts in public participation and deliberation. With expert guidance, facilitators can provide guidance for how to: take steps to mitigate the effects of unequal power in deliberative processes; incorporate appropriately trained facilitators and coaching participants in deliberations; recognize and curb disproportionate influence by morepowerful groups; use techniques to maximize the voices of less-powerful groups.•Leads should use their convening power to bring together A/IS creators and stakeholders, including critics of A/IS, for deliberations on well-being indicators, impacts, and other considerations for specific contexts and settings. Leads’ involvement would help bring actors to the table with a balance of power and encourage all actors to remain in conversation until robust, mutually agreeable definitionsare found.## Further Resources•D. E. Booher and J. E. Innes. Planning with Complexity: An Introduction to Collaborative Rationality for Public Policy. London:Routledge, 2010.•J. A. Leydens and J. C. Lucena. Engineering Justice: Transforming Engineering Education and Practice_. _Wiley-IEEE Press, 2018.•G. Ottinger. [Assessing Community Advisory ](https://www.sciencehistory.org/sites/default/files/studies-in-sustainability-ottinger2009.pdf)[Panels: A Case Study from Louisiana’s Industrial Corridor.](https://www.sciencehistory.org/sites/default/files/studies-in-sustainability-ottinger2009.pdf) Center for Contemporary History and Policy, 2008.•[Expert and Citizen Assessment of Science and ](https://ecastnetwork.org/about/)[Technology (ECAST) Network ](https://ecastnetwork.org/about/)\"p.82-83","id":"reczi7hdwkbu5t22v","dom_id":"item_reczi7hdwkbu5t22v"},{"Principles":["recOHnq45Fq7YWsRO","recZToVrPeFlFq0Aw"],"Sources":["recfYC5jjPmpLfSlM"],"title":"Principle of responsible delivery through human-centred implementation, key considerations","category":"Strategies","name":"recZPU5yTmrjPXlF2","tags":["reflection-discussion"],"created_at":"2023-05-19T12:03:02.000Z","description":"\"The demand for sensitivity to human factors should inform your approach to devising delivery and implementation processes from start to finish. To provide clear and effective explanations about the content and rationale of algorithmic outputs, you will have to begin by building _from the human ground up_. You will have to pay close attention to the circumstances, needs, competences, and capacities of the people whom your AI project aims to assist and serve.This means that _context will be critical_. By understanding your use case well and by drawing upon solid domain knowledge, you will be better able to define roles and relationships. You will better be able to train the users and implementers of your system. And, you will be better able to establish an effectual implementation platform, to clarify content, and to facilitate understanding of outcomes for users and affected stakeholders alike. Here is a diagram of what securing human-centred implementation protocols and practices might look like:           Let us consider these steps in turn by building a checklist of essential actions that should be taken to help ensure the human-centred implementation of your AI project. Because the specifics of your approach will depend so heavily on the context and potential impacts of your project, we’ll assume a generic case and construct the checklist around a hypothetical algorithmic decision-making system that will be used for predictive risk assessment.Step 1: Consider aspects of application type and domain context to define roles and determine user needsÿ(1) Assess which members of the communities you are serving will be most affected by the implementation of your AI system. Who are the most vulnerable among them? How will their socioeconomic, cultural, and education backgrounds affect their capacities to interpret and understand the explanations you intend to provide? How can you fine-tune your explanatory strategy to accommodate their requirements and provide them with clear and non-technical details about the rationale behind the algorithmically supported result?When thinking about providing explanations to affected stakeholders, you should start with the needs of the most disadvantaged first. Only in this way, will you be able to establish an acceptable baseline for the equitable delivery of interpretable AI.ÿ(2) After reviewing [Guideline 1](#_bookmark33) above, make a list of and define all the roles that will potentially be involved at the delivery stage of your AI project. As you go through each role, specify levels of technical expertise and domain knowledge as well as possible goals and objectives for each role. For instance, in our predictive risk assessment case:oDecision Subject (DS)-§Role: Subject of the predictive analytics.§Possible Goals and Objectives: To receive a fair, unbiased, and reasonable determination, which makes sense; to discover which factors might be changed to receive a different outcome.§Technical and Domain Knowledge: Most likely low to average technical expertise and average domain knowledge.oAdvocate for the DS-§Role: Support for the DS (for example, legal counsel or care worker) and concerned party to the automated decision.§Possible Goals and Objectives: To make sure the best interests of the DS are safeguarded throughout the process; to help make clear to the DS what is going on and how and why decisions are being made.§Technical and Domain Knowledge: Most likely average technical expertise and high level of domain knowledge.oImplementer-§Role: User of the AI system as decision support.§Possible Goals and Objectives: To make an objective and fair decision that is sufficiently responsive to the particular circumstances of the DS and that is anchored in solid reasoning and evidence-based judgement.§Technical and Domain Knowledge: Most likely average technical expertise and high level of domain knowledge.oSystem Operator/Technician-§Role: Provider of support and maintenance for the AI system and its use. §Possible Goals and Objectives: To make sure the machine learning system is performing well and running in accordance with its intended design; to handle the technical dimension of information processing for the DS’s particular case; to answer technical questions about the system and its results as they arise.§Technical and Domain Knowledge: Most likely high level of technical expertise and average domain knowledge.oDelivery Manager-§Role: Member of the implementation team who oversees its operation and responds to problems as they arise.§Possible Goals and Objectives: To ensure that the quality of the automation- supported assessment process is high and that the needs of the decision subject are being served as intended by the project; to oversee the overall quality of the relationships within the implementation team and between the members of that team and the communities they serve.§Technical and Domain Knowledge: Most likely average technical expertise and good to high level domain knowledgeStep 2: Define delivery relations and map delivery processesÿ(1) Assess the possible relationships between the defined roles that will have significantbearing on your project’s implementation and formulate a descriptive account of this relationship with an eye to the part it will play in the delivery process. For the predictive risk assessment example:oDecision Subject/Advocate to Implementer: This is the primary relationship of the implementation process. It should be information-driven and dialogue-driven with the implementer’s exercise of unbiased judgment and the DS’s comprehension of the outcome treated as the highest priorities. Implementers should be prepared to answer questions and to offer evidence-based clarifications and justifications for their determinations. The achievement of well-informed mutual understanding is a central aim.oImplementer to System Operator: This is the most critical operational relationship within the implementation team. Communication levels should be kept high from case to case, and the shared goal of the two parties should be to optimise the quality of the decisions by optimising the use of the algorithmic decision-support system in ways that are accessible both to the user and to the DS. The conversations between implementers and system operators should be problem-driven and should avoid, as much as possible, focus on the specialised vocabularies of each party’s domain of expertise.oDelivery Manager to Operator to Implementer: The quality of this cross-disciplinary relationship within the implementation team will have direct bearing on the overall quality of the delivery of the algorithmically supported decisions. Safeguarding the latter will require that open and easily accessible lines of communication be maintained between delivery managers, operators, and implementers, so that unforeseen implementation problems can be tackled from multiple angles and in ways that anticipate and stem future difficulties. Additionally, different use cases may present different explanatory challenges that are best addressed by multidisciplinary team input. Good communications within the implementation team will be essential to enable that such challenges are addressed in a timely and efficient manner.ÿ(2) Start building a map of the delivery process. This should involve incorporating your understanding of the needs, roles, and relationships of relevant actors involved in the implementation of your AI system into the wider objective of providing clear, informative, and understandable explanations of algorithmically supported decisions.It is vital to recognise, at this implementation-planning stage of your project, that the principal goal of the delivery process is two-fold: _to translate statistically expressed results into humanly significant reasons and to translate algorithmic outputs into socially meaningful outcomes_.These overlapping objectives should have a direct bearing on the way you build a map foryour project’s delivery process, because they organise the duties of implementation into two task-specific components:_1._A technical component, which involves determining the most effective way to convey and communicate to users and decision subjects the statistical results of your model’s information processing so that the factors that figured into the logic and rationale of those results can be translated into understandable reasons that can be subjected to rational evaluation and critical assessment; and_2._A social component, which involves clarifying the socially meaningful content of the outcome of a given algorithmically assisted decision by translating that model’s technical machinery—its input and output variables, parameters, and functional rationale—_back _into the everyday language of the humanly relevant categories and relationships that informed the formulation of its purpose, objective, and intended elements of design in the first place. Only through this re-translation will the effects ofthat model’s output on the real human life it impacts be understandable in terms of the specific social and individual context of that life and be conveyable as such.These two components of the delivery process will be fleshed out in turn.Technical component of responsible implementation: As a general rule, we use the results of statistical analysis to guide our actions, because, when done properly, this kind of analysis offers a solid basis of empirically derived evidence that helps us to exercise sound and well- supported judgment about the matters it informs.Having a good understanding of the factors that are at work in producing the result of a particular statistical analysis (such as in an algorithmic decision-support system) means that we are able to grasp these factors (for instance, input features that weigh heavily in determining a given algorithmically generated decision) as reasons that may warrant the rational acceptability of that result. After all, seen from the perspective of the interpretability of such an analysis, these factors are, in fact, nothing other than _reasons that are operating to support its conclusions_. Clearly understood, these factors that lie behind the logic of the result or decision are not ‘causes’ of it. Rather, they form the evidentiary basis of its rational soundness and of the goodness of the inferences that support it. Whether or not we ultimately agree with the decision or the result of the analysis, the reasons that work together to comprise its conclusions make _claims to validity _and can _as such _be called before a tribunal of _rational criticism_. These reasons, in other words, must bear the burden of continuous assessment, evaluation, and contestation.This is an element especially crucial for the responsible implementation of AI systems: Because they serve surrogate cognitive functions in society, their decisions and results are in no way immune from these demands for rational justification and thus must be delivered to be optimally responsive to such demands.The results of algorithmic decision support systems, in this sense, serve as stand-ins for acts of speech and representation and therefore bear the justificatory burdens of those cognitive functions. They must establish the validity of their conclusions and operate under the constraint of being surrogates of the dialogical goal to convince through good reasons.This charge to be responsive to the demands of rational justification should be essential to the way you map out your delivery strategy. When you devise how best to relay and explain the statistical results of your AI systems, you need to start from the role they play in supporting evidence-based reasoning.This, however, is no easy job. Interpreting the results of data scientific analysis is, more often than not, a highly technical activity and can depart widely from the conventional, everyday styles of reasoning that are familiar to most. Moreover, the various performance metrics deployed in AI systems can be confusing and, at times, seem to be at cross-purposes with each other, depending upon the metrics chosen. There is also an unavoidable dimension of uncertainty that must be accounted for and expressed in confidence intervals and error bars which may only bring further confusion to users and decision subjects.Be that as it may, by taking a deliberate and human-centred approach to the delivery process, you should be able to find the most effective way to convey your model’s statistical results to users and decision subjects in non-technical and socially meaningful language that enables them to understand and evaluate the rational justifiability of those results. A good point of departure for this is to divide your map-building task into the _means of content delivery _and the _substance of the content to be delivered_._Means of content delivery: _When you start mapping out serviceable ways of presenting and communicating your model’s results, you should consider the users’ and decision subjects’ perspectives to be of primary importance. Here are a few guiding questions to ask as you sketch out this dimension of your delivery process as well as some provisional answers to them:oHow can the delivery process of explaining the AI system’s results aid and augment the user’s and decision subject’s _mental models _(their ways of organising and filtering information), so that they can get a clear picture of the technical meaning of the assessment or explanation? What is the best way to frame the statistical inferences and meanings so that they can be effectively integrated into each user’s own cognitive _space of concepts and beliefs_?While answering these questions will largely depend both on your use case and on the type of AI application you are building, it is just as important that you start responding to them by concentrating on the differing needs and capabilities of your explainees. To do this properly, you should first seek input from domain experts, users, and affected stakeholders, so that you can suitably scan the horizons of existing needs and capabilities. Likewise, you should take a human-centred approach to exploring the types of explanation delivery methods that would best be suited for each of your target groups. Much valuable research has been done on this in the field of human-computer interaction and in the study of human factors. This work should be consulted when mapping delivery means.Once you have gathered enough background information, you should begin to plan out how you are going to line up your means of delivery with the varying levels of technical literacy, expertise, and cognitive need possessed by the relevant stakeholder groups, who will be involved in the implementation of your project. Such a _multi-tiered approach _minimally requires that individual attention be paid to the explanatory needs and capacities of implementers, system operators, and decision subjects and their advocates. This multi-tiered approach will pose different challenges at each different level.For instance, the mental models of implementers—i.e. their ways of conceptualising the information they are receiving from the algorithmic decision-support system— may, in some cases, largely be shaped by their accumulation of domain know-how and by the filter of on-the-job expertise that they have developed over long periods of practice. These users may have a predisposition to automation distrust or aversion bias, and this should be taken into account when you are formulating appropriate means of explanation delivery.In other contexts, the opposite may be the case. Where implementers tend to over- rely on or over-comply with automated systems, the means of explanation delivery must anticipate a different sort of mental model and adjust the presentation of information accordingly.In any event, you will need to have a good empirical understanding of yourimplementer’s decision-making context and maintain such knowledge through ongoing assessment. In both bias risk areas, the conveyance and communication of the assessments generated by algorithmic decision-support systems should attemptto bolster each user’s practical judgment in ways that mitigate the possibility of either sort of bias. These assessments should present results as evidence-based reasons that support and better capacitate the objectivity of these implementers’ reasoning processes. The story is different with regard to the cognitive life of the technically inclined user. The mental models of system operators, who are natives in the technical vocabulary and epistemic representations of the statistical results, may be adept at the model- based problem-solving tasks that arise during implementation but less familiar with identifying and responding to the cognitive needs and limitations of non-technical stakeholders. Incorporating ongoing communication exercises and training into their roles in the delivery process may capacitate them to better facilitate implementers’ and decision subjects’ understanding of the technical details of the assessments generated by algorithmic decision-support systems. These ongoing developmentactivities will not only helpfully enrich operators’ mental models, they may also inspire them to develop deeper, more responsive, and more effective ways of communicating the technical yields of the analytics they oversee.Finally, the mental models of decision subjects and their advocates will show the broadest range of conceptualisation capacities, so your delivery strategy should (1) prioritise the facilitation of optimal explanation at the baseline level of the needs of the most disadvantaged of them and (2) build the depth of your multi-tiered approach to providing effective explanations into the delivery options presented to decision subjects and their advocates. This latter suggestion entails that, beyond provision of the baseline explanation of the algorithmically generated result, options should be given to decision subjects and their advocates to view more detailed and technical presentations of the sort available to implementers and operators (with the proviso that reasonable limitations be placed on transparency in accordance with the need to protect the confidential personal and organisational information and to prevent gaming of the system).oHow can non-technical stakeholders be adequately prepared to gain baseline knowledge of the kinds of statistical and probabilistic reasoning that have factored into the technical interpretation of the system’s output, so that they are able to comprehend it on its own technical terms? How can the technical components be presented in a way that will enable explainees to easily translate the statistical inferences and meanings of the results into understandable and rationally assessable terms? What are the best available media for presenting the technical results in engaging and comprehensible ways?To meet these challenges, you should consider supplementing your implementation platform with knowledge-building and enrichment resources that will provide non- technical stakeholders with access to basic technical concepts and vocabulary. At a minimum, you should consider building a plain language glossary of basic terms and concepts that will include all of the technical ideas covered by the algorithmic component of a given explanation. If your explanation platforms are digital, you should also make them as user friendly as possible by hyperlinking the technical terms used in the explanations to their plain language glossary elaborations.Where possible, explanatory demonstrations of technical concepts (like performance metrics, formal fairness criteria, confidence intervals, etc.) should be provided to users and decision subjects in an engaging and easy-to-comprehend way, and graphical and visualisation techniques should be consistently used to make potentially difficult ideas more accessible. Moreover, the explanation interfaces themselves should be as simple, learnable, and usable as possible. They should be tested to measure the ease with which those with neither technical experience nor domain knowledge are able to gain proficiency in their use and in understanding their content.Substance of the technical content to be delivered: The overall interpretability of your AI system will largely hinge on the effectiveness and even-handedness of your technical content delivery. You will have to strike a balance between (1) determining how best to convey and communicate the rationale of the statistical results so that they may be treated appropriately as decision supporting and clarifying reasons and (2) being clear about the limitations of and potential uncertainties in the statistical results themselves so that the explanations you offer will not mislead implementers and decision subjects. These are not easy tasks and will require substantial forethought as you map out the content clarification aspect of your delivery process.To assist you in this, here is a non-exhaustive list of recommendations that you should consider as you map out the execution of the technical content delivery component of the responsible implementation of your AI project (This list will, for the sake of specificity, assume the predictive risk assessment example):·Each explanation should be presented in plain, non-technical language and in an optimally understandable way so that the results provided can enable the affordance of better judgment on the part of implementers and optimal understanding on the part of decision subjects. On the implementer’s side, the primary goal of the explanation should be to support the user’s ability to offer solid, coherent, and reasonable justifications oftheir determinations of decision outcomes. On the decision subject’s side, the primary goal of the explanation should be to make maximally comprehensible the rationale behind the algorithmic component of the decision process, so that the decision subject can undertake a properly informed critical evaluation of the decision outcome as a whole.·Each explanation should present its results as facts or evidence in as sparse but complete and sound a manner as possible with a clear indication of what components in the explanation are operating as premises, what components are operating as conclusions, and what the inferential rationale is that is connecting the premises to the conclusions. Each explanation should therefore make explicit the rational criteria for its determination whether this be, for example, global inferences drawn from the population-based reasoning of a demographic analysis or more locally or instance-based inferences drawn from the indication of feature significance by a proxy model. In all cases, the optimisation criteria of the operative algorithmic system should be specified, made explicit, and connected to the logic and rationale of the decision.·Each explanation should make available the records and activity-monitoring results that the design and development processes of your AI project yielded. Building this link between the process transparency dimension of your project and its outcome transparency will help to make its result, as a whole, more sufficiently interpretable. This can be done by simply linking or including the public-facing component of the process log of your PBG Framework.·Each explanation provided to an implementer should come with a standard Implementation Disclaimer that may read as follows:           ·Each explanation should specify and make explicit its governing performance metrics together with the acceptability criteria used to select those metrics and any standard benchmarks followed in establishing that criteria. Where appropriate and possible, fuller information about model validation measurement (including confusion matrix and ROC curve results) and any external validation results should be made available.·Each explanation should provide confirmatory information that the formal fairness criteriaspecified in your project’s Fairness Policy Statement has been met.·Each explanation should include clear representations of confidence intervals and error bars. These certainty estimates should make as quantitatively explicit as possible the confidence range of specific predictions, so that users and decision subjects can more fully understand their reliability and the levels of uncertainty surrounding them.·When an explanation offers categorically ordered scores (for instance, risk scores on a scale of 1 to 10), that explanation must also explicitly indicate the actual raw numerical probabilities for the labels (predicted outcomes) that have been placed into those categories. This will help your delivery process avoid producing confusion about the relative magnitudes of the categorical groupings under which the various scores fall. Information should also be provided about the relative distances between the risk scores of specific cases if the risk categories under which they are placed are unevenly distributed. It may be possible, for example, for two cases, which fall under the same high risk category (say, 9) to be farther apart in terms of the actual values of their risk probabilities than two other cases in two different categories (say 1 and 4). This may be misleading to the user. ·Each explanation should, where possible, include a counterfactual explanatory tool, so that implementers and affected individuals have the opportunity to gain a better contrastive understanding of the logic of the outcome and its alternative possibilities.Social component of responsible implementation: We have now established the first step in the delivery of a responsible implementation process: making clear the rationale behind the technical content of an algorithmic model’s statistical results and determining how best to convey and communicate it so that these results may be appropriately treated as decision supporting and clarifying reasons. This leaves us with a second related task of content clarification, which is only implicit in the first step but must be made explicit and treated reflectively in a second.Beyond translating statistically expressed results into humanly significant reasons, you will have to make sure that their _socially meaningful content _is clarified by implementers, so that they are able to thoughtfully apply these results to the real human lives they impact in terms of the specific societal and individual contexts in which those lives are situated.This will involve explicitly translating that model’s technical machinery—its input and output variables, parameters, and functional rationale—_back _into the everyday language of the humanly relevant meanings, categories, and relationships that informed the formulation of its purpose, objectives, and intended elements of design in the first place. It will also involve training and preparing implementers to intentionally assist in carrying out this translation in each particular case, so that due regard for the dignity of decision subjects can be supported by the interpretive charity, reasonableness, empathy, and context-specificity of the determination of the outcomes that affect them.Only through this re-translation will the internals, mechanisms, and output of the model become _useably interpretable _by implementers: Only then will they be able to apply input features of relevance to the specific situations and attributes of decision subjects. Only then will they be able to critically assess the manner of inference-making that led to its conclusion. And only then will they be able to adequately weigh the normative considerations (such as prioritising public interest or safeguarding individual well-being) that factored into thesystem’s original objectives.Having clarified the socially meaningful content of the model’s results, the implementer will be able to more readily apply its evidentiary contribution to a more holistic and wide-angled consideration of the particular circumstances of the decision subject while, at the same time, weighing these circumstances against the greater purpose of the algorithmically assisted assessment. It is important to note here that the understanding enabled by the clarification of the social context and stakes of an algorithmically supported decision-making process goes hand-in-glove with fuller considerations of the moral justifiability of the outcome of that process.A good starting point for considering how to integrate this clarification of the socially meaningful content of an algorithmic model’s output into your map of the delivery process is to consider what you might think of as your AI project’s content lifecycle. The content lifecycle: The output of an algorithmic system does not begin and end with the computation. Rather, it begins with the very human purposes, ideas, and initiatives that lay behind the conceptualisation and design of that system. Creating technology is a shared public activity, and it is animated by human objectives and beliefs. An algorithmic system is brought into the world as the result of this collective enterprise of ingenuity, intention, action, and collaboration.Human choices and values therefore punctuate the design and implementation of AI systems. These choices and values are inscribed in algorithmic models:·At the very inception of an AI project, human choices and values come into play when we formulate the goals and objectives to be achieved by our algorithmic technologies. They come into play when we define the optimal outcome of our use of such technologies and when we translate these goals and objectives into target variables and their measurable proxies.·Human choices and values come into play when decisions are made about the sufficiency, fit-for-purpose, representativeness, relevance, and appropriateness of the data sampled. They come into play in how we curate our data—in how we label, organise, and annotate them.·Such choices and values operate as well when we make decisions about how we craft a feature space—how we select or omit and aggregate or segregate attributes. Determinations of what is relevant, reasonable, desirable, or undesirable will factor into what kinds of inputs we are going to include in the processing and how we are going to group and separate them.·Moreover, the data points themselves are imbued with residua of human choices and values. They carry forward historical patterns of social and cultural activity that may contain configurations of discrimination, inequality, and marginalisation—configurations that must be thoughtfully and reflectively considered by implementers as they incorporate the analytics into their reasoned determinations.Whereas all of these human choices and values are translated in to the algorithmic systems we build, the responsible implementation of these systems requires that they be translated out. The rationale and logic behind an algorithmic model’s output can be properly understood as it affects the real existence of a decision subject only when we transform its variables, parameters, and analytical structures back into the human currency of values, choices, and norms that shaped the construction of its purpose, its intended design, and its optimisation logic from the start.It is only in virtue of this re-translation that an algorithmically supported outcome can afford stakeholders the degree of deliberation, dialogue, assessment, and mutual understanding that is necessary to make it fully comprehensible and justifiable to them. And, it is likewise only in virtue of this re-translation that the implementation process itself can, at once, secure end-to-end accountability and give due regard to the SUM values. The content lifecycle of algorithmic systems therefore has three phases: (1) The translation in of human purposes, values, and choices during the design process; (2) The digital processing of the quantified/mechanised proxies of these purposes, values, and choices in the statistical frame; (3) The translation out of the purposes, values, and choices in clarifying the socially meaningful content of the result as it affects the life of the decision subject through the implementation process. Here is a visualisation of these three phases of the content lifecycle:           The translation rule: A beneficial result of framing the implementation process in terms of the content lifecycle is that it gives us a clear and context-sensitive measure by which to identify the explanatory needs of any given AI application. We can think of this measurement as the translation rule. It states that:What is _translated in _to an algorithmic system with regard to the human choices and societal values that determine its content and purpose is directly proportional to what, in terms of the explanatory needs of clarification and justification, must be _translated out_.The translation rule organically makes two distinctions that have great bearing on the delivery process for responsible implementation. First, it divides the question of what needs explaining into two parts: (1) issues of socially meaningful content in need of clarification (i.e., the explanatory need that comes from the translation in to the AI model of the categories, meanings, and relations that originate in social practices, beliefs, and intentions)(2) issues of normative rightness in need of justification (i.e. the explanatory need that comes from translation in to the AI model of choices and considerations that have bearing on its ethical permissibility, discriminatory non-harm, and public trustworthiness). These two parts line up with what we have above called interpretable AI and justifiable AI respectively, and what we have also identified as [tasks 2 and 3](#_bookmark26) of delivering transparent AI.Secondly, the translation rule divides the two dimensions of translation (translation in and translation out) into aspects of intention-in-design and intention-in-application. _Translating in_ has to do with _intention-in-design_. It involves an active awareness of the human purposes, objectives, and intentions that factor into the construction of AI systems. _Translating out, _on the other hand, has directly to do with _intention-in-application_, or put differently, the intentional dimension of the implementation of an AI system by a user in a specific context and with direct consequences for a subject affected by its outcome.In human beings, intention-in-design and intention-in-application are _united in intelligent action_, and it is precisely this unity that enables people to reciprocally hold each other accountable for the consequences of what they say and what they do. By contrast, in artificial intelligence systems, which fulfil surrogate cognitive functions in society but are themselves neither intentional nor accountable, design and application are divided. In these systems, intention-in-design and intention-in-application are and must remain _punctuation points of human involvement and responsibility _that manifest on either side of the vacant mechanisms of data processing. This is why translation is so important, and this is why enabling theimplementer’s capacity to _intentionally translate out the social and normative content _of the model’s results is such a critical element of the responsible delivery of your AI project.It might be helpful to think more concretely about the translation rule by considering it in action. Let’s compare two hypothetical examples: (1) a use case about an early cancer detection system in radiomics (a machine learning application that uses high throughputcomputing to identify features of pathology that are undetectable to the trained radiological eye); and (2) a use case about a predictive risk assessment application that supports decision- making in child social care.In the radiomics case, the _translating in _dimension involves minimal social content: theclinical goal inscribed in the model’s objective is that of lesion detection and the features of relevance are largely voxels extracted from PET and CT scanner images. However, the normative aspect of _translating in _is, in this case, significant. Ethical considerations about looking after patient wellbeing and clinical safety are paramount and wider justice concerns about improving healthcare for all and health equity factor in as well.The explanatory needs of the physician/implementer receiving clinical decision support and of the clinical decision subject will thus lean less heavily on the dimension of the clarification of socially meaningful content than it will on the normative dimension of justifying the safety of the system, the priority of the patient’s wellbeing, and the issues of improved delivery and equitable access. The technical content of the decision support may be crucial here (Issues surrounding the reproducibility of the results and the robustness of the system may, in fact, be of great concern in the assessment of the validity of the outcome.), but the _translating out _component of the implementation remains directly proportional to the minimal social content and to the substantial ethical concerns and objectives that were _translated in _and that thus inform the explanatory and justificatory needs of the result in general.The explanatory demands in the child social care risk assessment use case are entirely different. The social content of the _translating in _dimension is intricate, multi-layered, and extensive. The chosen target variable may be child safety or the prevention of severe mistreatment and the measurable proxy, home removal of at-risk children within a certain timeframe. Selected features that are deemed relevant may include the age of the at-risk children, public health records, previous referrals, family history of violent crime, welfare records, juvenile criminal records, demographic information, and mental health records. Complex socioeconomic and cultural formations may additionally influence the representativeness and quality of the dataset as well as the substance of the data itself.The normative aspect of _translating in _here is also subtle and complicated. Ethical considerations about protecting the welfare of children at risk are combined with concerns that parents and guardians be treated fairly and without discrimination. Objectives of providing evidence-based decision support are also driven by hopes that accurate results and well-reasoned determinations will preserve the integrity and sanctity of familial relations where just, safe, and appropriate. Other goals and purposes may be at play as well such as making an overburdened system of service provision more efficient or accelerating real-time decision-making without harming the quality of the decisions themselves.In this case of predictive risk assessment, the _translating out _burdens of the frontline social worker are immense both in terms of clarifying content and in terms of moral justification. If, for example, analytical results yielding a high risk score were based on the relative feature importance of demographic information, welfare records, mental health records, and criminal history, the implementer would have to scrutinise the particular decision subject’s situation, so that the socially meaningful content of these factors could be clarified in terms of the living context, relevant relationships, and behavioural patterns of the stakeholders directly affected. Only then could the features of relevance be thoroughly and deliberatively assessed.The effective interpretability of the model’s result would, in this case, heavily depend on the implementer’s ability to apply domain-knowledge in order to reconstruct the meaningful social formations, intentions, and relationships that constituted the concrete form of life in which the predictive risk modelling applies. The implementer’s well-reasoned decision here would involve a careful weighing of this socially clarified content against the wider predictive patterns in the data distribution yielded by the model’s results—patterns that may have otherwise gone unnoticed.Such a weighing process would, in turn, be informed by the normative-explanatory need to translate out the morally implicating choices, concerns, and objectives that influenced and informed the predictive risk assessment model’s development in the first place. Again, the interpretive burden of the frontline social worker would be immense here. First, thisimplementer would have to deliberate with a critically informed awareness of the legacies of discrimination and inequity that tend to feed forward in the kinds of evidentiary sources drawn upon by the analytics. Such an active reflexivity is crucial for retaining the punctuating role of human involvement and responsibility in these sensitive and high-stakes environments.Just as importantly, the frontline social worker would have to evaluate the real impact of ethical objectives at the point of delivery. Not only would the results of the analytics have to be aligned with the ethical concerns and purposes that fostered the construction of the model, this implementer would have to reflectively align their own potentially diverging ethical point of view both with those results and with those objectives. This _normative_ _triangulation _between the original intention-in-design, the implementer’s intention-in- application, and the content clarification of the AI system’s results is, in fact, a crucial safeguard to the delivery of justifiable AI. It again enables a reanimation of moral involvement and responsibility at the most critical juncture of the content lifecycle.Step 3: Build an ethical implementation platform:ÿ(1) Train ethical implementation. The continuous challenges of translation, content clarification, and normative explanation should inform how you set up your implementation training to achieve optimal outcome transparency. In addition to the necessary [training to](#_bookmark13) [prevent implementation biases in the users of your AI system](#_bookmark13) (discussed above), you should prepare and train the implementers to be stewards of interpretable and justifiable AI. This entails that they be able to:oRationally evaluate and critically assess the logic and rationale behind the outputs of the AI systems;oConvey and communicate their algorithmically assisted decisions to the individuals affected by them in plain language. This includes explaining to them in an everyday, non-technical, and accessible way how and why the decision-supporting model performed the way it did in a specific context and how that result factored into the final outcome of the implementation;oApply the conclusions reached by the AI model to a more focused consideration of the particular social circumstances and life context of the decision subject and other affected parties;oTreat the inferences drawn from the results of the model’s computation as evidentiary contributions to a broader, more rounded, and coherent understanding of the individual situations of the decision subject and other affected parties;oWeigh the interpretive understanding gained by integrating the model’s insights into this rounded picture of the life context of the decision subject against the greater purpose and societal objective of the algorithmically assisted assessment;oJustify the ethical permissibility, the discriminatory non-harm, and the publictrustworthiness both of the AI system’s outcome and of the processes behind its design and useÿ(2) Make your implementation platform a relevant part and capstone of the sustainability track of your project. An important element of gauging the impacts of your AI technology on the individuals and communities it touches is having access to the frontlines of its potentially transformative and long-term effects. Your implementation platform should assist you in gaining this access by being a _two-way medium of application and communication_. It should both enable you to sustainably achieve the objectives and goals you set for your project through responsible implementation, but it should also be a sounding board as well as a site for feedback and cooperative sense-checking about the real-life effects of your system’s use. Your implementation platform should be dialogically and collaboratively connected to the stakeholders it effects. It should be bound to the communities it serves as part of a shared project to advance their immediate and long-run wellbeing.ÿ(3) Provide a model sheet to implementers and establish protocols for implementation reporting. As part of the roll-out of your AI project, you should prepare a summary/model sheet for implementers, which includes summation information about the system’s technical specifications and all of the relevant details indicated above in the section on _substance of the technical content to be delivered_. This should include relevant information about performance metrics, formal fairness criteria and validation, the implementation disclaimer, links or summaries to the relevant information from the process logs of your PBG Framework, and links or summary information from the Stakeholder Impact Assessment.You should also set up protocols for implementation reporting that are proportional to thepotential impacts and risks of the system’s use.ÿ(4) Foster outcome understanding through dialogue. Perhaps the single most important aspect of building a platform for ethical implementation is the awareness that the realisation of interpretable and justifiable AI is a dialogical and collaborative effort. Because all types of explanation are mediated by language, each and every explanatory effort is a participatory enterprise where understanding can be reached only through acts of communication. The interpretability and justifiability of AI systems depend on this shared human capacity to give and ask for reasons in the ends of reaching mutual understanding. Implementers and decision subjects are, in this respect, first and foremost participants in an explanatory dialogue, and the success of their exchange will hinge both on a reciprocal readiness take the other’s perspective and on a willingness to enlarge their respective mental models in accordance with new, communicatively achieved, insights and understandings.For these reasons, your implementation platform should encourage open, mutually respectful, sincere, and well-informed dialogue. Reasons from all affected voices must be heard and considered as demands for explanation arise, and manners of response and expression should remain clear, straightforward, and optimally accessible. Deliberations that have been inclusive, unfettered, and impartial tend to generate new ideas and insights as well as better and more inferentially sound conclusions, so approaching the interpretability and justifiability of your AI project in this manner will not only advance its responsible implementation, it will likely encourage further improvements in its design, delivery, and performance.Leslie, 2019, p.54-68","id":"reczpu5ytmrjpxlf2","dom_id":"item_reczpu5ytmrjpxlf2"},{"Principles":["recKdujFoPJr4ZAhZ","rec42P8U9usfYCtv9","reclPiw2VvNOSTzv5"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Minimising and reporting negative impacts","category":"Strategies","name":"recaTTvWN1olqx608","tags":["governance-question"],"created_at":"2023-05-28T19:50:07.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Accountability “Ability to redress: \u003cU+F0FC\u003e Did you establish an adequate set of mechanisms that allows for redress in case of the occurrence of any harm or adverse impact? \u003cU+F0FC\u003e Did you put mechanisms in place both to provide information to (end-)users/third parties about opportunities for redress?” (High-Level Expert Group on AI, 2019, p. 31)","id":"recattvwn1olqx608","dom_id":"item_recattvwn1olqx608"},{"Principles":["recsvi4LnhEEPyQ1h"],"Sources":["recQiVQ7CTC72xp6O"],"title":"Questions to consider regarding autonomy and informed consent","category":"Strategies","name":"recazD3B5XpqgOCGV","tags":["reflection-questions"],"created_at":"2023-05-19T08:28:21.000Z","description":"“How are we recognizing the autonomy of others and acknowledging that they are of equal worth to ourselves and should be treated so? § Will informed consent be required from participants? § If so, what procedures to obtain consent will be followed? (E.g., print or digital signatures, virtual consent tokens, click boxes or waiver of documented consent)28 § Will consent be obtained just from individuals or from communities and online system administrators? § In situations whereby consent is desired but written informed consent is impossible (or in regulatory criteria, impracticable) or potentially harmful, will procedures or requirements be modified? § What harm might result from asking for consent, or through the process of asking for consent?29 § What ethical concerns might arise if informed consent is not obtained? § If an ethics board deems no consent is required, will the researcher still seek subjects’/participants’ consent in a non-regulatory manner? § If informed consent is warranted, how will the researcher ensure that participants are truly informed?” (Markham and Buchanan, 2012, p. 11)","id":"recazd3b5xpqgocgv","dom_id":"item_recazd3b5xpqgocgv"},{"Principles":["recKdujFoPJr4ZAhZ","rec42P8U9usfYCtv9","recmzjcGKv3yNOxbl","recSqx6wklVpDzx3s"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Documenting trade-offs","category":"Strategies","name":"reccAKmFQRH7IiuJi","tags":["governance-question"],"created_at":"2023-05-28T19:49:50.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Accountability “Documenting trade-offs:\u003cU+F0FC\u003e Did you establish a mechanism to identify relevant interests and values implicated by the AI system and potential trade-offs between them? \u003cU+F0FC\u003e How do you decide on such trade-offs? Did you ensure that the trade-off decision was documented?” (High-Level Expert Group on AI, 2019, p. 31) c","id":"reccakmfqrh7iiuji","dom_id":"item_reccakmfqrh7iiuji"},{"Challenges":["recxjc79LvLdKa4rl"],"Sources":["recpXl48pJdKDhc6f"],"title":"Community norms should be identified, and expertise from intercultural information ethics practitioners embedded in ethics committees","category":"Strategies","name":"reccMqFCLOgQwWM5Q","tags":[],"created_at":"2023-06-05T10:21:20.000Z","description":"## RecommendationEstablish a leading role for [intercultural information ethics ](http://www.capurro.de/iie.html)(IIE) practitioners in ethics committees informing technologists, policy makers, and engineers. Clearly demonstrate through examples how cultural variation informs not only information flows and information systems, but also algorithmic decision-making and value by design.## Further Resources•D. J. Pauleen, et al. “[Cultural Bias in Information Systems Research and Practice: Are You Coming From the Same Place I Am? ](http://aisel.aisnet.org/cais/vol17/iss1/17/)” _Communications of the Association for Information Systems, _vol._ _17, no. 17, 2006.•J. Bielby, “[Comparative Philosophies in Intercultural Information Ethics](https://scholarworks.iu.edu/iupjournals/index.php/confluence/article/view/540),” _Confluence: Online Journal of World Philosophies _2, no. 1, pp. 233–253, 2016.p.124## RecommendationTo develop A/IS capable of following social and moral norms, the first step is to identify the norms of the specific community in which theA/IS are to be deployed and, in particular, norms relevant to the kinds of tasks and roles that the A/IS are designed for. This norm identification process must use appropriate scientific methods and continue through the system's life cycle.## Further Resources•Mack, Ed., “Changing social norms.” _Social Research: An International Quarterly,_ 85, no.1, 1–271, 2018.•I. Misra, C. L. Zitnick, M. Mitchell, and R. Girshick, (2016). Seeing through the human reporting bias: Visual Classifiers from Noisy Human-Centric Labels. In _Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition_ (CVPR), pp. 2930–2939. doi[:10.1109/CVPR.2016.320](https://doi.org/10.1109/CVPR.2016.320)•I. van de Poel, “[An Ethical Framework for Evaluating Experimental Technology,](https://link.springer.com/article/10.1007/s11948-015-9724-3)” _Science and Engineering Ethics_, 22, no. 3,pp. 667686, 2016.\"p.168-169## RecommendationA/IS developers should identify the ways in which people resolve norm conflicts and the ways in which they expect A/IS to resolve similar norm conflicts. A system’s resolution of norm conflicts must be transparent—that is, documented by the system and ready to be made available to users, the relevant community of deployment, and third-party evaluators.## Further resources•M. Velasquez, C. Andre, T. Shanks, S.J., and M. J. Meyer, “[The Common](https://legacy.scu.edu/ethics/publications/iie/v5n1/common.html) [Good](https://legacy.scu.edu/ethics/publications/iie/v5n1/common.html).” _Issues in Ethics_, vol._ _5, no. 1, 1992.•J. Van den Hoven, “Engineering and the Problem of Moral Overload.” _Science and Engineering Ethics, vol. _18, no. 1, pp.143–155, 2012.•D. Abel, J. MacGlashan, and M. L. Littman. “Reinforcement Learning as a Framework for Ethical Decision Making.” _AAAI Workshop AI, Ethics, and Society, Volume WS-16-02 of 13th AAAI Workshops_. Palo Alto, CA: AAAIPress, 2016.•O. Bendel, Die Moral in der Maschine: Beiträge zu Roboter- und Maschinenethik. Hannover, Germany: Heise Medien, 2016.Accessible popular-science contributions to philosophical issues and technical implementations of machine ethics•S. V. Burks, and E. L. Krupka. [“A Multimethod Approach to Identifying Norms and Normative Expectations within a Corporate Hierarchy: ](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1110.1478)[Evidence from the Financial Services Industry.”](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1110.1478) _Management Science, _vol. 58, pp. 203–217, 2012.Illustrates surveys and incentivized coordination games as methods to elicit norms in a large financial services firm•F. Cushman, V. Kumar, and P. Railton, “Moral Learning,” _Cognition_, vol._ _167, pp. 1–282, 2017.•M. Flanagan, D. C. Howe, and H. Nissenbaum, “Embodying Values in Technology: Theory and Practice.” _Information Technology and Moral __Philosophy_, J. van den Hoven and J. Weckert, Eds., Cambridge University Press, 2008, pp. 322–53. Cambridge Core, _Cambridge University Press._ Preprint available at[http://www.nyu.edu/projects/nissenbaum/ papers/Nissenbaum-VID.4-25.pdf](http://www.nyu.edu/projects/nissenbaum/papers/Nissenbaum-VID.4-25.pdf)•B. Friedman, P. H. Kahn, A. Borning, and A. Huldtgren. “Value Sensitive Design and Information Systems,” in _Early Engagement and New Technologies: Opening up the Laboratory, _N. Doorn, Schuurbiers, I. van de Poel, and M. Gorman, Eds., vol. 16, pp. 55–95. Dordrecht: Springer, 2013.A comprehensive introduction into Value Sensitive Design and three sample applications•G. Mackie, F. Moneti, E. Denny, and H. Shakya. “What Are Social Norms? How Are They Measured?” UNICEF Working Pape[r. ](http://dmeforpeace.org/sites/default/files/4%2009%2030%20Whole%20What%20are%20Social%20Norms.pdf)University of California at San Diego: UNICEF, Sept. 2014. [https://dmeforpeace.org/sites/ default/files/4%2009%2030%20Whole%20 What%20are%20Social%20Norms.pdf](https://dmeforpeace.org/sites/default/files/4%2009%2030%20Whole%20What%20are%20Social%20Norms.pdf)A broad survey of conceptual and measurement questions regarding social norms.•J. A. Leydens and J. C. Lucena. Engineering Justice: Transforming Engineering Education and Practice. Hoboken, NJ: John Wiley \u0026 Sons, 2018.Identifies principles of engineering for social justice.•B. F. Malle, “Integrating Robot Ethics and Machine Morality: The Study and Design of Moral Competence in Robots.” _Ethics and Information Technology, _vol._ _18, no. 4, pp. 243–256, 2016.Discusses how a robot’s norm capacity fits in the larger vision of a robot with moral competence.•K. W. Miller, M. J. Wolf, and F. Grodzinsky, “This ‘Ethical Trap’ Is for Roboticists, Not Robots: On the Issue of Artificial Agent Ethical DecisionMaking.” _Science and Engineering Ethics, _vol._ _23, pp. 389–401, 2017.This article raises doubts about the possibility of imbuing artificial agents with morality, or of claiming to have done so.•Open Roboethics Initiative: [www.openroboethics.org](http://www.openroboethics.org/). A series of poll results on differences in human moral decision-making and changes in priority order of values for autonomous systems (e.g., [on care](http://www.openroboethics.org/results-should-a-carebot-bring-an-alcoholic-a-drink-poll-says-it-depends-on-who-owns-the-robot/) [robots)](http://www.openroboethics.org/results-should-a-carebot-bring-an-alcoholic-a-drink-poll-says-it-depends-on-who-owns-the-robot/), 2019.•A. Rizzo and L. L. Swisher, “Comparing the Stewart–Sprinthall Management Survey and the Defining Issues Test-2 as Measures of Moral Reasoning in Public Administration.” _Journal of Public Administration Researchand Theory, _vol._ _14, pp. 335–348, 2004. Describes two assessment instruments of moral reasoning (including norm maintenance) based on Kohlberg’s theoryof moral development.•S. H. Schwartz, “A[n Overview of the Schwartz](https://scholarworks.gvsu.edu/orpc/vol2/iss1/11/)[Theory of Basic Values.” _Online Readings in Psychology and Culture _2, 2012](https://scholarworks.gvsu.edu/orpc/vol2/iss1/11/). •Comprehensive overview of a specific theory of values, understood as motivational orientations toward abstract outcomes (e.g., self-direction, power, security).•S. H. Schwartz and K. Boehnke. “[Evaluating the Structure of Human Values with Confirmatory Factor Analysis.” _Journal of Research in Personality, _vol. 38, ](http://www.sciencedirect.com/science/article/pii/S0092656603000692?via%3Dihub)pp. 230–255, 2004.•Describes an older method of subjective judgments of relations among valued outcomes and a newer, formal method of analyzing these relations.•W. Wallach and C. Allen. _Moral Machines: Teaching Robots Right from Wrong_. New York: Oxford University Press, 2008. This book describes some of the challenges of having a one-size-fits-all approach to embedding human values in autonomous systems. \"p.172-174","id":"reccmqfclogqwwm5q","dom_id":"item_reccmqfclogqwwm5q"},{"Principles":["recxcFmvPG5wrCqpO","recQEiU22Qy1E0YuA"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Traceability","category":"Strategies","name":"recdtW2BdWmY6WSP5","tags":["governance-question"],"created_at":"2023-05-28T19:33:40.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Transparency Traceability: \u003cU+F0FC\u003e Did you establish measures that can ensure traceability? This could entail documenting the following methods: \u003cU+F0D8\u003e Methods used for designing and developing the algorithmic system: o Rule-based AI systems: the method of programming or how the model was built; o Learning-based AI systems; the method of training the algorithm, including which input data was gathered and selected, and how this occurred.\u003cU+F0D8\u003e Methods used to test and validate the algorithmic system: o Rule-based AI systems; the scenarios or cases used in order to test and validate; o Learning-based model: information about the data used to test and validate. \u003cU+F0D8\u003e Outcomes of the algorithmic system: o The outcomes of or decisions taken by the algorithm, as well as potential other decisions that would result from different cases (for example, for other subgroups of users).” (High-Level Expert Group on AI, 2019, p. 28-29)","id":"recdtw2bdwmy6wsp5","dom_id":"item_recdtw2bdwmy6wsp5"},{"Cases":["reciNqxyfUgE5XM7t","recOOmVQviRyJGvea"],"Principles":["recLHILkx2JDFsLbX","reco4DUa3rsjP0hyg"],"Sources":["rec9jnxuHOioQn4DC"],"title":"Guiding questions for educators regarding Human Agency and Oversight of AI in Education","category":"Strategies","name":"recegTm800wJXGjO1","tags":["reflection-questions"],"created_at":"2023-05-19T13:32:23.000Z","description":"- Is the teacher role clearly defined so as to ensure that there is a teacher in the loop while the AI system is being used? How does the AI system affect the didactical role of the teacher?- Are the decisions that impact students conducted with teacher agency and is the teacher able to notice anomalies or possible discrimination? - Are procedures in place for teachers to monitor and intervene, for example in situations where empathy is required when dealing with learners or parents? - Is there a mechanism for learners to opt-out if concerns have not been adequately addressed? - Are there monitoring systems in place to prevent overconfidence in or overreliance on the AI system? - Do teachers and school leaders have all the training and information needed to effectively use the system and ensure it is safe and does not cause harms or violate rights of students?” (European Commission, 2022, p. 19)","id":"recegtm800wjxgjo1","dom_id":"item_recegtm800wjxgjo1"},{"Principles":["recKdujFoPJr4ZAhZ"],"Sources":["recQiVQ7CTC72xp6O"],"title":"Questions to consider regarding the norms and values of those involved in the study and alignment of these to the research","category":"Strategies","name":"recf50wvya0NXxdxz","tags":["reflection-questions"],"created_at":"2023-05-18T13:54:19.000Z","description":"“Who is involved in the study? § What are the ethical expectations of the community/participants/authors? § What is the ethical stance of the researcher? (For example, a mismatch between the ethical stance of the researcher and the community/participant/author may create ethical complications).18 § What are the ethical traditions of researchers’ and/or author/participants’ cultures or countries? § If research data is housed in a repository for reuse, how might individuals or communities be affected later? For example, data collected for one purpose might be reused later for a different purpose but the researcher’s relationship with the community from which the data came no longer exists. What possible risk or harm might result from reuse and publication of this information?1” (Markham and Buchanan, 2012, p. 9)","id":"recf50wvya0nxxdxz","dom_id":"item_recf50wvya0nxxdxz"},{"Challenges":["recqpYMjEFJLmyNaN","rect310qem9li3HKK","recA7Kh502s4UKWGo"],"Sources":["recpXl48pJdKDhc6f"],"title":"Corporations should implement \"ethics filters\" throughout the development process","category":"Strategies","name":"recfcXzM3foqFNNGN","tags":[],"created_at":"2023-06-05T10:26:39.000Z","description":"## RecommendationsThe building blocks of such practices include top-down leadership, bottom-up empowerment, ownership, and responsibility, along with the need to consider system deployment contexts and/or ecosystems. Corporations should identify stages in their processes in which ethical considerations, “ethics filters”, are in place before products are further developed and deployed. For instance, if an ethics review board comes in at the right time during the A/IS creation process, it would help mitigate the likelihood of creating ethically problematic designs. The institution of an ethical A/IS corporate culture would accelerate the adoption of the other recommendations within this section focused on business practices.## Further Resources•[ACM Code of Ethics and Professional Ethics,](https://ethics.acm.org/2018-code-draft-2/) which includes various references to human well-being and human rights, 2018.•Report of UN Special Rapporteur on [Freedom of Expression. _AI and Freedom of Expression_.](http://undocs.org/A/73/348) 2018.•The [website of the Benefit corporations ](https://www.bcorporation.net/)(B-corporations) provides a good overview of a range of companies that personify this type of culture.•R. Sisodia, J. N. Sheth and D. Wolfe, [Firms of ](http://www.firmsofendearment.com/)[Endearment](http://www.firmsofendearment.com/)_, _2nd edition. Upper Saddle River, NJ: FT Press, 2014. This book showcases how companies embracing values and a stakeholder approach outperform their competitors in the long run.\"p.127\"Organizations should identify points for formal review during product development. These reviews can focus on “red flags” that have been identified in advance as indicators of risk. For example, if the datasets involve minors or focus on users from protected classes, then it may require additional justification or alterations to the research or development protocols.\"## Further Resources•A. Sinclair, “[Approaches to Organizational Culture and Ethics,](https://doi.org/10.1007/BF01845788)” _Journal of Business Ethics, _vol._ _12, no. 1, pp. 63–73, 1993.•Al Y. S. Chen, R. B. Sawyers, and P. F. Williams. “[Reinforcing Ethical Decision Making Through Corporate Culture,](https://link.springer.com/article/10.1023/A:1017953517947)_” Journal of Business Ethics _16, no. 8, pp. 855–865, 1997. • K. Crawford and R. Calo, “[There Is a Blind Spot in AI Research,](http://www.nature.com/news/there-is-a-blind-spot-in-ai-research-1.20805)” _Nature _538, pp. 311–313, 2016. p.132","id":"recfcxzm3foqfnngn","dom_id":"item_recfcxzm3foqfnngn"},{"Principles":["reckb3cgfeDh1EeUP"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Society and democracy","category":"Strategies","name":"recffbHgJO0d179DG","tags":["governance-question"],"created_at":"2023-05-28T19:46:30.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Societal and environmental well-being “Society and democracy: \u003cU+F0FC\u003e Did you assess the broader societal impact of the AI system’s use beyond the individual (end-)user, such as potentially indirectly affected stakeholders?” (High-Level Expert Group on AI, 2019, p. 31)","id":"recffbhgjo0d179dg","dom_id":"item_recffbhgjo0d179dg"},{"Challenges":["recb50cfuQUWDAHZW"],"Sources":["recpXl48pJdKDhc6f"],"title":"Training in skills for adaptability to rapid technological changes informed by improved data regarding labour pattern shifts","category":"Strategies","name":"recg2u6ZXSRiUO8uF","tags":[],"created_at":"2023-06-05T11:49:40.000Z","description":"## \"RecommendationsTo thrive in the A/IS age, workers must be provided training in skills that improve their adaptability to rapid technological changes; programs should be available to any worker, with special attention to the low-skilled workforce. Those programs can be private, that is, sponsored by the employer, or publicly and freely offered through specific public channels and government policies, and should be available regardless of whether the worker is in between jobs or still employed. Specific measures include:•Offering new technical programs, possibly earlier than high school, to increase the workforce capacity to close the skills gap and thrive in employment alongside A/IS. •Creating opportunities for apprenticeships, pilot programs, and scaling up data-driven evidence-based solutions that increase employment and earnings.•Supporting new forms of public-private partnerships involving civil society, as well as new outcome-oriented financial mechanisms, e.g., social impact bonds, that help scale up successful innovations.•Supporting partnerships between universities, innovation labs in corporations, and governments to research and incubate startups for A/IS graduates.23•Developing regulations to hold corporations responsible for employee retraining necessary due to increased automation and other technological applications having impacton the workforce.•Facilitating private sector initiatives by public policy for co-investment in training and retraining programs through tax incentives.•Establishing and resourcing public policies that assure the survival and well-being of workers, displaced by A/IS and automation, who cannot be retrained.•Researching complementary areas, to lay solid foundations for the transformation outlined above.•Requiring more policy research on the dynamics of professional transitions in different labor market conditions.•Researching the fairest and most efficient public-private options for financing labor force transformation due to A/IS.•Developing national and regional future of work strategies based on sound research and strategic foresight.## Further Resources•V. Cerf and D. Norfors, The People-centered Economy: The New Ecosystem for Work. California: IIIJ Foundation, 2018.•Executive Office of the President. _Artificial Intelligence, Automation, and the Economy._ December 20, 2016.•S. Kilcarr, “Defining the American Dream for Trucking ... and the Nation, Too,” _FleetOwner_, April 26, 2016.•M. Mason, “Millions of Californians’ Jobs could be Affected by Automation—a Scenario the next Governor has to Address,”_Los Angeles Times_, October 14, 2018.•OECD, “Labor Market Programs: Expenditure and Participants,” _OECD Employment and Labor Market Statistics _(database), 2016.•M. Vivarelli, “Innovation and Employment: ASurvey,” Institute for the Study of Labor (IZA) Discussion Paper No. 2621, February 2007.\"p.147-149## RecommendationsWhile there is evidence that robots and automation are taking jobs away in various sectors, a more balanced, granular, analytical, and objective treatment of A/IS impact on the workforce is needed to effectively inform policy making and essential workforce reskilling. Specifics to accomplish this include:•Creating an international and independent agency able to properly disseminate objective statistics and inform the media, as well as the general public, about the impact of robotics and A/IS on jobs, tax revenue, growth,26 and well-being.•Analyzing and disseminating data on how current task content of jobs have changed, based on a clear assessment of the automatability of the occupationaldescription of such jobs.•Promoting automation with augmentation, as recommended in the _Future of Jobs Report 2018_ (see chart on page 154), to maximize the benefit of A/IS to employment and meaningful work.•Integrating more granulated dynamic mapping of the future jobs, tasks, activities, workplace-structures, associated work-habits, and skills base spurred by the A/IS revolution, in order to innovate, align, and synchronize skill development and training programs with future requirements. This workforce mapping is needed at the macro, but also crucially at the micro, levels where labor market programsare deployed.•Considering both product and process innovation, and looking at them from a global perspective in order to understand properly the global impact of A/IS on employment.•Proposing mechanisms for redistribution of productivity increases and developing an adaptation plan for the evolving labor market.## Further Resources•E. Brynjolfsson and A. McAfee. The Second Age of Machine Intelligence: Work Progress and Prosperity in a Time of Brilliant Technologies. New York, NY: W. W. Norton \u0026 Company, 2014.•P.R. Daugherty, and H.J. Wilson, Human + Machine: Reimagining Work in the Age of AI_. _Watertown, MA:_ _Harvard Business Review Press, 2018.•International Federation of Robotics. “The Impact of Robots on Productivity, Employment and Jobs,” A positioning paper by the International Federation of Robotics, April 2017.•RockEU. “Robotics Coordination Action for Europe Report on Robotics and Employment,” Deliverable D3.4.1, June 30, 2016.•World Economic Forum, Centre for the New Economy and Society, _The Future of Jobs 2018_, Geneva: WEF 2018.\"150-152","id":"recg2u6zxsriuo8uf","dom_id":"item_recg2u6zxsriuo8uf"},{"Principles":["rec42P8U9usfYCtv9"],"Sources":["rec6r8OkE2Q2EdiM3"],"title":"Questions to consider in key stages of AI and machine learning based research, regarding project data governance","category":"Strategies","name":"recgNdOlcMTiussUk","tags":["reflection-questions","data-governance","internet-research"],"created_at":"2023-05-19T12:45:30.000Z","description":"The close of the project includes data archiving and model storage for future access, deployment and development, and for third parties to replicate similar results on other datasets or reuse the dataset (e.g. with permission) on other research questions. Established repositories such as data archives can assist researchers in post-research data governance.__Post-research Data Governance Systems and research design will never be as robust as intended. To mitigate unforeseen risks, researchers must be prepared and manage the unknown, also after the project has been completed. For example, when a dataset containing sensitive information is disclosed by a third-party unexpectedly, researchers must alert data subjects so they can take precautions.\u003cU+25CF\u003eAre the datasets and models stored securely? \u003cU+25CF\u003eAre some datasets more sensitive than others and do they warrant special security precautions? \u003cU+25CF\u003eWill the data be destroyed at a specific date? How will this data be destroyed? Or will they be anonymized and archived at a specific date? \u003cU+25CF\u003eHow might the data and model be accessed through an application process and what potential harm to data subjects and/or society might this future access have? \u003cU+25CF\u003eIs there a containment policy for unexpected breaches or malicious uses and what does it oblige the researcher to do or will this responsibility go to the archival organization? \u003cU+25CB\u003e Will researchers contact the data subjects and/or the relevant privacy regulator directly about a breach?\u003cU+25CB\u003e To what extent does this depend on the seriousness of the disclosure or the sensitivity of the data? \u003cU+25CB\u003e How will harmed data subjects or stakeholders be compensated? (franzke, 2020, p.46-47)","id":"recgndolcmtiussuk","dom_id":"item_recgndolcmtiussuk"},{"Principles":["recsvi4LnhEEPyQ1h","recPg7Ov0priGGtLm"],"Sources":["recQiVQ7CTC72xp6O"],"title":"Questions to consider regarding data management and participant re-identification","category":"Strategies","name":"recgWFCdfcVaeaPQO","tags":["reflection-questions"],"created_at":"2023-05-19T07:41:10.000Z","description":"“How are data being managed, stored, and represented? § What method is being used to secure and manage potentially sensitive data? § What unanticipated breaches might occur during or after the collection and storage of data or the production of reports?20 (For example, if an audience member recorded and posted sensitive material presented during an in-house research presentation, what harms might result? § If the researcher is required to deposit research data into a repository for future use by other researchers (or wishes to do so), what potential risks might arise? What steps should be taken to ensure adequate anonymity of data or to unlink this data from individuals? § What are the potential ethical consequences of stripping data of personally identifiable information? § How might removal of selected information from a dataset distort it such that it no longer represents what it was intended to represent? § If future technologies (such as automated textual analysis or facial recognition software) make it impossible to strip personally identifiable information from data sets in repositories, what potential risks might arise for individuals? Can this be addressed by the original researcher? If so, how? How will this impact subsequent researchers and their data management?” (Markham and Buchanan, 2012, p. 9-10)","id":"recgwfcdfcvaeapqo","dom_id":"item_recgwfcdfcvaeapqo"},{"Challenges":["recdmBNNa98cN8Sda"],"Principles":["recLHILkx2JDFsLbX"],"Sources":["recfYC5jjPmpLfSlM","recpXl48pJdKDhc6f"],"title":"Principle of transparency, key considerations","category":"Strategies","name":"recgn2UvSD4OhzGI4","tags":["reflection-discussion"],"created_at":"2023-05-19T11:54:47.000Z","description":"\"It is important to remember that _transparency as a principle of AI ethics _differs a bit in meaning from the everyday use of the term. The common dictionary understanding of transparency defines it as _either _(1) the quality an object has when one can see clearly through it or (2) the quality of a situation or process that can be clearly justified and explained because it is open to inspection and free from secrets.Transparency as a principle of AI ethics encompasses _both _of these meanings:On the one hand, transparent AI involves the interpretability of a given AI system, i.e. the ability to know how and why a model performed the way it did in a specific context and therefore to understand the rationale behind its decision or behaviour. This sort of transparency is often referred to by way of the metaphor of ‘opening the black box’ of AI. It involves _content clarification and intelligibility _or explicability.On the other hand, transparent AI involves the justifiability both of the processes that go into its design and implementation and of its outcome. It therefore involves the _soundness of the justification of its use_. In this more normative meaning, transparent AI is _practically justifiable _in an unrestricted way if one can demonstrate that both the design and implementation processes that have gone into the particular decision or behaviour of a system and the decision or behaviour itself are ethically permissible, non-discriminatory/fair, and worthy of public trust/safety-securing._Three critical tasks for designing and implementing transparent AI___This two-pronged definition of transparency as a principle of AI ethics asks that you to think about transparent AI both in terms of the _process _behind it (the design and implementation practices that lead to an algorithmically supported outcome) and in terms of its _product _(the content and justification of that outcome). Such a process/product distinction is crucial, because it clarifies the three tasks that your team will be responsible for in safeguarding the transparency of your AI project:·Process Transparency, Task 1: Justify Process. In offering an explanation to affected stakeholders, you should be able to demonstrate that considerations of ethical permissibility, non-discrimination/fairness, and safety/public trustworthiness were operative end-to-end in the design and implementation processes that lead to an automated decision or behaviour. This task will be supported both by following the best practices outlined herein throughout the AI project lifecycle and by putting into place robust auditability measures through an accountability-by-design framework.·Outcome Transparency, Task 2: Clarify Content and Explain Outcome. In offering an explanation to affected stakeholders, you should be able to show in plain language that is understandable to non-specialists how and why a model performed the way it did in a specific decision-making or behavioural context. You should therefore be able to clarify and communicate the rationale behind its decision or behaviour. This explanation should be _socially meaningful _in the sense that the terms and logic of the explanation should not simply reproduce the formal characteristics or the technical meanings and rationale of the mathematical model but should rather be translated into the everyday language of human practices and therefore be understandable in terms of the societal factors and relationships that the decision or behaviour implicates.·Outcome Transparency, Task 3: Justify Outcome. In offering an explanation to affected stakeholders, you should be able to demonstrate that a specific decision or behaviour of your system is ethically permissible, non-discriminatory/fair, and worthy of public trust/safety- securing. This outcome justification should take the content clarification/explicated outcome from task 2 as its starting point and weigh that explanation against the justifiability criteria adhered to throughout the design and use pipeline: ethical permissibility, non- discrimination/fairness, and safety/public trustworthiness. Undertaking an optimal approach to process transparency from the start should support and safeguard this demand for normative explanation and outcome justification.\"Leslie, 2019, p.30-31## \"Process Transparency: Establishing a Process-Based Governance FrameworkThe central importance of the end-to-end operability of good governance practices should guide your strategy to build out responsible AI project workflow processes. Three components are essential to creating a such a responsible workflow: (1) Maintaining strong regimes of professional and institutional transparency; (2) Having a clear and accessible Process-Based Governance Framework (PBG Framework); (3) Establishing a well-defined auditability trail in your PBG Framework through robust activity logging protocols that are consolidated digitally in a process log.1\\.Professional and Institutional Transparency: At every stage of the design and implementation of your AI project, team members should be held to rigorous standards of conduct that secure and maintain professionalism and institutional transparency. These standards should include the core values of integrity, honesty, sincerity, neutrality, objectivity and impartiality. All professionals involved in the research, development, production, and implementation of AI technologies are, first and foremost, acting as fiduciaries of the public interest and must, in keeping with these core values of the Civil Service, put the obligations to serve that interest above any other concerns.Furthermore, from start to finish of the AI project lifecycle, the design and implementation process should be as transparent and as open to public scrutiny as possible with restrictions on accessibility to relevant information limited to the reasonable protection of justified public sector confidentiality and of analytics that may tip off bad actors to methods of gaming the system of service provision.2\\.Process-Based Governance Framework: So far, this guide has presented some of the main steps that are necessary for establishing responsible innovation practices in your AI project. Perhaps the most vital of these measures is the effective operationalisation of the values and principles that underpin the development of ethical and safe AI. By organising all of your governance considerations and actions into a PBG Framework, you will be better able to accomplish this task.The purpose of a PBG Framework is to provide a template for the integrations of the norms, values, and principles, which motivate and steer responsible innovation, with the actual processes that characterise the AI design and development pipeline. While the accompanying Guide has focused primarily on the Cross Industry Standard Process for Data Mining (CRISP-DM), keep in mind that such a structured integration of values and principles with innovation processes is just as applicable in other related workflow models like Knowledge Discovery in Databases (KDD) and Sample, Explore, Modify, Model, and Assess (SEMMA).Your PBG Framework should give you a landscape view of the governance procedures and protocols that are organising the control structures of your project workflow. Constructing a good PBG Framework will provide you and your team with a big picture of:·The relevant team members and roles involved in each governance action.·The relevant stages of the workflow in which intervention and targeted consideration are necessary to meet governance goals·Explicit timeframes for any necessary follow-up actions, re-assessments, and continual monitoring·Clear and well-defined protocols for logging activity and for instituting mechanisms to assure end-to-end auditability\"(Leslie, 2019, p31-33)\"1.Enabling Auditability with a Process Log: With your controls in place and your governance framework organised, you will be better able to manage and consolidate the information necessary to assure end-to-end auditability. This information should include both the records and activity-monitoring results that are yielded by your PBG Framework and the model development data gathered across the modelling, training, testing, verifying, and implementation phases.By centralising your information digitally in a process log, you are preparing the way for optimal process transparency. A process log will enable you to make available, in one place, information that may assist you in demonstrating to concerned parties and affected decision subjects both the responsibility of design and use practices and the justifiability of the outcomes of your system’s processing behaviour.Such a log will also allow you to differentially organise the accessibility and presentation of the information yielded by your project. Not only is this crucial to preserving and protecting data that legitimately should remain unavailable for public view, it will afford your team the capacity to cater the presentation of results to different tiers of stakeholders with different interests and levels of expertise. This ability to curate your explanations with the user- receiver in mind will be vital to achieving the goals of interpretable and justifiable AI.## Outcome transparency: Explaining outcomeand clarifying contentBeyond enabling process transparency through your PBG Framework, you must also put in place standards and protocols to ensure that clear and understandable explanations of the outcomes of your AI system’s decisions, behaviours, and problem-solving tasks can: 1\\.Properly inform the evidence-based judgments of the implementers that they are designed to support;2\\.Be offered to affected stakeholders and concerned parties in an accessible way.This is a multifaceted undertaking that will demand careful forethought and participation across your entire project team.There is no simple technological solution for how to effectively clarify and convey the rationalebehind a model’s output in a particular decision-making or behavioural context. Your team will have to use sound judgement and common sense in order to bring together the technical aspects of choosing, designing, using a sufficiently interpretable AI system and the delivery aspects of being able to clarify and communicate in plain, non-technical, and socially meaningful language how and why that system performed the way it did in a specific decision-making or behavioural context.Having a good grasp of the rationale and criteria behind the decision-making and problem-solving behaviour of your system is essential for producing safe, fair, and ethical AI. If your AI model is not sufficiently interpretable—if you aren’t able to draw from it humanly understandable explanations of the factors that played a significant role in determining its behaviours—then you may not be able to tell how and why things go wrong in your system when they do.This is a crucial and unavoidable issue for reasons we have already explored. Ensuring the safety of high impact systems in transportation, medicine, infrastructure, and security requires human verification that these systems have properly learned the critical tasks they are charged to complete. It also requires confirmation that when confronted with unfamiliar circumstances, anomalies, and perturbations, these systems will not fail or make unintuitive errors. Moreover, ensuring that these systems operate without causing discriminatory harms requires effective ways to detect and to mitigate sources of bias and inequitable influence that may be buried deep within their feature spaces, inferences, and architectures. Without interpretability each one of these tasks necessary for delivering safe and morally justifiable AI will remain incomplete.Defining Interpretable AITo gain a foothold in both the technical and delivery dimensions of AI interpretability, you will first need a solid working definition of what interpretable AI is. To this end, it may be useful to recall once again the definition of AI offered in the accompanying Guide: ‘Artificial Intelligence is the science of _making computers do things that require intelligence when done by humans_.’This characterisation is important, because it brings out an essential feature of the explanatory demands of interpretable AI: to do things that require intelligence when done by humans means to do things that require _reasoning processes and cognitive functioning_. This cognitive dimension has a direct bearing on how you should think about offering suitable explanations about algorithmically generated outcomes:Explaining an algorithmic model’s decision or behaviour should involve making explicit how the particular set of factors which determined that outcome can play the role of evidence in supporting the conclusion reached. It should involve making intelligible to affected individuals the rationale behind that decision or behaviour as if it had been produced by a reasoning, evidence-using, and inference-making person.What makes this explanation-giving task so demanding when it comes to AI systems is that reasoning processes do not occur, for humans, at just one level. Rather, human-scale reasoning and interpreting includes:1\\.Aspects of logic (applying the basic principles of validity that lie behind and give form to sound thinking): _This aspect aligns with the need for formal or logical explanations of AI systems.___2\\.Aspects of semantics (gaining an understanding of how and why things work the way they do and what they mean): _This aspect aligns with the need for explanations of the technical rationale behind the outcomes AI systems.___3\\.Aspects of the social understanding of practices, beliefs, and intentions (clarifying the content of interpersonal relations, societal norms, and individual objectives): _This aspect aligns with the need for the clarification of the socially meaningful content of the outcomes of AI systems.___4\\.Aspects of moral justification (making sense of what should be considered right and wrong in our everyday activities and choices): _This aspect aligns with the justifiability of AI systems.___There are good reasons why _all four of these dimensions of human reasoning processes _must factor in to explaining the decisions and behaviours of AI systems: First and most evidently, understanding the logic and technical innerworkings (i.e. semantic content) of these systems is a precondition for ensuring their safety and fairness. Secondly, because they are designed and used to achieve human objectives and to fulfil surrogate cognitive functions _in the everyday social world_, we need to make sense of these systems in terms of the consequential roles that their decisions and behaviours play in that human reality. The social context of these outcomes matters greatly. Finally, because they actually affect individuals and society in direct and morally consequential ways, we need to be able to understand and explain their outcomes not just in terms of their mathematical logic, technical rationale, and social context but also in terms of the justifiability of their impacts on people.Delving more deeply into the technical and delivery aspects of interpretable AI will show how these four dimensions of human reasoning directly line up with the different levels of demand for explanations of the outcomes of AI systems. In particular, the logical and semantic dimensions will weigh heavily in technical considerations whereas the social and moral dimensions will be significant at the point of delivery.Note here, though, that these different dimensions of human reasoning are not necessarily mutually exclusive but build off and depend upon each other in significant and cascading ways. Approaching explanations of interpretable AI should therefore be treated holistically and inclusively. Technical explanation of the logic and rationale of a given model, for instance, should be seen as a support for the context-based clarification of its socially meaningful content, just as that socially meaningful content should be viewed as forming the basis of explaining an outcome’s moral justifiability. When considering how to make the outcomes of decision-making and problem-solving AI systems maximally transparent to affected stakeholders, you should take this rounded view of human reasoning into account, because it will help you address more effectively the spectrum of concerns that these stakeholders may have.Technical aspects of choosing, designing, and using an interpretable AI systemKeep in mind that, while, on the face of it, the task of choosing between the numerous AI and machine learning algorithms may seem daunting, it need not be so. By sticking to the priority of outcome transparency, you and your team will be able to follow some straightforward and simple guidelines for selecting sufficiently interpretable but optimally performing algorithmic techniques.Before exploring these guidelines, it is necessary to provide you with some background information to help you better understand what facets of explanation are actually involved in technically interpretable AI. A good grasp of what is actually needed from such an explanation will enable you to effectively target the interpretability needs of your AI project.Facets of explanation in technically interpretable AI: A good starting point for understanding how the technical dimension of explanation works in interpretable AI systems is to remember that these systems are largely mathematical models that carry out step-by-step computations in transforming sets of statistically interacting or independent inputs into sets of target outputs. Machine learning is, at bottom, just applied statistics and probability theory fortified with several other mathematical techniques. As such, it is subject to same methodologically rigorous requirements of logical validation as other mathematical sciences.Such a demand for rigour informs the facet of formal and logical explanation of AI systems that is sometimes called the _mathematical glass box_. This characterisation refers to the transparency of strictly formal explanation: No matter how complicated it is (even in the case of a deep neural net with a hundred million parameters), an algorithmic model is a closed system of effectively computable operations where rules and transformations are mechanically applied to inputs to determine outputs. In this restricted sense, all AI and machine learning models are fully intelligible and mathematically transparent if only _formally and logically _so.This is an important characteristic of AI systems, because it makes it possible for supplemental and eminently interpretable computational approaches to model, approximate, and simplify even the most complex and high dimensional among them. In fact, such a possibility fuels some of the technical approaches to interpretable AI that will soon be explored.This formal way of understanding the technical explanation of AI and machine learning systems, however, has immediate limitations. It can tell us that a model is mathematically intelligible because it operates according to a collection of fixed operations and parameters, but it cannot tell us much about how or why the components of the model transformed a specified group of inputs into their corresponding outputs. It cannot tell us anything about the _rationale behind the algorithmic generation of a given outcome_.This second dimension of technical explanation has to do with the _semantic facet _of interpretable AI. A semantic explanation offers an interpretation of the functions of the individual parts of the algorithmic system in the generation of its output. Whereas formal and logical explanation presents an account of the stepwise application of the procedures and rules that comprise the formal framework of the algorithmic system, semantic explanation helps us to understand the meaning of those procedures and rules in terms of their purpose in the input-output mapping operation of the system, i.e. what role they play in determining the outcome of the model’s computation.The difficulties surrounding the interpretability of algorithmic decisions and behaviours arise in this semantic dimension of technical explanation. It is easiest to illustrate this by starting from the simplest case.When a machine learning model is very basic, the task of following the rationale of how it transforms a given set of inputs into a given set of outputs can be relatively unproblematic. For instance, in the simple linear regression, _y = a + bx + e_, with a single predictor variable _x _and a response variable _y_, the predictive relationship of x to y is directly expressed in a regression coefficient _b_, representing the rate and direction at which _y _is predicted to change as x changes. This hypothetical model is completely interpretable from the technical perspective for the following reasons:·Linearity: Any change in the value of the predictor variable is directly reflected in a change in the value of the response variable at a constant rate _b_. The interpretable prediction yielded by the model can therefore be directly inferred_. _This linearity dimension of predictive models has been an essential feature of the automated decision-making systems in many heavily regulated and high-impact sectors, because the predictions yielded have high inferential clarity and strength.·Monotonicity: When the value of the predictor changes in a given direction, the value of the response variable changes consistently either in the same or opposite direction. The interpretable prediction yielded by the model can thus be directly inferred. This monotonicity dimension is also a highly desirable interpretability condition of predictive models in many heavily regulated sectors, because it incorporates reasonable expectations about the consistent application of sector specific selection constraints into automated decision-making systems. So, for example, if the selection criteria to gain employment at an agency or firm includes taking an exam, a reasonable expectation of outcomes would be that if candidate A scored better than candidate B, then candidate B, all other things being equal, would not be selected for employment when A is not. A monotonic predictive model that uses the exam score as the predictor variable and application success as the response variable would, in effect, guarantee this expectation is met by disallowing situations where A scores better than B but B gets selected and A does not.·Non-Complexity: The number of features (dimensionality) and feature interactions is low enough and the mapping function is simple enough to enable a clear ‘global’ understanding of the function of each part of the model in relation to its outcome.While, all three of these desirable interpretability characteristics of the imagined model allow for direct and intuitive reasoning about the relation of the predictor and response variables, the model itself is clearly too minimal to capture the density of relationships and interactions between attributes in complex real-world situations where some degree of noisiness is unavoidable and the task of apprehending the subtleties of underlying data distributions is tricky. In fact, one of the great strides forward that has been enabled by the contemporary convergence of expanding computing power and big data availability with more advanced machine learning models has been exactly this capacity to better capture and model the intricate and complicated dynamics of real-world situations. Still, this incorporation of the complexity of scale into the models themselves has also meant significant challenges to the semantic dimension of the technical explanation of AI systems.As machine learning systems have come to possess both ever greater access to big data and increasing computing power, their designers have correspondingly been able both to enlarge the feature spaces (the number of input variables) of these systems and to turn to gradually more complex mapping functions. In many cases, this has meant vast improvements in the predictive and classificatory performance of more accurate and expressive models, but this has also meant the growing prevalence of non-linearity, non-monotonicity, and high-dimensional complexity in an expanding array of so-called ‘black-box’ models.Once high-dimensional feature spaces and complex functions are introduced into machine learning systems, the effects of changes in any given input become so entangled with the values and interactions of other inputs that understanding how individual components are transformed into outputs becomes extremely difficult. The complex and unintuitive curves of the decision functions of many of these models preclude linear and monotonic relations between their inputs and outputs.Likewise, the high-dimensionality of their optimisation techniques—frequently involving millions of parameters and complex correlations—ranges well beyond the limits of human-scale cognition and understanding.\"Leslie, 2019, p33-39\"These rising tides of computational complexity and algorithmic opacity consequently pose a key challenge for the responsible design and deployment of safe, fair, and ethical AI systems: how should the potential to advance the public interest through the implementation of high performing but increasingly uninterpretable machine learning models be weighed against the tangible risks posed by the lack of interpretability of such systems? A careful answer to this question is, in fact, not so simple. While the trade-off between performance and interpretability may be real and important in _some domain-specific applications_, in others there exist increasingly sophisticated developments of standard interpretable techniques such as regression extensions, decision trees, and rule lists that may prove just as effective for use cases where the need for transparency is paramount. Furthermore, supplemental interpretability tools, which function to make ‘black box’ models more semantically and qualitatively explainable are rapidly advancing day by day.These are all factors that you and your team should consider as you work together to decide on which models to use for your AI project. As a starting point for those considerations, let us now turn to some basic guidelines that may help you to steer that dialogue toward points of relevance and concern.Guidelines for designing and delivering a sufficiently interpretable AI systemYou should use the table below to begin thinking about how to integrate interpretability into your AI project. While aspects of this topic can become extremely technical, it is important to make sure that dialogue about making your AI system interpretable remains multidisciplinary and inclusive.Moreover, it is crucial that key stakeholders be given adequate consideration when deciding upon the delivery mechanisms of your project. These should include policy or operational design leads, the technical personnel in charge of operating the trained models, the implementers of the models, and the decision subjects, who are affected by their outcomes.Note that the first three guidelines focus on the big picture issues you will need to consider in order to incorporate interpretability needs into your project planning and workflow, whereas the last two guidelines shift focus to the user-centred requirements of designing and implementing a sufficiently interpretable AI system.\" (Leslie, 2019, p.39-40)## Guidelines for designing and delivering a sufficiently interpretable AI system   Guideline 1: Look first to context, potential impact, and domain-specific need when determining the interpretability requirements of your projectThere are several related factors that should be taken into account as you formulate your project’sapproach to interpretability:1\\. Type of application: Start by assessing both the kind of tool you are building and the environment in which it will apply. Clearly there is a big difference between a computer vision system that sorts handwritten employee feedback forms and one that sorts safety risks at a security checkpoint. Likewise, there is a big difference between a random forest model that triages applicants at a licencing agency and one that triages sick patients in an emergency department.Understanding your AI system’s purpose and context of application will give you abetter idea of the stakes involved in its use and hence also a good starting point to thinkabout the scope of its interpretability needs. For instance, low-stakes AI models that are     not safety-critical, do not directly impact the lives of people, and do not process potentially sensitive social and demographic data will likely have a lower need for extensive resources to be dedicated to a comprehensive interpretability platform.2\\.Domain specificity: By acquiring solid domain knowledge of the environment in which your AI system will operate, you will gain better insight into any potential sector-specific standards of explanation or benchmarks of justification which should inform your approach to interpretability. Through such knowledge, you may also obtain useful information about organisational and public expectations regarding the scope, content, and depth of explanations that have been previously offered in relevant use cases.3\\.Existing technology: If one of the purposes of your AI project is to replace an existing algorithmic technology that may not offer the same sort of expressive power or performance level as the more advanced machine learning techniques that you are planning to deploy, you should carry out an assessment of the performance and interpretability levels of the existing technology. Acquiring this knowledge will provide you with an important reference point when you are considering possible trade-offs between performance and interpretability that may occur in your own prospective system. It will also allow you to weigh the costs and benefits of building a more complex system with higher interpretability-support needs in comparison to the costs and benefits of using a simpler model.   Guideline 2: Draw on standard interpretable techniques when possibleIn order to actively integrate the aim of sufficient interpretability into your AI project, your team should approach the model selection and development process with the goal of finding the right fit between (1) domain-specific risks and needs, (2) available data resources and domain knowledge, and (3) task appropriate machine learning techniques. Effectively assimilating these three aspects of your use case requires open-mindedness and practicality.Often times, it may be the case that high-impact, safety-critical, or other potentially sensitive environments heighten demands for the thoroughgoing accountability and transparency of AI projects. In some of these instances, such demands may make choosing standard but sophisticated non-opaque techniques an overriding priority. These techniques may include decisions trees, linear regression and its extensions like generalised additive models, decision/rule lists, case-based reasoning, or logistic regression. In many cases, reaching for the ‘black box’ model first may not be appropriate and may even lead to inefficiencies in project development, because more interpretable models, which perform very well but do not require supplemental tools and techniques for facilitating interpretable outcomes, are also available.Again, solid domain knowledge and context awareness are key components here. In use cases where data resources lend to well-structured, meaningful representations and domain expertise can be incorporated into model architectures, interpretable techniques may often be more desirable than opaque ones. Careful data pre-processing and iterative model development can, in these cases, hone the accuracy of such interpretable systems in ways that may make the advantages gained by the combination of their performance and transparency outweigh thebenefits of more semantically intransparent approaches.     In other use cases, however, data processing needs may disqualify the deployment of these sorts of straightforward interpretable systems. For instance, when AI applications are sought for classifying images, recognising speech, or detecting anomalies in video footage, the most effective machine learning approaches will likely be opaque. The feature spaces of these kinds of AI systems grow exponentially to hundreds of thousands or even millions of dimensions. At this scale of complexity, conventional methods of interpretation no longer apply. Indeed, it is the unavoidability of hitting such an interpretability wall for certain important applications of supervised, unsupervised, and reinforcement learning that has given rise to an entire subfield of machine learning research which focuses on providing technical tools to facilitate interpretable and explainable AI.When the use of ‘black box’ models best fits the purpose of your AI project, you should proceed diligently and follow the procedures recommended in Guideline 3. For clarity, let us define a ‘black box’ model as any AI system whose innerworkings and rationale are opaque or inaccessible to human understanding. These systems may include neural networks (including recurrent, convolutional, and deep neural nets), ensemble methods (an algorithmic technique such as the random forest method that strengthens an overall prediction by combining and aggregating the results of several or many different base models), and support vector machines (a classifier that uses a special type of mapping function to build a divider between two sets of features in a high dimensional feature space).   Guideline 3: When considering the use of ‘black box’ AI systems, you should:1\\.Thoroughly weigh up impacts and risks;2\\.Consider the options available for supplemental interpretability tools that will ensure a level of semantic explanation which is both _domain appropriat_e and _consistent with the design and implementation of safe, fair, and ethical AI_;3\\.Formulate an interpretability action plan, so that you and your team can put adequate forethought into how explanations of the outcomes of your system’s decisions, behaviours, or problem-solving tasks can be optimally provided to users, decision subjects, and other affected parties.It may be helpful to explore each of these three suggested steps of assessing the viability of theresponsible design and implementation of a ‘black box’ model in greater detail.(1)Thoroughly weigh up impacts and risks: Your first step in evaluating the feasibility of using a complex AI system should be to focus on issues of ethics and safety. As a general policy, you and your team should utilise ‘black box’ models only:·where their potential impacts and risks have been thoroughly considered in advance, and you and your team have determined that your use case and domain specific needs support the responsible design and implementations of these systems;   ·where supplemental interpretability tools provide your system with a domain appropriate level of semantic explainability that is reasonably sufficient to mitigate its potential risks and that is therefore consistent with the design and implementation of safe, fair, and ethical AI.(2) Consider the options available for supplemental interpretability tools: Next, you and your team should assess whether there are technical methods of explanation-support that _both _satisfy the specific interpretability needs of your use case as determined by the deliberations suggested in Guideline 1 _and _are appropriate for the algorithmic approach you intend to use. You should consult closely with your technical team at this stage of model selection. The exploratory processes of trial-and-error, which often guide this discovery phase in the innovation lifecycle, should be informed and constrained by a solid working knowledge of the technical art of the possible in the domain of available and useable interpretability approaches.The task of lining up the model selection process with the demands of interpretable AI requires a few conceptual tools that will enable thoughtful evaluation of whether proposed supplemental interpretability approaches sufficiently meet your project’s explanatory needs. First and most importantly, you should be prepared to ask the right questions when evaluating any given interpretability approach. This involves establishing with as much clarity as possible how the explanatory results of that approach can contribute to the user’s ability to offer solid, coherent, and reasonable accounts of the rationale behind any given algorithmically generated output. Relevant questions to ask that can serve this end are:·What sort of explanatory resources will the interpretability tool provide users and implementers in order (1) to enable them to exercise better-informed evidence-based judgments and (2) to assist them in offering plausible, sound, and reasonable accounts of the logic behind algorithmically generated output to affected individuals and concerned parties?·Will the explanatory resources that the interpretability tool offers be useful for providing affected stakeholders with a sufficient understanding of a given outcome?·How, if at all, might the explanatory resources offered by the tool be misleading or confusing?You and your team should take these questions as a starting point for evaluating prospective interpretability tools. These tools should be assessed in terms of their capacities to render the reasoning behind the decisions and behaviours of the uninterpretable ‘black box’ systems sufficiently intelligible to users and affected stakeholders given use case and domain specific interpretability needs.Keeping this in mind, there are two technical dimensions of supplemental interpretability approaches that should be systematically incorporated into evaluation processes at this stage of the innovation workflow. The first involves the possible explanatory strategies you choose to pursue over the course of the design and implementation lifecycle. Such strategies will largely determine the paths to understanding you will be able to provide for its users and decision subjects. They will largely define _how you explain your model and its outcomes _and hence _what kinds of explanation you are able offer_.The second involves the coverage and scope of the actual explanations themselves. The choices you make about explanatory coverage will determine the extent to which the kinds of explanations you are planning to pursue will address _single instances _of the model’s outputs or range more broadly to cover the _underlying rationale of its behaviour in general and across instances_. Choices you make about explanatory coverage will largely govern the extent to which your AI system is locally and/or globally interpretable.The very broad-brushed overview of these two dimensions that follows is just meant to orient you to some of the basic concepts in an expanding field of research, so that you are more prepared for working with your technical team to think through the strengths and weaknesses of various approaches. Note, additionally, that this is a rapidly developing area. Relevant members of your team should keep abreast of the latest developments in the field of interpretable AI or XAI (Explainable AI):Two technical dimensions of supplemental interpretability approaches:1\\.Determining explanatory strategies: To achieve the goal of securing a sufficiently interpretable AI system, you and your team will need to get clear on how to explain your model and its outcomes. The explanatory strategies you decide to pursue will shape the paths to understanding you are able to provide for the users of your model and for its decision subjects.There are four such explanatory strategies to which you should pay special attention:a)_Internal explanation: _Pursuing the internal explanation of an opaque model involves making intelligible how the components and relationships within it function. There are two ways that such a goal of internal explanation can be interpreted. On the one hand, it can be seen as an endeavour to explain the operation of the model by considering it globally _as a comprehensible whole_. Here, the aspiration is to ‘pry open the black box’ by building an explanatory model that enables a full grasp of the opaque system’s internal contents. The strengths and weaknesses of such an approach will be discussed in the next section on global interpretability.On the other hand, the search for internal explanation can indicate the pursuit a kind of _engineering insight_. In this sense, internal explanation can be seen as attempting to shed descriptive and inferential light on the parts and operation of the system as a whole in order to try to make it work better. Acquiring this sort of internal understanding of the more general relationships that the working parts of a trained model have with patterns of its responses can allow researchers to advance step-by-step in gaining a better data scientific grasp on why it does what it does and how to improve it. Similarly, this type of internal explanation can be seen as attempting to shed light on an opaque model’s operation by breaking it down into more understandable, analysable, and digestible parts (for instance, in the case of a DNN: into interpretable characteristics of its vectors, features, layers, parameters, etc.).From a practical point of view, this kind of aspiration to _engineering insight _in the ends of data scientific advancement should inform the goals of your technical team throughout the model selection and design workflow.Numerous methods exist to help provide informative representations of the innerworkings of various ‘black box’ systems. Gaining a clearer descriptive understanding of the internal composition of your system will contribute greatly to your project’s ability to achieve a higher degree of outcome transparency and to its capacity to foster best practices in the pursuit of responsible data science in general.b)_External or post-hoc explanation: _External or post-hoc explanation attempts to capture essential attributes of the observable behaviour of a ‘black box’ system by subjecting it to a number of different techniques that reverse engineer explanatory insight. Some post-hoc approaches test the sensitivity of the outputs of an opaque model to perturbations in its inputs; others allow for the interactive probing of its behavioural characteristics; others, still, build proxy- based models that utilise simplified interpretable techniques to gain a better understanding of particular instances of its predictions and classifications.This external or post-hoc approach has, at present, established itself in machine learning research as a go-to explanatory strategy and for good reason. It allows data scientists to pose mathematical questions to their opaque systems by testing them and by building supplemental models which enable greater insight through the inferences drawn from their experimental interventions. Such a post-hoc approach allows them, moreover, to seek out evidence for the reasoning behind a given opaque model’s prediction or classification by utilising maximally interpretable techniques like linear regression, decision trees, rule lists, or case-based reasoning. Several examples of post-hoc explanation will be explored below in the section on local interpretability.Take note initially though that, as some critics have rightly pointed out, because they are approximations or simplified supplemental models of the more complex originals, many post-hoc explanations can fail to accurately represent certain areas of the opaque model’s feature space. This deterioration ofaccuracy in parts of the original model’s domain can frequently producemisleading and uncertain results in the post-hoc explanations of concern.c)_Supplemental explanatory infrastructure_: A different kind of explanatory strategy involves actually incorporating secondary explanatory facilities into the system you are building. For instance, an image recognition system could have a primary component, like a convolutional neural net, that extracts features from its inputs and classifies them while a secondary component, like a built-in recurrent neural net with an ‘attention-directing’ mechanism, translates the extracted features into a natural language representation that produces a sentence-long explanation of the result to the user. In other words, a system like this is designed to provide simple explanations of its own data processing results.Research into integrating ‘attention-based’ interfaces like this in AI systems is continuing to advance toward making their implementations more sensitive to user needs, more explanation-forward, and more human-understandable. For instance, multimodal methods of combining visualisation tools and textual interface are being developed that may make the provision of explanations more interpretable for both implementers and decision subjects. Furthermore, the incorporation of domain knowledge and logic-based or convention-based structures into the architectures of complex models are increasingly allowing for better and more user-friendly representations and prototypes to be built into them. This is gradually enabling more sophisticated explanatory infrastructures to be integrated into opaque systems and makes it essential to think about building explanation-by-design into your AI projects.d)_Counterfactual explanation_: While counterfactual explanation is a kind of post- hoc approach, it deserves special attention insofar as it moves beyond other post-hoc explanations to provide affected stakeholders with clear and precise options for actionable recourse and practical remedy.Counterfactual explanations are contrastive explanations: They offer succinct computational reckonings of how specific factors that influenced an algorithmic decision can be changed so that better alternatives can be realised by the subject of that decision. Incorporating counterfactual explanations into your AI system at its point of delivery would allow stakeholders to see what input variables of the model can be modified, so that the outcome could be altered to their benefit. Additionally, from a responsible design perspective, incorporating counterfactual explanation into the development and testing phases of your system would allow your team to build a model that incorporates _actionable variables_, i.e. input variables that will afford decision subjects with concise options for making practical changes that would improve their chances of obtaining the desired outcome. Counterfactual explanatory strategies can be used as way to incorporate reasonableness and the encouragement of agency into the design and implementation of your AI project.All that said, it is important to recognise that, while counterfactual explanation does offer an innovative way to contrastively explore how feature importance may influence an outcome, it is not a complete solution to the problem of AI interpretability. In certain cases, for instance, the sheer number of potentially significant features that could be at play in counterfactual explanations of a given result can make a clear and direct explanation difficult to obtain and selected sets of explanations seem potentially arbitrary. Moreover, there are as yet limitations on the types of datasets and functions to which these kinds of explanations are applicable. Finally, because this kind of explanation concedes the opacity of the algorithmic model outright, it is less able to address concerns about potentially harmful feature interactions and multivariate relationshipsthat may be buried deep within the model’s architecture.Here is an at-a-glance view of a typology of these explanatory strategies:2\\.Coverage and Scope: The main questions you will need to broach in the dimension of the coverage and scope of your supplemental interpretability approach are: To what extent does our interpretability approach cover the explanation of _singe predictions or classifications _of the model and to what extent does it cover the explanation of the _innerworkings and rationale of the model as a whole and across predictions_? To what extent does it cover both?This distinction between single instance and total model explanation is often characterised as the difference between local interpretability and the global interpretability. Both types of explanation offer potentially helpful support for the provision of significant information about the rationale behind an algorithmic decision or behaviour, but both, in their own ways, also face difficulties.Local Interpretability: A local semantic explanation aims to enable the interpretability of individual cases. The general idea behind attempts to explain a ‘black box’ system in terms of specific instances is that, regardless of how complex the architecture or decision function of that system may be, it is possible to gain interpretive insight into its innerworkings by focusing on single data points or neighbourhoods in its feature space. In other words, even if the high dimensionality and curviness of a model makes it opaque _as a whole_, there is an expectation that insight-generating interpretable methods can be applied _locally _to smaller sections of the model, where changes in isolated or grouped variables are more manageable and understandable.This general explanatory perspective has yielded several different interpretivestrategies that have been successfully applied in significant areas of ‘black box’ machine learning. One family of such strategies has zeroed in on neural networks (DNNs, in particular) by identifying what features of an input vector’s data points make it representative of the target concept that a given model is trying to classify. So, for example, if we have a digital image of a dog that is converted into a vector of pixel values and then processed it through a dog-classifying deep neural net, this interpretive approach will endeavour to tell us why the system yielded a ‘dog- positive’ output by isolating the slices of this set of data points that are most relevant to its successful classification by the model.This can be accomplished in several related ways. What is called sensitivity analysis identifies the most relevant features of an input vector by calculating local gradients to determine how a data point has to be moved to change the output label. Here, an output’s sensitivity to such changes in input values identifies the most relevant features. Another method to identify feature relevance that is downstream from sensitivity analysis is called salience mapping, where a strategy of moving backward through the layers of a neural net graph allows for the mapping of patterns of high activation in the nodes and ultimately generates interpretable groupings of salient input variables that can be visually represented in a heat or pixel attribution map.A second local interpretive strategy also seeks to explain feature importance in a single prediction or classification by perturbing input variables. However, instead of using these nudges in the feature space to highlight areas of saliency, it uses them to prod the opaque model in the area around the relevant prediction, so that a supplemental interpretable model can be constructed which establishes the relative importance of features in the black box model’s output.The most well-known example of this strategy is called LIME (Local Interpretable Model-Agnostic Explanation). LIME works by fitting an interpretable model to a specific prediction or classification produced by the opaque system of concern. It does this by sampling data points at random around the target prediction or classification and then using them to build a local approximation of the decision boundary that can account for the features which figure prominently in the specific prediction or classification under scrutiny.The way this works is relatively uncomplicated: LIME generates a simple linear regression model by weighting the values of the data points, which were produced by randomly perturbing the opaque model, according to their proximity to the original prediction or classification. The closest of these values to the instance being explained are weighted the heaviest, so that the supplemental model can produce an explanation of feature importance that is locally faithful to that instance. Note that the type of model that LIME uses most prominently is a sparse linear regression function for reasons of semantic transparency that were discussed above. Other interpretable models such as decision trees can likewise be employed.While LIME does indeed appear to be a step in the right direction for the future of interpretable AI, a host of issues that present challenges to the approach remains unresolved. For instance, the crucial aspect of how to properly define the proximity measure for the ‘neighbourhood’ or ‘local region’ where the explanation applies remains unclear, and small changes in the scale of the chosen measure can lead to greatly diverging explanations. Likewise, the explanation produced by the supplemental linear model can quickly become unreliable even with small and virtually unnoticeable perturbations of the system it is attempting to approximate. This challenges the basic assumption that that there is always some simplified linear model that successfully approximates the underlying model reasonably well near any given data point.LIME’s creators have largely acknowledged these shortcomings and have recently offered a new explanatory approach that they call ‘anchors’. These ‘high precision rules’ incorporate into their formal structures ‘reasonable patterns’ that areoperating within the underlying model (such as the implicit linguistic conventions that are at work in a sentiment prediction model), so that they can establish suitable and faithful boundaries of their explanatory coverage of its predictions or classifications.A related and equally significant local interpretive strategy is called SHAP (Shapley Additive exPlanations). SHAP uses concepts from game theory to define a ‘Shapley value’ for a feature of concern that provides a measurement of its influence on the underlying model’s prediction. Broadly, this value is calculated for a feature byaveraging its marginal contribution to _every possible prediction _for the instance under consideration.This might seem impossible, but the strategy is straightforward. SHAP calculates the marginal contribution of the relevant feature for all possible combinations of inputs in the feature space of the instance. So, if the opaque model that it is explaining has 15 features, SHAP would calculate the marginal contribution of the feature under consideration 32,768 times (i.e. one calculation for each combination of all possible combinations of features: 215, or 2_k _when _k = _15).This method then allows SHAP to estimate the Shapley values for all input features in the set to produce the complete distribution of the prediction for the instance. In our example, this would entail 491,520 calculations. While such a procedure is computationally burdensome and becomes intractable beyond a certain threshold, this means that _locally_, that is, for the calculation of the specific instance, SHAP can axiomatically guarantee the consistency and accuracy of its reckoning of the marginal effect of the feature. (Note that the SHAP platform does offer methods of approximation to avoid this excessive computational expense.)Despite this calculational robustness, SHAP also faces some of the same kinds of difficulties that LIME does. The way SHAP calculates marginal contributions is by constructing two instances: the first instance includes the feature being measured while the second leaves it out. After calculating the prediction for each of these instances by plugging their values into the underlying model, the result of the second is subtracted from that of the first to determine the marginal contribution of the feature. This procedure is then repeated for all possible combinations of features so that the weighted average of all of the marginal contributions of the feature of concern can be computed.The contestable part of this process comes with how SHAP defines the _absence _of variables under consideration. To leave out a feature—whether it’s the one being directly measured or one of the others not included in the combination under consideration—SHAP replaces it with a _stand-in feature value _drawn from a selected donor sample (that is itself drawn from the existing dataset). This method of sampling values assumes feature independence (i.e. that values sampled are not correlated in ways that might significantly affect the output for a particular calculation). As a consequence, the interaction effects engendered by and between stand-in variables are necessarily unaccounted for when conditional contributions are approximated.The result is the introduction of uncertainty into the explanation that is produced because the complexity of multivariate interactions in the underlying model may not be sufficiently captured by the simplicity of this supplemental interpretability technique. This drawback in sampling (as well as a certain degree of arbitrariness in domain definition) can cause SHAP to become unreliable even with minimal perturbations of the model it is approximating.Despite these limitations in the existing tools of local interpretability, it is important that you think ‘local-first’ when considering the issue of the coverage and scope of the explanatory approaches you plan to incorporate into your project. Being able to provide explanations of specific predictions and classifications is of paramountimportance both to securing optimal outcome transparency and also to ensuring that your AI system will be implemented responsibly and reasonably.Global interpretability: The motivation behind the creation of local interpretability tools like LIME or SHAP (as well as many others not mentioned here) has derived, at least in part, from a need to find a way of avoiding the kind of difficult _double bind _faced by the alternative approach to the coverage and scope of interpretable AI: global interpretability.On the prevailing view, providing a global explanation of a ‘black box’ model entails offering an alternative interpretable model that captures the innerworkings and logic of a ‘black box’ model _in sum _and across predictions or classifications. The difficulty faced by global interpretability arises in the seemingly unavoidable trade-off between the need for the global explanatory model to be sufficiently simple so that it is understandable by humans and the need for that model to be sufficiently complex so that it can capture the intricacies of how the mapping function of a ‘black box’ model works as a whole. While this is clearly a real problem that appears to be theoretically inevitable, it is important to keep in mind that, _from a practical standpoint_, a serviceable notion of global interpretability need not be limited to such a conceptual puzzle. There are at least two less ambitious but more constructive ways to view global interpretability as a potentially meaningful contributor to the responsible design and implementation of interpretable AI.First, many useful attempts have already been made at building explanatory models that employ interpretable methods (like decision trees, rule lists, and case-based classification) to globally approximate neural nets, tree ensembles, and support vector machines. These results have enabled a deeper understanding of the way human interpretable logics and conventions (like if-then rules and representationally generated prototypes) can be measured against or mapped onto high dimensional computational structures and even allow for some degree of targeted comprehensibility of the logic of their parts.This capacity to ‘peek into the black box’ is of great practical importance in domains where trust, user-confidence, and public acceptance are critical for the realisation optimal outcomes. Moreover, this ability to move back and forth between interpretable architectures and high-dimensional processing structures can enable knowledge discovery as well as insights into the kinds of dataset-level and population-level patterns, which are crucial for well-informed macroscale decision- making in areas ranging from public health and economics to the science of climate change.Being able to uncover global effects and relationships between complex model behaviour and data distributions at the demographic and ecological level may prove vital for establishing valuable and practically useful knowledge about unobservable but significant biophysical and social configurations. Hence, although these models have not solved the understandability-complexity puzzle as such, they have opened up new pathways for innovative thinking in the applied data sciences that may be of immense public benefit in the future.Secondly, as mentioned above, under the auspices of the aspiration to engineering insight, a _descriptive and analytical kind of global interpretability _can be seen as a driving force of data scientific advancement. When seen through a practitioner- centred lens, this sort of global interpretability allows data scientists to take a wide- angled and discovery-oriented view of a ‘black box’ model’s relationship to patterns that arise across the range of its predictions. Figuring out how an opaque system works and how to make it work better by more fully understanding these patterns is a continuous priority of good research. So too is understanding the relevance of features and of their complex interactions through dataset level measurement and analysis. These dimensions of incorporating the explanatory aspirations of global interpretability into best practices of research and innovation should be encouraged in your AI project.   (3) Formulate an interpretability action plan: The final step you will need to take to ensure aresponsible approach to using ‘black box’ models is to formulate an interpretability action plan so that you and your team can put adequate forethought into how explanations of the outcomes of your system’s decisions, behaviours, or problem-solving tasks can be optimally provided to users, decision subjects, and other affected parties.This action plan should include the following:·A clear articulation of the explanatory strategies your team intends to use and a detailed plan that indicates the stages in the project workflow when the design and development of these strategies will need to take place.·A succinct formulation of your explanation delivery strategy, which addresses the special provisions for clear, simple, and user-centred explication that are called for whensupplemental interpretability tools for ‘black box’ models are utilised. See more about delivery and implementation in Guideline 5.·A detailed timeframe for evaluating your team’s progress in executing its interpretability action plan and a role responsibility list, which maps in detail the various task-specific responsibilities that will need to be fulfilled to execute the plan.   Guideline 4: Think about interpretability in terms of the capacities of human understandingWhen you begin to deliberate about the specific scope and content of your interpretability platform, it is important to reflect on what it is that you are exactly aiming to do in making your model sufficiently interpretable. A good initial step to take in this process is to think about what makes even the simplest explanations clear and understandable. In other words, you should begin by thinking about interpretability in terms of the capacities and limitations of human cognition.From this perspective, it becomes apparent that even the most straightforward model like a linear regression function or a decision tree can become uninterpretable when its dimensionality presses beyond the cognitive limits of a thinking human. Recall our example of the simple linear regression: 𝑦 = 𝑎 + 𝑏𝑥 + 𝜖. In this instance, only one feature _x _relates to the response variable _y_, so understanding the predictive relationship is easy. The model is parsimonious.However, if we started to add more features as covariates, even though the model would remain linear and hence intuitively predictable, being able to understand the relationship between the response variable and all the predictors and their coefficients (feature weights) would quickly become difficult. So, say we added ten thousand features and trained the model: 𝑦 = 𝑎 + 𝑏0𝑥0 +𝑏1𝑥1 + \u003cU+22EF\u003e + 𝑏10000𝑥10000 + 𝜖. Understanding _how _this model’s prediction comes about—what role each of the individual parts play in producing the prediction—would become difficult because of a certain cognitive limit in the quantity of entities that human thinking can handle at any given time. This model would lose a significant degree of interpretability.Seeing interpretability as a continuum of comprehensibility that is dependent on the capacities and limits of the individual human interpreter should key you in to what is needed in order todeliver an interpretable AI system. Such limits to consider should include not only cognitive boundaries but also varying levels of access to relevant vocabularies of explanation; an explanation about the results of a trained model that uses a support vector machine to divide a 26-dimensional feature space with a planar separator, for instance, may be easy to understand for a technical operator or auditor but entirely inaccessible to a non-specialist. Offering good explanations should take expertise level into account. Your interpretability platform should be cognitively equitable.\"(Leslie, 2019, p.40-53)  ## IEEE report\"for transparency from implementation to deployment## BackgroundWhen A/IS become part of social communities and behave according to the norms of their communities, people will want to understand the A/IS decisions and actions, just as they want to understand each other’s decisions and actions. This is particularly true for morally significant actions or omissions: an ethical reasoning system should be able to explain its own reasoning to a user on request. Thus, transparency, or “explainability”, of A/IS is paramount (Chaudhuri 201727; Wachter, Mittelstadt, and Floridi 201728), and it will allow a community to understand, predict, and modify the A/IS (see Section 1, Issue 2; for a nuanced discussion see Selbst and Barocas29). Moreover, as the norms embedded in A/IS are continuously updated and refined (see Section 1, Issue 2), transparency allows for appropriate trust to be developed (Grodzinsky, Miller, and Wolf 201130), and, where necessary, allows the community to modify a system’s norms, reasoning, and behavior.Transparency can occur at multiple levels, e.g., ordinary language or coder verification, and for multiple stakeholders, e.g., user, engineer, and attorney. (See [IEEE P7001](https://standards.ieee.org/develop/project/7001.html)™, IEEE Standards Project for Transparency of Autonomous Systems). It should be noted that transparency to all parties may not always be advisable, such as in the case of security programs that prevent a system from being hacked (Kroll et al. 201631). Here we briefly illustrate the broad range of transparency by reference to four ways in which systems can be transparent—traceability, verifiability, honest design, and intelligibility—and apply these considerations to the implementation of norms in A/IS._Transparency as traceability_—Most relevant for the topic of implementation is the transparency of the software engineering process during implementation (Cleland-Huang, Gotel, and Zisman201232). It allows for the originally identified norms (Section 1, Issue 1) to be traced through to the final system. This allows technical inspection of which norms have been implemented, for which contexts, and how norm conflicts are resolved, e.g., priority weights given to different norms. Transparency in the implementation process may also reveal biases that were inadvertently built into systems, such as racism and sexism, in search engine algorithms (Noble 201333). (See Section 3, Issue 2.) Such traceability in turn calibrates a community’s trust about whether A/IS are conforming to the norms and values relevant in their use contexts (Fleischmann and Wallace 200534)._Transparency as verifiability_—Transparency concerning how normative reasoning is approached in the implementation is important as we wish to verify that the normative decisions the system makes match the required norms and values. Explicit and exact representations of these normative decisions can then provide the basis for a range of strong mathematical techniques, such as formal verification (Fisher, Dennis, and Webster 201335). Even if a system cannot explain every single reasoning step in understandable human terms, a log of ethical reasoning should be available for inspection of later evaluation purposes (Hind et al. 201836)._Transparency as honest design_—German designer Dieter Rams coined the term “honest design” to refer to design that “does not make a product more innovative, powerful or valuable than it really is” (Vitsoe 201837; see also Donelli 201538; Jong 201739). Honest design of A/IS is one aspect of their transparency, because it allows the user to “see through” the outward appearance and accurately infer the A/IS’ actual capacities. At times, however, the physical appearance of a system does not accurately represent what the system is capable of doing—e.g., the agent displays signs of a certain human-like emotion but its internal state does not represent that human emotion. Humans are quick to make strong inferences from outward appearances of human-likeness to the mental and social capacities the A/IS might have. Demands for transparency in design therefore put a responsibility on the designer to “not attempt to manipulate the consumer with promises that cannot be kept” (Vitsoe 201840)._Transparency as intelligibility_—As mentioned above, humans will want to understand theA/IS’ decisions and actions, especially the morally significant ones. A clear requirement for an ethical A/IS is that the system be able to explain its own reasoning to a user, when asked—or, ideally, also when suspecting the user’s confusion, and the system should do so at a level of ordinary human reasoning, not with incomprehensible technical detail (Tintarev and Kutlak 201441). Furthermore, when the system cannot explain some of its actions, technicians or designers should be available to make those actions intelligible. Along these lines, the European Union’s General Data Protection Regulation (GDPR), in effect since May 2018, states that, for automated decisions based on personal data, individuals have a right to “an explanation of the [algorithmic] decision reached after such assessment and to challenge the decision”. (See boyd [sic] 201642, for a critical discussion of this regulation.)## RecommendationA/IS, especially those with embedded norms, must have a high level of transparency, shown as traceability in the implementation process, mathematical verifiability of their reasoning, honesty in appearance-based signals,and intelligibility of the systems’ operationand decisions.## Further Resources•d. boyd, “[Transparency \u003cU+2260\u003e Accountability](https://points.datasociety.net/transparency-accountability-3c04e4804504).”_Data \u0026 Society: Points_, November 29, 2016.•A. Chaudhuri, “ Philosophical Dimensions of Information and Ethics in the Internet of Things (IoT) Technology,”The EDP Audit, Control, and SecurityNewsletter, vol. 56, no. 4, pp. 7-18, DOI:10.1080/07366981.2017.1380474, 2017.•J. Cleland-Huang, O. Gotel, and A. Zisman, eds. _Software and Systems Traceability_. London: Springer, 2012. doi:10.1007/978-1-4471-2239-5•G. Donelli, “Good design is honest.” (blog). March 13, 2015. Accessed Oct 22, 2018. [https://blog.astropad.com/good-design-ishonest/ ](https://blog.astropad.com/good-design-is-honest/)•M. Fisher, L. A. Dennis, and M. P. Webster. “Verifying Autonomous Systems.” _Communications of the ACM_, vol. 56,no. 9, pp. 84–93, 2013.•K. R. Fleischmann and W. A. Wallace. “ACovenant with Transparency: Opening the Black Box of Models.” _Communications of the ACM_, vol. 48, no. 5, pp. 93–97, 2005.•F. S. Grodzinsky, K. W. Miller, and M. J. Wolf.“Developing Artificial Agents Worthy of Trust:Would You Buy a Used Car from This Artificial Agent?” _Ethics and Information Technology_, vol. 13, pp. 17–27, 2011.•M. Hind, et al. “Increasing Trust in AI Services through Supplier’s Declarations of Conformity.” _ArXiv E-Prints_, Aug. 2018. [Online] Available: [https://arxiv.org/abs/1808.07261.](https://arxiv.org/abs/1808.07261) [Accessed October 28, 2018].•C. W. De Jong, ed., _Dieter Rams: Ten Principles for Good Design_. New York, NY: Prestel Publishing, 2017.•J. A. Kroll, J. Huey, S. Barocas et al. “Accountable Algorithms.” University of Pennsylvania Law Review 165 2017.•S. U. Noble, “Google Search: Hyper-Visibility as a Means of Rendering Black Women and Girls Invisible.” InVisible Culture 19, 2013.• D. Selbst and S. Barocas, “The Intuitive Appeal of Explainable Machines,” _87Fordham Law Review 1085_, Available at SSRN: [https:// ssrn.com/abstract=3126971 ](https://ssrn.com/abstract=3126971)or [http://dx.doi.org/10.2139/ssrn.3126971,](http://dx.doi.org/10.2139/ssrn.3126971) Feb. 19, 2018.•N. Tintarev and R. Kutlak. “Demo: Making Plans Scrutable with Argumentation and Natural Language Generation.” _Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces, _pp. 29–32, 2014.•Vitsoe. “The Power of Good Design.” _Vitsoe_, 2018. Retrieved Oct 22, 2018 from [https:// www.vitsoe.com/us/about/good-design](https://www.vitsoe.com/us/about/good-design).•S.Wachter, B. Mittelstadt, and L. Floridi, “Transparent, Explainable, and Accountable AI for Robotics.” Science Robotics, vol. 2, no. 6, eaan6080. doi:10.1126/scirobotics. aan6080, 2017.\"p.177-179","id":"recgn2uvsd4ohzgi4","dom_id":"item_recgn2uvsd4ohzgi4"},{"Challenges":["recrk0Tgfwe5J9xfI","rececsX8igwNqhhkC"],"Sources":["recpXl48pJdKDhc6f"],"title":"Independent standards body for ethical criteria","category":"Strategies","name":"recgqEckuFTILlOnz","tags":[],"created_at":"2023-06-05T10:57:02.000Z","description":"## RecommendationAn independent, internationally coordinated body—akin to ISO—should be formed to oversee whether A/IS products actually meet ethical criteria, both when designed, developed, deployed, and when considering their evolution after deployment and during interaction with other products. It should also includea certification process.## Further Resources•A. Tutt, “An FDA for Algorithms,” _Administrative Law Review _69, 83–123, 2016.•M. U. Scherer, “[Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies,](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2609777)” _Harvard Journal of Law and Technology _vol._ _29, no. 2, 354–400, 2016.•D. R. Desai and J. A. Kroll, “[Trust But Verify: A Guide to Algorithms and the Law](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2959472).” _Harvard Journal of Law and Technology_, Forthcoming; Georgia Tech Scheller College of Business Research Paper No. 17-19, 2017.p.133-134","id":"recgqeckuftillonz","dom_id":"item_recgqeckuftillonz"},{"Principles":["reczVPIH1y2OMpAJH","rec42P8U9usfYCtv9","rec7n2TGrH9RHYpQj"],"Sources":["recfYC5jjPmpLfSlM"],"title":"Principle of Discriminatory non-harm for fairness, key considerations for design fairness","category":"Strategies","name":"recgswAsiepwEclOd","tags":["reflection-discussion"],"created_at":"2023-05-19T11:12:50.000Z","description":"“Because human beings have a hand in all stages of the construction of AI systems, fairness-aware design must take precautions across the AI project workflow to prevent bias from having a discriminatory influence:- Problem Formulation: At the initial stage of problem formulation and outcome definition, technical and non-technical members of your team should work together to translate project goals into measurable targets. This will involve the use of both domain knowledge and technical understanding to define what is being optimised in a formalisable way and to translate the project’s objective into a target variable or measurable proxy, which operates as a statistically actionable rendering of the defined outcome. At each of these points, choices must be made about the design of the algorithmic system that may introduce structural biases which ultimately lead to discriminatory harm. Special care must be taken here to identify affected stakeholders and to consider how vulnerable groups might be negatively impacted by the specification of outcome variables and proxies. Attention must also be paid to the question of whether these specifications are reasonable and justifiable given the general purpose of the project and the potential impacts that the outcomes of the system’s use will have on the individuals and communities involved. These challenges of fairness aware design at the problem formulation stage show the need for making diversity and inclusive participation a priority from the start of the AI project lifecycle. This involves both the collaboration of the entire team and the attainment of stakeholder input about the acceptability of the project plan. This also entails collaborative deliberation across the project team and beyond about the ethical impacts of the design choices made.- Data Pre-Processing: Human judgment enters into the process of algorithmic system construction at the stage of labelling, annotating, and organising the training data to be utilised in building the model. Choices made about how to classify and structure raw inputs must be taken in a fairness aware manner with due consideration given to the sensitive social contexts that may introduce bias into such acts of classification. Similar fairness aware processes should be put in place to review automated or outsourced classifications. Likewise, efforts should be made to attach solid contextual information and ample metadata to the datasets, so that downstream analyses of data processing have access to properties of concern in bias mitigation.”- Feature Determination and Model-Building: The constructive task of selecting the attributes or features that will serve as input variables for your model involves human decisions be made about what sorts of information may or may not be relevant or rationally required to yield an accurate and unbiased classification or prediction. Moreover, the feature engineering tasks of aggregating, extracting, or decomposing attributes from datasets may introduce human appraisals that have biasing effects. For this reason, discrimination awareness should play a large role at this stage of the AI model-building workflow as should domain knowledge and policy expertise. Your team should proceed in the modelling stage aware that choices made about grouping or separating and including or excluding features as well as more general judgements about the comprehensiveness or coarseness of the total set of features may have significant consequences for vulnerable or protected groups. The process of tuning hyperparameters and setting metrics at the modelling, testing, and evaluation stages also involves human choices that may have discriminatory effects in the trained model. Your technical team should proceed with an attentiveness to bias risk, and continual iterations of peer review and project team consultation should be encouraged to ensure that choices made in adjusting the dials and metrics of the model are in line with bias mitigation and discriminatory non-harm.- Evaluating Analytical Structures: Design fairness also demands close assessment of the existence in the trained model of lurking or hidden proxies for discriminatory features that may act as significant factors in its output. Including such hidden proxies in the structure of the model may lead to implicit ‘redlining’ (the unfair treatment of a sensitive group on the basis of an unprotected attribute or interaction of attributes that ‘stands in’ for a protected or sensitive one). Designers must additionally scrutinise the moral justifiability of the significant correlations and inferences that are determined by the model’s learning mechanisms themselves. In cases of the processing of social or demographic data related to human features, where the complexity and high dimensionality of machine learning models preclude the confirmation of the discriminatory non-harm of these inferences (for reason of their uninterpretability by human assessors), these models should be avoided. In AI systems that process and draw analytics from data arising from human relationships, societal patterns, and complex socioeconomic and cultural formations, designers must prioritise a degree of interpretability that is sufficient to ensure that the inferences produced by these systems are nondiscriminatory. In cases where this is not possible, a different, more transparent and explainable model or portfolio of models should be chosen. Analytical structures must also be confirmed to be procedurally fair. Any rule or procedure employed in an AI system should be consistently and uniformly applied to every decision subject whose information is being processed by that system. Your team should be able to certify that when a rule or procedure has been used to render an outcome for any given individual, the same rule or procedure will be applied to any other individual in the same way regardless of that other subject’s similarities with or differences from the first. Implementers, in this respect, should be able to show that any algorithmic output is replicable when the same rules and procedures are applied to the same inputs. Such a uniformity of the application of rules and procedures secures the equal procedural treatment of decision subjects and precludes any rule-changes in the algorithmic processing targeted at a specific person that may disadvantage that individual vis-à-vis any other.(Leslie, 2019, p. 16-18)","id":"recgswasiepweclod","dom_id":"item_recgswasiepweclod"},{"Cases":["reciNqxyfUgE5XM7t","recmS3zSMbR3ofAR5"],"Challenges":["recdmBNNa98cN8Sda","recHsgB7ki6GknJnX"],"Principles":["recLHILkx2JDFsLbX","recmzjcGKv3yNOxbl","recgDkzdE9dfpTxCK","recwjv8IMAZFWfMSr"],"Sources":["rec9jnxuHOioQn4DC"],"title":"Guiding questions for educators regarding Transparency of AI in Education","category":"Strategies","name":"rechaQXedBh3OsMjZ","tags":["reflection-questions"],"created_at":"2023-05-19T13:36:31.000Z","description":"• Are teachers and school leaders aware of the AI methods and features being utilised by the system? • Is it clear what aspects AI can take over and what not within the system? • Do teachers and school leaders understand how specific assessment or personalisation algorithms work within the AI system? • Are the system processes and outcomes focussed on the expected learning outcomes for the learners? How reliable are the predictions, assessments and classifications of the AI system in explaining and evaluating the relevance of its use? • Are the instructions and information accessible and presented in a way that is clear both for teachers and learners?(European Commission, 2022, p. 19)","id":"rechaqxedbh3osmjz","dom_id":"item_rechaqxedbh3osmjz"},{"Principles":["recint2IxoR8aILCp","recQ9DIFEsOEkCx3O","recLHILkx2JDFsLbX"],"Sources":["recQzldmBLByP78Uu"],"title":"Questions to consider in SoTL research inception","category":"Strategies","name":"recinajIdAe7hRxjN","tags":["reflection-questions","education-research"],"created_at":"2023-05-19T12:56:14.000Z","description":"“Purpose, Participation, and Consent - What is the question or problem you want to investigate, and why is it essential to spend your own and others’ time and energy on it? - Whose consent, permission, cooperation, involvement, or collaboration will be required to conduct your project? How can roles and permissions be negotiated and renegotiated over time? - What concerns might students have about your work and their participation in it? What choices do students have if they are uncomfortable? - Whose perspectives will be represented in the work? How can various perspectives be honoured? What unique concerns do you have about representing individuals or groups with less power in the educational system? - What power relationships need to be considered when negotiating roles, permissions, and involvements by various participants in your work? Are there issues of gender, race, culture, and status difference that need to be considered?” (Fedoruk, 2017, p. 2)","id":"recinajidae7hrxjn","dom_id":"item_recinajidae7hrxjn"},{"Principles":["recPg7Ov0priGGtLm","recOHnq45Fq7YWsRO","reczVPIH1y2OMpAJH"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Quality and integrity of data","category":"Strategies","name":"reciw4r1bRvx6A6Fq","tags":["governance-question"],"created_at":"2023-05-28T19:31:36.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Privacy and data governance Respect for privacy and data Protection: “Quality and integrity of data: \u003cU+F0FC\u003e Did you align your system with relevant standards (for example ISO, IEEE) or widely adopted protocols for daily data management and governance? \u003cU+F0FC\u003e Did you establish oversight mechanisms for data collection, storage, processing and use? \u003cU+F0FC\u003e Did you assess the extent to which you are in control of the quality of the external data sources used? \u003cU+F0FC\u003e Did you put in place processes to ensure the quality and integrity of your data? Did you consider other processes? How are you verifying that your data sets have not been compromised or hacked?” (High-Level Expert Group on AI, 2019, p. 28)","id":"reciw4r1brvx6a6fq","dom_id":"item_reciw4r1brvx6a6fq"},{"Challenges":["recPqsTlFK76oGD9C"],"Principles":["recKWrfJzX52AXSIf","recQ9DIFEsOEkCx3O"],"Sources":["recpXl48pJdKDhc6f"],"title":"Ethics must be embedded in STEM education and professional accreditation","category":"Strategies","name":"reck6xeMUc9jtmLr3","tags":[],"created_at":"2023-06-05T10:14:39.000Z","description":"•Ethics training needs to be a core subjectfor all those in the STEM field, beginning atthe earliest appropriate level and for all advanced degrees.•Effective STEM ethics curricula should be informed by experts outside the STEM community_ _from a variety of cultural and educational backgrounds to ensure that students acquire sensitivity to a diversityof robust perspectives on ethics and design.•Such curricula should teach aspiring engineers, computer scientists, and statisticians about the relevance and impact of their decisions in designing A/IS technologies. Effective ethics education in STEM contexts and beyond should span primary, secondary, and postsecondary education, and include both universities and vocational training schools.•Relevant accreditation bodies should reinforce this integrated approach as outlined above.## Further Resources•[IEEE P7000TM Standards Project for a Model ](https://standards.ieee.org/develop/project/7000.html)[Process for Addressing Ethical Concerns During System Design.](https://standards.ieee.org/develop/project/7000.html) IEEE P7000 aims to enhance corporate IT innovation practices by providing processes for embedding a values- and virtue-based thinking, culture, and practice into them.•Z. Lipton and J. Steinhardt, [Troubling Trends in Machine Learning Scholarship.](https://www.dropbox.com/s/ao7c090p8bg1hk3/Lipton%20and%20Steinhardt%20-%20Troubling%20Trends%20in%20Machine%20Learning%20Scholarship.pdf?dl=0) ICML conference paper, July 2018.•J. Holdren, and M. Smith. “[Preparing for the ](https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_future_of_ai.pdf)[Future of Artificial Intelligence.”](https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_future_of_ai.pdf) Washington, DC: Executive Office of the President, National Science and Technology Council, 2016.•Comparing the UK, EU, and US approaches to AI and ethics: C. Cath, S. Wachter, B. Mittelstadt, et al., “[Artificial Intelligence and the ‘Good Society’: The US, EU, and UK Approach.](https://link.springer.com/article/10.1007/s11948-017-9901-7)” _Science and Engineering Ethics, _vol._ _24_, _pp. 505-528, 2017.p122-123","id":"reck6xemuc9jtmlr3","dom_id":"item_reck6xemuc9jtmlr3"},{"Principles":["recMGB4iC5oaCtr5x","rec42P8U9usfYCtv9","rec6O9e1nYBJtQUTj"],"Sources":["recQzldmBLByP78Uu"],"title":"Questions to consider in SoTL research dissemination","category":"Strategies","name":"reclplj55gpqdYGTr","tags":["reflection-questions","education-research"],"created_at":"2023-05-19T13:01:27.000Z","description":"Results and the Presentation of Results to Various Audiences - What negative or embarrassing data can you anticipate emerging from your scholarship of teaching and learning, and who might be harmed as a consequence? How can you create a context for understanding “bad news”? How in particular, can examples of work by students who are novices or who are struggling with new material be treated with respect? - Who will see the results and products of your work? What conclusions might be drawn by various audiences: About students? About teaching? About your department, discipline, or campus? About higher education? About you? How is your choice of medium (e.g., video recording) related to those concerns? - How can contributions to your work by various participants (including both colleagues and students) be acknowledged and/or cited, while maintaining appropriate confidentiality?(Fedoruk, 2017, p. 2-3)","id":"reclplj55gpqdygtr","dom_id":"item_reclplj55gpqdygtr"},{"Challenges":["recKzhZVabDuYM6rG","recroNon39TCBeC88"],"Sources":["recpXl48pJdKDhc6f"],"title":"Educational data should be classified as sensitive and held in 'escrow' not available for commercial purposes","category":"Strategies","name":"reclzrMrQSQls3PI9","tags":["education-research"],"created_at":"2023-06-05T09:41:58.000Z","description":"\"Educational data offer a unique opportunity to model individuals’ thought processes and could be used to predict or change individuals’ behavior in many situations. Governments and organizations should classify educational dataas being sensitive and implement special protective standards.Children’s data should be held in “escrow”and not used for any commercial purposesuntil a child reaches the age of majority and is able to authorize use as they choose.\"p.115 IEEE report","id":"reclzrmrqsqls3pi9","dom_id":"item_reclzrmrqsqls3pi9"},{"Principles":["reczVPIH1y2OMpAJH","rec42P8U9usfYCtv9","rec7n2TGrH9RHYpQj"],"Sources":["recfYC5jjPmpLfSlM"],"title":"Principle of Discriminatory non-harm for fairness, key considerations for data fairness","category":"Strategies","name":"recmhwo8kmYkBZ7Sy","tags":["reflection-discussion"],"created_at":"2023-05-19T11:03:50.000Z","description":"“Principle of Discriminatory Non-Harm: The designers and users of AI systems, which process social or demographic data pertaining to features of human subjects, societal patterns, or cultural formations, should prioritise the mitigation of bias and the exclusion of discriminatory influences on the outputs and implementations of their models. Prioritising discriminatory non-harm implies that the designers and users of AI systems ensure that the decisions and behaviours of their models do not generate discriminatory or inequitable impacts on affected individuals and communities. This entails that these designers and users ensure that the AI systems they are developing and deploying: 1\\. Are trained and tested on properly representative, relevant, accurate, and generalisable datasets (Data Fairness) 2\\. Have model architectures that do not include target variables, features, processes, or analytical structures (correlations, interactions, and inferences) which are unreasonable, morally objectionable, or unjustifiable (Design Fairness)3\\. Do not have discriminatory or inequitable impacts on the lives of the people they affect (Outcome Fairness) 4\\. Are deployed by users sufficiently trained to implement them responsibly and without bias (Implementation Fairness)” (Leslie, 2019, p. 14)\\*Data Fairness\\*\"Responsible data acquisition, handling, and management is a necessary component of algorithmic fairness. If the results of your AI project are generated by biased, compromised, or skewed datasets, affected stakeholders will not adequately be protected from discriminatory harm. Your project team should keep in mind the following key elements of data fairness:- Representativeness: Depending on the context, either underrepresentation or overrepresentation of disadvantaged or legally protected groups in the data sample may lead to the systematic disadvantaging of vulnerable stakeholders in the outcomes of the trained model. To avoid such kinds of sampling bias, domain expertise will be crucial to assess the fit between the data collected or procured and the underlying population to be modelled. Technical team members should, if possible, offer means of remediation to correct for representational flaws in the sampling. - Fit-for-Purpose and Sufficiency: An important question to consider in the data collection and procurement process is: Will the amount of data collected be sufficient for the intended purpose of the project? The quantity of data collected or procured has a significant impact on the accuracy and reasonableness of the outputs of a trained model. A data sample not large enough to represent with sufficient richness the significant or qualifying attributes of the members of a population to be classified may lead to unfair outcomes. Insufficient datasets may not equitably reflect the qualities that should rationally be weighed in producing a justified outcome that is consistent with the desired purpose of the AI system. Members of the project team with technical and policy competences should collaborate to determine if the data quantity is, in this respect, sufficient and fit-for-purpose. - Source Integrity and Measurement Accuracy: Effective bias mitigation begins at the very commencement of data extraction and collection processes. Both the sources and instruments of measurement may introduce discriminatory factors into a dataset. When incorporated as inputs in the training data, biased prior human decisions and judgments such as prejudiced scoring, ranking, interview-data or evaluation—will become the ‘ground truth’ of the model and replicate the bias in the outputs of the system. In order to secure discriminatory non-harm, you must do your best to make sure your data sample has optimal source integrity. This involves securing or confirming that the data gathering processes involved suitable, reliable, and impartial sources of measurement and sound methods of collection. - Timeliness and Recency: If your datasets include outdated data then changes in the underlying data distribution may adversely affect the generalisability of your trained model. Provided these distributional drifts reflect changing social relationship or group dynamics, this loss of accuracy with regard to the actual characteristics of the underlying population may introduce bias into your AI system. In preventing discriminatory outcomes, you should scrutinise the timeliness and recency of all elements of the data that constitute your datasets. - Relevance, Appropriateness and Domain Knowledge: The understanding and utilisation of the most appropriate sources and types of data are crucial for building a robust and unbiased AI system. Solid domain knowledge of the underlying population distribution and of the predictive or classificatory goal of the project is instrumental for choosing optimally relevant measurement inputs that contribute to the reasonable determination of the defined solution. You should make sure that domain experts collaborate closely with your technical team to assist in the determination of the optimally appropriate categories and sources of measurement.\"(Leslie, 2019, p. 15)\"To ensure the uptake of best practices for responsible data acquisition, handling, and management across your AI project delivery workflow, you should initiate the creation of a Dataset Factsheet at the alpha stage of your project. This factsheet should be maintained diligently throughout the design and implementation lifecycle in order to secure optimal data quality, deliberate bias-mitigation aware practices, and optimal auditability. It should include a comprehensive record of data provenance, procurement, pre-processing, lineage, storage, and security as well as qualitative input from team members about determinations made with regard to data representativeness, data sufficiency, source integrity, data timeliness, data relevance, training/testing/validating splits, and unforeseen data issues encountered across the workflow.\" (Leslie, 2019, p. 16)","id":"recmhwo8kmykbz7sy","dom_id":"item_recmhwo8kmykbz7sy"},{"Principles":["recLHILkx2JDFsLbX","recmzjcGKv3yNOxbl","recSqx6wklVpDzx3s","recOHnq45Fq7YWsRO"],"Sources":["rec6r8OkE2Q2EdiM3"],"title":"Questions to consider in key stages of AI and machine learning based research, regarding the Research Analytical Process","category":"Strategies","name":"recnFHf9V2VtAVJMx","tags":["reflection-questions","project-analysis"],"created_at":"2023-05-19T12:34:45.000Z","description":"\"The research analytical process includes selecting the training data, cleaning the data, developing the model through steps of training, evaluating, adjusting, re-training the model. Source of Training Data The inferences and predictions of an AI system are closely connected to the source of the training data and here especially issues on systemic discrimination or biases are interesting to disclose and reflect upon as many previous studies have shown such effects (Barocas \u0026 Selbst, 2016; Bechmann \u0026 Bowker, 2019; boyd \u0026 Crawford, 2012; Crawford \u0026 Calo, 2016; Kroll et al., 2017; Sweeney, 2013). The use of AI systems to uncover or predict social phenomena can thus be tainted by biases in the training data set on certain demographics or proxies thereof, which may lead to unfair and unjust outcomes.\u003cU+25CF\u003eWhat is the cultural and sociodemographic profile of the datasets used by the researcher to train the models? \u003cU+25CF\u003eTo what extent does the cultural and sociodemographic profile of the training data allow for generalizability of the resulting findings or predictors from the research study? \u003cU+25CF\u003eAre there particular groups which may be advantaged or disadvantaged, in the context in which the researcher is deploying an AI-system? What is the potential damaging effect of uncertainty and error-rates to different groups? \u003cU+25CF\u003eHow has the demographic profile of the researcher(s) affected the composition of the training data? \u003cU+25CF\u003eHow does the training data as ‘ground truth’ affect different demographic profiles and proxies thereof?Data Cleaning Data cleaning is the process of detecting, correcting, replacing and even removing inaccurate and incomplete records from a database and structuring the data in a consistent way that makes it processable in the model. Researchers typically find data cleaning a difficult, timeconsuming, though necessary and important part of creating an AI-system. It is therefore tempting for some to cut corners or otherwise speed up the process, which can lead to concerns about the rigor and validity of the study because it is seldom accounted for in details. The time spent on cleaning a dataset and the assumptions that go into this process should be communicated more clearly in the resulting research paper. A descriptive analysis of the study datasets may help to identify missing information, incorrect coding, outliers, and misaligned data by the reader. \u003cU+25CF\u003eHow would you characterize the datasets and their cleaning processes? For which variables was the cleaning process optimized? (Features, labels etc.) \u003cU+25CF\u003eHow have (small) adjustments to the training data to make data fit into a model logic potentially influenced the outcome of the model calculations and predictions? \u003cU+25CF\u003eIf the researcher used the raw data to train the model, to what extent could the resulting model be inaccurate, inappropriate, or dysfunctional? \u003cU+25CF\u003eSpecifically, which actions have been taken by the research team in the process of cleaning the dataset and what potential consequences do these choices have on the predictions and/or findings made in the study? \u003cU+25CF\u003eHow do the data cleaning actions normalize data and what are the potential consequences of taking out outliers in terms of minority representation in the model? \u003cU+25CF\u003eTo what extent does the data cleaning process reflect the character of the data collected and the context in which it was provided? \u003cU+25CF\u003eWhat actions have been taken to anonymize/pseudonymize the data and to what extent is it possible to de-identify data subjects? Does the anonymization prevent certain types of analysis and what is the argument for the decisions taken? \u003cU+25CF\u003eHow has the data been stored in order to safeguard the privacy of the data subjects? \u003cU+25CF\u003eIf the research team consists of multiple parties and/or distributed calculations how has access to data been negotiated and established in a safe space solution for data subjects? Model The researcher’s model, based on cleaned training data, will likely have utility in predicting behaviours, or finding correlations in datasets. Such inferences may not be tailored to individuals or be based on anonymized data. Ethical issues may still remain, however, with regard to (1) the privacy considerations of groups on their collective behaviour and the resulting shifts of power balances, (2) the automation of inferences or decision-making, and (3) biases as well as errors in the output data. These issues may also arise if researchers choose to work with a pretrained model on different datasets, for instance open source models.**Group Privacy and Power Dynamics **\u003cU+25CF\u003eCan the knowledge that is generated and inferred from the model shift power balances with regard to specific communities and societies in the training data or as data subjects in terms of predictive power over their behaviour?\u003cU+25CF\u003eCould the increased power be operationalized maliciously if the model or inferred data was shared with a third-party, and how could such problems be mitigated? \u003cU+25CF\u003eCould the predictors identified by the model be operationalized maliciously by a third party when published and how could such use potentially be mitigated? \u003cU+25CF\u003eTo what extent is the organization or the AI-system making decisions for data subjects?**Automation **\u003cU+25CF\u003eTo what extent is human deliberation being replaced by automated systems and what consequences does it have for the research results? \u003cU+25CF\u003eCan the researcher override the automated inferences procedure, how will this be documented and justified for later reproducibility?\u003cU+25CF\u003eAre the automated inferences explainable? \u003cU+25CF\u003eIs there a strong incentive for the researcher to take the automated inferences as a base truth? How was the ground-truth identified and is this ground-truth adequate to predict the whole spectrum of the problem and/or population behaviour? \u003cU+25CF\u003eCan the data subjects influence the reach of the AI-system on their lives, their physical environments, and their own decisions? Should the researchers provide such functionality? **Biases and Errors **\u003cU+25CF\u003eTo what extent has the researcher accounted for false positives and false negatives in the output of the model, and to what extent can the researcher mitigate their negative impacts? \u003cU+25CF\u003eCan the researchers use their model to infer social biases and communicate them? \u003cU+25CF\u003eHow have steps of re-training the model to improve accuracy influenced the outcome and what considerations on representation/non-representation have been made in this practice? \u003cU+25CF\u003eIf the research team uses a pretrained model, are the datasets well-documented and how can the character of the datasets influence the predictions of the research in question and the study of another context/practice?**Model Training **\u003cU+25CF\u003eHow many instances of re-training have taken place, what was the reason for each retraining and the result, what were the choices made for changing the settings, and what was the specific type of data added to the training loop? \u003cU+25CF\u003eHow do the re-training choices align with the cultural and sociodemographic profile of the research group, and how does this affect the robustness/generalizability of the predictions and/or the findings of the study? \u003cU+25CF\u003eWhat would be the consequences of manually tweaking certain weights in the model and feeding the model with different training data? How would this affect the predictions of the model? \"(franzke, et al., 2020 p.41-44)","id":"recnfhf9v2vtavjmx","dom_id":"item_recnfhf9v2vtavjmx"},{"Challenges":["rec0BAjUqQrSIMAEG"],"Sources":["recpXl48pJdKDhc6f"],"title":"Foster equitable benefits of AI through global standards, regional investment, and justice with respect to the burdens and benefits of AI development ","category":"Strategies","name":"recnLauSoXMQCSK5c","tags":[],"created_at":"2023-06-05T11:46:23.000Z","description":"## **\"**RecommendationsA/IS benefits should be equally available to populations in HIC and LMIC, in the interest of universal human dignity, peace, prosperity, and planet protection. Specific measures for LMIC should include:•Deploying A/IS to detect fraud and corruption, to increase the transparency of power structures, to contribute to a favorable investment, governance, and innovation environment. •Supporting LMIC in the development of their own A/IS strategies, and in the retention or return of their A/IS talent to prevent “brain drain”.•Encouraging global standardization/ harmonization and open source A/IS software.•Promoting distribution of knowledge and wealth generated by the latest A/IS, including through formal public policy and financial mechanisms to advance equity worldwide.•Developing public datasets to facilitate the access of people from LMIC to data resources to facilitate their applied research, while ensuring the protection of personal data.•Creating A/IS international research centers in every continent, that promote culturally appropriate research, and allow the remote access of LMIC's communities to high-end technology.16•Facilitating A/IS access in LMIC through online courses in local languages.•Ensuring that, along with the use of A/IS, discussions related to identity, platforms, and blockchain are conducted, such that core enabling technologies are designed to meet the economic, social, and cultural needs of LMIC.•Diminishing the barriers and increase LMIC access to technological products, including the formation of collaborative networks between developers in HIC and LMIC, supporting the latter in attending global A/IS conferences.17•Promoting research into A/IS-based technologies, for example, mobile lightweight A/IS applications, that are readily availablein LMIC.•Facilitating A/IS research and development in LMIC through investment incentives, public-private partnerships, and/or joint grants, and collaboration between international organizations, government bodies, universities, and research institutes.•Prioritizing A/IS infrastructure in international development assistance, as necessary to improve the quality and standard of living and advance progress towards the SDGs in LMIC.•Recognizing data issues that may be particular to LMIC contexts, i.e., insufficient sample size for machine learning which sometimes results in _de facto_ discrimination, and inadequate laws for, and the practice of, data protection.•Supporting research on the adaptation ofA/IS methods to scarce data environmentsand other remedies that facilitate an optimalA/IS enabling environment in LMIC.## Further Resources•A. Akubue, “Appropriate Technology for Socioeconomic Development in Third World Countries.” _The Journal of Technology Studies _26, no. 1, pp. 33–43, 2000.•O. Ajakaiye and M. S. Kimenyi. “Higher Education and Economic Development in Africa: Introduction and Overview.” _Journal of African Economies _20, no. 3, iii3–iii13, 2011.•D. Allison-Hope and M. Hodge, \"Artificial Intelligence: A Rights-Based Blueprint for Business,” San Francisco: BSF, Aug. 28, 2018•D. E. Bloom, D. Canning, and K. Chan. _Higher Education and Economic Development in Africa _(Vol. 102). Washington, DC: World Bank, 2006.•N. Bloom, “Corporations in the Age of Inequality.” _Harvard Business Review, _April 21, 2017.•C. Dahlman, _Technology, Globalization, and Competitiveness: Challenges for Developing Countries. Industrialization in the 21st Century_. New York: United Nations, 2006.•M. Fong, _Technology_ _Leapfrogging for Developing Countries. Encyclopedia of Information Science and Technology_, 2nd ed. Hershey, PA: IGI Global, 2009 (pp. 3707– 3713).•C. B. Frey and M. A. Osborne. “The Future of Employment: How Susceptible Are Jobs to Computerisation?” (working paper). Oxford, U.K.: Oxford University, 2013.•B. Hazeltine and C. Bull. _Appropriate Technology: Tools, Choices, and Implications. _New York: Academic Press, 1999.•McKinsey Global Institute. “Disruptive Technologies: Advances That Will Transform Life, Business, and the Global Economy” (report), May 2013.•D. Rotman, “How Technology Is Destroying Jobs.” _MIT Technology Review_, June 12, 2013.•R. Sauter and J. Watson. “Technology Leapfrogging: A Review of the Evidence, A Report for DFID.” Brighton, England: University of Sussex. October 3, 2008.•“The Rich and the Rest.” _The Economist. _October 13, 2012.•“Wealth without Workers, Workers without Wealth.” _The Economist_. October 4, 2014.•World Bank. “Global Economic Prospects 2008: Technology Diffusion in the Developing World.” Washington, DC: World Bank, 2008.•World Development Report 2016: Digital Dividends_. _Washington, DC: World Bank. doi:10.1596/978-1-4648-0671-1.•World Wide Web Foundation “Artificial Intelligence: The Road ahead in Low and Middle-income Countries,” webfoundation.org, June 2017.\"p.145-147","id":"recnlausoxmqcsk5c","dom_id":"item_recnlausoxmqcsk5c"},{"Principles":["recU6u0AZbcNj1ik9"],"Sources":["recQiVQ7CTC72xp6O"],"title":"Questions to consider regarding how a context is defined and conceptualised with respect to relevant stakeholder agency","category":"Strategies","name":"recobVSYWj9jYvbgF","tags":["reflection-questions"],"created_at":"2023-05-18T13:54:19.000Z","description":"“How is the context defined and conceptualized? § Does the research definition of the context match the way owners, users, or members might define it?13 (Parameters such as ‘culture,’ ‘person,’ ‘data set,’ and ‘public text’ each carry different ethical expectations for researchers) § Are there distinctions between local contextual norms for how a venue is conceptualized and jurisdictional frameworks (e.g., Terms of Service, other regulations)? For example, if the TOS defines the space as off limits for researchers but the individuals want to participate in public research of this space, what risk might exist for either the researcher or individuals involved?14 § What are the ethical expectations users attach to the venue in which they are interacting, particularly around issues of privacy? Both for individual participants as well as the community as a whole?” (Markham and Buchanan, 2012, p. 8)","id":"recobvsywj9jyvbgf","dom_id":"item_recobvsywj9jyvbgf"},{"Challenges":["recj64vAVJSm5B2ba"],"Sources":["recpXl48pJdKDhc6f"],"title":"Explicitly consider how AI may foster and hamper SDG progress","category":"Strategies","name":"recouZjokdKQz88z1","tags":[],"created_at":"2023-06-05T11:16:58.000Z","description":"## **\"**RecommendationsThe current range of A/IS applications in sectors crucial to the SDGs, and to excluded populations everywhere, should be studied, with the strengths, weaknesses, and potential of the most significant recent applications analyzed, and the best ones developed at scale. Specific objectives to consider include:•Identifying and experimenting withA/IS technologies relevant to the SDGs,such as: big data for development relevant to, for example, agriculture and medical tele-diagnosis; geographic information systems needed in public service planning, disaster prevention, emergency planning, and disease monitoring; control systems used in, for example, naturalizing intelligent cities through energy and traffic control and management of urban agriculture; applications that promote human empathy focused on diminishing violence and exclusion and increasing well-being.•Promoting the potential role of A/IS in sustainable development by collaboration between national and international government agencies and non-governmental organizations (NGOs) in technology sectors.•Analyzing the cost of and proposing strategies for publicly providing internet access forall, as a means of diminishing the gap inA/IS’ potential benefit to humanity, particularly between urban and rural populations in HIC and LMIC alike.•Investing in the documentation and dissemination of innovative applications ofA/IS that advance the resolution of identified societal issues and the SDGs.•Researching sustainable energy to power A/IS computational capacity.•Investing in the development of transparent monitoring frameworks to track the concrete results of donations by international organizations, corporations, independent agencies, and the State, to ensure efficiency and accountability in applied A/IS.•Developing national legal, policy, and fiscal measures to encourage competition in theA/IS domestic markets and the flourishingof scalable A/IS applications.•Integrating the SDGs into the core of private sector business strategies and adding SDG indicators to companies’ key performance indicators, going beyond corporate social responsibility (CSR).•Applying the well-being indicators10 to evaluate A/IS’ impact from multiple perspectives in HIC and LMIC alike.## Further reading•R. Van Est and J.B.A. Gerritsen, with assistance of L. Kool, Human Rights in the Robot Age: Challenges arising from the use of Robots, Artificial Intelligence and Augmented Reality Expert Report written for the Committee on Culture, Science, Education and Media of the Parliamentary Assembly of the Council of Europe (PACE), The Hague: Rathenau Instituut 2017.•World Economic Forum Global Future Council on Human Rights 2016-18, “White Paper: How to Prevent Discriminatory Outcomes in Machine Learning,” World Economic Forum, March 2018.•United Nations General Assembly, _Transforming Our World: The 2030 Agenda for Sustainable Development_ (A/RES/70/1: 21 October 2015) Preamble. [http://www.un.org/ en/development/desa/population/migration/ generalassembly/docs/globalcompact/ A\\_RES\\_70\\_1\\_E.pdf](http://www.un.org/en/development/desa/population/migration/generalassembly/docs/globalcompact/A_RES_70_1_E.pdf).•United Nations Global Pulse, Big Data for Development: Challenges and Opportunities, 2012.\"p139-140","id":"recouzjokdkqz88z1","dom_id":"item_recouzjokdkqz88z1"},{"Principles":["recsvi4LnhEEPyQ1h","recPg7Ov0priGGtLm"],"Sources":["recQiVQ7CTC72xp6O"],"title":"Questions to consider regarding the nature of the data (as (dis)aggregated, private/public, reidentifiable, etc.)","category":"Strategies","name":"recpUrzG3GpRiwqnz","tags":["reflection-questions"],"created_at":"2023-05-19T07:39:07.000Z","description":"“What is the primary object of study? § What are the ethical expectations commonly associated with these types of data? (For example, working with aggregated, de-identified data carries different ethical expectations than working with interview data.) § Does the object of analysis include persons or texts beyond the immediate parameters outlined by the study? What are the potential ethical consequences and how might these be addressed? (For example, collecting data from a blog often includes comments; collecting data from one social media stream reveals links to people or data outside the specific scope of the study.) § If information collected in the course of a study can be linked back to an individual by means of internet search or other technology, what process will the researcher use to determine how that information will be treated? (For example, many challenges surround the responsible use of images and video). To what extent might data be considered by participants to be personal and private, or public and freely available for analysis and republication? § What other questions might arise as a result of the particular context from which this data was collected?” (Markham and Buchanan, 2012, p. 9)","id":"recpurzg3gpriwqnz","dom_id":"item_recpurzg3gpriwqnz"},{"Principles":["recint2IxoR8aILCp","recheWZC64aZRgmpo","recgDkzdE9dfpTxCK"],"Sources":["rec6r8OkE2Q2EdiM3"],"title":"Questions to consider in key stages of AI and machine learning based research, regarding methodological scope","category":"Strategies","name":"recpvGxQzTNQS1smH","tags":["reflection-questions","project-design"],"created_at":"2023-05-19T12:32:46.000Z","description":"“Assessing Alternative Methodologies and Scope of Research Internet experimentation projects can be scaled to a worldwide level (e.g. Kramer, Guillory \u0026 Hancock, 2014) and engineers are typically incentivized to deploy a project as widely as possible to maximize their reach. It is sometimes also just easier to let a project operate without limitations and to see later which data is collected, rather than limiting its scope artificially. However, the knowledge gained using this collection method can have exposed some problems in specific political and cultural contexts. Risk levels can vary widely based on target countries or for particular target groups (see also Dwork, 2006). Therefore, trying to mitigate the risks and shifts in values in all areas will result in a race to appease the lowest common denominator (or: reduce the utility of the project to appease the context with the highest risk factors). - How can the researcher limit the scope of the research questions and the project’s aim to avoid some risks of harm or negatively affected values? - How can the researcher limit the scope of stakeholders, by excluding particular groups or countries? If so, would the data collected still be a representative sample to answer the research question? - Are any risks averted if the researcher limits the project duration to a shorter amount of operation time? And does this conflict with the ability of the researcher to conduct the research in question?” (franzke et al., 2020, p. 40)","id":"recpvgxqztnqs1smh","dom_id":"item_recpvgxqztnqs1smh"},{"Principles":["recmzjcGKv3yNOxbl","recScYLR2TNiv7iKf","reclPiw2VvNOSTzv5","rec42P8U9usfYCtv9","recSqx6wklVpDzx3s"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Accessibility and universal design","category":"Strategies","name":"recq6GeKNegFQdzS9","tags":["governance-question"],"created_at":"2023-05-28T19:39:26.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Diversity, non-discrimination and fairness “Accessibility and universal design: \u003cU+F0FC\u003e Did you ensure that the AI system accommodates a wide range of individual preferences and abilities? \u003cU+F0A7\u003e Did you assess whether the AI system usable by those with special needs or disabilities or those at risk of exclusion? How was this designed into the system and how is it verified? \u003cU+F0A7\u003e Did you ensure that information about the AI system is accessible also to users of assistive technologies? \u003cU+F0A7\u003e Did you involve or consult this community during the development phase of the AI system? \u003cU+F0FC\u003e Did you take the impact of your AI system on the potential user audience into account? \u003cU+F0A7\u003e Did you assess whether the team involved in building the AI system is representative of your target user audience? Is it representative of the wider population, considering also of other groups who might tangentially be impacted? \u003cU+F0A7\u003e Did you assess whether there could be persons or groups who might be disproportionately affected by negative implications? \u003cU+F0A7\u003e Did you get feedback from other teams or groups that represent different backgrounds and experiences?” (High-Level Expert Group on AI, 2019, p. 30)","id":"recq6geknegfqdzs9","dom_id":"item_recq6geknegfqdzs9"},{"Challenges":["recJV8yTX0MnF3rGD"],"Sources":["recpXl48pJdKDhc6f"],"title":"Legislate to prevent misinformation and hate speech spread","category":"Strategies","name":"recqJAVTtqOUfypNI","tags":[],"created_at":"2023-06-05T11:43:25.000Z","description":"## \"RecommendationsTo protect democracy, respect fundamental rights, and promote sustainable development, governments should implement a legislative agenda which prevents the spread of misinformation and hate speech, by:•Ensuring more control and transparency in the use of A/IS techniques for user profiling in order to protect privacy and prevent user manipulation.•Using A/IS techniques to detect untruthful information circulating in the infrastructures, overseen by a democratic body to prevent potential censorship.•Obliging companies owning A/IS infrastructures to provide more transparency regarding their algorithms, sources of funding, services, and clients.•Defining a new legal status somewhere between \"platforms\" and \"content providers\" for A/IS infrastructures.•Reformulating the deontological codes of the journalistic profession to take into account the intensive use of A/IS techniques foreseenin the future. •Promoting the right to information in official documents, and developing A/IS techniques to automate journalistic tasks such as verification of sources and checking the accuracy of the information in official documents, or in the selection, hierarchy, assessment, and development of news, thereby contributing to objectivity and reliability.## Further Resources•M. Broussard, “Artificial Iintelligence for Investigative Reporting: Using an expert system to enhance journalists’ ability to discover original public affairs stories.” Digital Journalism, vol. 3, no. 6, pp. 814-831, 2015.•M. Carlson, “The robotic reporter: Automated journalism and the redefinition of labor, compositional forms, and journalistic authority.” Digital Journalism, vol. 3, no. 3, pp. 416-431, 2015.•A. López Barriuso, F. de la Prieta Pintado, Á. Lozano Murciego, , D. Hernández de la Iglesia and J. Revuelta Herrero, JOUR-MAS: A Multiagent System Approach to Help Journalism Management, vol. 4, no. 4, 2015.•P. Mozur, ”A Genocide Incited on Facebook with Posts from Myanmar’s Military,” _The New York Times,_ Oct. 15 2018. [https:// www.nytimes.com/2018/10/15/technology/ myanmar-faceboo.k-genocide.html](https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html)•UK Parliament, House of Commons, Digital, Culture, Media and Sport Committee Disinformation and ‘fake news’: Interim Report, Fifth Report of Session 2017–19UK Parliament, Published on July 29, 2018\"p.142-144","id":"recqjavttqoufypni","dom_id":"item_recqjavttqoufypni"},{"Principles":["recOHnq45Fq7YWsRO"],"Sources":["recY4zreDoWrsbsv7"],"title":"Align AI models to a shared vision of education","category":"Strategies","name":"recqY2lEigz82Xmh5","tags":[],"created_at":"2023-05-29T08:14:09.000Z","description":"“AI technologies are grounded in models, and these models are inevitably incomplete in some way. It is up to humans to name educational goals and measure the degree to which models fit and are useful—or don’t fit and might be harmful. Such an assessment of how well certain tools serve educational priorities may seem obvious, but the romance of technology can lead to a “let’s see what the tech can do'' attitude, which can weaken the focus on goals and cause us to adopt models that fit our priorities poorly.” (Cardona et al., 2023, p. 54)“align priorities, educational strategies, and technology adoption decisions to place the educational needs of students ahead of the excitement about emerging AI capabilities.” (Cardona et al., 2023, p. 54)“Every conversation about AI (or any emerging technology) should start with the educational needs and priorities of students front and center and conclude with a discussion about the evaluation of effectiveness re-centered on those needs and priorities. Equity, of course, is one of those priorities that requires constant attention, especially given the worrisome consequences of potentially biased AI models. We especially call upon leaders to avoid romancing the magic of AI or only focusing on promising applications or outcomes, but instead to interrogate with a critical eye how AI-enabled systems and tools function in the educational environment.” (Cardona et al., 2023, p. 54)“we center teaching and learning in all considerations about the suitability of an AI model for an educational use. Humans remain in the loop of defining, refining, and using AI models. We highlight the six desirable characteristics of AI models for education” (Cardona et al., 2023, p. 55)“1. Alignment of the AI Model to Educators’ Vision for Learning: When choosing to use AI in educational systems, decision makers prioritize educational goals, the fit to all we know about how people learn, and alignment to evidence-based best practices in education. 2. Data Privacy: Ensuring security and privacy of student, teacher, and other human data in AI systems is essential. 3. Notice and Explanation: Educators can inspect edtech to determine whether and how AI is being incorporated within edtech systems. Educators’ push for AI models can explain the basis for detecting patterns and/or for making recommendations, and people retain control over these suggestions. 4. Algorithmic Discrimination Protections: Developers and implementers of AI in education take strong steps to minimizing bias and promoting fairness in AI models.” (Cardona et al., 2023, p. 55)“5. Safe and Effective Systems: The use of AI models in education is based on evidence of efficacy (using standards already established in education for this purpose) and work for diverse learners and in varied educational settings. 6. Human Alternatives, Consideration and Feedback: AI models that support transparent, accountable, and responsible use of AI in education by involving humans in the loop to ensure that educational values and principles are prioritized.” (Cardona et al., 2023, p. 56)","id":"recqy2leigz82xmh5","dom_id":"item_recqy2leigz82xmh5"},{"Challenges":["rec3Fm8dyG49YU137"],"Sources":["recpXl48pJdKDhc6f"],"title":"Consider where AI-mediation is, and is not, appropriate with respect to human relationships and autonomy","category":"Strategies","name":"recrBZOfrDC2lKpRM","tags":[],"created_at":"2023-06-05T11:40:38.000Z","description":"## \"Recommendations1\\.It is important that human workers’ interaction with other workers not always be intermediated by affective systems (or other technology) which may filter out autonomy, innovation, and communication.2\\.Human points of contact should remain available to customers and other organizations when using A/IS.3\\.Affective systems should be designed to support human autonomy, sense of competence, and meaningful relationships as these are necessary to support a flourishing life.4\\.Even where A/IS are less expensive, more predictable, and easier to control than human employees, a core network of human employees should be maintained at every level of decision-making in order to ensure preservation of human autonomy, communication, and innovation.5\\.Management and organizational theorists should consider appropriate use of affective and autonomous systems to enhance their business models and the efficacy of their workforce within the limits of the preservation of human autonomy.## Further reading•J. J. Bryson, “Artificial Intelligence and Pro-Social Behavior,” in _Collective Agency and Cooperation in Natural and Artificial Systems, _C. Misselhorn, Ed., pp. 281–306, Springer, 2015.•D. Peters, R.A. Calvo, and R.M. Ryan,“[Designing for Motivation, Engagement and ](https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00797/full)[Wellbeing in Digital Experience](https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00797/full),”_ Frontiers in Psychology_– Human Media Interaction, vol. 9, pp 797, 2018.\"\"p.100-101","id":"recrbzofrdc2lkprm","dom_id":"item_recrbzofrdc2lkprm"},{"Principles":["recgDkzdE9dfpTxCK"],"Sources":["rec6r8OkE2Q2EdiM3"],"title":"Questions to consider in key stages of AI and machine learning based research, regarding merit ","category":"Strategies","name":"recsAAZKwCyesrHK6","tags":["reflection-questions","project-design"],"created_at":"2023-05-19T12:24:49.000Z","description":"\"“Quantitative research is typically guided by stated hypotheses, which may be written down before data collection and analysis begins. In qualitative research, researchers often do not work with clear hypotheses to be tested but instead enter the field in order to learn something about the practice in a particular social setting. They conduct self-reflection and state their assumptions as well as the effect of their presence in the research domain. These processes add accountability and transparency to the research process. In technical domains and data science, even though relying on massive amounts of data, it is also common for researchers to collect data over a period of time without a clear stated goal in order to find strong predictors, correlations or interesting cluster phenomena. We suggest that researchers who use AI systems in their research consider a hybrid of an ex ante hypothesis (still making room for exploration) as well as documenting actions taken and choices made throughout the process along with self-reflections on how this research practice has affected their findings and research questions.- How do the research questions or hypotheses affect and control the outcome? - If the researcher did not store a fixed hypothesis, 1) how has the researcher’s choices been documented? And 2) how has the researcher affected the outcome by choosing this practice (e.g. discussing the presence of proxies and spurious correlations)?” (franzke et al., 2020, p. 38)","id":"recsaazkwcyesrhk6","dom_id":"item_recsaazkwcyesrhk6"},{"Challenges":["rech9vLLbQgO3OY0i","recaFyWRROaJ1ZFyY","recdlFwsToNXtcvGC"],"Principles":["recjViPnz3atRIOpD"],"Sources":["recpXl48pJdKDhc6f"],"title":"Cross-disciplinary work should be incentivised","category":"Strategies","name":"recsonQhKLmX4D1ao","tags":[],"created_at":"2023-06-05T10:19:10.000Z","description":"## RecommendationsFunding models and institutional incentive structures should be reviewed and revised to prioritize projects with interdisciplinary ethics components to encourage integration of ethics into projects at all levels.## Further Resources•S. Barocas, Course Material for Ethics and Policy in Data Science, Cornell University, 2017.•L. Floridi, and M. Taddeo. “What Is Data Ethics?” _Philosophical Transactions of the Royal Society, _vol._ _374, no. 2083, 1–4. DOI[10.1098/ rsta.2016.0360,](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5124072/) 2016.•S. Spiekermann, Ethical IT Innovation: A ValueBased System Design Approach. Boca Raton, FL: Auerbach Publications, 2015.•K. Crawford, “[Artificial Intelligence’s White Guy Problem](http://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html?_r=1)”, _New York Times_, July 25, 2016. [Online]. Available: [http://www.nytimes. com/2016/06/26/opinion/sunday/artificialintelligences-white-guy-problem.html?\\_r=1](http://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html?_r=1). [Accessed October 28, 2018].\"p.123-124","id":"recsonqhklmx4d1ao","dom_id":"item_recsonqhklmx4d1ao"},{"Principles":["recU6u0AZbcNj1ik9"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - human agency","category":"Strategies","name":"recspvYTySr0ANH6j","tags":["governance-question"],"created_at":"2023-05-28T19:19:22.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION)“Human agency: \u003cU+F0FC\u003e Is the AI system implemented in work and labour process? If so, did you consider the task allocation between the AI system and humans for meaningful interactions and appropriate human oversight and control? \u003cU+F0A7\u003e Does the AI system enhance or augment human capabilities? \u003cU+F0A7\u003e Did you take safeguards to prevent overconfidence in or overreliance on the AI system for work processes?” (High-Level Expert Group on AI, 2019, p. 26)","id":"recspvytysr0anh6j","dom_id":"item_recspvytysr0anh6j"},{"Principles":["recOHnq45Fq7YWsRO"],"Sources":["rec6r8OkE2Q2EdiM3"],"title":"Questions to consider in key stages of AI and machine learning based research, regarding reproducibility and replicability","category":"Strategies","name":"recvVsXM40mlnmhqP","tags":["reflection-questions","project-dissemination"],"created_at":"2023-05-19T12:34:49.000Z","description":"The scientific requirements of reproducibility and replicability demand from researchers to describe their experiment in such a way that another person could achieve at least similar results. For social science and humanities research using AI tools, this includes for instance making available the training data, the model and test prediction results if deemed safe for the data subjects (Zimmer, 2010).\u003cU+25CF\u003eCan the researcher make datasets available without violating the privacy of data subjects or revealing other sensitive information? \u003cU+25CF\u003eTo what extent would rigorous anonymization of research data affect the utility of the data to allow for reproducibility and replicability? \u003cU+25CF\u003eWhat exact version of the model did the researcher use, was this model pre-trained and if so, what are the precise specifications of that particular dataset(s), what is the cultural and sociodemographic profile of the dataset?\u003cU+25CF\u003eHas the journal/conference in question established procedures for uploading material to safe space solution or other process for safe access for review purposes? Are there established repositories that offer sharing solutions appropriate for the specific material that might be used? \u003cU+25CF\u003eDoes the journal/conference offer guidance for how to safely and adequately document AI based research? (The NeurIPS conference for example requires completion of a “reproducibility checklist,” https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf) (franzke, 2020, p.44-45)","id":"recvvsxm40mlnmhqp","dom_id":"item_recvvsxm40mlnmhqp"},{"Challenges":["recvWM2glArsVhaye","recdZI38VrUaUKRYf"],"Sources":["recpXl48pJdKDhc6f"],"title":"Sustainability should be part of AI impact design and evaluation","category":"Strategies","name":"recwQrzqPp6C7jyJs","tags":[],"created_at":"2023-06-05T11:23:27.000Z","description":"**\"**A/IS creators need to recognize and prioritize the stewardship of the Earth’s natural systems to promote human and ecological well-being. Specifically:_ _•Human well-being should be defined to encompass ecological health, access to nature, safe climate and natural environments, biosystem diversity, and other aspects of a healthy, sustainable natural environment.•A/IS systems should be designed to use, support, and strengthen existing ecological sustainability standards with a certification or similar system, e.g., [LEED,](https://new.usgbc.org/leed) [Energy Star,](https://www.energystar.gov/) or [Forest Stewardship Council.](https://us.fsc.org/en-us) This directs automation and machine intelligence to follow the principle of doing no harm and to safeguard environmental, social, and economic systems.•A/IS creators should prioritize doing no harm to the Earth’s natural systems, both intended and unintended harm.•A committee should be convened to issue findings on ways in which A/IS can be used by business, NGOs, and governmental agencies to promote stewardship and restoration of natural systems while reducing the harmful impact of economic development on ecological sustainability and environmental justice.## Further reading•D. Austin and M. Macauley. \"[Cutting Through Environmental Issues: Technology as a double-edged sword.](https://www.brookings.edu/articles/cutting-through-environmental-issues-technology-as-a-double-edged-sword/)” The Brookings Institution, Dec. 2001 [Online]. Available: [https://www.brookings.edu/articles/cuttingthrough-environmental-issues-technology-asa-double-edged-sword/.](https://www.brookings.edu/articles/cutting-through-environmental-issues-technology-as-a-double-edged-sword/) [Accessed Dec. 1, 2018].•J. Newton, _[Well-being and the Natural Environment: An Overview of the Evidence](http://resolve.sustainablelifestyles.ac.uk/sites/default/files/JulieNewtonPaper.pdf)_[. ](http://resolve.sustainablelifestyles.ac.uk/sites/default/files/JulieNewtonPaper.pdf)August 20, 2007.•P. Dasgupta, [Human Well-Being and the Natural Environment.](https://books.google.com/books?id=OuMTDAAAQBAJ\u0026amp;dq=wellbeing%2Band%2Bthe%2Bnatural%2Benvironment\u0026amp;lr\u0026amp;source=gbs_navlinks_s) Oxford, U.K.: Oxford University Press, 2001.•R. Haines-Young and M. Potschin. “[The Links ](https://www.pik-potsdam.de/news/public-events/archiv/alter-net/former-ss/2009/10.09.2009/10.9.-haines-young/literature/haines-young-potschin_2009_bes_2.pdf)[Between Biodiversity, Ecosystem Services and Human Well-Being,](https://www.pik-potsdam.de/news/public-events/archiv/alter-net/former-ss/2009/10.09.2009/10.9.-haines-young/literature/haines-young-potschin_2009_bes_2.pdf)” in _Ecosystem Ecology: A New Synthesis_, D. Raffaelli, and C. Frid, Eds. Cambridge, U.K.: Cambridge University Press, 2010.•S. Hart, _[Capitalism at the Crossroads: Next Generation Business Strategies for a PostCrisis World.](https://www.pearson.com/us/higher-education/program/Hart-Capitalism-at-the-Crossroads-Next-Generation-Business-Strategies-for-a-Post-Crisis-World-3rd-Edition/PGM9671.html)_ Upper Saddle River, NJ: Pearson Education, 2010.•United Nations Department of Economic and Social Affairs. “[Call for New Technologies to Avoid Ecological Destruction.](http://www.un.org/en/development/desa/news/policy/wess-2011.html)” Geneva, Switzerland, July 5, 2011.•Pope Francis. [Encyclical Letter Laudato Si’ of the Holy Father Francis On the Care for Our Common Home.](http://w2.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html) May 24, 2015.•“[Environment,](https://www.dalailama.com/messages/environment)” The 14th Dalai Lama. Accessed Dec. 9, 2018. [https://www.dalailama.com/ messages/environment.](https://www.dalailama.com/messages/environment)- Why Islam.org, Environment and Islam, 2018._ \"__p.74-75__\"_•To avoid potential negative, unintended consequences, and secure and safeguard positive impacts, A/IS creators, end-users, and stakeholders should be aware of possible well-being impacts when designing, using, and monitoring A/IS systems. This includes being aware of existing cases and possible areas of impact, measuring impacts on wellbeing outcomes, and developing regulations to promote beneficent uses of A/IS. Specifically\"•A/IS creators should protect human dignity, autonomy, rights, and well-being of those directly and indirectly affected by the technology. As part of this effort, it is important to include multiple stakeholders, minorities, marginalized groups, and those often without power or a voice in consultation.•Policymakers, regulators, monitors, and researchers should consider issuing guidance on areas such as A/IS labor and the proper role of humans vs. A/IS in work transparency, trust, and explainability; manipulation and deception; and other areas that emerge.•Ongoing literature review and analysis should be performed by research and other communities to curate and aggregate information on positive and negative A/IS impacts, along with demonstrated approaches to realize positive ones and amelioratenegative ones.•A/IS creators working toward computational sustainability should integrate well-being concepts, scientific findings, and indicators into current computational sustainability models. They should work with well-being experts, researchers, and practitioners to conduct research and develop and apply models inA/IS development that prioritize and increase human well-being.•Cross-pollination should be developed between computational sustainability and well-being professionals to ensure integration of well-being into computational sustainability frameworks, and vice versa. Where feasible and reasonable, do the same for conceptual models such as doughnut economics and systems thinking.## Further Resources•[AI Safety Research](https://futureoflife.org/ai-safety-research/) by The Future of Life Institute•D. Helbing, et al. “[Will Democracy Survive Big Data and Artificial Intelligence?”](https://www.scientificamerican.com/article/will-democracy-survive-big-data-and-artificial-intelligence/) _Scientific American_, February 25, 2017.•J. L. Schenker, “[Can We Balance Human Ethics with Artificial Intelligence?](http://techonomy.com/2017/01/how-will-ai-decide-who-lives-and-who-dies/)” _Techonomy, _January 23, 2017_._•M. Bulman, “[EU to Vote on Declaring Robots To Be ‘Electronic Persons](http://www.independent.co.uk/life-style/gadgets-and-tech/robots-eu-vote-electronic-persons-european-union-ai-artificial-intelligence-a7527106.html)’.” _Independent, _January 14, 2017.•N. Nevejan, for the European Parliament.“[European Civil Law Rules in Robotics.](http://www.europarl.europa.eu/RegData/etudes/STUD/2016/571379/IPOL_STU(2016)571379_EN.pdf)” October 2016.•University of Oxford. “Social media manipulation rising globally, new report warns,” [https://phys.org/news/2018-07social-media-globally.html.](https://phys.org/news/2018-07-social-media-globally.html) July 20, 2018.•“[The AI That Pretends To Be Human,](http://lesswrong.com/lw/n99/the_ai_that_pretends_to_be_human/)” _LessWrong _blog post, February 2, 2016.•C. Chan, “[Monkeys Grieve When Their Robot Friend Dies.](http://sploid.gizmodo.com/monkeys-grieve-when-their-robot-friend-dies-1791076966)” _Gizmodo_, January 11, 2017.•Partnership on AI, “AI, Labor, and theEconomy” Working Group launches in New York City,” [https://www.partnershiponai.org/ aile-wg-launch/.](https://www.partnershiponai.org/aile-wg-launch/) April 25, 2018.•C.Y. Johnson, “[Children can be swayed by robot peer pressure,study says,](https://www.washingtonpost.com/news/speaking-of-science/wp/2018/08/15/robot-overlords-may-sound-scary-but-robot-friends-could-be-just-as-bad/?utm_term=.d07ad598531c)” The Washington Post, August 15, 2018. [Online].Available: [www.WashingtonPost.com.](http://www.WashingtonPost.com/)[Accessed 2018].## **Further Resources for**Computational Sustainability•Stanford Engineering Department, [Topics in Computational Sustainability Course Presentation,](https://cs.stanford.edu/~ermon/cs325/slides/lecture1-S16.pdf) 2016.•Computational Sustainability, [Computational Sustainability: Computational Methods for a Sustainable Environment, Economy, and Society Project Summary. ](http://computational-sustainability.cis.cornell.edu/FILES/gomes-computational-sustainability-summary-NSF-Exp08.pdf)•C. P. Gomes, “[Computational Sustainability: Computational Methods for a Sustainable ](https://www.nae.edu/File.aspx?id=17673)[Environment, Economy, and Society](https://www.nae.edu/File.aspx?id=17673)” in The Bridge: Linking Engineering and Society_.\"__p.83-86_","id":"recwqrzqpp6c7jyjs","dom_id":"item_recwqrzqpp6c7jyjs"},{"Challenges":["recO1L6GoFMkA6Lt4"],"Principles":["recPg7Ov0priGGtLm"],"Sources":["recpXl48pJdKDhc6f"],"title":"Tools for individuals to create custom machine-readable dynamic terms and conditions that respect their preferences for data collection and use","category":"Strategies","name":"recwWov6KzmwU0FQW","tags":[],"created_at":"2023-06-05T09:31:04.000Z","description":"\"Individuals should be provided tools that produce machine-readable terms and conditions that are dynamic in nature and serve to protect their data and honor their preferences for its use.•Personal data access and consent should be managed by the individual using their curated terms and conditions that provide notification and an opportunity for consent at the time data are exchanged, versus outside actors being able to access personal data without an individual’s awareness or control.•Terms should be presented in a way that allows a user to easily read, interpret, understand, and choose to engage with any A/IS. Consent should be both conditional and dynamic, where “dynamic” means downstream uses of a person’s data must be explicitly called out, allowing them to cancel a service and potentially rescind or “kill” any data they have shared with a service to date via the use of a “Smart Contract” or specific conditions as described in mutual terms and conditions between two parties at the time of exchange.•For further information on these issues, please see the following section in regard to algorithmic agents and their application.\"p.109 IEEE report","id":"recwwov6kzmwu0fqw","dom_id":"item_recwwov6kzmwu0fqw"},{"Principles":["recPg7Ov0priGGtLm"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Respect for privacy and data protection","category":"Strategies","name":"recypFGFqJzmgFmWV","tags":["governance-question"],"created_at":"2023-05-28T19:30:10.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Privacy and data governance Respect for privacy and data Protection: \u003cU+F0FC\u003e Depending on the use case, did you establish a mechanism allowing others to flag issues related to privacy or data protection in the AI system’s processes of data collection (for training and operation) and data processing? \u003cU+F0FC\u003e Did you assess the type and scope of data in your data sets (for example whether they contain personal data)? \u003cU+F0FC\u003e Did you consider ways to develop the AI system or train the model without or with minimal use of potentially sensitive or personal data? \u003cU+F0FC\u003e Did you build in mechanisms for notice and control over personal data depending on the use case (such as valid consent and possibility to revoke, when applicable)? \u003cU+F0FC\u003e Did you take measures to enhance privacy, such as via encryption, anonymisation and aggregation? \u003cU+F0FC\u003e Where a Data Privacy Officer (DPO) exists, did you involve this person at an early stage in the process?” (High-Level Expert Group on AI, 2019, p. 28)","id":"recypfgfqjzmgfmwv","dom_id":"item_recypfgfqjzmgfmwv"},{"Principles":["recKdujFoPJr4ZAhZ","recxcFmvPG5wrCqpO"],"Sources":["recnCULdYQ36cpZR7"],"title":"Considerations in assessing trustworthy AI - Auditability","category":"Strategies","name":"reczFKqCos9f1opXO","tags":["governance-question"],"created_at":"2023-05-28T19:47:29.000Z","description":"“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“Compliance with this assessment list is not evidence of legal compliance, nor is it intended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. In addition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level.” (High-Level Expert Group on AI, 2019, p. 24)“TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION) “Accountability Auditability: \u003cU+F0FC\u003e Did you establish mechanisms that facilitate the system’s auditability, such as ensuring traceability and logging of the AI system’s processes and outcomes? \u003cU+F0FC\u003e Did you ensure, in applications affecting fundamental rights (including safety-critical applications) that the AI system can be audited independently?” (High-Level Expert Group on AI, 2019, p. 31)","id":"reczfkqcos9f1opxo","dom_id":"item_reczfkqcos9f1opxo"},{"Principles":["recsvi4LnhEEPyQ1h","recPg7Ov0priGGtLm","recy4stJ6Y4e2Fezp","rec42P8U9usfYCtv9"],"Sources":["recQiVQ7CTC72xp6O"],"title":"Questions to consider regarding re-identification and disciplinary methodological norms","category":"Strategies","name":"reczG9x5YfScZJdCo","tags":["reflection-questions"],"created_at":"2023-05-19T07:44:09.000Z","description":"“How are texts/persons/data being studied? § Does one’s method of analysis require exact quoting and if so, what might be the ethical consequence of this in the immediate or long term? (For example, would quoting directly from a blog cause harm to the blogger and if so, could another method of representation be less risky?21) § What are the ethical expectations of the research community associated with a particular approach (e.g, ethnographic, survey, linguistic analysis)? § Do one’s disciplinary requirements for collecting, analyzing, or representing information clash with the specific needs of the context? If so, what are the potential ethical consequences?” (Markham and Buchanan, 2012, p. 10)","id":"reczg9x5yfsczjdco","dom_id":"item_reczg9x5yfsczjdco"},{"Stakeholder-actors":["recwbivinQr5lOP5r"],"Stakeholders-impacted":["reci4jOiaZbCd37eY","rec0af1wTjdFwMqsw"],"title":"A local school has installed a tool to help tackle antisocial behaviour, and truantism risk","category":"Cases","name":"rec1RzTKH08fWjSbT","tags":["Safety-and-wellbeing"],"created_at":"2023-05-19T14:13:27.000Z","id":"rec1rztkh08fwjsbt","dom_id":"item_rec1rztkh08fwjsbt"},{"Sources":["recH8KmnURSknCr5y"],"title":"Tracking performance limits access to education","category":"Cases","name":"rec8YSr2IJxVndTKb","tags":["Dropout-risk-and-grade-prediction"],"created_at":"2023-05-29T07:26:14.000Z","description":"“AI can fundamentally violate the principle of equal access. Universities in the U.S. are using deterministic algorithmic systems to recommend applicants they should admit. These are often custom-built to meet the school’s preferences, and have a host of issues that can lead to discrimination, including use of historical data of previously admitted students to inform the model. Since many elite universities have historically been attended by wealthy white males, any model that uses these data risks perpetuating past trends.105 Such systems will likely employ ML in the future, which would make bias harder to detect. This could result in universities discriminating under the guise of objectivity. Looking forward: If AI is used to track and predict student student performance in such a way that limits the eligibility to study certain subjects or have access to certain educational opportunities, the right to education will be put at risk. Given the growth of research into early childhood predictors of success, it is likely that such a system could be used to restrict the opportunities of students at increasingly younger ages, resulting in significant discrimination, with students coming from underprivileged backgrounds ultimately being denied opportunities because people from that background tend to have more negative outcomes. Such a system would ignore the students that overcome adversity to achieve academic and professional success, and would entrench existing educational inequalities.” ([Access Now, 2018, p. 28](zotero://select/groups/4907410/items/KIU6F9PR)) ([pdf](zotero://open-pdf/groups/4907410/items/598C5P37?page=28\u0026annotation=DR727J3N))","id":"rec8ysr2ijxvndtkb","dom_id":"item_rec8ysr2ijxvndtkb"},{"Challenges":["recHsgB7ki6GknJnX"],"Principles":["recqdGhz7l1cGnQeU","recWLcMWDPE9Fd1pE","recUYS0TFpk2MhVD7"],"Sources":["rec9jnxuHOioQn4DC"],"Stakeholder-actors":["recGRt9lWJvxx5GAE","recTYTKZphOqdzYoL","recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","recSaPtMj9rxfu4uu"],"Stakeholders-impacted":["recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","reci4jOiaZbCd37eY","recdkIzRDcuurhTmM","rec0af1wTjdFwMqsw"],"Strategies":["rec7daqDHSCuc70yS","recTWhZ88TbQLcNaQ","recOo7Pmo4FBYcu7P"],"title":"Using chatbots to guide learners and parents through administrative tasks","category":"Cases","name":"recAlOHJhEy5nDwA6","tags":["Resource-management-and-administration"],"created_at":"2023-05-19T13:58:49.000Z","description":"“A school uses a chatbot virtual assistant on its website to guide learners and parents through administrative tasks such as enrolment for courses, paying course fees or logging technical support issues. The system is also used to help students to find learning opportunities, provide feedback on pronunciation or comprehension. The virtual assistant is also used to support students with special educational needs through administrative tasks. The following guiding questions highlight areas that require attention: • Does the AI system clearly signal that its social interaction is simulated and that it has no capacities of feeling or empathy? Societal and environmental wellbeing • Is there a strategy to monitor and test if the AI system is meeting the goals, purposes and intended applications? Technical robustness and safety • Is there a mechanism to allow teachers and school leaders to flag issues related to privacy or data protection? Privacy and data governance” ([European Commission, 2022, p. 25](zotero://select/groups/4907410/items/3BCJVLT9)) ([pdf](zotero://open-pdf/groups/4907410/items/6A6DKVJ2?page=25\u0026annotation=ERDZJU8P))","id":"recalohjhey5ndwa6","dom_id":"item_recalohjhey5ndwa6"},{"Challenges":["recHHr97jsyNDnlsJ","recefglLZ3oJWw2SZ"],"Sources":["recH8KmnURSknCr5y"],"title":"Perpetuating bias in criminal justice","category":"Cases","name":"recDAvfsflBF0WKpf","tags":["non-education"],"created_at":"2023-05-29T07:21:09.000Z","description":"“Perpetuating bias in criminal justice: There are many documented cases of AI gone wrong in the criminal justice system. The use of AI in this context often occurs in two different areas: risk scoring—evaluating whether or not a defendant is likely to reoffend in order to recommend sentencing and set bail—or so-called “predictive policing,” using insights from various data points to predict where or when crime will occur and direct law enforcement action accordingly.38 In many cases, these efforts are likely well-intentioned. Use of machine learning for risk scoring of defendants is advertised as removing the known human bias of judges in their sentencing and bail decisions.39 And predictive policing efforts seek to best allocate often-limited police resources to prevent crime, though there is always a high risk of mission creep.40 However, the recommendations of these AI systems often further exacerbate the very bias they are trying to mitigate, either directly or by incorporating factors that are proxies for bias.” ([Access Now, 2018, p. 15](zotero://select/groups/4907410/items/KIU6F9PR)) ([pdf](zotero://open-pdf/groups/4907410/items/598C5P37?page=15\u0026annotation=RY637D35))","id":"recdavfsflbf0wkpf","dom_id":"item_recdavfsflbf0wkpf"},{"Sources":["recY4zreDoWrsbsv7"],"title":"AI for Adaptive Learning","category":"Cases","name":"recHAwvOykeiUxFsY","tags":[],"created_at":"2023-05-29T08:00:33.000Z","description":"“AI Enables Adaptivity in Learning Adaptivity has been recognized as a key way in which technology can improve learning.25 AI can be a toolset for improving the adaptivity of edtech. AI may improve a technology’s ability to meet students where they are, build on their strengths, and grow their knowledge and skills. Because of AI’s powers of work with natural forms of input and the foundational strengths of AI models (as discussed in the What is AI? section), AI can be an especially strong toolkit for expanding the adaptivity provided to students. And yet, especially with AI, adaptivity is always more specific and limited than what a broad phrase like “meet students where they are” might suggest. Core limits arise from the nature of the model at the heart of any specific AI-enabled system. Models are approximations of reality. When important parts of human learning are left out of the model or less fully developed, the resulting adaptivity will also be limited, and the resulting supports for learning may be brittle or narrow. Consequently, this section on Learning focuses on one key concept: Work toward AI models that fit the fullness of visions for learning—and avoid limiting learning to what AI can currently model well. AI models are demonstrating greater skills because of advances in what are called “large language models” or sometimes “foundational models.” These very general models still have limits. For example, generative AI models discussed in the mainstream news can quickly generate convincing essays about a wide variety of topics while other models can draw credible images based on just a few prompts. Despite the excitement about foundational models, experts in our” ([Cardona et al., 2023, p. 18](zotero://select/groups/4907410/items/ZI7HP57C)) ([pdf](zotero://open-pdf/groups/4907410/items/4YFQW35Q?page=22\u0026annotation=39EVQUQW))“listening sessions warned that AI models are narrower than visions for human learning and that designing learning environments with these limits in mind remains very important. The models are also brittle and can’t perform well when contexts change. In addition, they don’t have the same “common sense” judgment that people have, often responding in ways that are unnatural or incorrect.26 Given the unexpected ways in which foundational models miss the mark, keeping humans in the loop remains highly important.” ([Cardona et al., 2023, p. 19](zotero://select/groups/4907410/items/ZI7HP57C)) ([pdf](zotero://open-pdf/groups/4907410/items/4YFQW35Q?page=23\u0026annotation=XGC6UNEN))","id":"rechawvoykeiuxfsy","dom_id":"item_rechawvoykeiuxfsy"},{"title":"AES white paper","category":"Cases","name":"recKURlVVYGCInW8y","tags":["Automated-feedback"],"created_at":"2023-05-19T14:13:58.000Z","id":"reckurlvvygcinw8y","dom_id":"item_reckurlvvygcinw8y"},{"Stakeholder-actors":["recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","recGRt9lWJvxx5GAE"],"Stakeholders-impacted":["reci4jOiaZbCd37eY","recdkIzRDcuurhTmM","recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl"],"title":"Teacher uses tool to automate creation of some material","category":"Cases","name":"recKWQBBplr1vXmRc","tags":["Automated-content-creation"],"created_at":"2023-05-19T14:15:04.000Z","id":"reckwqbbplr1vxmrc","dom_id":"item_reckwqbbplr1vxmrc"},{"Stakeholder-actors":["recSaPtMj9rxfu4uu","recGRt9lWJvxx5GAE"],"Stakeholders-impacted":["reckhslKpo5ZlVNpl"],"title":"A regional government is investing in a tool to automate content creation for all teachers","category":"Cases","name":"recMalmQowxcka9V6","tags":["Automated-content-creation"],"created_at":"2023-05-18T18:42:40.000Z","id":"recmalmqowxcka9v6","dom_id":"item_recmalmqowxcka9v6"},{"Sources":["recH8KmnURSknCr5y"],"title":"Financial discrimination","category":"Cases","name":"recNNFrnCiGxTP4WE","tags":[],"created_at":"2023-05-29T07:22:53.000Z","description":"“Driving financial discrimination against the marginalized: Algorithms have long been used to create credit scores and inform loan screening. However, with the rise of big data, systems are now using machine learning to incorporate and analyze non-financial data points to determine creditworthiness, from where people live, to their internet browsing habits, to their purchasing decisions. The outputs these systems produce are known as e-scores, and unlike formal credit scores they are largely unregulated. As data scientist Cathy O’Neil has pointed out, these scores are often discriminatory and create pernicious feedback loops.4” ([Access Now, 2018, p. 16](zotero://select/groups/4907410/items/KIU6F9PR)) ([pdf](zotero://open-pdf/groups/4907410/items/598C5P37?page=16\u0026annotation=GTXRLF2L))","id":"recnnfrncigxtp4we","dom_id":"item_recnnfrncigxtp4we"},{"Challenges":["rec5VzvnIZcaNKqDh","recL5M2Hye2XHK7zg","recvQ90DajNCwPiGP","recefglLZ3oJWw2SZ","recdmBNNa98cN8Sda","recxjc79LvLdKa4rl"],"Principles":["reco4DUa3rsjP0hyg","recWLcMWDPE9Fd1pE"],"Sources":["rec9jnxuHOioQn4DC"],"Stakeholder-actors":["recGRt9lWJvxx5GAE","recTYTKZphOqdzYoL","recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","recSaPtMj9rxfu4uu"],"Stakeholders-impacted":["recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","reci4jOiaZbCd37eY","recdkIzRDcuurhTmM"],"Strategies":["recTWhZ88TbQLcNaQ","recegTm800wJXGjO1"],"title":"Providing individualised interventions for special needs","category":"Cases","name":"recOOmVQviRyJGvea","tags":["Dropout-risk-and-grade-prediction"],"created_at":"2023-05-19T13:56:36.000Z","description":"“A school is considering how AI systems can help reduce barriers for students with special educational needs. The school is currently trialling an AI system to detect student support demands early on and provide tailored instructional support. By detecting patterns of corresponding characteristics from measures such as learning performance, standardised tests attention span or reading speed, the system suggests probabilities of specific diagnoses and related recommendations for interventions. The following guiding questions highlight areas that require attention: • Are procedures in place for teachers to monitor and intervene, for example in situations where empathy is required when dealing with learners or parents? Human agency and oversight • Is information available to assure learners and parents of the system’s technical robustness and safety? Technical robustness and safety • Is the teacher role clearly defined so as to ensure that there is a teacher in the loop while the AI system is being used? How does the AI system affect the didactical role of the teacher? Human agency and oversight” ([European Commission, 2022, p. 23](zotero://select/groups/4907410/items/3BCJVLT9)) ([pdf](zotero://open-pdf/groups/4907410/items/6A6DKVJ2?page=23\u0026annotation=QT7FXESI))","id":"recoomvqviryjgvea","dom_id":"item_recoomvqviryjgvea"},{"Challenges":["recBc3GCNokDL220T","recHsgB7ki6GknJnX"],"Principles":["reczcRriFbQQpn8iX","recUYS0TFpk2MhVD7"],"Sources":["rec9jnxuHOioQn4DC"],"Stakeholder-actors":["recGRt9lWJvxx5GAE","recTYTKZphOqdzYoL","recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","recSaPtMj9rxfu4uu","recwbivinQr5lOP5r"],"Stakeholders-impacted":["recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","reci4jOiaZbCd37eY","recdkIzRDcuurhTmM","rec0af1wTjdFwMqsw"],"Strategies":["recOo7Pmo4FBYcu7P","recRTVqtvPcBS6zps"],"title":"Managing student enrolment and resource planning","category":"Cases","name":"recSXcY4cnofb4zTP","tags":["Resource-management-and-administration"],"created_at":"2023-05-19T13:58:44.000Z","description":"A school uses the data collected when students enrol to predict and better organise the number of students who will attend in the coming year. The AI system is also used to assist with forward planning, resource allocation, class allocations and budgeting. This has enabled the school to consider more student attributes than before, for example, to increase gender parity and student diversity. The school is now considering using prior grades and other metrics like standardised tests to develop targets for their students to achieve and to support educators to predict student success on a per subject basis. The following guiding questions highlight areas that require attention: • Who is responsible for the ongoing monitoring of results produced by the AI system and how the results are being used to enhance teaching, learning and assessment? Accountability • Are there mechanisms to ensure that sensitive data is kept anonymous? Are there procedures in place to limit access to the data only to those who need it? Privacy and data governance • How is the effectiveness and impact of the AI system being evaluated and how does this evaluation consider key values of education? Accountability","id":"recsxcy4cnofb4ztp","dom_id":"item_recsxcy4cnofb4ztp"},{"title":"Teacher evaluation fails to acknowledge individual contexts and teacher autonomy","category":"Cases","name":"recTQ7eABknthRQo8","tags":[],"created_at":"2023-06-05T15:18:06.000Z","id":"rectq7eabknthrqo8","dom_id":"item_rectq7eabknthrqo8"},{"Sources":["recH8KmnURSknCr5y"],"title":"Disinformation spread","category":"Cases","name":"recVLyzjb1UfqbrIh","tags":[],"created_at":"2023-05-29T07:22:23.000Z","description":"“Assisting the spread of disinformation: AI can be used to create and disseminate targeted propaganda, and that problem is compounded by AI-powered social media algorithms driven by “engagement,” which promote content most likely to be clicked on. Machine learning powers the data analysis social media companies use to create profiles of users for targeted advertising. In addition, bots disguised as real users further spread content outside of narrowly targeted social media circles by both sharing links to false sources and actively interacting with users as chatbots using natural language processing.45 In addition, the specter of “deep fakes,” AI systems capable of creating realistic-sounding video and audio recordings of real people, is causing many to believe the technology will be used in the future to create forged videos of world leaders for malicious ends. Although it appears that deep fakes have yet to be used as part of real propaganda or disinformation campaigns, and the forged audio and video is still not good enough to seem completely human, the AI behind deep fakes continues to advance, and there is potential for sowing chaos, instigating conflict, and further causing a crisis of truth that should not be discounted.46” ([Access Now, 2018, p. 16](zotero://select/groups/4907410/items/KIU6F9PR)) ([pdf](zotero://open-pdf/groups/4907410/items/598C5P37?page=16\u0026annotation=9DGUCL8R))","id":"recvlyzjb1ufqbrih","dom_id":"item_recvlyzjb1ufqbrih"},{"Sources":["recH8KmnURSknCr5y"],"title":"Hiring bias","category":"Cases","name":"recWNt6W4v0xr3x0Z","tags":[],"created_at":"2023-05-29T07:22:41.000Z","description":"“Perpetuating bias in the job market: Hiring processes have long been fraught with bias and discrimination. In response, an entire industry has emerged that uses AI with the goal of removing human bias from the process. However, many products ultimately risk perpetuating the very bias they seek to mitigate. As in other areas a major cause of this is the prevalent use of historical data of past “successful” employees to train the ML models, thus naturally reproducing the bias in prior hiring.47” ([Access Now, 2018, p. 16](zotero://select/groups/4907410/items/KIU6F9PR)) ([pdf](zotero://open-pdf/groups/4907410/items/598C5P37?page=16\u0026annotation=DQUC6GAH))","id":"recwnt6w4v0xr3x0z","dom_id":"item_recwnt6w4v0xr3x0z"},{"Stakeholder-actors":["recGRt9lWJvxx5GAE","recz1WGGiX1qkZTxC","recTYTKZphOqdzYoL"],"Stakeholders-impacted":["recVHjstaBpfkQwvp","reci4jOiaZbCd37eY","rec0af1wTjdFwMqsw","recwbivinQr5lOP5r"],"title":"A company has approached researchers to fund a small evaluation of a wellbeing app being deployed in local schools","category":"Cases","name":"reccWWRCdr9ZKJBZE","tags":["Safety-and-wellbeing"],"created_at":"2023-05-18T18:42:27.000Z","id":"reccwwrcdr9zkjbze","dom_id":"item_reccwwrcdr9zkjbze"},{"Challenges":["rec5VzvnIZcaNKqDh","recL5M2Hye2XHK7zg","recvQ90DajNCwPiGP","recxjc79LvLdKa4rl"],"Principles":["recwjv8IMAZFWfMSr","recaMsksKInFYbnCL","reco4DUa3rsjP0hyg"],"Sources":["rec9jnxuHOioQn4DC"],"Stakeholder-actors":["recGRt9lWJvxx5GAE","recTYTKZphOqdzYoL","recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","recSaPtMj9rxfu4uu"],"Stakeholders-impacted":["recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","reci4jOiaZbCd37eY","recdkIzRDcuurhTmM"],"Strategies":["rechaQXedBh3OsMjZ","recegTm800wJXGjO1","rec35PeHdUmtalypk"],"title":"Using adaptive learning technologies to adapt to each learner’s ability","category":"Cases","name":"reciNqxyfUgE5XM7t","tags":["Adaptive-and-personalised-learning"],"created_at":"2023-05-19T13:52:56.000Z","description":"“A primary school is using an Intelligent Tutoring System to automatically direct learners to resources specific to their learning needs. The AI based system uses learner data to adapt problems to the learner’s predicted knowledge levels. As well as providing constant feedback to the learner, the system provides real-time information on their progress on a teacher dashboard. The following guiding questions highlight areas that require attention: • Are the system processes and outcomes focussed on the expected learning outcomes for the learners? How reliable are the predictions, assessments and classifications of the AI system in explaining and evaluating the relevance of its use? Transparency • Does the system provide appropriate interaction modes for learners with disabilities or special education needs? Is the AI system designed to treat learners respectfully adapting to their individual needs? Diversity, non-Discrimination and Fairness • Are there monitoring systems in place to prevent overconfidence in or overreliance on the AI system? Human agency and oversight” (European Commission, 2022, p. 22)","id":"recinqxyfuge5xm7t","dom_id":"item_recinqxyfuge5xm7t"},{"Challenges":["rec5VzvnIZcaNKqDh","recL5M2Hye2XHK7zg","recvQ90DajNCwPiGP","recefglLZ3oJWw2SZ","recdmBNNa98cN8Sda","recxjc79LvLdKa4rl"],"Principles":["recaMsksKInFYbnCL","reczcRriFbQQpn8iX","recwjv8IMAZFWfMSr"],"Sources":["rec9jnxuHOioQn4DC"],"Stakeholder-actors":["recGRt9lWJvxx5GAE","recTYTKZphOqdzYoL","recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","recSaPtMj9rxfu4uu"],"Stakeholders-impacted":["recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","reci4jOiaZbCd37eY","recdkIzRDcuurhTmM"],"Strategies":["rechaQXedBh3OsMjZ","recRTVqtvPcBS6zps","rec81gtnlFS5W2BBF"],"title":"Scoring essays using automated tools","category":"Cases","name":"recmS3zSMbR3ofAR5","tags":["Automated-feedback"],"created_at":"2023-05-19T13:58:20.000Z","description":"“A school is looking at how AI systems can support the assessment of student written assignments. A provider has recommended an automated essay scoring system which uses large natural language models to assess various aspects of text with high accuracy. The system can be used to check student assignments, automatically identify errors, and assign grades. The system can also be used to generate sample essays. Over time, the system can train large artificial neural networks with historical cases that contain various types of student mistakes to provide even more accurate grading. The system has a plagiarism detection option which can be used to automatically detect instances of plagiarism or copyright infringement in written work submitted by students. The following guiding questions highlight areas that require attention: • Are there procedures in place to ensure that AI use will not lead to discrimination or unfair behaviour for all users? Diversity, non-discrimination and fairness • Who is responsible for the ongoing monitoring of results produced by the AI system and how the results are being used to enhance teaching, learning and assessment? Accountability • Do teachers and school leaders understand how specific assessment or personalisation algorithms work within the AI system? Transparency” ([European Commission, 2022, p. 24](zotero://select/groups/4907410/items/3BCJVLT9)) ([pdf](zotero://open-pdf/groups/4907410/items/6A6DKVJ2?page=24\u0026annotation=7VK8CJ4X))","id":"recms3zsmbr3ofar5","dom_id":"item_recms3zsmbr3ofar5"},{"Sources":["recY4zreDoWrsbsv7"],"title":"ITS","category":"Cases","name":"reco7BYWc7reRuaCh","tags":[],"created_at":"2023-05-29T08:02:31.000Z","description":"“One long-standing type of AI-enabled technology is an Intelligent Tutoring System (ITS).27 In an early success, scientists were able to build accurate models of how human experts solve mathematical problems. The resulting model was incorporated into a system that would observe student problem solving as they worked on mathematical problems on a computer. Researchers who studied human tutors found that feedback on specific steps (and not just right or wrong solutions) is a likely key to why tutoring is so effective.28 For example, when a student diverged from the expert model, the system gave feedback to help the student get back on track.29 Importantly, this feedback went beyond right or wrong, and instead, the model was able to provide feedback on specific steps of a solution process. A significant advancement of AI, therefore, can be its ability to provide adaptivity at the step-by-step level and its ability to do so at scale with modest cost. As a research and development (R\u0026D) field emerged to advance ITS, the work has gone beyond mathematics problems to additional important issues beyond step-by-step problem solving. In the early work, some limitations can be observed. The kinds of problems that an ITS could support were logical or mathematical, and they were closed tasks, with clear expectations for what a solution and solution process should look like. Also, the “approximation of reality” in early AI models related to cognition and not to other elements of human learning, for example, social or motivational aspects. Over time, these early limitations have been addressed in two ways: by expanding the AI models and by involving humans in the loop, a perspective that is also important now. Today, for example, if an ITS specializes in feedback as a student practices, a human teacher could still be responsible for motivating student engagement and self-regulation along with other aspects of instruction. In other contemporary examples, the computer ITS might focus on problem solving practice, while teachers work with students in small groups. Further, students can be in the loop with AI, as is the case with “open learner models”—a type of AI-enabled system that provides information to support student self-monitoring and reflection.30” ([Cardona et al., 2023, p. 19](zotero://select/groups/4907410/items/ZI7HP57C)) ([pdf](zotero://open-pdf/groups/4907410/items/4YFQW35Q?page=23\u0026annotation=AIT4G5YT))“Although R\u0026D along the lines of an ITS should not limit the view of what’s possible, such an example is useful because so much research and evaluation has been done on the ITS approach. Researchers have looked across all the available high-quality studies in a meta-analysis and concluded that ITS approaches are effective.31 Right now, many school systems are looking at high-intensity human tutoring to help students with unfinished learning. Human tutoring is very expensive, and it is hard to find enough high-quality human tutors. With regard to large-scale needs, if it is possible for an ITS to supplement what human tutors do, it might be possible to extend beyond the amount of tutoring that people can provide to students.” ([Cardona et al., 2023, p. 20](zotero://select/groups/4907410/items/ZI7HP57C)) ([pdf](zotero://open-pdf/groups/4907410/items/4YFQW35Q?page=24\u0026annotation=TJ94T9HD))","id":"reco7bywc7reruach","dom_id":"item_reco7bywc7reruach"},{"Stakeholder-actors":["recGRt9lWJvxx5GAE","recTYTKZphOqdzYoL","recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","recSaPtMj9rxfu4uu","reci4jOiaZbCd37eY"],"Stakeholders-impacted":["recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","reci4jOiaZbCd37eY","recdkIzRDcuurhTmM","rec0af1wTjdFwMqsw"],"title":"UK exam algorithm","category":"Cases","name":"recpFE7h6QhaEpNAQ","tags":["Dropout-risk-and-grade-prediction"],"created_at":"2023-05-19T14:13:50.000Z","id":"recpfe7h6qhaepnaq","dom_id":"item_recpfe7h6qhaepnaq"},{"Challenges":["rec5VzvnIZcaNKqDh","recL5M2Hye2XHK7zg","recvQ90DajNCwPiGP","recxjc79LvLdKa4rl"],"Principles":["recqdGhz7l1cGnQeU","recUYS0TFpk2MhVD7","reczcRriFbQQpn8iX"],"Sources":["rec9jnxuHOioQn4DC"],"Stakeholder-actors":["recGRt9lWJvxx5GAE","recTYTKZphOqdzYoL","recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","recSaPtMj9rxfu4uu"],"Stakeholders-impacted":["recVHjstaBpfkQwvp","reckhslKpo5ZlVNpl","reci4jOiaZbCd37eY","recdkIzRDcuurhTmM"],"Strategies":["recOo7Pmo4FBYcu7P","recRTVqtvPcBS6zps","rec7daqDHSCuc70yS"],"title":"Using student dashboards to guide learners through their learning","category":"Cases","name":"recrVkbG0XGe2Ca0v","tags":["Automated-feedback"],"created_at":"2023-05-19T13:55:23.000Z","description":"“A post-primary school is considering the use of a personalised online student dashboard which will provide feedback to learners and support the development of their self-regulation skills. Instead of focusing on what the learner has learned, the visualisations provide the student with a view of how they are learning. The following guiding questions highlight areas that require attention: • Does the AI system clearly signal that its social interaction is simulated and that it has no capacities of feeling or empathy? Societal and environmental wellbeing • Is access to learner data protected and stored in a secure location and used only for the purposes for which the data was collected? Privacy and data governance • Is there a Service Level Agreement in place, clearly outlining the Support and Maintenance Services and steps to be taken to address reported problems? Accountability” ([European Commission, 2022, p. 23](zotero://select/groups/4907410/items/3BCJVLT9)) ([pdf](zotero://open-pdf/groups/4907410/items/6A6DKVJ2?page=23\u0026annotation=WEY6JTEU))","id":"recrvkbg0xge2ca0v","dom_id":"item_recrvkbg0xge2ca0v"},{"Principles":["recUYS0TFpk2MhVD7"],"category":"Cases","name":"recskjpMFnQ0emSKI","tags":[],"created_at":"2023-06-08T06:38:41.000Z","id":"recskjpmfnq0emski","dom_id":"item_recskjpmfnq0emski"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recnLauSoXMQCSK5c"],"title":"Inequities in access to AI may increase, not tackle, inequality","category":"Challenges","name":"rec0BAjUqQrSIMAEG","tags":[],"created_at":"2023-06-05T11:18:00.000Z","description":"**\"Issue:** Vastly different power structures among and within countries create risk that A/IS deployment accelerates, rather than reduces, inequality in the pursuit of a sustainable future. It is unclear how LMIC can best implement A/IS via existing resources and take full advantage of the technology’s potential to achieve a sustainable future.## BackgroundThe potential use of A/IS to create sustainable economic growth for LMIC is uniquely powerful. Yet, many of the debates surrounding A/IS take place within HIC, among highly educated and financially secure individuals. It is imperative that all humans, in any condition around the world, are considered in the general development and application of these systems to avoid the risk of bias, excessive inequality, classism, and general rejection of these technologies. With much of the financial and technical resources for A/IS development and deployment residing in HIC, not only are A/IS benefits more difficult to access for LMIC populations, but those A/IS applications that are_ _deployed outside of HIC realities may not be appropriate. This is for reasons of cultural/ethnic bias, language difficulties, or simply an inability to adapt to local internet infrastructure constraints.Furthermore, technological innovation in LMIC comes up against many potential obstacles, which could be considered when undertaking initiatives aimed at enhancing LMIC access:•Reluctance to provide open source licensing of technological development innovations,•Lack of the human capital and knowledge required to adapt HIC-developed technologies to resolving problems in the LMIC context, or to develop local technological solutions to these problems,•Retention of A/IS capacity in LMIC due to globally uncompetitive salaries,•Lack of infrastructure for deployment, and difficulties in taking technological solutions to where they are needed,•Lack of organizational and business models for adapting technologies to the specific needs of different regions,•Lack of active participation of the target population,•Lack of political will to allow people to have access to technological resources,•Existence of oligopolies that hinder new technological development,•Lack of inclusive and high-quality education at all levels, and•Bureaucratic policies ill-adapted to highly dynamic scenarios.For A/IS capacities and benefits to become equally available worldwide, training, education, and opportunities should be provided particularly for LMIC. Currently, access to products that facilitate A/IS research of timely topics is quite limited for researchers in LMIC, due to cost considerations.If A/IS capacity and governance problems, such as relevant laws, policies, regulations, and anticorruption safeguards, are addressed, LMIC could have the ability to use A/IS to transform their economies and leapfrog into a new era of inclusive growth. Indeed, A/IS itself can contribute to good governance when applied to the detection of corruption in state and banking institutions, one of the most serious recognized constraints to investment in LMIC. Particular attention, however, must be paid to ensure that the use of A/IS is for the common good—especially in the context of LMIC—and does not reinforce existing socioeconomic inequities through systematic discriminatory bias in both design and application, or undermine fundamental rights through, among other issues, lax data privacy laws and practice.## RecommendationsA/IS benefits should be equally available to populations in HIC and LMIC, in the interest of universal human dignity, peace, prosperity, and planet protection. Specific measures for LMIC should include:• Deploying A/IS to detect fraud and corruption, to increase the transparency of power structures, to contribute to a favorable investment, governance, and innovation environment. •Supporting LMIC in the development of their own A/IS strategies, and in the retention or return of their A/IS talent to prevent “brain drain”.•Encouraging global standardization/ harmonization and open source A/IS software.•Promoting distribution of knowledge and wealth generated by the latest A/IS, including through formal public policy and financial mechanisms to advance equity worldwide.•Developing public datasets to facilitate the access of people from LMIC to data resources to facilitate their applied research, while ensuring the protection of personal data.•Creating A/IS international research centers in every continent, that promote culturally appropriate research, and allow the remote access of LMIC's communities to high-end technology.16•Facilitating A/IS access in LMIC through online courses in local languages.•Ensuring that, along with the use of A/IS, discussions related to identity, platforms, and blockchain are conducted, such that core enabling technologies are designed to meet the economic, social, and cultural needs of LMIC.•Diminishing the barriers and increase LMIC access to technological products, including the formation of collaborative networks between developers in HIC and LMIC, supporting the latter in attending global A/IS conferences.17•Promoting research into A/IS-based technologies, for example, mobile lightweight A/IS applications, that are readily availablein LMIC.•Facilitating A/IS research and development inLMIC through investment incentives, public-private partnerships, and/or joint grants, and collaboration between international organizations, government bodies, universities, and research institutes.•Prioritizing A/IS infrastructure in international development assistance, as necessary to improve the quality and standard of living and advance progress towards the SDGs in LMIC.•Recognizing data issues that may be particular to LMIC contexts, i.e., insufficient sample size for machine learning which sometimes results in _de facto_ discrimination, and inadequate laws for, and the practice of, data protection. •Supporting research on the adaptation ofA/IS methods to scarce data environmentsand other remedies that facilitate an optimalA/IS enabling environment in LMIC.## Further Resources•A. Akubue, “Appropriate Technology for Socioeconomic Development in Third World Countries.” _The Journal of Technology Studies _26, no. 1, pp. 33–43, 2000. •O. Ajakaiye and M. S. Kimenyi. “Higher Education and Economic Development in Africa: Introduction and Overview.” _Journal of African Economies _20, no. 3, iii3–iii13, 2011.•D. Allison-Hope and M. Hodge, \"Artificial Intelligence: A Rights-Based Blueprint for Business,” San Francisco: BSF, Aug. 28, 2018•D. E. Bloom, D. Canning, and K. Chan. _Higher Education and Economic Development in Africa _(Vol. 102). Washington, DC: World Bank, 2006.•N. Bloom, “Corporations in the Age of Inequality.” _Harvard Business Review, _April 21, 2017.•C. Dahlman, _Technology, Globalization, and Competitiveness: Challenges for Developing Countries. Industrialization in the 21st Century_. New York: United Nations, 2006.•M. Fong, _Technology_ _Leapfrogging for Developing Countries. Encyclopedia of Information Science and Technology_, 2nd ed. Hershey, PA: IGI Global, 2009 (pp. 3707– 3713).•C. B. Frey and M. A. Osborne. “The Future of Employment: How Susceptible Are Jobs to Computerisation?” (working paper). Oxford, U.K.: Oxford University, 2013.•B. Hazeltine and C. Bull. _Appropriate Technology: Tools, Choices, and Implications. _New York: Academic Press, 1999.•McKinsey Global Institute. “Disruptive Technologies: Advances That Will Transform Life, Business, and the Global Economy” (report), May 2013.•D. Rotman, “How Technology Is Destroying Jobs.” _MIT Technology Review_, June 12, 2013.•R. Sauter and J. Watson. “Technology Leapfrogging: A Review of the Evidence, A Report for DFID.” Brighton, England: University of Sussex. October 3, 2008.•“The Rich and the Rest.” _The Economist. _October 13, 2012.•“Wealth without Workers, Workers without Wealth.” _The Economist_. October 4, 2014.•World Bank. “Global Economic Prospects 2008: Technology Diffusion in the Developing World.” Washington, DC: World Bank, 2008.•World Development Report 2016: Digital Dividends_. _Washington, DC: World Bank. doi:10.1596/978-1-4648-0671-1.•World Wide Web Foundation “Artificial Intelligence: The Road ahead in Low and Middle-income Countries,” webfoundation.org, June 2017.\"p.145-147","id":"rec0bajuqqrsimaeg","dom_id":"item_rec0bajuqqrsimaeg"},{"Principles":["receFm7cGasHwpJZO","recint2IxoR8aILCp"],"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recZI7HDWKBU5T22v"],"title":"How do we foster AI to support wellbeing in context of short-term growth priorities?","category":"Challenges","name":"rec1QZHHMARBQcZoo","tags":["wellbeing"],"created_at":"2023-06-05T05:40:06.000Z","description":"\"Increased awareness and application of well-being metrics by A/IS creators can create greater value, safety, and relevance to corporate communities and other organizations in the algorithmic age.\" ## \"BackgroundWhile many organizations in the private and public sectors are increasingly aware of the need to incorporate well-being measures as part of their efforts, the reality is that bottom line, quarterly-driven shareholder growth remains a dominant goal and metric. Short term growth is often the priority in the private sector and public sector. As long as organizations exist in a larger societal system which prioritizes financial success, these companies will remain under pressure to deliver financial results that do not fully incorporate societal and environmental impacts, measurements, or priorities.Rather than focus solely on the negative aspects of how A/IS could harm humans and environments, we seek to explore how the implementation of well-being metrics can help A/IS to have a measurable, positive impact on human well-being as well as on systems and organizations. Incorporation of well-being goals and measures beyond what is strictly required can benefit both private sector organizations’ brands and public sector organizations’ stability and reputation, as well as help realize financial savings, innovation, trust, and many other benefits. For instance, a companion robot outfitted to support seniors in assisted living situations might traditionally be launched with a technology development model that was popularized by Silicon Valley known as “move fast and break things”. The A/IS creator who rushed to bring the robot to market faster than the competition and who was unaware of well-being metrics, may have overlooked critical needs of the seniors. The robot might actually hurt the senior instead of helping by exacerbating isolation or feelings of loneliness and helplessness. While this is a hypothetical scenario, it is intended to demonstrate the value of linking A/IS design to well-being indicators.By prioritizing largely fiscal metrics of success,A/IS devices might fail in the market because of limited adoption and subpar reception. However, if during use of the A/IS product, success were measured in terms of relevant aspects of wellbeing, developers and researchers could be in a better position to attain funding and public support. Depending on the intended use of the A/IS product, well-being measures that could be used extend to emotional levels of calm or stress; psychological states of thriving or depression; behavioral patterns of engagement in community or isolation; eating, exercise and consumption habits; and many other aspects of human well-being. The A/IS product could significantly improve quality of life guided by metrics from trusted sources, such as the [World Health Organization,](http://www.who.int/en/) [European Social Survey,](https://www.europeansocialsurvey.org/)and [Sustainable Development Goal Indicators.](https://unstats.un.org/sdgs/)Thought leaders in the corporate arena have recognized the multifaceted need to utilize metrics beyond fiscal indicators. PricewaterhouseCoopers defines “[total impact” ](https://www.pwc.com/gx/en/services/sustainability/total-impact-measurement-management/measuring-and-managing-total-impact-a-new-language-for-business-decisions.html)as a “holistic view of social, environmental, fiscal and economic dimensions—the big picture”. Other thought-leading organizations in the public sector, such as the OECD, demonstrate the desire for business leaders to incorporate metrics of success beyond fiscal indicators for their efforts, exemplified in their 2017 workshop, [Measuring Business Impacts on People’s WellBeing.](http://www.oecd.org/statistics/Biz4WB-Highlights-OECD.pdf) The [B-Corporation movement h](https://www.bcorporation.net/)as created a new legal status for “a new type of company that uses the power of business to solve social and environmental problems”. Focusing on increasing stakeholder value versus shareholder returns alone, B-Corps are defining their brands by provably aligning their efforts with wider measures of well-being.****## RecommendationsA/IS creators should work to better understand and apply well-being metrics in the algorithmic age. Specifically:•A/IS creators should work directly with experts, researchers, and practitioners in wellbeing concepts and metrics to identify existing metrics and combinations of indicators that would bring support a “triple bottom line”, i.e., accounting for economic, social, and environmental impacts, approach to wellbeing. However, well-being metrics should only be used with consent, respect for privacy, and with strict standards for collection and use of these data.•For A/IS to promote human well-being, the well-being metrics should be chosen in collaboration with the populations most affected by those systems—the A/IS stakeholders—including both the intended end-users or beneficiaries and those groups whose lives might be unintentionally transformed by them. This selection process should be iterative and through a learningand continually improving process. In addition, “metrics of well-being” should be treated as vehicles for learning and potential mid- course corrections. The effects of A/IS on human well-being should be monitored continuously throughout their life cycles, byA/IS creators and stakeholders, and both A/IS creators and stakeholders should be prepared to significantly modify, or even roll back, technology that is shown to reduce well-being, as defined by affected populations.•A/IS creators in the business or academic, engineering, or policy arenas are advised to review the additional resources on standards development models and frameworks at the end of this chapter to familiarize themselves with existing indicators relevant to their work.## Further Resources•PricewaterhouseCoopers (PwC). [Managing and Measuring Total Impact: A New Language for Business Decisions](https://www.pwc.com/gx/en/services/sustainability/total-impact-measurement-management/measuring-and-managing-total-impact-a-new-language-for-business-decisions.html), 2017.•World Economic Forum. [The Inclusive Growth and Development Report 2017](https://www.weforum.org/reports/the-inclusive-growth-and-development-report-2017), Geneva, Switzerland: World Economic Forum, January 16, 2017.•[OECD Guidelines on Measuring Subjective Well-being,](http://www.oecd.org/statistics/oecd-guidelines-on-measuring-subjective-well-being-9789264191655-en.htm) 2013.• National Research Council. [Subjective WellBeing: Measuring Happiness, Suffering, and Other Dimensions of Experience. D](https://www.nap.edu/catalog/18548/subjective-well-being-measuring-happiness-suffering-and-other-dimensions-of)C: The National Academies Press, 2013.\"p73-74","id":"rec1qzhhmarbqczoo","dom_id":"item_rec1qzhhmarbqczoo"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["rec5CFheImo8onTcd"],"title":"How might AI deployed in care settings to foster intimate relationships impact on relationships among humans?","category":"Challenges","name":"rec2ULcHUjSbbSnHL","tags":["affective-computing"],"created_at":"2023-06-05T06:05:11.000Z","description":"Are moral and ethical boundaries crossed when the design of affective systems allows them to develop intimate relationships with their users?## Background There are many robots in development or production designed to focus on intimate care of children, adults, and the elderly2. While robots capable of participating fully in intimate relationships are not currently available, the potential use of such robots routinely captures the attention of the media. It is important that professional communities, policy makers, and the general public participate in development of guidelines for appropriate use of A/IS in this area. Those guidelines should acknowledge fundamental human rights to highlight potential ethical benefits and risks that may emerge, ifand when affective systems interact intimately with users. Among the many areas of concern are the representation of care, embodiment of caringA/IS, and the sensitivity of data generated through intimate and caring relationships withA/IS. The literature suggests that there are some potential benefits to individuals and to society from the incorporation of caring A/IS, along with duly cautionary notes concerning the possibility that these systems could negatively impact human-to-human intimate relations3.## RecommendationsAs this technology develops, it is important to monitor research into the development of intimate relationships between A/IS and humans. Research should emphasize any technical and normative developments that reflect use ofA/IS in positive and therapeutic ways while also creating appropriate safeguards to mitigate against uses that contribute to problematic individual or social relationships:1\\.Intimate systems must not be designed or deployed in ways that contribute to stereotypes, gender or racial inequality,or the exacerbation of human misery.2\\.Intimate systems must not be designed to explicitly engage in the psychological manipulation of the users of these systems unless the user is made aware they are being manipulated and consents to this behavior. Any manipulation should be governedthrough an opt-in system.3\\.Caring A/IS should be designed to avoid contributing to user isolation from society.4\\.Designers of affective robotics must publicly acknowledge, for example, within a notice associated with the product, that these systems can have side effects, such as interfering with the relationship dynamics between human partners, causing attachments between the user and the A/IS that are distinct from human partnership.5\\.Commercially marketed A/IS for caring applications should not be presented to be a person in a legal sense, nor marketed as a person. Rather its artifactual, that is, authored, designed, and built deliberately, nature should always be made as transparent as possible, at least at point of sale and in available documentation, as noted in Section 4, Systems Supporting Human Potential.6.Existing laws regarding personal imagery need to be reconsidered in light of caring A/IS.In addition to other ethical considerations, it will also be necessary to establish conformance with local laws and mores in the context of caring A/IS systems.## Further Resources•M. Boden, J. Bryson, D. Caldwell, K. Dautenhahn, L. Edwards, S. Kember, P.Newman, V. Parry, G. Pegman, T. Rodden and T. Sorrell, Principles of robotics: regulating robots in the real world. Connection Science, vol. 29, no. 2, pp. 124-129, April 2017.•J. J. Bryson, M. E. Diamantis, and T. D. Grant, “Of, For, and By the People: The Legal Lacuna of Synthetic Persons.” _Artificial Intelligence \u0026 Law_, vol. 25, no. 3, pp. 273–291, Sept. 2017.•M. Scheutz, “The Inherent Dangers ofUnidirectional Emotional Bonds between Humans and Social Robots,” in _Robot Ethics: The Ethical and Social Implications of Robotics,_ P. Lin, K. Abney, and G. Bekey, Eds., pp. 205. Cambridge, MA: MIT Press, 2011.","id":"rec2ulchujsbbsnhl","dom_id":"item_rec2ulchujsbbsnhl"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["rec9TbOu2ZXUZG4A5"],"title":"Should affective AI nudge users for personal or societal benefit?","category":"Challenges","name":"rec2ajglpzbFYRivi","tags":["nudge-deception"],"created_at":"2023-06-05T06:13:01.000Z","description":"\"Should affective systems be designed to nudge peoplefor the user’s personal benefit and/or for the benefit of others?## BackgroundManipulation can be defined as an exercise of influence by one person or group, with the intention to attempt to control or modify the actions of another person or group. Thaler and Sunstein (2008) call the tactic of subtly modifying behavior a “nudge4”. Nudging mainly operates through the affective elements of a human rational system. Making use of a nudge might be considered appropriate in situations like teaching children, treating drug dependency, and in some healthcare settings. While nudges can be deployed to encourage individuals to express behaviors that have community benefits, a nudge could have unanticipated consequences for people whose backgrounds were not well considered in the development of the nudging system5. Likewise, nudges may encourage behaviors with unanticipated long-term effects, whether positive or negative, for theindividual and/or society. The effect ofA/IS nudging a person, such as potentially eroding or encouraging individual liberty, or expressing behaviors that are for the benefit others, should be well characterized in the design of A/IS.## Recommendations1\\.Systematic analyses are needed that examine the ethics and behavioral consequences of designing affective systems to nudge human beings prior to deployment.2\\.The user should be empowered, through an explicit opt-in system and readily available, comprehensible information, to recognize different types of A/IS nudges, regardless of whether they seek to promote beneficial social manipulation or to enhance consumer acceptance of commercial goals. The user should be able to access and check facts behind the nudges and then make a conscious decision to accept or reject a nudge. Nudging systems must be transparent, with a clear chain of accountability that includes human agents: data logging is required so users can know how, why, and by whom they were nudged.3\\.A/IS nudging must not become coercive and should always have an opt-in system policy with explicit consent.4\\.Additional protections against unwanted nudging must be put in place for vulnerable populations, such as children, or when informed consent cannot be obtained. Protections against unwanted nudging should be encouraged when nudges alter long-term behavior or when consent alone may not bea sufficient safeguard against coercionor exploitation.5\\.Data gathered which could reveal an individual or groups’ susceptibility to a nudge or their emotional reaction to a nudge should not be collected or distributed without opt-in consent, and should only be retained transparently, with access restrictions in compliance with the highest requirements of data privacy and law.## Further Resources•R. Thaler, and C. R. Sunstein, _Nudge: Improving Decision about Health, Wealth and Happiness_, New Haven, CT: Yale University Press, 2008.•L. Bovens, “The Ethics of Nudge,” in _Preference change: Approaches from Philosophy, Economics and Psychology_, T. Grüne-Yanoff and S. O. Hansson, Eds., Berlin, Germany: Springer, 2008 pp. 207–219.•S. D. Hunt and S. Vitell. \"A General Theory of Marketing Ethics.\" Journal of Macromarketing, vol.6, no. 1, pp. 5-16, June 1986.•A. McStay, [Empathic Media and Advertising: Industry, Policy, Legal and Citizen Perspectives (the Case for Intimacy)](http://journals.sagepub.com/doi/pdf/10.1177/2053951716666868), Big Data \u0026 Society, pp. 1-11, December 2016.•J. de Quintana Medina and P. Hermida Justo, “Not All Nudges Are Automatic: Freedom of Choice and Informative Nudges.” Working paper presented to the European Consortium for Political Research, Joint Session of Workshops, 2016 Behavioral Change and Public Policy, Pisa, Italy, 2016.• M. D. White, _[The Manipulation of Choice. Ethics and Libertarian Paternalism. ](http://www.palgraveconnect.com/doifinder/10.1057/9781137313577)_New York: Palgrave Macmillan, 2013•C.R. Sunstein, The Ethics of Influence: Government in the Age of Behavioral Science. New York: Cambridge, 2016•M. Scheutz, “[The Affect Dilemma for Artificial Agents: Should We Develop Affective Artificial Agents? ](http://ieeexplore.ieee.org/document/6296668/)” _IEEE Transactions on Affective Computing, _vol._ _3, no. 4,pp. 424–433,Sept. 2012.•A. Grinbaum, R. Chatila, L. Devillers, J.G. Ganascia, C. Tessier and M. Dauchet. “[Ethics in Robotics Research: CERNA Recommendations,](http://ieeexplore.ieee.org/document/7822928/)” _IEEE Robotics and Automation Magazine, _vol._ _24, no. 3,pp. 139–145, Sept. 2017.“Designing Moral Technologies: Theoretical, Practical, and Ethical Issues” Conference July 10–15, 2016, Monte Verità, Switzerland\"p.96-97\"Governmental entities may potentially use nudging strategies, for example to promote the performance of charitable acts. Does the practice of nudging for the benefitof society, including nudgesby affective systems, raiseethical concerns?## BackgroundA few scholars have noted a potentially controversial practice of the future: allowing a robot or another affective system to nudge a user for the good of society6. For instance, if it is possible that a well-designed robot could effectively encourage humans to perform charitable acts, would it be ethically appropriate for the robot to do so? This design possibility illustrates just one behavioral outcome that a robot could potentially elicit from a user.Given the persuasive power that an affective system may have over a user, ethical concerns related to nudging must be examined. This includes the significant potential for misuse.## Recommendations1\\.As more and more computing devices subtly and overtly influence human behavior, it is important to draw attention to whether it is ethically appropriate to pursue this type of design pathway in the context of governmental actions.2\\. There needs to be transparency regarding who the intended beneficiaries are, and whether any form of deception or manipulation is going to be used to accomplish the intended goal.## Further Resources•J. Borenstein and R. Arkin, “[Robotic Nudges: Robotic Nudges: The Ethics of Engineering a More Socially Just Human Being Just Human Being.](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)” _Science and Engineering Ethics, _vol._ _22, no. 1,pp. 31–46, Feb. 2016.•J. Borenstein and R. Arkin. “[Nudging for Good: Robots and the Ethical Appropriateness of Nurturing Empathy and Charitable Behavior ](https://link.springer.com/article/10.1007/s00146-016-0684-1?no-access=true).” _AI and Society, _vol._ _32, no. 4, pp. 499–507, Nov. 2016.\"p.97-98","id":"rec2ajglpzbfyrivi","dom_id":"item_rec2ajglpzbfyrivi"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recrBZOfrDC2lKpRM"],"title":"Will AI reduce autonomy through reducing creative, affective, and empathetic elements of management?","category":"Challenges","name":"rec3Fm8dyG49YU137","tags":["human-potential"],"created_at":"2023-06-05T06:13:18.000Z","description":"\"Will extensive use ofA/IS in society make our organizations more brittle by reducing human autonomy within organizations, and by replacing creative, affective, empathetic componentsof management chains?## BackgroundIf human workers are replaced by A/IS, the possibility of corporations, governments, employees, and customers discovering new equilibria outside the scope of what the organizations’ past leadership originally foresaw may be unduly limited. A lack of empathy based on shared needs, abilities, and disadvantages between organizations and customers causes disequilibria between the individuals and corporations and governments that exist to serve them. Opportunities for useful innovation may therefore be lost through automation. Collaboration requires enough commonalityof collaborating intelligences to create empathy— the capacity to model the other’s goals basedon one’s own.According to scientists within several fields, autonomy is a psychological need. Without it, humans fail to thrive, create, and innovate.Ethically aligned design should support, not hinder, human autonomy or its expression.## Recommendations1\\.It is important that human workers’ interaction with other workers not always be intermediated by affective systems (or other technology) which may filter out autonomy, innovation, and communication.2\\.Human points of contact should remain available to customers and other organizations when using A/IS.3\\.Affective systems should be designed to support human autonomy, sense of competence, and meaningful relationships as these are necessary to support a flourishing life.4\\.Even where A/IS are less expensive, more predictable, and easier to control than human employees, a core network of human employees should be maintained at every level of decision-making in order to ensure preservation of human autonomy, communication, and innovation.5\\.Management and organizational theorists should consider appropriate use of affective and autonomous systems to enhance their business models and the efficacy of their workforce within the limits of the preservation of human autonomy.## Further reading•J. J. Bryson, “Artificial Intelligence and Pro-Social Behavior,” in _Collective Agency and Cooperation in Natural and Artificial Systems, _C. Misselhorn, Ed., pp. 281–306, Springer, 2015.•D. Peters, R.A. Calvo, and R.M. Ryan,“[Designing for Motivation, Engagement and  Wellbeing in Digital Experience](https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00797/full),”_ Frontiers in Psychology_– Human Media Interaction, vol. 9, pp 797, 2018.\"\"p.100-101","id":"rec3fm8dyg49yu137","dom_id":"item_rec3fm8dyg49yu137"},{"Cases":["reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5"],"Principles":["recLHILkx2JDFsLbX"],"title":"Tyranny of averages","category":"Challenges","name":"rec5VzvnIZcaNKqDh","tags":[],"created_at":"2023-05-18T14:51:53.000Z","id":"rec5vzvnizcankqdh","dom_id":"item_rec5vzvnizcankqdh"},{"Principles":["recQ9DIFEsOEkCx3O"],"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recZI7HDWKBU5T22v"],"title":"How do we incorporate stakeholder processes to determine key wellbeing considerations in AI design/development/use?","category":"Challenges","name":"rec5nbWC0ZbVwnmxJ","tags":["wellbeing"],"created_at":"2023-06-05T05:56:59.000Z","description":"\"**Issue: **Decision processes for determining relevant well-being indicators through stakeholder deliberations need to be established.## BackgroundA/IS stakeholder involvement is necessary to determine relevant well-being indicators, for a number of reasons:•“Well-being” will be defined differently by different groups affected by A/IS. The most relevant indicators of well-being may vary according to country, with concerns of wealthy nations being different than those of low- and middle-income countries. Indicators may vary based on geographical region or unique circumstances. The indicators may also be different across social groups, including gender, race, ethnicity, and disability status.•Common indicators of well-being include satisfaction with life, healthy life expectancy, government, social support, perceived freedom to make life decisions, income equality, access to education, and poverty rates. Applying them in particular settings necessarily requires judgment, to ensure that assessments of well-being are in fact meaningful in context and reflective of the life circumstances of the diverse groups in question.•Not all aspects of well-being are easily quantifiable. The importance of hard-to-quantify aspects of well-being is most likely to become apparent through interaction with those more directly affected by A/IS in specific settings.•Engineers and corporate employees frequently misunderstand stakeholders’ needs and expectations, especially when the stakeholders are very different from them in terms of educational and cultural background, social location, and/or economic status.The processes through which stakeholders become involved in determining relevant wellbeing indicators will affect the quality of the indicators selected and assessed. Stakeholders should be empowered to define well-being, assess the appropriateness of existing indicators and propose new ones, and highlight context-specific factors that bear on issues of well-being, whether or not the issues have been recognized previously or are amenable to measurement. Interactive, open-ended discussions or deliberations among a wide variety of stakeholders and system designers are more likely to yield robust, widely-shared understandings of well-being and how to measure it in context. Closed-ended or over-determined methods for soliciting stakeholder input are likely to miss relevant information that system designers have not anticipated.deliberation is one model for collective decisionmaking. Parties in such deliberation come together as equals. Their goal is to set aside their immediate, personal interests in order to think together about the common good. Participants in a stakeholder engagement and deliberation learn from one another’s perspectives and experiences.**In the real world, stakeholder engagement and deliberation may run into the following challenges:**•Individuals with more education, power, or higher social status may—intentionally or unintentionally—dominate the discussion, undermining their ability to learn from less powerful participants.•Topics may be preemptively ruled “out of bounds”, to the detriment of collective problem-solving. An example would be if, in a deliberation on well-being and A/IS, participants were told that worries about the costs of health insurance were unrelated toA/IS and thus could not be discussed.•Engineers and scientists may claim authority over technical issues and be willing to deliberate only on social issues, obscuring the ways that technical and social issues are intertwined.•Less powerful groups may be unable to keep more powerful ones “at the table” when discussions get contentious, and vice versa.•Participants may not agree on who can legitimately be involved in the conversation. For example, the consensual spirit of deliberation is often used as a justification for excluding activists and others who already hold a position on the issue.**Stakeholder engagement and deliberative processes can be effective when:**•Their design is guided by experts or practitioners who are experienced in deliberation models.•Deliberations are facilitated by individuals sensitive to issues of power and are skilled in mediating deliberation sessions.•Less powerful actors participate with the help of allies who can amplify their voices.•More powerful actors participate with an awareness of their own power and make a commitment to listen with humility, curiosity, and open-mindedness.•Deliberations are convened by institutions or individuals who are trusted and respected by all parties and who hold all actors accountable for participating constructively.Ethically aligned design of A/IS would be furthered by thoughtfully constructed, context-specific deliberations on well-being and the best indicators for assessing it.## RecommendationAppoint a lead team or person, “leads”, to facilitate stakeholder engagement and to serve as a resource for A/IS creators who use stakeholderbased processes to establish well-being indicators. Specifically:•Leads should solicit and collect lessons learned from specific applications of stakeholder engagement and deliberation in order to continually refine its guidance.• When determining well-being indicators, the leads should enlist the help of experts in public participation and deliberation. With expert guidance, facilitators can provide guidance for how to: take steps to mitigate the effects of unequal power in deliberative processes; incorporate appropriately trained facilitators and coaching participants in deliberations; recognize and curb disproportionate influence by morepowerful groups; use techniques to maximize the voices of less-powerful groups.•Leads should use their convening power to bring together A/IS creators and stakeholders, including critics of A/IS, for deliberations on well-being indicators, impacts, and other considerations for specific contexts and settings. Leads’ involvement would help bring actors to the table with a balance of power and encourage all actors to remain in conversation until robust, mutually agreeable definitionsare found.## Further Resources•D. E. Booher and J. E. Innes. Planning with Complexity: An Introduction to Collaborative Rationality for Public Policy. London:Routledge, 2010.•J. A. Leydens and J. C. Lucena. Engineering Justice: Transforming Engineering Education and Practice_. _Wiley-IEEE Press, 2018.•G. Ottinger. [Assessing Community Advisory ](https://www.sciencehistory.org/sites/default/files/studies-in-sustainability-ottinger2009.pdf)[Panels: A Case Study from Louisiana’s Industrial Corridor.](https://www.sciencehistory.org/sites/default/files/studies-in-sustainability-ottinger2009.pdf) Center for Contemporary History and Policy, 2008.•[Expert and Citizen Assessment of Science and ](https://ecastnetwork.org/about/)[Technology (ECAST) Network ](https://ecastnetwork.org/about/)\"p.82-83","id":"rec5nbwc0zbvwnmxj","dom_id":"item_rec5nbwc0zbvwnmxj"},{"Principles":["recKdujFoPJr4ZAhZ"],"Reference":"Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector. The Alan Turing Institute. https://doi.org/10.5281/ZENODO.3240529","Sources":["recfYC5jjPmpLfSlM"],"title":"Lack of accountability through automated decisions leading to loss of autonomy","category":"Challenges","name":"rec8hxSSEudaplJiA","tags":[],"created_at":"2023-05-18T19:10:49.000Z","description":"“When citizens are subject to decisions, predictions, or classifications produced by AI systems, situations may arise where such individuals are unable to hold directly accountable the parties responsible for these outcomes. AI systems automate cognitive functions that were previously attributable exclusively to accountable human agents. This can complicate the designation of responsibility in algorithmically generated outcomes, because the complex and distributed character of the design, production, and implementation processes of AI systems may make it difficult to pinpoint accountable parties. In cases of injury or negative consequence, such an accountability gap may harm the autonomy and violate the rights of the affected individuals.” (Leslie, 2019, p. 4)","id":"rec8hxsseudapljia","dom_id":"item_rec8hxsseudapljia"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["rec81gtnlFS5W2BBF"],"title":"How do we ensure assumptions and limitations are clear in technical documentation?","category":"Challenges","name":"rec9Cuz3HWc9lWW23","tags":["corporate-ethics"],"created_at":"2023-06-05T10:57:55.000Z","description":"**\"Issue:** Need for bettertechnical documentation ## BackgroundA/IS are often construed as fundamentally opaque and inscrutable. However, lack of transparency is often the result of human decision. The problem can be traced to avariety of sources, including poor documentation that excludes vital information about the limitations and assumptions of a system.Better documentation combined withinternal and external auditing are crucial tounderstanding a system’s ethical impact.## RecommendationEngineers should be required to thoroughly document the end product and related data flows, performance, limitations, and risks ofA/IS. Behaviors and practices that have been prominent in the engineering processes should also be explicitly presented, as well as empirical evidence of compliance and methodology used, such as training data used in predictive systems, algorithms and components used, and results of behavior monitoring. Criteria for such documentation could be: auditability, accessibility, meaningfulness, and readability.Companies should make their systems auditable and should explore novel methods for external and internal auditing.## Further reading•S. Wachter, B. Mittelstadt, and L. Floridi. “[Transparent, Explainable, and Accountable AI for Robotics](http://robotics.sciencemag.org/content/2/6/eaan6080).” _[Science Robotics, v](http://robotics.sciencemag.org/content/2/6/eaan6080)ol. _2, no. 6, May 31, 2017. [Online]. Available: DOI: 10.1126/scirobotics.aan6080. [Accessed Nov.•S. Barocas, and A. D. Selbst, “[Big Data’s Disparate Impact.](http://www.californialawreview.org/wp-content/uploads/2016/06/2Barocas-Selbst.pdf)” _California Law Review_ 104, 671-732, 2016.•J. A. Kroll, J. Huey, S. Barocas, E. W. Felten, J. R. Reidenberg, D. G. Robinson, and H. Yu. “[Accountable Algorithms.](https://www.pennlawreview.com/print/165-U-Pa-L-Rev-633.pdf)” _University of Pennsylvania Law Review _165, no. 1, 633– 705, 2017.•J. M. Balkin, “[Free Speech in the Algorithmic Society: Big Data, Private Governance, and New School Speech Regulation.](https://ssrn.com/abstract%3D3038939)” _UC Davis Law Review_, 2017.\"p.135","id":"rec9cuz3hwc9lww23","dom_id":"item_rec9cuz3hwc9lww23"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recCN9eqkgKT3KjCB"],"title":"How do we encourage taking ownership and responsibility for AI impacts?","category":"Challenges","name":"rec9qgAZS5U8GL137","tags":["corporate-ethics"],"created_at":"2023-06-05T10:33:11.000Z","description":"\"Ownership and responsibility## BackgroundThere is variance within the technology community on how it sees its responsibility regarding A/IS. The difference in values and behaviors are not necessarily aligned with the broader set of social concerns raised by public, legal, and professional communities. The current makeup of most organizations has clear delineations among engineering, legal, and marketing functions. Thus, technologists will often be incentivized in terms of meeting functional requirements, deadline, and financial constraints, but for larger social issues may say, “Legal will handle that.” In addition, in employment and management technology or work contexts, “ethics” typically refers to a code of conduct regarding professional behavior versus a values-driven design process mentality.As such, ethics regarding professional conduct often implies moral issues such as integrity or the lack thereof, in the case of whistleblowing, for instance. However, ethics in A/IS design include broader considerations about the consequences of technologies.## RecommendationsOrganizations should clarify the relationship between professional ethics and appliedA/IS ethics by helping or enabling designers, engineers, and other company representatives to discern the differences between these kinds of ethics and where they complement each other.Corporate ethical review boards, or comparable mechanisms, should be formed to addressethical and behavioral concerns in relation toA/IS design, development and deployment. Such boards should seek an appropriately diverse composition and use relevant criteria, including both research ethics and product ethics, at the appropriate levels of advancement of research and development. These boards should examine justifications of research or industrial projects.## Further Resources• HH van der Kloot Meijberg and RHJ ter Meulen, “[Developing Standards for Institutional](https://jme.bmj.com/content/27/suppl_1/i36.full) [Ethics Committees: Lessons from the Netherlands,](https://jme.bmj.com/content/27/suppl_1/i36.full)” _Journal of Medical Ethics_ 27 i36-i40, 2001.\"p.130","id":"rec9qgazs5u8gl137","dom_id":"item_rec9qgazs5u8gl137"},{"Principles":["recQ9DIFEsOEkCx3O","receFm7cGasHwpJZO"],"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recZI7HDWKBU5T22v","recfcXzM3foqFNNGN"],"title":"How do we incorporate wellbeing considerations into AI impact measurement and monitoring?","category":"Challenges","name":"recA7Kh502s4UKWGo","tags":["wellbeing"],"created_at":"2023-06-05T05:53:34.000Z","description":"\"How can A/IS creators incorporate well-being intotheir work?## BackgroundWithout practical ways of incorporating well-being metrics to guide, measure, and monitor impact, A/IS will likely lack fall short of its potential to avoid harm and promote well-being. Incorporating well-being thinking into typical organizational processes of design, prototyping, marketing, etc., suggests a variety of adaptations.Organizations and A/IS creators should consider clearly defining the type of A/IS product or service that they are developing, including articulating its intended stakeholders and uses. By defining typical uses, possible uses, and finally unacceptable uses of the technology, creators will help to spell out the context of well-being. This can help to identify possible harms and risks given the different possible uses and end users, as well as intended and unintended positive consequences.Additionally, internal and external stakeholders should be extensively consulted to ensure that impacts are thoroughly considered through an iterative and learning stakeholder engagement process. After consultation, A/IS creators should select appropriate well-being indicators based on the possible scope and impact of their A/IS product or service. These well-being indicators can be drawn from mainstream sources and models and adapted as necessary. They can be used to engage in pre-assessment of the intended user population, projection of possible impacts, and post-assessment. Development of a well-being indicator measurement plan and relevant data infrastructure will support a robust integration of well-being. A/IS models can also be trained to explicitly include well-being indicators as subgoals.Data and discussions on well-being impacts can be used to suggest improvements and modifications to existing A/IS products and services throughout their lifecycle. For example, a [team](https://www.aaai.org/Papers/Symposia/Fall/2008/FS-08-02/FS08-02-024.pdf) [seeking to increase the well-being o](https://www.aaai.org/Papers/Symposia/Fall/2008/FS-08-02/FS08-02-024.pdf)f people using wheelchairs found that when provided the opportunity to use a smart wheelchair, some users were delighted with the opportunity for more mobility, while others felt it would decrease their opportunities for social contact, increase their sense of isolation, and lead to an overall decrease in their well-being. Therefore, even though a product modification may increase well-being according to one indicator or set of A/IS stakeholders, it does not mean that this modification should automatically be adopted.Finally, organizational processes can be modified to incorporate the above strategies. Appointment of an organizational lead person for well-being impacts, e.g., a well-being lead, ombudsman,or officer can help to facilitate this effort.## RecommendationA/IS creators should adjust their existing development, marketing, and assessment cycles to incorporate well-being concerns throughout their processes. This includes identification of an A/IS lead ombudsperson or officer; identification of stakeholders and end users; determination of possible uses, harm and risk assessment; robust stakeholder engagement; selection of well-being indicators; development of a well-being indicator measurement plan; and ongoing improvement of A/IS products and services throughout the lifecycle.## Further Resources•[Peter Senge and the Learning Organization](http://infed.org/mobi/peter-senge-and-the-learning-organization/)[ -](https://api.ag.purdue.edu/api/depotws/File.ashx?t=f\u0026i=11736)(synopsis) Purdue University•Stakeholder Engagement: A Good Practice Handbook for Companies Doing Business in Emerging Markets. International Finance Corporation, May 2007.•[Global Reporting Initiative](https://www.globalreporting.org/Pages/default.aspx)• [GNH Certification](http://www.bhutanstudies.org.bt/gnh-certification/), Centre for Bhutanand GNH Studies, 2018.•J. Helliwell, R. Layard, and J. Sachs, Eds., “The Objective Benefits of Subjective Well-Being,” in [World Happiness Report ](http://worldhappiness.report/ed/2013/)2013. New York: UN Sustainable Development Solutions Network, pp. 54-79, 2013.•[Global Happiness and Well-being Policy Report](http://www.happinesscouncil.org/) by the Global Happiness Council, 2018.\"p.78-79","id":"reca7kh502s4ukwgo","dom_id":"item_reca7kh502s4ukwgo"},{"Principles":["recPg7Ov0priGGtLm"],"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recG86YsfXnKY9kNc"],"title":"How do we know how our data is being used?","category":"Challenges","name":"recAoyMGCCLhaSqEb","tags":[],"created_at":"2023-06-05T09:34:56.000Z","description":"\"**Issue: **What would it mean for a person to have an algorithmic agent helping them actively represent and curate their terms and conditions at all times?## BackgroundWhile it’s essential to create your own terms and conditions to broadcast your preferences, it’s also important to recognize that humans do not operate at an algorithmic speed or level. A significant part of retaining your agency in this way involves identifying trusted services that can essentially act on your behalf when making decisions about your data.Part of this logic entails putting you “at the center of your data”. One of the greatest challenges to user agency is that once you give your data away, you do not know how it is being used or by whom. But when all transactions about your data go through your A/IS agent honoring your preferences, you have better opportunities to control how your information is shared.As an example, with medical data—while it is assumed most would share all their medical data with their spouse—most would also not wish to share that same amount of data with their local gym. This is an issue that extends beyond privacy, meaning one’s cultural or individual preferences about what personal information to share, to utility and clarity. This type of sharing also benefits users or organizations on the receiving end of data from these exchanges. For instance, the local gym in the previous example may only need basic heart or general health information and would actually not wish to handle or store sensitive cancer or other personal health data for reasons of liability.A precedent for this type of patient- or usercentric model comes from Gliimpse, a service described by Jordan Crook from _TechCrunch_ in his article, “[Apple acquired Gliimpse, a personal health data startup”](https://techcrunch.com/2016/08/22/apple-acquired-gliimpse-a-personal-health-data-startup/): “Gliimpse works by letting users pull their own medical info into a single virtual space, with the ability to add documents and pictures to fill out the profile. From there, users can share that data (as a comprehensive picture) to whomever they wish.” The fact that Apple acquired the startup points to the potential for the successful business model of user-centric data exchange and putting individuals at the center of their data. A person’s A/IS agent is a proactive algorithmic tool honoring their terms and conditions in the digital, virtual, and physical worlds. Any public space where a user may not be aware they are under surveillance by facial recognition, biometric, or other tools that could track, store, and utilize their data can now provide overt opportunity for consent via an A/IS agent platform. Even where an individual is not sure they are being tracked, by broadcasting their terms and conditions via digital means, they can demonstrate their preferences in the public arena. Via Bluetooth or similar technologies, individuals could offer their terms and conditions in a ubiquitous and always-on manner. This means even when an individual’s terms and conditions are not honored, people would have the ability to demonstrate their desire not to be tracked which could provide a methodology for the democratic right to protest in a peaceful manner. And where those terms and conditions are recognized- meaning technically recognized even if they are not honored-one’s opinions could be formally logged via GPS and timestamp data.The A/IS agent could serve as an educator and negotiator on behalf of its user by suggesting how requested data could be combined with other data that has already been provided, inform the user if data are being used in a way that was not authorized, or make recommendations to the user based on a personal profile. As a negotiator, the agent could broker conditions for sharing data and could include payment to the user as a term, or even retract consent for the use of data previously authorized, for instance, if a breach of conditions was detected.## RecommendationsAlgorithmic agents should be developed for individuals to curate and share their personal data. Specifically:•For purposes of privacy, a person must be able to set up complex permissions that reflect a variety of wishes.•The agent should help a person foresee and mitigate potential ethical implications of specific machine learning data exchanges.•A user should be able to override his/her personal agents should he/she decide that the service offered is worth the conditions imposed.•An agent should enable machine-to-machine processing of information to compare, recommend, and assess offers and services.•Institutional systems should ensure support for and respect the ability of individuals to bring their own agent to the relationship without constraints that would make some guardians inherently incompatible or subject to censorship.•Vulnerable parts of the population will need protection in the process of granting access.## Further Resources•IEEE P7006™ - IEEE [Standards Project on Personal Data AI Agent Working Group](https://standards.ieee.org/develop/project/7006.html). Designed as a tool to allow any individual to create their own personal “terms and conditions” for their data, the AI Agent will also provide a technological tool for individuals to manage and control their identity in the digital and virtual world.•Tools allowing an individual to create a form of an algorithmic guardian are often labeled as PIMS, or Personal Information Management Services. [Nesta in the United Kingdom was one of the funders of early research about PIMS ](http://www.nesta.org.uk/publications/personal-information-management-services-analysis-emerging-market)conducted by [CtrlShift](https://www.ctrl-shift.co.uk/).\"p.111-112","id":"recaoymgcclhasqeb","dom_id":"item_recaoymgcclhasqeb"},{"Cases":["recSXcY4cnofb4zTP"],"Principles":["recPg7Ov0priGGtLm"],"Reference":"Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector. The Alan Turing Institute. https://doi.org/10.5281/ZENODO.3240529","Sources":["recfYC5jjPmpLfSlM"],"title":"Privacy breaches","category":"Challenges","name":"recBc3GCNokDL220T","tags":[],"created_at":"2023-05-18T19:10:38.000Z","description":"“Threats to privacy are posed by AI systems both as a result of their design and development processes, and as a result of their deployment. As AI projects are anchored in the structuring and processing of data, the development of AI technologies will frequently involve the utilisation of personal data. This data is sometimes captured and extracted without gaining the proper consent of the data subject or is handled in a way that reveals (or places under risk the revelation of) personal information. On the deployment end, AI systems that target, profile, or nudge data subjects without their knowledge or consent could in some circumstances be interpreted as infringing upon their ability to lead a private life in which they are able to intentionally manage the transformative effects of the technologies that influence and shape their development. This sort of privacy invasion can consequently harm a person’s more basic right to pursue their goals and life plans free from unchosen influence.” (Leslie, 2019, p. 5) ","id":"recbc3gcnokdl220t","dom_id":"item_recbc3gcnokdl220t"},{"Principles":["recKdujFoPJr4ZAhZ"],"Reference":"Fedoruk, L. (2017b). Ethics in The Scholarship of Teaching and Learning. University of Calgary Taylor Institute for Teaching and Learning. https://taylorinstitute.ucalgary.ca/resources/ethics-scholarship-teaching-and-learning","Sources":["recQzldmBLByP78Uu"],"title":"SoTL power","category":"Challenges","name":"recCkVZngvMA5I5uv","tags":[],"created_at":"2023-05-19T13:05:35.000Z","description":"“Because Instructors typically conduct SoTL in their classrooms (current or former), SoTL practitioners frequently find themselves in the dual role of instructor and researcher. Ultimately, the instructor-researcher in SoTL is an instructor first. As MacLean and Poole (2010, pg. 3) explain, “The teacher’s responsibility to hold students’ educational interests paramount provides an important perspective when considering ethical issues for research in teaching and learning.” This dual role can raise a set of specific ethical dilemmas that require instructor-researchers to plan parts of the research carefully and to ask themselves challenging questions. Potential ethical dilemmas can arise concerning the following areas of ethical consideration. In the table below, we articulate several core principles for ethical practice that respond to these potentially dilemmatic areas of consideration and elaborate on them in the remainder of the document.” (Fedoruk, 2017, p. 4)““Dual roles of researchers and their associated obligations (e.g., acting as both a researcher and a therapist, health care provider, caregiver, teacher, advisor, consultant, supervisor, student or employer) may create conflicts, undue influences, power imbalances or coercion that could affect relationships with others and affect decision-making procedures (e.g., consent of participants). Article 3.2(e) reminds researchers of relevant ethical duties that govern real, potential or perceived conflicts of interest as they relate to the consent of participants. To preserve and not abuse the trust on which many professional relationships rest, researchers should be fully cognizant of conflicts of interest that may arise from their dual or multiple roles, their rights and responsibilities, and how they can manage the conflict. When acting in dual or multiple roles, the researcher shall disclose the nature of the conflict to the participant in the consent process” (TCPS2, Chapter 7, D. Researchers and Conflicts of Interest).” (Fedoruk, 2017, p. 5)““The approach to recruitment is an important element in assuring voluntariness. In particular, how, when and where participants are approached, and who recruits them are important elements in assuring (or undermining) voluntariness. In considering the voluntariness of consent, REBs and researchers should be cognizant of situations where undue influence, coercion, or the offer of incentives may undermine the voluntariness of a participants’ consent to participate in research” (TCPS2, Chapter 3, A. General Principles, “Consent Should Be Given Voluntarily”).” (Fedoruk, 2017, p. 7)““Consent shall be maintained throughout the research project. Researchers have an ongoing duty to provide participants with all information relevant to their ongoing consent to participate in the research” (TCPS2, Chapter 3, A. General Principles, “Consent Shall Be an Ongoing Process”).” (Fedoruk, 2017, p. 8)““Taking into account the scope and objectives of their research, researchers should be inclusive in selecting participants. Researchers shall not exclude individuals from the opportunity to participate in research on the basis of attributes such as culture, language, religion, race, disability, sexual orientation, ethnicity, linguistic proficiency, gender or age, unless there is a valid reason for the exclusion. Application ... The focus, objective, nature of research and context in which the research is conducted inform the inclusion and exclusion criteria for a specific research project... Other examples include research focused on specific cultural traditions or languages, or on one age group...Such research should not be precluded so long as the selection criteria for those included in the research are germane to answering the research question. Researchers who plan to actively exclude particular groups should clarify to their REBs the grounds for the exclusion” (TCPS2, Chapter 4, A. Appropriate Inclusion).” (Fedoruk, 2017, p. 9)““Researchers should anticipate, to the best of their ability, needs of participants, groups and their communities that might arise in any given research project. ... Researchers should consider ways to ensure the equitable distribution of any benefits of participation in research” (TCPS2, Chapter 4, B. Inappropriate Exclusion, “Participants’ Vulnerability and Research”).” (Fedoruk, 2017, p. 9)““Researchers should normally provide copies of publications, or other research reports or products, arising from the research to the institution or organization – normally the host institution – that is best suited to act as a repository and disseminator of the results within the participating communities. This may not be necessary for jurisdictions where the results are readily available in print or electronically. In general, researchers should ensure that participating individuals, groups and communities are informed of how to access the results of the research. Results of the research should be made available to them in a culturally appropriate and meaningful format, such as reports in plain language in addition to technical reports” (TCPS2, Chapter 4, B. Inappropriate Exclusion, “Equitable Distribution of Research Benefits”).” (Fedoruk, 2017, p. 10)““Researchers shall safeguard information entrusted to them and not misuse or wrongfully disclose it. Institutions shall support their researchers in maintaining promises of confidentiality” (TCPS2, Chapter 5, B. Ethical Duty of Confidentiality).” (Fedoruk, 2017, p. 11)““Researchers shall describe measures for meeting confidentiality obligations and explain any reasonably foreseeable disclosure requirements: a. in application materials they submit to the REB; and b. during the consent process with prospective participants” (TCPS2, Chapter 5, B. Ethical Duty of Confidentiality).” (Fedoruk, 2017, p. 11)““Researchers shall provide details to the REB regarding their proposed measures for safeguarding information, for the full life cycle of information: its collection, use, dissemination, retention and/or disposal” (TCPS2, Chapter 5, C. Safeguarding Information).” (Fedoruk, 2017, p. 12)““Institutions or organizations where research data are held have a responsibility to establish appropriate institutional security safeguards” (TCPS2, Chapter 5, C. Safeguarding Information).” (Fedoruk, 2017, p. 12)","id":"recckvzngvma5i5uv","dom_id":"item_recckvzngvma5i5uv"},{"Principles":["recU6u0AZbcNj1ik9","reczVPIH1y2OMpAJH","rec42P8U9usfYCtv9"],"title":"Incidental findings","category":"Challenges","name":"recCpTJXuuowqsqQa","tags":["AI"],"created_at":"2023-05-18T13:41:32.000Z","id":"reccptjxuuowqsqqa","dom_id":"item_reccptjxuuowqsqqa"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recI7WdfNhrQxRt7F"],"title":"How can institutional ethics review committees provide oversight of AI research?","category":"Challenges","name":"recELObWGfkhXzFG2","tags":["research"],"created_at":"2023-06-05T10:22:04.000Z","description":"**Issue: **Institutional ethics committees in the A/IS fields## BackgroundIt is unclear how research on the interface of humans and A/IS, animals and A/IS, and biological hazards will impact research ethical review boards. Norms, institutional controls, and risk metrics appropriate to the technology are not well established in the relevant literature and research governance infrastructure. Additionally, national and international regulations governing review of human-subjects research may explicitly or implicitly exclude A/IS research from their purview on the basis of legal technicalities or medical ethical concerns, regardless of the potential harms posed by the research.Research on A/IS human-machine interaction, when it involves intervention or interaction with identifiable human participants or their data, typically falls to the governance of research ethics boards, e.g., institutional review boards. The national level and institutional resources, e.g., hospitals and universities, necessary to govern ethical conduct of Human-Computer Interaction (HCI), particularly within the disciplines pertinent to A/IS research, are underdeveloped.First, there is limited international or national guidance to govern this form of research. Sections of IEEE standards governing research on A/IS in medical devices address some of the issues related to the security of A/ISenabled devices. However, the ethics of testing those devices for the purpose of bringing them to market are not developed into policies or guidance documents from recognized national and international bodies, e.g., U.S. Food and Drug Administration (FDA) and EU European Medicines Agency (EMA). Second, the bodies that typically train individuals to be gatekeepers for the research ethics bodies are under-resourced in terms of expertise for A/IS development, e.g., Public Responsibility in Medicine and Research (PRIM\u0026R) and the Society of Clinical Research Associates (SoCRA). Third, it is not clear whether there is sufficient attention paid to A/IS ethics by research ethics board members or by researchers whose projects involve the use of human participants or their identifiable data.For example, research pertinent to the ethics-governing research at the interface of animalsand A/IS research is underdeveloped with respect to systematization for implementation by the Institutional Animal Care and Use Committee (IACUC) or other relevant committees. In institutions without a veterinary school, it is unclear that the organization would have the relevant resources necessary to conduct an ethical review of such research.Similarly, research pertinent to the intersection of radiological, biological, and toxicological research —ordinarily governed under institutional biosafety committees—and A/IS research is not oftenfound in the literature pertinent to researchethics or research governance.## RecommendationsThe IEEE and other standards-setting bodies should draw upon existing standards, empirical research, and expertise to identify prioritiesand develop standards for the governance ofA/IS research and partner with relevant national agencies, and international organizations,when possible.## Further Resources•S. R. Jordan, “The Innovation Imperative.” _Public Management Review _16, no. 1,pp. 67–89, 2014.•B. Schneiderman, “[The Dangers of Faulty, Biased, or Malicious Algorithms Requires Independent Oversight.](http://www.pnas.org/content/113/48/13538.long)” _Proceedings of the National Academy of Sciences of the United States of America _113, no. 48, 13538–13540, 2016.•J. Metcalf and K. Crawford, “[Where are Human Subjects in Big Data Research? The Emerging Ethics Divide](http://papers.ssrn.com/abstract%3D2779647).” _Big Data \u0026 Society,_ May 14, 2016._ _[Online]. Available: SSRN: [https://ssrn. com/abstract=2779647](https://ssrn.com/abstract=2779647). [Accessed Nov. 1, 2018].•R. Calo, “[Consumer Subject Review Boards: A Thought Experiment](https://www.stanfordlawreview.org/online/privacy-and-big-data-consumer-subject-review-boards/).” _Stanford Law Review Online _66 97, Sept. 2013. p.125","id":"recelobwgfkhxzfg2","dom_id":"item_recelobwgfkhxzfg2"},{"Principles":["recSqx6wklVpDzx3s","recMGB4iC5oaCtr5x"],"title":"Digital divides in burden of data capture","category":"Challenges","name":"recENQXCSOQQ3Y8et","tags":[],"created_at":"2023-05-18T18:54:06.000Z","id":"recenqxcsoqq3y8et","dom_id":"item_recenqxcsoqq3y8et"},{"Principles":["recKdujFoPJr4ZAhZ"],"title":"Autonomy reduced through power relationships such as teacher-student, administrator-teacher, or perception that a tool cannot be questioned","category":"Challenges","name":"recGEFZ3EynXjRaa9","tags":[],"created_at":"2023-05-18T18:55:13.000Z","id":"recgefz3eynxjraa9","dom_id":"item_recgefz3eynxjraa9"},{"Principles":["recQ9DIFEsOEkCx3O","receFm7cGasHwpJZO"],"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recZI7HDWKBU5T22v"],"title":"How do we incorporate wellbeing considerations into AI design/development/use?","category":"Challenges","name":"recGRpoF7DA23ODSj","tags":["wellbeing"],"created_at":"2023-06-05T05:54:42.000Z","description":"\"How can A/IS creators influence A/IS goals to ensure well-being, and what can A/IS creators learn or borrow from existing models in the well-being and other arenas?## BackgroundAnother way to incorporate considerations of well-being is to include well-being measuresin the development, goal setting, and trainingof the A/IS systems themselves.Identified metrics of well-being could be formulated as auxiliary objectives of the A/IS. As these auxiliary well-being objectives will be only a subset of the intended goals of the system, the architecture will need to balance multiple objectives. Each of these auxiliary objectives may be expressed as a goal, set of rules, set of values, or as a set of preferences, which can be weighted and combined using established methodologies from intelligent systems engineering.For example, an educational A/IS tool could not only optimize learning outcomes, but also incorporate measures of student social and emotional education, learning, and thriving.A/IS-related data relates both to the individual— through personalized algorithms, in conjunction with affective sensors measuring and influencing emotion, and other aspects of individual well-being —and to society as large data sets representing aggregate individual subjective and objective data. As the exchange of this data becomes more widely available via establishing tracking methodologies, the data can be aligned within A/IS products and services to increase human well-being. For example, robots like [Pepper](https://www.sbs.com.au/news/dateline/article/2017/04/11/love-intimacy-and-companionship-tale-robots-japan) are equipped to share data regarding their usage and interaction with humans to the cloud. This allows almost instantaneous innovation, as once an action is validated as useful for one Pepper robot, all other Pepper units (and ostensibly their owners) benefit as well. As long as this data exchange happens with the predetermined consent of the robots’ owners, this innovation in real time model can be emulated for the large-scale aggregation of information relating to existing well-being metrics.A/IS creators can also help to operationalize well-being metrics by providing stakeholders with reports on the expected or actual outcomes of the A/IS and the values and objectives embedded in the systems. This transparency will help creators, users, and third parties assess the state of well-being produced by A/IS and make improvements in A/IS. In addition, A/IS creators should consider allowing end users to layer on their own preferences, such as allowing users to limit their use of an A/IS product if it leads to increased sustained stress levels, sustained isolation, development of unhealthy habits, or other decreases to well-being.Incorporating well-being goals and metrics into broader organizational values and processes would support the use of well-being metrics as there would be institutional support. A key factor in industrial, corporate, and societal progress is cross-dissemination of concepts and models from one industry or field to another. To date, a number of successful concepts and models exist in the fields of sustainability, economics, industrial design and manufacturing, architecture and urban development, and governmental policy. These concepts and models can provide a foundationfor building a metrics standard and the use of wellbeing metrics by A/IS creators, from conception and design to marketing, product updates, and improvements to the user experience.## RecommendationCreate technical standards for representing goals, metrics, and evaluation guidelines for well-being metrics and their precursors and components within A/IS that include:•[O](https://en.wikipedia.org/wiki/Ontology_(information_science))ntologies for representing technological requirements.•A testing framework for validating adherence to well-being metrics and ethical principles such as [IEEE P7010™ Standards Project for Wellbeing Metric for Autonomous and Intelligent Systems](https://standards.ieee.org/project/7010.html).above as well as others as a basis for a wellbeing metrics standard for A/IS creators. _(See page 191, [Additional Resources: Additional Resources: Standards Development Models and Frameworks)](https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/ead1e_standards_development_models_frameworks.pdf)_•The development of a well-being metrics standard for A/IS that encompasses an understanding of well-being as holistic and interlinked to social, economic, and ecological systems.## Further Resources•A.F.T Winfield, C. Blum, and W. Liu. “[Towards an Ethical Robot: Internal Models, Consequences and Ethical Action Selection](https://link.springer.com/chapter/10.1007/978-3-319-10401-0_8),” in Advances in Autonomous Robotics Systems. Springer, 2014, pp. 85–96•R. A. Calvo, and D. Peters. [Positive Computing: Technology for Well-Being and Human Potential](https://mitpress.mit.edu/books/positive-computing)._ _Cambridge MA: MIT Press, 2014.•Y. Collette, and P. Slarry. [Multiobjective Optimization: Principles and Case Studies ](https://link.springer.com/book/10.1007%2F978-3-662-08883-8)(Decision Engineering Series). Berlin, Germany:Springer, 2004. doi: 10.1007/978-3-662-08883-8.•J. Greene, et al. “[Embedding Ethical Principles in Collective Decision Support Systems](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12457),” in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence_, _4147–4151. Palo Alto, CA: AAAI Press, 2016.•L. Li, I. Yevseyeva, V. Basto-Fernandes, H. Trautmann, N. Jing, and M. Emmerich,“[Building and Using an Ontology of Preference-Based Multiobjective Evolutionary Algorithms.](https://dl.acm.org/citation.cfm?id=3088704)” In 9th International Conference on Evolutionary Multi-Criterion Optimization—Volume 10173 (EMO O. Schütze, M. Wiecek, Y. Jin, and C. Grimme, Eds., Vol. 10173. Springer-Verlag, Berlin, Heidelberg, 406-421, 2017.•[PositiveSocialImpact:](https://invis.io/9874GSJS6) Empowering people, organizations and planet with information and knowledge to make a positive impact to sustainable development, 2017.•D.K. Ura, Bhutan’s [Gross National Happiness Policy Screening Tool](http://www.grossnationalhappiness.com/docs/GNH/PDFs/PoliSTools.pdf).\"p.79-81","id":"recgrpof7da23odsj","dom_id":"item_recgrpof7da23odsj"},{"Cases":["recDAvfsflBF0WKpf"],"Principles":["recK5zFiq18A3wHAE","reczVPIH1y2OMpAJH","recDRQE1qQQNI65Xn","recZbEXiEs1AlDdn3","recScYLR2TNiv7iKf","rec42P8U9usfYCtv9"],"Reference":"Access Now. (2018). HUMAN RIGHTS IN THE AGE OF ARTIFICIAL INTELLIGENCE. Access Now. https://www.accessnow.org/cms/assets/uploads/2018/11/AI-and-Human-Rights.pdf","Sources":["recH8KmnURSknCr5y"],"Strategies":["recTjwhqfrJoaRxYo"],"title":"Bias","category":"Challenges","name":"recHHr97jsyNDnlsJ","tags":[],"created_at":"2023-05-29T07:14:32.000Z","description":"“AI can be biased both at the system and the data or input level. Bias at the system level involves developers building their own personal biases into the parameters they consider or the labels they define. Although this rarely occurs intentionally, unintentional bias at the system level is common. This often occurs in two ways: • When developers allow systems to conflate correlation with causation. Take credit scores as an example. People with a low income tend to have lower credit scores, for a variety of reasons. If an ML system used to build credit scores includes the credit scores of your Facebook friends as a parameter, it will result in lower scores among those with low-income backgrounds, even if they have otherwise strong financial indicators, simply because of the credit scores of their friends. • When developers choose to include parameters that are proxies for known bias. For example, although developers of an algorithm may intentionally seek to avoid racial bias by not including race as a parameter, the algorithm will still have racially biased results if it includes common proxies for race, like income, education, or postal code.26 Bias at the data or input level occurs in a number of ways:27 • The use of historical data that is biased. Because ML systems use an existing body of data to identify patterns, any bias in that data is naturally reproduced. For example, a system used to recommend admissions at a top university that uses the data of previously admitted students to train the model is likely to recommend upper class males over women and traditionally underrepresented groups. • When the input data are not representative of the target population. This is called selection bias, and results in recommendations that favor certain groups over another. For example, if a GPS-mapping app used only input data from smartphone users to estimate travel times and distances, it could be more accurate in wealthier areas of cities that have a higher concentration of smartphone users, and less accurate in poorer areas or informal settlements, where smartphone penetration is lower and there is sometimes no official mapping. • When the input data are poorly selected. In the GPS mapping app example, this could involve including only information related to cars, but not public transportation schedules or bike paths, resulting in a system that favored cars and was useless for buses or biking. • When the data are incomplete, incorrect, or outdated. If there is insufficient data to make certain conclusions, or the data are out of date, results will naturally be inaccurate. And if a machine learning model is not continually updated with new data that reflects current reality, it will naturally become less accurate over time. Unfortunately, biased data and biased parameters are the rule rather than the exception. Because data are produced by humans, the information carries all the natural human bias within it. Researchers have begun trying to figure out how to best deal with and mitigate bias, including whether it is possible to teach ML systems to learn without bias;28 however, this research is still in its nascent stages. For the time being, there is no cure for bias in AI systems” (Access Now, 2018, p. 12)","id":"rechhr97jsyndnlsj","dom_id":"item_rechhr97jsyndnlsj"},{"Cases":["recSXcY4cnofb4zTP","recAlOHJhEy5nDwA6"],"Principles":["recKdujFoPJr4ZAhZ"],"Strategies":["rechaQXedBh3OsMjZ"],"title":"Reification of structures","category":"Challenges","name":"recHsgB7ki6GknJnX","tags":[],"created_at":"2023-05-18T18:53:59.000Z","id":"rechsgb7ki6gknjnx","dom_id":"item_rechsgb7ki6gknjnx"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recHsUV9w4HKBA1w1"],"title":"How do we assess the impact of AI without cultural sensitivity, on value systems of communities in which they are deployed?","category":"Challenges","name":"recIUf0R50ogibwe6","tags":["affective-computing"],"created_at":"2023-06-05T06:05:10.000Z","description":"\"When affective systems are deployed across cultures, they could adversely affect the cultural, social, or religious values of the community in which they interact.## BackgroundSome philosophers argue that there are no universal ethical principles and that ethical norms vary from society to society. Regardless of whether universalism or some form of ethical relativism is true, affective systems need to respect the values of the cultures within which they are embedded. How systems should effectively reflect the values of the designers or the users of affective systems is not a settled discussion. There is general agreement that developers of affective systems should acknowledge that the systems should reflect the values of those with whom the systems are interacting. There is a high likelihood that when spanning different groups, the values imbued by the developer will be different from the operator or customer of that affective system, and that end-user values should be actively considered. Differences between affective systems and societal values may generate conflict situations producing undesirable results, e.g., gestures or eye contact being misunderstood as rude or threatening. Thus, affective systems should adapt to reflect the values of the community and individuals where they will operate in order to avoid misunderstanding.## RecommendationsAssuming that well-designed affective systems have a minimum subset of configurable norms incorporated in their knowledge base:1\\.Affective systems should have capabilities to identify differences between the values they are designed with and the differing values of those with whom the systems are interacting.2\\.Where appropriate, affective systems will adapt accordingly over time to better fit the norms of their users. As societal values change, there needs to be a means to detect and accommodate such cultural change in affective systems.3\\.Those actions undertaken by an affective system that are most likely to generate an emotional response should be designed to be easily changed in appropriate ways by the user without being easily hacked by actors with malicious intentions. Similar to how software today externalizes the language and vocabulary to be easily changeable based on location, affective systems should externalize someof the core aspects of their actions.## Further Resources- J. Bielby, “Comparative Philosophies in Intercultural Information Ethics.” _Confluence: Online Journal of World Philosophies _2, no. 1, pp. 233–253, 2015.- M. Velasquez, C. Andre, T. Shanks, and M. J. Meyer. “[Ethical Relativism.](https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/ethical-relativism/)” Markkula Center for Applied Ethics, Santa Clara, CA: Santa Clara University, August 1, 1992.- Culture reflects the moral values and ethical norms governing how people should behave and interact with others. “[Ethics, an Overview](https://courses.lumenlearning.com/boundless-management/chapter/ethics-an-overview/).” Boundless Management. - T. Donaldson, “[Values in Tension: Ethics Away from Home Away from Home](https://hbr.org/1996/09/values-in-tension-ethics-away-from-home).” _Harvard Business Review. _September– October 1996.","id":"reciuf0r50ogibwe6","dom_id":"item_reciuf0r50ogibwe6"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recqJAVTtqOUfypNI"],"title":"AI poses risks to the right to truthful information","category":"Challenges","name":"recJV8yTX0MnF3rGD","tags":[],"created_at":"2023-06-05T11:18:00.000Z","description":"\"**Issue:** The right to truthful information is key to a democratic society and to achieving sustainable development and a more equal world, but A/IS poses risks to this right that must be managed.## BackgroundSocial media have become the dominant technological infrastructure for the dissemination of information such as news, opinion, advertising, etc., and are currently in the vanguard of the movement toward customized/targeted information based on user profiling that involves significant use of A/IS techniques. Analysis of opinion polls and trends in social networks, blogs, etc., and of the emotional response to news items can be used for the purposes of manipulation, facilitating both the selection of news that guides public opinion in the desired direction and the practice of sensationalism.The \"personalization of the consumer experience\", that is, the adaptation of articles to the interests, political vision, cultural level, education, and geographic location of the reader, is a new challenge for the journalism profession that expands the possibilities of manipulation.The information infrastructure is currently lacking in transparency, such that it is difficult or impossible to know (except perhaps for the infrastructure operator):•what private information is being collected foruser profiling and by whom,•which groups are targeted and by whom,•what information has been received by any given targeted group,•who financed the creation and dissemination of this information,•the percentage of the information being disseminated by bots, and•who is financing these bots.Many actors have found this opaque infrastructure ideal for spreading politically motivated disinformation, which has a negative effect on the creation of a more equal world, democracy, and the respect for fundamental rights. This disinformation can have tragic consequences. For instance, human rights groups have unearthed evidence that the military authorities of Myanmar used Facebook for inciting hatred against the Rohingya Muslim minority, hatred which facilitated an ethnic cleansing campaign and the murder of up to 50,000 people.14 The UN determined that these actions constituted genocide, crimes against humanity, and war crimes.15## RecommendationsTo protect democracy, respect fundamental rights, and promote sustainable development, governments should implement a legislative agenda which prevents the spread of misinformation and hate speech, by:•Ensuring more control and transparency in the use of A/IS techniques for user profiling in order to protect privacy and prevent user manipulation.•Using A/IS techniques to detect untruthful information circulating in the infrastructures, overseen by a democratic body to prevent potential censorship.•Obliging companies owning A/IS infrastructures to provide more transparency regarding their algorithms, sources of funding, services, and clients.•Defining a new legal status somewhere between \"platforms\" and \"content providers\" for A/IS infrastructures.• Reformulating the deontological codes of the journalistic profession to take into account the intensive use of A/IS techniques foreseenin the future. •Promoting the right to information in official documents, and developing A/IS techniques to automate journalistic tasks such as verification of sources and checking the accuracy of the information in official documents, or in the selection, hierarchy, assessment, and development of news, thereby contributing to objectivity and reliability.## Further Resources•M. Broussard, “Artificial Iintelligence for Investigative Reporting: Using an expert system to enhance journalists’ ability to discover original public affairs stories.” Digital Journalism, vol. 3, no. 6, pp. 814-831, 2015.•M. Carlson, “The robotic reporter: Automated journalism and the redefinition of labor, compositional forms, and journalistic authority.” Digital Journalism, vol. 3, no. 3, pp. 416-431, 2015.•A. López Barriuso, F. de la Prieta Pintado, Á. Lozano Murciego, , D. Hernández de la Iglesia and J. Revuelta Herrero, JOUR-MAS: A Multiagent System Approach to Help Journalism Management, vol. 4, no. 4, 2015.•P. Mozur, ”A Genocide Incited on Facebook with Posts from Myanmar’s Military,” _The New York Times,_ Oct. 15 2018. [https:// www.nytimes.com/2018/10/15/technology/ myanmar-faceboo.k-genocide.html](https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html)•UK Parliament, House of Commons, Digital, Culture, Media and Sport Committee Disinformation and ‘fake news’: Interim Report, Fifth Report of Session 2017–19UK Parliament, Published on July 29, 2018\"p.142-144","id":"recjv8ytx0mnf3rgd","dom_id":"item_recjv8ytx0mnf3rgd"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["rec2679E9TXh1DCL8"],"title":"How do we educate the workforce for design for, or work with, ethical AI?","category":"Challenges","name":"recJfAMUB8HdjVQYD","tags":[],"created_at":"2023-06-05T11:54:10.000Z","description":"\"**Issue:** Education to prepare the future workforce, in both HIC and LMIC, to design ethical A/IS applications or to have a comparative advantage in working alongside A/IS, is either lacking or unevenly available, risking inequality perpetuated across generations, within and between countries, constraining equitable growth, supporting a sustainable future, and achievement of the SDGs.## BackgroundMultiple international institutions, in particular educational engineering organizations,27 have called on universities to play an active role, both locally and globally, in the resolution of the enormous problems that the world faces in securing peace, prosperity, planet protection, and universal human dignity: armed conflict, social injustice, rapid climate change, abuse of human rights, etc. Addressing global social problems is one of the central objectives of many universities, transversal to their other functions, including research in A/IS. UNESCO points out that universities’ preparation of future scientists and engineers for social responsibility is presently very limited, in view of the enormous ethical and social problems associated with technology.28 Enhancing the global dimension of engineeringin undergraduate and postgraduate A/IS education is necessary, so that students canbe prepared as technical professionals, awareof the opportunities and risks that A/IS present, and ready for work anywhere in the world inany sector.Engineering studies at the university and postgraduate levels is just one dimension of the A/IS education challenge. For instance, business, law, public policy, and medical students will also need to be prepared for professions where A/IS are a partner, and to have internalized ethical principles to guide the deployment of such technologies. LMIC need financial and academic support to incorporate global A/IS professional curricula in their own universities, and all countries need to develop the pipeline by preparing elementary and secondary school students to access such professional programs. While the need for curriculum reform is recognized, the impact of A/IS on various professions and socioeconomic contexts is, at this time, both evolving and largely undocumented. Thus, the overhaul of education systems at all levels should be preceded by A/IS research.Much of LMIC education is not globally competitive today, so there is a risk that the global advent of A/IS could negatively affect the chances of young people in LMIC finding productive employment, further fueling global inequality. Education systems worldwide have to be reformed and transformed to fit the new demands of the information age, in view of the changing mix of skills demanded from the workforce.29 In 21st century education, it has been observed that children need less rote knowledge, given so much is instantly accessible on the web and more tools to network and innovate are available; less memory and more imagination should be developed; and fewer physical books and more internet access is required. Young people everywhere need to develop their capacities for creativity, human empathy, ethics, and systems thinking in order to work productively alongside robots and A/IS technologies. Science, Technology, Engineering, Art/design, and Math (STEAM) subjects need to be more extensive and more creatively taught.30 In addition, research is needed to establish ways that a new subject, empathy, can be added to these crucial 21st century subjects in order to educate the future A/IS workforce in social skills. Instead, in rich and poor countries alike, children are continuing to be educated for an industrial age which has disappeared or never even arrived. LMIC education systems, being less entrenched in many countries, may have the potential to be more flexible than those in HIC. Perhaps A/IS can be harnessed to help educational systems to leapfrog into the 21st century, just as mobile phone technology enabled LMIC leapfrog over the phase of wired communication infrastructure.## RecommendationsEducation with respect to A/IS must be targeted to three sets of students: the general public, present and future professionals in A/IS, and present and future policy makers. To prepare the future workforce to develop culturally appropriate A/IS, to work productively and ethically alongside such technologies, and to advance the UN SDGs, the curricula in HIC and LMIC universities and professional schools require innovation. Equally importantly, preuniversity education systems, starting with early childhood education, need to be reformed to prepare society for the risks and opportunities of the A/IS age, rather than the current system which prepares society for work in an industrial age that ended with the 20th century. Specific recommendations include:•Preparing future managers, lawyers, engineers, civil servants, and entrepreneurs to work productively and ethically as global citizens alongside A/IS, through reform of undergraduate and graduate curricula as well as of preschool, primary, and secondary school curricula. This will require:•Fomenting interaction between universities and other actors such as companies, governments, NGOs, etc., with respect to A/IS research through definition of research priorities and joint projects, subcontracts to universities, participation in observatories, and co-creation of curricula, cooperative teaching, internships/service learning, and conferences/seminars/courses.•Establishing and supporting more multidisciplinary degrees that includeA/IS, and adapting university curricula to provide a broad, integrated perspective which allows students to understand the impact of A/IS in the global, economic, environmental, and sociocultural domains and trains them as future policy makers in A/IS fields.•Integrating the teaching of ethics andA/IS across the education spectrum, from preschool to postgraduate curricula, instead of relegating ethics to a standalone module with little direct practical application.•Promoting service learning opportunities that allow A/IS undergraduate and graduate students to apply their knowledge to meet the needs of a community.•Creating international exchange programs, through both private and public institutions, which expose students to different cultural contexts for A/IS applications in both HIC and LMIC.•Creating experimental curricula to prepare people for information-based work in the 21st century, from preschool through postgraduate education.•Taking into account transversal competencies students need to acquire to become ethical global citizens, i.e., critical thinking, empathy, sociocultural awareness, flexibility, and deontological reasoningin the planning and assessment ofA/IS curricula.• Training teachers in teaching methodologies suited to addressing challenges imposed in the age of A/IS. •Stimulating STEAM courses in preuniversity education.•Encouraging high-quality HIC-LMIC collaborative A/IS research in both private and public universities.•Conducting research to support innovation in education and business for the A/IS world, which could include:•Researching the impact of A/IS on the governance and macro/micro strategies of companies and organizations, together with those companies, in an interdisciplinary manner which harnesses expertise of both social scientists and technology experts.•Researching the impact of A/IS on the business model for the development of new products and services through the collaborative efforts of management, operations, and the technical research and development function.•Researching how empathy can be taught and integrated into curricula, starting at the preschool level.•Researching how schools and education systems in low-income settings of both HIC and LMIC can leverage their lessentrenched interests to leapfrog into a 21st century-ready education system.•Establishing ethics observatories in universities with the purpose of fostering an informed public opinion capableof participating in policy decisionsregarding the ethics and social impactof A/IS applications.•Creating professional continuing education and employment opportunities in A/IS for current professionals, including through online and executive education courses.•Creating educative mass media campaigns to elevate society’s ongoing baseline level of understanding of A/IS systems, including what it is, if and how it can be trusted in various contexts, and what are its limitations.## Further resources•ABET Computing and Engineering Accreditation Criteria 2018. Available at: [http://www.abet.org/accreditation/ accreditation-criteria/](http://www.abet.org/accreditation/accreditation-criteria/)•ABET, 2017 ABET Impact Report, Working Together for a Sustainable Future_, _2017.•emlyon business school, Artificial Intelligence in Management (AIM) Institute [http://aim. em-lyon.com](http://aim.em-lyon.com/)UNESCO,_ The UN Decade of Education for Sustainable Development, Shaping the Education of Tomorrow_. UNESCO 2012.\"p.153-156","id":"recjfamub8hdjvqyd","dom_id":"item_recjfamub8hdjvqyd"},{"Principles":["recOHnq45Fq7YWsRO"],"Reference":"Access Now. (2018). HUMAN RIGHTS IN THE AGE OF ARTIFICIAL INTELLIGENCE. Access Now. https://www.accessnow.org/cms/assets/uploads/2018/11/AI-and-Human-Rights.pdf","Sources":["recH8KmnURSknCr5y"],"title":"Accuracy paradox and the risks of false positives","category":"Challenges","name":"recJjTMjpfE0WHSWu","tags":[],"created_at":"2023-05-29T07:17:18.000Z","description":"“Because of their statistical basis, all ML systems have error rates. Even though in many cases ML systems are far more accurate than human beings, there is danger in assuming that simply because a system’s predictions are more accurate than a human’s, the outcome is necessarily better. Even if the error rate is close to zero, in a tool with millions of users, thousands could be affected by error rates. Consider the example of Google Photos. In 2015 Google Photos’ image recognition software was found to have a terribly prejudicial and offensive error: it was occasionally labeling photos of black people as gorillas. Because the system used a complex ML model, engineers were unable to figure out why this was happening. The only “solution” they could work out to this “racist” ML was merely a band-aid: they removed any monkey-related words from the list of image tags. Now, imagine a similar software system used by U.S Customs and Border Patrol that photographs every person who enters and exits the U.S. and cross-references it with a database of photos of known or suspected criminals and terrorists. In 2016, an estimated 75.9 million people arrived in the United States.31 Even if the facial recognition system was 99.9% accurate, the 0.1% error rate would result in 75,900 people being misidentified. How many of these people would be falsely identified as wanted criminals and detained? And what would the impact be on their lives? Conversely, how many known criminals would get away? Even relatively narrow error rates in cases such as these can have severe consequences.” (Access Now, 2018, p. 13)","id":"recjjtmjpfe0whswu","dom_id":"item_recjjtmjpfe0whswu"},{"Principles":["reczVPIH1y2OMpAJH"],"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["reclzrMrQSQls3PI9"],"title":"Collecting data to support learning without falling into social profiling of learners","category":"Challenges","name":"recKzhZVabDuYM6rG","tags":["education"],"created_at":"2023-06-05T09:40:36.000Z","description":"\"**Issue: **Mass personalizationof instruction## BackgroundThe mass personalization of education offers better education for all at very low cost through A/IS-enabled computer-based instruction that promises to free up teachers to work with kids individually to pursue their passions. These applications will rely on the continuous gathering of personal data regarding mood, thought processes, private stories, physiological data, and more. The data will be used to construct a computational model of each child’s interests, understanding, strengths, and weaknesses. The model provides an intimate understanding of how they think, what they understand, how they process information, or react to new information; all of which can be used to drive instructional content and feedback. Sharing of this data between classes, enabling it to follow students through their schooling, will make the models more effective and beneficial to children, but it also exposes children and their families to social control. If performance data are correlated with social data on a family, it could be used by social authorities in decision-making about the family. For example, since 2015-2018, well-being digital tests were performed in schools in Denmark. Children were asked about everything from bullying, loneliness, and stomach aches. Recently it was disclosed that although the collected data was presented as anonymous, they were not. Data were stored with social security numbers, correlated with other test data, and even used in case management by some Danish municipalities.5 Commercial profiling and correlation of different sets of personal data may further affect these children in future job or educational situations.## RecommendationEducational data offer a unique opportunity to model individuals’ thought processes and could be used to predict or change individuals’ behavior in many situations. Governments and organizations should classify educational dataas being sensitive and implement special protective standards.Children’s data should be held in “escrow”and not used for any commercial purposesuntil a child reaches the age of majority and is able to authorize use as they choose.## Further Resources•The journal of the International Artificial Intelligence in Education Society[:](http://iaied.org/journal/)\u003chttp://iaied.org/journal/\u003e•Deeper discussion and bibliography of future trends of AI-based education with utopian and dystopian case scenarios: N. Pinkwart, “Another 25 Years of AIED? Challenges andOpportunities for Intelligent EducationalTechnologies of the Future,” International Journal of Artificial Intelligence in Education, vol. 26, no. 2, pp. 771–783, 2016.[ ](https://doi.org/10.1007/s40593-016-0099-7)[Online].Available: [https://doi.org/10.1007/s40593016-0099-7](https://doi.org/10.1007/s40593-016-0099-7) [Accessed Dec. 2018].•Information Commissioners Office (ico.),“What if we want to profile children or make automated decisions about them?” [https://ico. org.uk/for-organisations/guide-to-the-generaldata-protection-regulation-gdpr/children-andthe-gdpr/what-if-we-want-to-profile-childrenor-make-automated-decisions-about-them/](https://ico.org.uk/for-organisations/guide-to-the-general-data-protection-regulation-gdpr/children-and-the-gdpr/what-if-we-want-to-profile-children-or-make-automated-decisions-about-them/)•K. Firth-Butterfield, “What happens when your child’s friend is an AI toy that talks back?” in World Economic Forum: Generation AI, [https://www.weforum.org/agenda/2018/05/ generation-ai-what-happens-when-your-childsinvisible-friend-is-an-ai-toy-that-talks-back/,](https://www.weforum.org/agenda/2018/05/generation-ai-what-happens-when-your-childs-invisible-friend-is-an-ai-toy-that-talks-back/)May 22, 2018.\"p.114-115","id":"reckzhzvabduym6rg","dom_id":"item_reckzhzvabduym6rg"},{"Cases":["reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5"],"Principles":["recSqx6wklVpDzx3s","recMGB4iC5oaCtr5x","recK5zFiq18A3wHAE","recZbEXiEs1AlDdn3"],"title":"Digital divides in quality of experience with technology","category":"Challenges","name":"recL5M2Hye2XHK7zg","tags":[],"created_at":"2023-05-18T18:54:16.000Z","id":"recl5m2hye2xhk7zg","dom_id":"item_recl5m2hye2xhk7zg"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["rec9UfowPkXtPUBC9"],"title":"How do we represent cross-cultural differences in communication through AI systems?","category":"Challenges","name":"recL5sNl6bi1vrIzj","tags":["affective-computing"],"created_at":"2023-06-05T06:03:37.000Z","description":"\"Should affective systems interact using the norms for verbal and nonverbal communication consistent with the norms of the society in which they are embedded?## BackgroundIndividuals around the world express intentions differently, including the ways that they make eye contact, use gestures, or interpret silence. These particularities are part of an individual’s and a society's culture and are incorporated into their affective systems in order to convey the intended message. To ensure that the emotional systems of autonomous and intelligent systems foster effective communication within a specific culture, an understanding of the norms/values of the community where the affective system will be deployed is essential.## Recommendations1\\. A well-designed affective system will have a set of essential norms, specific to its intended cultural context of use, in its knowledge base. Research has shown that A/IS technologies can use at least five types of cues to simulate social interactions.2\\.These include: physical cues such as simulated facial expressions, psychological cues such as 1simulated humor or other emotions, use of language, use of social dynamics like taking turns, and through social roles such as acting as a tutor or medical advisor. Further examples are listed below:a.Well-designed affective systems will use language with affective content carefully and within the contemporaneous expectations of the culture. An example is small talk. Although small talk is useful for establishing a friendly rapport in many communities, some communities see people that use small talk as insincere and hypocritical. Other cultures may consider people that do not use small talk as unfriendly, uncooperative, rude, arrogant, or ignorant. Additionally, speaking with proper vocabulary, grammar, and sentence structure may contrast with the typical informal interactions between individuals. For example, the latest trend, TV show, or other media may significantly influence what is viewed as appropriate vocabulary and interaction style.b.Well-designed affective systems will recognize that the amount of personal space (proxemics) given by individuals in an important part of culturally specific human interaction. People from varying cultures maintain, often unknowingly, different spatial distances between themselves to establish smooth communication. Crossing these limits may require explicit or implicit consent, which A/IS must learn to negotiate to avoid transmitting unintended messages.c.Eye contact is an essential component for culturally sensitive social interaction. For some interactions, direct eye contact is needed but for others it is not essential and may even generate misunderstandings. It is important that A/IS be equipped to recognize the role of eye contact in the development of emotional interaction.d.Hand gestures and other non-verbal communication are very important for social interaction. Communicative gestures are culturally specific and thus should be used with caution in cross-cultural situations. The specificity of physical communication techniques must be acknowledged in the design of functional affective systems. For instance, although a “thumbs-up” sign is commonly used to indicate approval, in some countries this gesture can be considered an insult.e. Humans use facial expressions to detect emotions and facilitate communication. Facial expressions may not be universal across cultures, however, and A/IS trained with a dataset from one culture may not be readily usable in another culture. Well-developed A/IS will be able to recognize, analyze, and even display facial expressions essential for culturally specific social interaction.3\\.Engineers should consider the need for cross-cultural use of affective systems.Well-designed systems will have options innate to facilitate flexibility in cultural programming. Mechanisms to enable and disable culturally specific “add-ons” should be considered an essential part of A/IS development.## Further Resources•G. Cotton, “[Gestures to Avoid in Cross-Cultural Business: In Other Words, ‘Keep Your Fingers to Yourself!’](http://www.huffingtonpost.com/gayle-cotton/cross-cultural-gestures_b_3437653.html)” _Huffington Post, _June 13, 2013.•“[Paralanguage Across Cultures,](https://cultureplusconsulting.com/2015/04/16/paralanguage-across-cultures/)” Sydney, Australia: Culture Plus Consulting, 2016.•G. Cotton, _[Say Anything to Anyone, Say Anything to Anyone, Anywhere: 5 Keys to Successful Cross-Cultural Communication](http://www.wiley.com/WileyCDA/WileyTitle/productCd-111842042X.html)_[.](http://www.wiley.com/WileyCDA/WileyTitle/productCd-111842042X.html) Hoboken, NJ: Wiley, 2013.•D. Elmer, _[Cross-Cultural Connections: ](https://www.ivpress.com/cross-cultural-connections)__[Stepping Out and Fitting In Around the World](https://www.ivpress.com/cross-cultural-connections)_[.](https://www.ivpress.com/cross-cultural-connections) Westmont, IL: InterVarsity Press, 2002.•B. J. Fogg, [Persuasive Technology.](https://dl.acm.org/citation.cfm?id=763957) _Ubiquity_, December 2, 2002.•A. McStay, Emotional AI: The Rise of Empathic Media. London: Sage, 2018.M. Price, “[Facial Expressions—Including Fear— May Not Be as Universal as We Thought.](http://www.sciencemag.org/news/2016/10/facial-expressions-including-fear-may-not-be-universal-we-thought)” _Science, _October 17, 2016.\"p.90-91","id":"recl5snl6bi1vrizj","dom_id":"item_recl5snl6bi1vrizj"},{"Principles":["recPg7Ov0priGGtLm","recsvi4LnhEEPyQ1h"],"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recwWov6KzmwU0FQW"],"title":"Individuals don't have control over their own data with generic T\u0026C statements","category":"Challenges","name":"recO1L6GoFMkA6Lt4","tags":["consent"],"created_at":"2023-06-05T09:28:22.000Z","description":"\"**Issue**: What would it mean for a person to have individually controlled terms and conditions for their personal data? ## BackgroundPart of providing individually controlled terms and conditions for personal data is to help each person consider what their preferences are regarding their data versus dictating how they need to share it. While questions along these lines are framed in light of a person’s privacy, their preferences also reveal larger values for individuals. The ethical issue is whether A/IS act in accordance with these values.This process of investigating one’s values to identify these preferences is a powerful step towards regaining data agency. The point is not only that a person’s data are protected, but also that by curating these answers they become educated about how important their information is in the context of how it is shared. Most individuals also believe controlling their personal data only happens on the sites or social networks to which they belong and have no idea of the consequences of how that data may be used by others in the future. Agreeing to most standard terms and conditions on these sites largely means users consent to give up control of their personal data rather than play a meaningful role in defining and curating its downstream use.The scope of how long one should or could control the downstream use of their data can be difficult to calculate as consent-based models of personal data have trained users to release rights on any claims for use of their data which are entirely provided to the service, manufacturer, and their partners. However, models like YouTube’s [Content ID ](https://www.youtube.com/t/contentid)provide a form of precedent for thinking about how an individual’s data could be technically protected where it is considered as an asset they could control and copyright. Here is language from [YouTube’s site about the service:](https://support.google.com/youtube/answer/2797370?hl=en) “Copyright owners can use a system called Content ID to easily identify and manage their content on YouTube. Videos uploaded to YouTube are scanned against a database of files that have been submitted to us by content owners.” In this sense, the question of how long or how far downstream one’s personal data should be protected takes on the same logic of how long a corporation’s intellectual property or copyrights could be protected based on initial legal terms set.One challenge is how to define use of data that can affect the individual directly, versus use of aggregated data. For example, an individual subway user’s travel card, tracking their individual movements, should be protected from uses that identify or profile that individual to make inferences about his/her likes or location generally. But data provided by a user could be included in an overall travel system’s management database, aggregated into patterns for scheduling and maintenance as long as the individual-level data are deleted. Where users have predetermined via their terms and conditions that they are willing to share their data for these travel systems, they can meaningfully articulate how to share their information.Under current business models, it is common for people to consent to the sharing of discrete data like credit card transaction data, answers to test questions, or how many steps they walk. However, once aggregated these data and the associated insights may lead to complex and sensitive conclusions being drawn about individuals. This end use of the individual’s data may not have been part of the initial sharing agreement. This is why models for terms and conditions created for user control typically alert people via onscreen or other warning methods when their predetermined preferences arenot being honored.## RecommendationIndividuals should be provided tools that produce machine-readable terms and conditions that are dynamic in nature and serve to protect their data and honor their preferences for its use.•Personal data access and consent should be managed by the individual using their curated terms and conditions that provide notification and an opportunity for consent at the time data are exchanged, versus outside actors being able to access personal data without an individual’s awareness or control.•Terms should be presented in a way that allows a user to easily read, interpret, understand, and choose to engage with any A/IS. Consent should be both conditional and dynamic, where “dynamic” means downstream uses of a person’s data must be explicitly called out, allowing them to cancel a service and potentially rescind or “kill” any data they have shared with a service to date via the use of a “Smart Contract” or specific conditions as described in mutual terms and conditions between two parties at the time of exchange.•For further information on these issues, please see the following section in regard to algorithmic agents and their application.## Further Resources•IEEE P7012™ - IEEE [Standards Project for Machine Readable Personal Privacy Terms.](https://development.standards.ieee.org/get-file/P7012.pdf%3Ft=95323600003)This approved standardization project (currently in development) directly honorsthe goals laid out in Section One ofthis document.•[The Personalized Privacy Assistant Project](https://privacyassistant.org/) Carnegie Mellon University. [https:// privacyassistant.org,](https://privacyassistant.org/) 2019.\\\\•M. Orcutt, “[Personal AI Privacy Watchdog Could Help You Regain Control of Your Data”](https://www.technologyreview.com/s/607830/personal-ai-privacy-watchdog-could-help-you-regain-control-of-your-data/) MIT Technology Review, May 11, 2017.•M. Hintze, [Privacy Statements: Purposes, Requirements, and Best Practices.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2927105) Cambridge, U.K.: Cambridge University Press, 2017.•D. J. Solove, “Privacy self-management and the consent dilemma, Harvard Law Review, vol. 126, no. 7, pp. 1880–1903, May 2013.•N. Sadeh, M. Degeling, A. Das, A. S. Zhang, A. Acquisti, L. Bauer, L. Cranor, A. Datta, and D. Smullen, A Privacy Assistant for the Internet of Things: [https://www.usenix.org/sites/default/ files/soups17\\_poster\\_sadeh.pdf](https://www.usenix.org/sites/default/files/soups17_poster_sadeh.pdf)•H. Lee, R. Chow, M. R. Haghighat, H. M. Patterson and A. Kobsa, “IoT Service Store: A Web-based System for Privacy-aware IoT Service Discovery and Interaction,_” 2018 IEEE International Conference on Pervasive Computing and CommunicationsWorkshops (PerCom Workshops),_Athens, pp. 107-112, 2018.•L. Cranor, M. Langheinrich, M. Marchiori, M. Presler-Marshall, and J. Reagle, “The Platform for Privacy Preferences 1.0 (P3P1.0) Specification,” W3C Recommendation, [Online]. Available: www.w3.org/TR/P3P/, Apr. 2002.L. F. Cranor, “Personal Privacy Assistants in the Age of the Internet of Things,” in World Economic Forum Annual Meeting, 2016\"p.108-110","id":"reco1l6gofmka6lt4","dom_id":"item_reco1l6gofmka6lt4"},{"Principles":["recMGB4iC5oaCtr5x"],"title":"Injustice in intellectual property (e.g., failure to acknowledge participant contributions)","category":"Challenges","name":"recOeanQuUyB6Dx8g","tags":[],"created_at":"2023-05-18T19:10:21.000Z","id":"recoeanquuyb6dx8g","dom_id":"item_recoeanquuyb6dx8g"},{"Principles":["recNB5h9bK4gEE9uc"],"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["rec81gtnlFS5W2BBF","rec9SSyrfFkmuJkCe","recZI7HDWKBU5T22v"],"title":"How do we understanding indirect or dual-use impacts of AI?","category":"Challenges","name":"recOpn4I30te1qiKl","tags":[],"created_at":"2023-06-05T11:17:59.000Z","description":"**\"Issue: **A/IS are often viewed only as having impact in market contexts, yet these technologies also have an impact on social relations and culture.## BackgroundA/IS are expected to have an impact beyond market domains and business models, diffusing throughout the global society. For instance,A/IS have and will impact social relationshipsin a way similar to how mobile phones changed our daily lives, reflecting directly on our culture, customs, and language. The extent and direction of this impact is not yet clear, but documented experience in HIC and high internet-penetration environments of trolls, “fake news,” and cyberbullying on social media offer a cautionary tale.11 Depression, social isolation, aggression, and the dissemination of violent behavior with damage to human relations, so extreme that, in some cases, it has resulted in suicide, are all correlated with the internet.12 As an example, the technology for “smart homes” has been used for inflicting domestic violence by remotely locking doors, turning off heat/AC, and otherwise harassing a partner. This problem could be easily extended to include elder and child abuse.13 Measures need to be developed to prevent A/IS from contributing to the emergence or amplification of social disorders.To understand the impact of A/IS on society, it is necessary to consider product and process innovation, as well as wider sociocultural and ethical implications, from a global perspective, including the following:•Exploring the development of algorithms capable of detecting and reporting discrimination, cyberbullying, deceptive content and identities, etc., and of notifying competent authorities; recognizing that the use of such algorithms must address ethical concerns related to algorithm explainability as well as take into account the risk to certain aspects of human rights, notably to privacy and freedom from oppression.•Developing a globally recognized professional Code of Ethics with and for technology companies.•Identifying social disorders, such as depression, anxiety, psychological violence, political manipulation, etc., correlated with the use ofA/IS-based technologies as a world health problem; monitoring and measuring their impact.•Elaborating metrics measuring how, where and on whom there is a cultural impact of newA/IS-based technologies.•T. Luong, “Thermostats, Locks and Lights: Digital Tools of Domestic Abuse,” _The New York Times_, June 23, 2018, [https://www. nytimes.com/2018/06/23/technology/smarthome-devices-domestic-abuse.html](https://www.nytimes.com/2018/06/23/technology/smart-home-devices-domestic-abuse.html).•J. Naughton, “The internet of things has opened up a new frontier of domestic abuse.” The Guardian, July 2018.•M. Pianta, Innovation and Employment, Handbook of Innovation_. _Oxford, U.K.: Oxford University Press, 2003.•M.J. Salganik, Bit by Bit. Princeton, NJ: Princeton University Press 2018.•J. Torresen, “A Review of Future and Ethical Perspectives of Robotics and AI” Frontiers in Robotics and AI, Jan. 15, 2018. [Online]. Available: [https://doi.org/10.3389/ frobt.2017.00075](https://doi.org/10.3389/frobt.2017.00075). [Accessed Nov. 1, 2018].\"p.141-142","id":"recopn4i30te1qikl","dom_id":"item_recopn4i30te1qikl"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["reck6xeMUc9jtmLr3"],"title":"AI teams may not thoroughly explore the ethical considerations in their work","category":"Challenges","name":"recPqsTlFK76oGD9C","tags":["research"],"created_at":"2023-06-05T10:12:55.000Z","description":"\"**Issue:** Integration of ethics inA/IS-related degree programs## BackgroundA/IS engineers and design teams do not always thoroughly explore the ethical considerations implicit in their technical work and design choices. Moreover, the overall science, technology, engineering, and mathematics (STEM) field struggles with the complexity of ethical considerations, which cannot be readily articulated and translated into the formal languages of mathematics and computer programming associated with algorithms and machine learning.Ethical issues can easily be rendered invisible or inappropriately reduced and simplified in the context of technical practice. For the dangers of this approach see for instance, Lipton and Steinhardt (2018), listed under “Further Resources”. This problem is further compounded by the fact that many STEM programs do not sufficiently integrate applied ethics throughout their curricula. When they do, often ethics is relegated to a stand-alone course or module that gives students little or no direct experience in ethical decision-making. Ethics education should be meaningful, applicable, and incorporate best practices from the broader field.The aim of these recommendations is to prepare students for the technical training and engineering development methods that incorporate ethics as essential so that ethics,and relevant principles, like human rights, become naturally a part of the design process.## Recommendations•Ethics training needs to be a core subjectfor all those in the STEM field, beginning atthe earliest appropriate level and for all advanced degrees.•Effective STEM ethics curricula should be informed by experts outside the STEM community_ _from a variety of cultural and educational backgrounds to ensure that students acquire sensitivity to a diversityof robust perspectives on ethics and design.•Such curricula should teach aspiring engineers, computer scientists, and statisticians about the relevance and impact of their decisions in designing A/IS technologies. Effective ethics education in STEM contexts and beyond should span primary, secondary, and postsecondary education, and include both universities and vocational training schools.•Relevant accreditation bodies should reinforce this integrated approach as outlined above.## Further Resources•[IEEE P7000TM Standards Project for a Model Process for Addressing Ethical Concerns During System Design.](https://standards.ieee.org/develop/project/7000.html) IEEE P7000 aims to enhance corporate IT innovation practices by providing processes for embedding a values- and virtue-based thinking, culture, and practice into them.•Z. Lipton and J. Steinhardt, [Troubling Trends in Machine Learning Scholarship.](https://www.dropbox.com/s/ao7c090p8bg1hk3/Lipton%20and%20Steinhardt%20-%20Troubling%20Trends%20in%20Machine%20Learning%20Scholarship.pdf?dl=0) ICML conference paper, July 2018.•J. Holdren, and M. Smith. “[Preparing for the Future of Artificial Intelligence.”](https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_future_of_ai.pdf) Washington, DC: Executive Office of the President, National Science and Technology Council, 2016.•Comparing the UK, EU, and US approaches to AI and ethics: C. Cath, S. Wachter, B. Mittelstadt, et al., “[Artificial Intelligence and the ‘Good Society’: The US, EU, and UK Approach.](https://link.springer.com/article/10.1007/s11948-017-9901-7)” _Science and Engineering Ethics, _vol._ _24_, _pp. 505-528, 2017.\"p.122","id":"recpqstlfk76ogd9c","dom_id":"item_recpqstlfk76ogd9c"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"title":"How do we navigate individual choice of technology engagement and school or system-wide procurement?","category":"Challenges","name":"recUKAxgCSxylEiDm","tags":["education"],"created_at":"2023-06-05T09:43:22.000Z","description":"\"**Issue: **Technology choice-making in schools## BackgroundChildren, as minors, have no standing to give or deny consent, or to control the use of their personal data. Parents only have limited choices in what are often school-wide implementations of educational technology. Examples include the use of Google applications, face recognition in security systems, and computer driven instruction as described above. In many cases, parents’ only choice would be to send their children to a different school, but that choice is seldom available.How should schools make these choices? How much input should parents have? Should parents be able to demand technology-free teaching?There are many gaps in current student data regulation. In June 2018, CLIP, The Center on Law and Information Policy at Fordham Law School published, ”Transparency and the Marketplace for Student Data”.6 This study concluded that “student lists are commercially available for purchase on the basis of ethnicity, affluence, religion, lifestyle, awkwardness, and even a perceived or predicted need for family planning services”. Fordham found that the data market is becoming one of the largest and most profitable marketplaces in the United States. Data brokers have databases that store billions of data elements on nearly every United States consumer. However, information from students in the pursuit of an education should not be exploited and commercialized without restraint.Fordham researchers found at least 14 data brokers who advertise the sale of student information. One sold lists of students as young as two years old. Another sold lists of student profiles on the basis of ethnicity, religion, economic factors, and even gawkiness.## RecommendationLocal and national educational authorities must work to develop policies surrounding students’ personal data with all stakeholders: administrators, teachers, technology providers, students, and parents in order to balance the best educational interests of each child with the best practices to ensure safety of their personal data. Such efforts will raise awareness among all stakeholders of the promise and the compromises inherent in new educational technologies.## Further reading•Common Sense Media privacy evaluation project: [https://www.commonsense.org/ education/privacy](https://www.commonsense.org/education/privacy)•D. T. Ritvo, L. Plunkett, and P. Haduong,”Privacy and Student Data: Companion Learning Tools.” Berkman Klein Center for Internet and Society at Harvard University, 2017. [Online]. Available: [http://blogs.harvard. edu/youthandmediaalpha/files/2017/03/ PrivacyStudentData\\_Companion\\_Learning\\_ Tools.pdf ](http://blogs.harvard.edu/youthandmediaalpha/files/2017/03/PrivacyStudentData_Companion_Learning_Tools.pdf)[Accessed Dec. 2018][.](http://blogs.harvard.edu/youthandmediaalpha/files/2017/03/PrivacyStudentData_Companion_Learning_Tools.pdf)•F. Alim, N. Cardozo, G. Gebhart, K. Gullo, and A. Kalia, “Spying on Students: School-Issued Devices and Student Privacy,” Electronic Frontier Foundation, [https://www.eff.org/wp/ school-issued-devices-and-student-privacy,](https://www.eff.org/wp/school-issued-devices-and-student-privacy) April 13, 2017.•N. C. Russell, J. R. Reidenberg, E. Martin, and T. Norton, “Transparency and the Marketplace for Student Data,” _Virginia Journal of Law and Technology_, Forthcoming. Available at SSRN: [https://ssrn.com/abstract=3191436,](https://ssrn.com/abstract=3191436) June 6, 2018.\"","id":"recukaxgcsxyleidm","dom_id":"item_recukaxgcsxyleidm"},{"title":"Indirect impacts (including long-range, and dual-use)","category":"Challenges","name":"recWPbL5kB1Yh5OPf","tags":[],"created_at":"2023-05-18T18:54:26.000Z","id":"recwpbl5kb1yh5opf","dom_id":"item_recwpbl5kb1yh5opf"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recJyzFLE9ry4YEbb"],"title":"How do we provide consistent oversight of AI to ensure they are accountable to end-users for any conclusions?","category":"Challenges","name":"recX6r1O4jcsp0nIM","tags":["corporate-ethics"],"created_at":"2023-06-05T10:49:39.000Z","description":"**Issue:** Oversight for algorithms## BackgroundThe algorithms behind A/IS are not subject to consistent oversight. This lack of assessment causes concern because end users have no account of how a certain algorithm or system came to its conclusions. These recommendations are similar to those made in the “General Principles” and “Embedding Values into Autonomous and Intelligent Systems” chapters of _Ethically Aligned Design_, but here the recommendations are used as they apply to the narrow scope of this chapter .## RecommendationsAccountability: As touched on in the General Principles chapter of _Ethically Aligned Design_, algorithmic transparency is an issue of concern. It is understood that specifics relating to algorithms or systems contain intellectual property that cannot, or will not, be released to the general public. Nonetheless, standards providing oversight of the manufacturing process of A/IS technologies need to be created to avoid harm and negative consequences. We can look to other technical domains, such as biomedical, civil, and aerospace engineering, where commercial protections for proprietary technology are routinely and effectively balanced with the need for appropriate oversight standards and mechanisms to safeguard the public.Human rights and algorithmic impact assessments should be explored as a meaningful way to improve the accountability of A/IS.These need to be paired with public consultations, and the final impactassessments must be made public.## Further Resources•F. Pasquale, The Black Box Society: The Secret Algorithms That Control Money and Information. Cambridge, MA: Harvard University Press, 2016.•R. Calo, “Artificial Intelligence Policy: A Primer and Roadmap,” _UC Davis Law Review,_ 52: pp. 399–435, 2017.•ARTICLE 19. “Privacy and Freedom of Expression in the Age of Artificial Intelligence,” Privacy International, April 2018. [Online]. Available: [https://www.article19.org/wpcontent/uploads/2018/04/Privacy-andFreedom-of-Expression-In-the-Age-of-ArtificialIntelligence-1.pdf.](https://www.article19.org/wp-content/uploads/2018/04/Privacy-and-Freedom-of-Expression-In-the-Age-of-Artificial-Intelligence-1.pdf) [Accessed October 28, 2018].p.132-133","id":"recx6r1o4jcsp0nim","dom_id":"item_recx6r1o4jcsp0nim"},{"Principles":["recsvi4LnhEEPyQ1h"],"title":"Incidental data capture (e.g. capturing non-consenting persons in the background of video capture, and challenges of disaggregating group-level data)","category":"Challenges","name":"recYHYaeqVHR93Kuo","tags":["AI"],"created_at":"2023-05-18T13:41:32.000Z","id":"recyhyaeqvhr93kuo","dom_id":"item_recyhyaeqvhr93kuo"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["rec9SSyrfFkmuJkCe"],"title":"How can we incentivise reporting ethical concerns?","category":"Challenges","name":"recZ43i8Cnhohwb2x","tags":["corporate-ethics"],"created_at":"2023-06-05T10:30:49.000Z","description":"**\"Issue:** Empowerment to raise ethical concerns## BackgroundEngineers and design teams may encounter obstacles to raising ethical concerns regarding their designs or design specifications within their organizations. Corporate culture should incentivize technical staff to voice the full range of ethical questions to relevant corporate actors throughout the full product lifecycle, including the design, development, and deployment phases. Because raising ethical concerns can be perceived as slowing or halting a design project, organizations need to consider how they can recognize and incentivize values-based design as an integral component of product development.## RecommendationsEmployees should be empowered and encouraged to raise ethical concerns inday-to-day professional practice.To be effective in ensuring adoption of ethical considerations during product development or internal implementation of A/IS, organizations should create a company culture and set of norms that encourage incorporating ethical considerations in the design and implementation processes.New categories of considerations around these issues need to be accommodated, along with updated Codes of Conduct, company value-statements, and other management principles so individuals are empowered to share their insights and concerns in an atmosphere of trust. Additionally, bottom-up approaches like company “town hall meetings” should be explored that reward, rather than punish, those who bring up ethical concerns.## Further Resources•[The British Computer Society (BCS),](http://www.bcs.org/category/6030) Codeof Conduct, 2019.•C. Cath, and L. Floridi, “[The Design of the Internet’s Architecture by the Internet  Engineering Task Force (IETF) and Human Rights](http://link.springer.com/article/10.1007%2Fs11948-016-9793-y),” _Science and Engineering Ethics, _vol._ _23, no. 2, pp. 449–468, Apr. 2017.\"p.129","id":"recz43i8cnhohwb2x","dom_id":"item_recz43i8cnhohwb2x"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recV8WjuiXlfbVN5d"],"title":"How do leaders provide direction regarding promotion of human values in AI design?","category":"Challenges","name":"recaBNAcact8Bz6dg","tags":["corporate-ethics"],"created_at":"2023-06-05T10:28:38.000Z","description":"\"**Issue:** Values-based leadership## BackgroundTechnology leadership should give innovation teams and engineers direction regarding which human values and legal norms should be promoted in the design of A/IS. Cultivating an ethical corporate culture is an essential component of successful leadership in theA/IS domain.## RecommendationsCompanies should create roles for senior-level marketers, engineers, and lawyers who can collectively and pragmatically implement ethically aligned design. There is also a need for more in-house ethicists, or positions that fulfill similar roles. One potential way to ensure values are on the agenda in A/IS development is to have a Chief Values Officer (CVO), a role first suggested by Kay Firth-Butterfield, see “Further Resources”. However, ethical responsibility should not be delegated solely to CVOs. They can support the creation of ethical knowledge in companies, but in the end, all members of an organization will need to act responsibly throughout the design process.Companies need to ensure that their understanding of values-based system innovation is based on _de jure _and _de facto _international human rights standards.•K. Firth-Butterfield, “[How IEEE Aims to Instill Ethics in Artificial Intelligence Design,](http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design)” The Institute. Jan. 19, 2017. [Online]. Available: [http://theinstitute.ieee.org/ieee-roundup/ blogs/blog/how-ieee-aims-to-instill-ethicsin-artificial-intelligence-design](http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design). [Accessed October 28, 2018]. •United Nations, [Guiding Principles on Business and Human Rights: Implementing the United Nations “Protect, Respect and Remedy” Framework,](http://www.ohchr.org/Documents/Publications/GuidingPrinciplesBusinessHR_EN.pdf) New York and Geneva: UN, 2011.•Institute for Human Rights and Business(IHRB), and Shift, ICT [Sector Guide on ](https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf)[Implementing the UN Guiding Principles on Business and Human Rights,](https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf) 2013.•C. Cath, and L. Floridi, “[The Design of the Internet’s Architecture by the Internet ](http://europepmc.org/abstract/med/27255607)[Engineering Task Force (IETF) and Human Rights](http://europepmc.org/abstract/med/27255607).” _Science and Engineering Ethics, _vol._ _23, no. 2, pp. 449–468, Apr. 2017.\"p.128","id":"recabnacact8bz6dg","dom_id":"item_recabnacact8bz6dg"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recsonQhKLmX4D1ao","recZI7HDWKBU5T22v"],"title":"How do we assess the impact of AI without cultural sensitivity, on culturally situated human social interaction?","category":"Challenges","name":"recaFyWRROaJ1ZFyY","tags":["affective-computing"],"created_at":"2023-06-05T06:05:09.000Z","description":"\"It is presently unknown** **whether long-term interaction with affective artifacts that lack cultural sensitivity could alter human social interaction.## BackgroundSystems that do not have cultural knowledge incorporated into their knowledge base may or may not interact effectively with humans for whom emotion and culture are significant. Given that interaction with A/IS may affect individuals and societies, it is imperative that we carefully evaluate mechanisms to promote beneficial affective interaction between humans andA/IS. Humans often use mirroring in order to understand and develop their norms for behavior. Certain machine learning approaches also address improving A/IS interaction with humans through mirroring human behavior. Thus, we must remember that learning via mirroring can go in both directions and that interacting with machines has the potential to impact individuals’ norms, as well as societal and cultural norms. If affective artifacts with enhanced, different, or absent cultural sensitivity interact with impressionable humans this could alter their responses to social and cultural cues and values. The potential for A/IS to exert cultural influencein powerful ways, at scale, is an area of substantial concern.## Recommendations1\\.Collaborative research teams must research the effects of long-term interaction of people with affective systems. This should be done using multiple protocols, disciplinary approaches, and metrics to measure the modifications of habits, norms, and principles as well as careful evaluation of the downstream cultural and societal impacts.2\\.Parties responsible for deploying affective systems into the lives of individuals or communities should be trained to detectthe influence of A/IS, and to utilize mitigation techniques if A/IS effects appear to be harmful. It should always be possible toshut down harmful A/IS.## Further Resources•T. Nishida and C. Faucher, Eds., [Modelling Machine Emotions for Realizing Intelligence: Foundations and Applications.](https://www.springer.com/us/book/9783642126031) Berlin, Germany: Springer-Verlag, 2010.•D. J. Pauleen, et al. “Cultural Bias in Information Systems Research and Practice: Are You Coming from the Same Place I Am?” _Communications of the Association for Information Systems_, vol._ _17,)pp. 1–36, 2006. J. Bielby, “Comparative Philosophies in Intercultural Information Ethics.” _Confluence: Online Journal of World Philosophies _2, no. 1, pp. 233–253, 2015.•J. Bryson, [“Why Robot Nannies Probably Won’t Do Much Psychological Damage.” ](http://www.cs.bath.ac.uk/~jjb/ftp/Bryson-SharkeyIS09.pdf)A commentary on an article by N. Sharkey and A. Sharkey, _The Crying Shame of Robot Nannies. [Interaction Studies](http://www.ingentaconnect.com/content/jbp/is)_, vol. 11, no. 2 pp. 161–190, July 2010.•A. Sharkey, and N. Sharkey, “Children, the Elderly, and Interactive Robots.” _IEEE Robotics \u0026 Automation Magazine, _vol.18, no. 1, pp. 32–38, March 2011.\"p.91-92","id":"recafywrroaj1zfyy","dom_id":"item_recafywrroaj1zfyy"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recg2u6ZXSRiUO8uF"],"title":"AI poses threats to labour","category":"Challenges","name":"recb50cfuQUWDAHZW","tags":[],"created_at":"2023-06-05T11:48:28.000Z","description":"\"**Issue: **A/IS are changing the nature of work, disrupting employment, while technological change is happening too fast for existing methods of (re)training the workforce.## BackgroundThe current pace of technological development will heavily influence changes in employment structure. In order to properly prepare the workforce for such evolution, actions should be proactive and not only reactive. The wave of automation caused by the A/IS revolution will displace a very large share of jobs across domains and value chains. The U.S. “automated vehicle” case study analyzed in the White House 2016 report_ Artificial Intelligence, Automation, and the Economy _is emblematic of what is at stake: “2.2 to 3.1 million existing part- and full-time U.S. jobs are exposed over the next two decades, although the timeline remains uncertain.”18The risk of unemployment for LMIC is more serious than for developed countries. The industry of most LMIC is labor intensive. While labor may be cheap(er) in LMIC economies, the ripple effects of A/IS and automation will be felt much more than in the HIC economies. The 2016 World Bank Development Report stated that the share of occupations susceptible to automation and A/IS is higher in LMIC than in HIC, where such jobs have already disappeared. In addition, the qualities which made certain jobs easy to outsource to LMIC where wages are lower are those that may make them easy to automate.19 An offsetting factor is the reality that many LMIC lack the communication, energy, and IT infrastructure required to support highly automated industries.20 Notwithstanding this reality, the World Bank estimated the automatable share of employment, unadjusted for adoption time lag, for LMIC ranges from 85% in Ethiopia to 62% in Argentina, compared to the OECD average of 57%.21In the coming decades, the automation wave calls for higher investment and the transformation of labor market capacity development programs. Innovative and fair ways of funding such an investment are required; the solutions should be designed in cooperation with the companies benefiting from the increase of profitability, thanks to automation. This should be done in a responsible way so that the innovation cycle is not broken, and yet workforce capacity does not fall behind the needs of 21st century employment. At the same time, A/IS and other digital technologies offer real potential to innovate new approaches to job-search assistance, placement, and hiring processes in the age of personalized services. The efficiency of matching labor supply and demand can be tremendously enhanced by the rise of multisided platforms and predictive analytics, provided they do not entrench discrimination.22 The case of platforms, such as LinkedIn, for instance, with its 470 million registered users, and online job consolidators such as indeed.com and Simply Hired, are interesting as an evolution in hiring practices,at least for those able to access the internet.Tailored counseling and integrated retraining programs also represent promising grounds for innovation. In addition, much will have to be done to create fair and effective lifelong skill development/training, infrastructures, and mechanisms capable of empowering millions of people to viably transition jobs, sectors, and potentially locations, and to address differential geographic impacts that exacerbate income and wealth disparities. Effectively enabling the workforce to be more mobile—physically, legally, and virtually—will be crucial. This implies systemic policy approaches which encompass housing, transportation, licensing, tax incentives, and crucially in the age of A/IS, universal broadband access, especially in rural areas of both HICand LMIC.## RecommendationsTo thrive in the A/IS age, workers must be provided training in skills that improve their adaptability to rapid technological changes; programs should be available to any worker, with special attention to the low-skilled workforce. Those programs can be private, that is, sponsored by the employer, or publicly and freely offered through specific public channels and government policies, and should be available regardless of whether the worker is in between jobs or still employed. Specific measures include:• Offering new technical programs, possibly earlier than high school, to increase the workforce capacity to close the skills gap and thrive in employment alongside A/IS. •Creating opportunities for apprenticeships, pilot programs, and scaling up data-driven evidence-based solutions that increase employment and earnings.•Supporting new forms of public-private partnerships involving civil society, as well as new outcome-oriented financial mechanisms, e.g., social impact bonds, that help scale up successful innovations.•Supporting partnerships between universities, innovation labs in corporations, and governments to research and incubate startups for A/IS graduates.23•Developing regulations to hold corporations responsible for employee retraining necessary due to increased automation and other technological applications having impacton the workforce.•Facilitating private sector initiatives by public policy for co-investment in training and retraining programs through tax incentives.•Establishing and resourcing public policies that assure the survival and well-being of workers, displaced by A/IS and automation, who cannot be retrained.•Researching complementary areas, to lay solid foundations for the transformation outlined above.•Requiring more policy research on the dynamics of professional transitions in different labor market conditions.•Researching the fairest and most efficient public-private options for financing labor force transformation due to A/IS.•Developing national and regional future of work strategies based on sound research and strategic foresight.## Further Resources•V. Cerf and D. Norfors, The People-centered Economy: The New Ecosystem for Work. California: IIIJ Foundation, 2018.•Executive Office of the President. _Artificial Intelligence, Automation, and the Economy._ December 20, 2016.•S. Kilcarr, “Defining the American Dream for Trucking ... and the Nation, Too,” _FleetOwner_, April 26, 2016.•M. Mason, “Millions of Californians’ Jobs could be Affected by Automation—a Scenario the next Governor has to Address,”_Los Angeles Times_, October 14, 2018.•OECD, “Labor Market Programs: Expenditure and Participants,” _OECD Employment and Labor Market Statistics _(database), 2016.•M. Vivarelli, “Innovation and Employment: ASurvey,” Institute for the Study of Labor (IZA) Discussion Paper No. 2621, February 2007.\"p.147-149\"**Issue: **Analysis of theA/IS impact on employment is too focused on the number and category of jobs affected, whereas more attention should be addressed to the complexities of changing the task contentof jobs.## BackgroundCurrent attention on automation and employment tends to focus on the sheer number of jobs lost or gained. It is important to focus the analysis on how employment structures will be changed by A/IS, rather than solely dwelling on the number of jobs that might be impacted. For example, rather than carrying out a task themselves, workers will need to shift to supervision of robots performing that task. Other concerns include changes in traditional employment structures, with an increase in flexible, contract-based temporary jobs, without employee protection, and a shift in task composition away from routine/repetitive and toward complex decision-making. This is in addition to the enormous need for the aforementioned retraining. Given the extent of disruption, workforce trends will need to measure time spent unemployed or underemployed, labor force participation rates, and other factors beyond simple unemployment numbers.The _Future of Jobs 2018 _report of the World Economic Forum highlights:“...the potential of new technologies to create as well as disrupt jobs and to improve the quality and productivity of the existing work of human employees. Our findings indicate that, by 2022, _augmentation _of existing jobs through technology may free up workers from the majority of data processing and information search tasks—and may also increasingly support them in high-value tasks such as reasoning and decision-making as augmentation becomes increasingly common over the coming years as a way to supplement and complement human labour.The report predicts the shift in skill demand between today and 2022 will be significant and that “proactive, strategic and targeted efforts will be needed to map and incentivize workforce redeployment… [and therefore]... investment decisions [on] whether to prioritize automation or augmentation and the question of whether or not to invest in workforce reskilling.## RecommendationsWhile there is evidence that robots and automation are taking jobs away in various sectors, a more balanced, granular, analytical, and objective treatment of A/IS impact on the workforce is needed to effectively inform policy making and essential workforce reskilling. Specifics to accomplish this include:•Creating an international and independent agency able to properly disseminate objective statistics and inform the media, as well as the general public, about the impact of robotics and A/IS on jobs, tax revenue, growth,26 and well-being.•Analyzing and disseminating data on how current task content of jobs have changed, based on a clear assessment of the automatability of the occupationaldescription of such jobs.•Promoting automation with augmentation, as recommended in the _Future of Jobs Report 2018_ (see chart on page 154), to maximize the benefit of A/IS to employment and meaningful work.•Integrating more granulated dynamic mapping of the future jobs, tasks, activities, workplace-structures, associated work-habits, and skills base spurred by the A/IS revolution, in order to innovate, align, and synchronize skill development and training programs with future requirements. This workforce mapping is needed at the macro, but also crucially at the micro, levels where labor market programsare deployed.•Considering both product and process innovation, and looking at them from a global perspective in order to understand properly the global impact of A/IS on employment.•Proposing mechanisms for redistribution of productivity increases and developing an adaptation plan for the evolving labor market.## Further Resources•E. Brynjolfsson and A. McAfee. The Second Age of Machine Intelligence: Work Progress and Prosperity in a Time of Brilliant Technologies. New York, NY: W. W. Norton \u0026 Company, 2014.•P.R. Daugherty, and H.J. Wilson, Human + Machine: Reimagining Work in the Age of AI_. _Watertown, MA:_ _Harvard Business Review Press, 2018.•International Federation of Robotics. “The Impact of Robots on Productivity, Employment and Jobs,” A positioning paper by the International Federation of Robotics, April 2017.•RockEU. “Robotics Coordination Action for Europe Report on Robotics and Employment,” Deliverable D3.4.1, June 30, 2016.•World Economic Forum, Centre for the New Economy and Society, _The Future of Jobs 2018_, Geneva: WEF 2018.\"150-152","id":"recb50cfuquwdahzw","dom_id":"item_recb50cfuquwdahzw"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recwQrzqPp6C7jyJs"],"title":"How do we assess long-range and indirect impacts of AI?","category":"Challenges","name":"recdZI38VrUaUKRYf","tags":["wellbeing"],"created_at":"2023-06-05T05:58:44.000Z","description":"\"There are insufficient mechanisms to foresee and measure negative impacts, andto promote and safeguard positive impacts of A/IS.## BackgroundA/IS technologies present great opportunity for positive change in every aspect of society. However, they can—by design or unintentionally— cause harm as well. While it is important to consider and make sense of possible benefits, harms, and trade-offs, it is extremely challengingto foresee all of the relevant, direct, andsecondary impacts.However, it is prudent to review case studies of similar products and the impacts they have hadon well-being, as well as to consider possibletypes of impacts that could apply. Issues to consider include:•Economic and labor impacts, including labor displacement, unemployment, and inequality,•Accountability, transparency, and explainability,•Surveillance, privacy, and civil liberties,•Fairness, ethics, and human rights,•Political manipulation, deception, “nudging”,and propaganda,•Human physical and psychological health,•Environmental impacts,•Human dignity, autonomy, and human vs.A/IS roles,•Security, cybersecurity, and autonomous weapons, and•Existential risk and super intelligence.While this is a partial list, it is important to be aware of and reflect on possible and actual cases. For example:•A prominent concern related to A/IS is oflabor displacement and economic and social impacts at an individual and a systems level.A/IS technologies designed to replicate human tasks, behavior, or emotion have the potential to increase or decrease human well-being. These systems could complement human work and increase productivity, wages, and leisure time; or they could be used to supplement and displace human workers, leading to unemployment, inequality, and social strife.It is important for A/IS creators to think about possible uses of their technology and whether they want to encourage or design in restrictions in light of these impacts.•Another example relates to manipulation. Sophisticated manipulative technologies utilizing A/IS can restrict the fundamental freedom of human choice by manipulating humans who consume content without them recognizing the extent of the manipulation. Software platforms are moving from targeting and customizing content to much more powerful and potentially harmful “persuasive computing” that leverages psychological data and methods. While these approaches may be effective in encouraging use of a product, they may come at significant psychological and social costs.•A/IS may deceive and harm humans by posing as humans. With the increased ability of artificial systems to meet the Turing test, an intelligence test for a computer that allows a human to distinguish human intelligence from artificial intelligence, there is a significant risk that unscrupulous operators will abuse the technology for unethical commercial or outright criminal purposes. Without taking action to prevent it, it is highly conceivable that A/IS will be used to deceive humans by pretending to be another human being in a plethora of situations and via multiple mediums.A potential entry point for exploring these unintended consequences is computational sustainability.[Computational-Sustainability.org](http://www.computational-sustainability.org/) defines the term as an “interdisciplinary field that aims to apply techniques from computer science, information science, operations research, applied mathematics, and statistics for balancing environmental, economic, and societal needs for sustainable development”. [The Institute of Computational Sustainability](http://computational-sustainability.cis.cornell.edu/about.php) states that the intent of computational sustainability is provide “computational models for a sustainable environment, economy, and society”. Examples of applied computational sustainability can be seen in the[ Stanford University Engineering Department’s course in computational sustainability presentation](https://cs.stanford.edu/~ermon/cs325/slides/lecture1-S16.pdf). Computational sustainability technologies designed to increase social good could also be tied to existing well-being metrics.## Recommendation• To avoid potential negative, unintended consequences, and secure and safeguard positive impacts, A/IS creators, end-users, and stakeholders should be aware of possible well-being impacts when designing, using, and monitoring A/IS systems. This includes being aware of existing cases and possible areas of impact, measuring impacts on wellbeing outcomes, and developing regulations to promote beneficent uses of A/IS. Specifically\"•A/IS creators should protect human dignity, autonomy, rights, and well-being of those directly and indirectly affected by the technology. As part of this effort, it is important to include multiple stakeholders, minorities, marginalized groups, and those often without power or a voice in consultation.•Policymakers, regulators, monitors, and researchers should consider issuing guidance on areas such as A/IS labor and the proper role of humans vs. A/IS in work transparency, trust, and explainability; manipulation and deception; and other areas that emerge.•Ongoing literature review and analysis should be performed by research and other communities to curate and aggregate information on positive and negative A/IS impacts, along with demonstrated approaches to realize positive ones and amelioratenegative ones.•A/IS creators working toward computational sustainability should integrate well-being concepts, scientific findings, and indicators into current computational sustainability models. They should work with well-being experts, researchers, and practitioners to conduct research and develop and apply models inA/IS development that prioritize and increase human well-being.•Cross-pollination should be developed between computational sustainability and well-being professionals to ensure integration of well-being into computational sustainability frameworks, and vice versa. Where feasible and reasonable, do the same for conceptual models such as doughnut economics and systems thinking.## Further Resources•[AI Safety Research](https://futureoflife.org/ai-safety-research/) by The Future of Life Institute•D. Helbing, et al. “[Will Democracy Survive Big Data and Artificial Intelligence?”](https://www.scientificamerican.com/article/will-democracy-survive-big-data-and-artificial-intelligence/) _Scientific American_, February 25, 2017.•J. L. Schenker, “[Can We Balance Human Ethics with Artificial Intelligence?](http://techonomy.com/2017/01/how-will-ai-decide-who-lives-and-who-dies/)” _Techonomy, _January 23, 2017_._•M. Bulman, “[EU to Vote on Declaring Robots To Be ‘Electronic Persons](http://www.independent.co.uk/life-style/gadgets-and-tech/robots-eu-vote-electronic-persons-european-union-ai-artificial-intelligence-a7527106.html)’.” _Independent, _January 14, 2017.•N. Nevejan, for the European Parliament.“[European Civil Law Rules in Robotics.](http://www.europarl.europa.eu/RegData/etudes/STUD/2016/571379/IPOL_STU(2016)571379_EN.pdf)” October 2016.•University of Oxford. “Social media manipulation rising globally, new report warns,” [https://phys.org/news/2018-07social-media-globally.html.](https://phys.org/news/2018-07-social-media-globally.html) July 20, 2018.•“[The AI That Pretends To Be Human,](http://lesswrong.com/lw/n99/the_ai_that_pretends_to_be_human/)” _LessWrong _blog post, February 2, 2016.•C. Chan, “[Monkeys Grieve When Their Robot Friend Dies.](http://sploid.gizmodo.com/monkeys-grieve-when-their-robot-friend-dies-1791076966)” _Gizmodo_, January 11, 2017.  •Partnership on AI, “AI, Labor, and theEconomy” Working Group launches in New York City,” [https://www.partnershiponai.org/ aile-wg-launch/.](https://www.partnershiponai.org/aile-wg-launch/) April 25, 2018.•C.Y. Johnson, “[Children can be swayed by robot peer pressure,study says,](https://www.washingtonpost.com/news/speaking-of-science/wp/2018/08/15/robot-overlords-may-sound-scary-but-robot-friends-could-be-just-as-bad/?utm_term=.d07ad598531c)” The Washington Post, August 15, 2018. [Online].Available: [www.WashingtonPost.com.](http://www.WashingtonPost.com/)[Accessed 2018].## **Further Resources for**Computational Sustainability•Stanford Engineering Department, [Topics in Computational Sustainability Course Presentation,](https://cs.stanford.edu/~ermon/cs325/slides/lecture1-S16.pdf) 2016.•Computational Sustainability, [Computational Sustainability: Computational Methods for a Sustainable Environment, Economy, and Society Project Summary. ](http://computational-sustainability.cis.cornell.edu/FILES/gomes-computational-sustainability-summary-NSF-Exp08.pdf)•C. P. Gomes, “[Computational Sustainability: Computational Methods for a Sustainable ](https://www.nae.edu/File.aspx?id=17673)[Environment, Economy, and Society](https://www.nae.edu/File.aspx?id=17673)” in The Bridge: Linking Engineering and Society_.\"__p.83-86_","id":"recdzi38vruaukryf","dom_id":"item_recdzi38vruaukryf"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recsonQhKLmX4D1ao"],"title":"How can AI model norms to govern its action","category":"Challenges","name":"recdlFwsToNXtcvGC","tags":[],"created_at":"2023-06-05T12:09:07.000Z","description":"**\"Issue 1:** Many approaches to norm implementation are currently available, and it isnot yet settled which onesare most suitable.## BackgroundThe prospect of developing A/IS that are sensitive to human norms and factor them into morally or legally significant decisions has intrigued science fiction writers, philosophers, and computer scientists alike. Modest efforts to realize this worthy goal in limited or bounded contexts are already underway. This emerging field of research appears under many names, including: machine morality, machine ethics, moral machines, value alignment, computational ethics, artificial morality, safe AI, and friendly AI.There are a number of different implementation routes for implementing ethics into autonomous and intelligent systems. Following Wallach and Allen (2008)14, we might begin to categorize these as either:A.Top-down approaches, where the system,e. g., a software agent, has some symbolic representation of its activity, and so can identify specific states, plans, or actions as ethical or unethical with respect to particular ethical requirements (Dennis, Fisher, Slavkovik, Webster 201615; Pereira and Saptawijaya 201616; Rötzer, 201617; Scheutz, Malle, and Briggs 201518); orB.Bottom-up approaches, where the system,e. g., a learning component, builds up, through experience of what is to be considered ethical and unethical in certain situations, an implicit notion of ethical behavior (Anderson and Anderson 201419; Riedl and Harrison 201620).Relevant examples of these two are: (A) symbolic agents that have explicit representations of plans, actions, goals, etc.; and (B) machine learning systems that train subsymbolic mechanisms with acceptable ethical behavior. For more detailed discussion, see Charisi et al. 201721.Many of the existing experimental approaches to building moral machines are top-down, in the sense that norms, rules, principles, or procedures are used by the system to evaluate the acceptability of differing courses of action, or as moral standards or goals to be realized. Increasingly, however, A/IS will encounter situations that initially programmed norms do not clearly address, requiring algorithmic procedures to select the better of two or more novel courses of action. Recent breakthroughs in machine learning and perception enable researchers to explore bottom-up approaches in which the A/IS learn about their context and about human norms, similar to the manner in which a child slowly learns which forms of behavior are safe and acceptable. Of course, unlike currentA/IS, children can feel pain and pleasure, and empathize with others. Still, A/IS can learn to detect and take into account others’ pain and pleasure, thus at least achieving some of the positive effects of empathy. As research on A/IS progresses, engineers will explore new ways to improve these capabilities.Each of the first two options has obvious limitations, such as option A’s inability to learn and adapt and option B’s unconstrained learning behavior. A third option tries to address these limitations:C.Hybrid approaches, combining (A) and (B).For example, the selection of action might be carried out by a subsymbolic system, but this action must be checked by a symbolic “gateway” agent before being invoked. This is a typical approach for “Ethical Governors” (Arkin, 200822; Winfield, Blum, and Liu 201423) or “Guardians” (Etzioni 201624) that monitor, restrict, and even adapt certain unacceptable behaviors proposed by the system (see Issue 3). Alternatively, action selection in light of norms could be done in a verifiable logical format, while many of the norms constraining those actions can be learned through bottom-up learning mechanisms (Arnold, Kasenberg, and Scheutz 201725).These three architectures do not cover all possible techniques for implementing norms into A/IS. For example, some contributors to the multi-agent systems literature have integrated norms into their agent specifications (Andrighetto et al. 201326), and even though these agents live in societal simulations and are too underspecified to be translated into individual A/IS such as robots, the emerging work can inform cognitive architectures of such A/IS that fully integrate norms. Of course, none of these experimental systems should be deployed outside of the laboratory before testing or before certain criteria are met, which we outline in the remainder of this section and in Section 3.## RecommendationIn light of the multiple possible approaches to computationally implement norms, diverse research efforts should be pursued, especially collaborative research between scientists from different schools of thought and different disciplines.## Further Resources•M. Anderson, and S. L. Anderson, “GenEth: A General Ethical Dilemma Analyzer,”_Proceedings of the Twenty-Eighth AAAI __Conference on Artificial Intelligence_, Québec City, Québec, Canada, July 27 –31, 2014, pp.253–261, Palo Alto, CA, The AAAI Press, 2014.•G. Andrighetto, G. Governatori, P. Noriega, and L. W. N. van der Torre, eds. Normative Multi-Agent Systems. Saarbrücken/Wadern, Germany: Dagstuhl Publishing, 2013.•R. Arkin, “Governing Lethal Behavior:Embedding Ethics in a Hybrid Deliberative/ Reactive Robot Architecture.” _Proceedings of the 2008 3rd ACM/IEEE International Conference on Human-Robot Interaction_ _(HRI)_, Amsterdam, Netherlands, March 12 -15, 2008, IEEE, pp. 121–128, 2008.•T. Arnold, D. Kasenberg, and M. Scheutz.“Value Alignment or Misalignment—What Will Keep Systems Accountable_?” The Workshops of the Thirty-First AAAI Conference on Artificial Intelligence: Technical Reports_, WS-17-02: AI, Ethics, and Society, pp. 81–88. Palo Alto, CA: The AAAI Press, 2017.•V. Charisi, L. Dennis, M. Fisher, et al. “[Towards Moral Autonomous Systems](https://arxiv.org/abs/1703.04741),” 2017.•A. Conn, “[How Do We Align Artificial Intelligence with Human Values?](https://futureoflife.org/2017/02/03/align-artificial-intelligence-with-human-values/)” _Futureof Life Institute_, Feb. 3, 2017.•L. Dennis, M. Fisher, M. Slavkovik, and M. Webster, “Formal Verification of Ethical Choices in Autonomous Systems.” _Robotics and Autonomous Systems, _vol. 77, pp. 1–14, 2016.•A. Etzioni and O.** **Etzioni, “Designing AISystems That Obey Our Laws and Values.” _Communications_ _of the ACM_, vol._ _59, no. 9, pp. 29–31, Sept. 2016.•L. M. Pereira and A. Saptawijaya, Programming Machine Ethics. Cham, Switzerland: Springer International, 2016.•M. O. Riedl and B. Harrison. “Using Stories toTeach Human Values to Artificial Agents.” _AAAI Workshops 2016_. Phoenix, Arizona, February 12–13, 2016.•F. Rötzer, ed. Programmierte Ethik: Brauchen Roboter Regeln oder Moral? Hannover, Germany: Heise Medien, 2016.•M. Scheutz, B. F. Malle, and G. Briggs.“Towards Morally Sensitive Action Selection for Autonomous Social Robots.” _Proceedings of the 24th International_ _Symposium on Robot and Human Interactive Communication,RO-MAN 2015 _(2015): 492–497.•U. Sommer, Werte: Warum Man Sie Braucht, Obwohl es Sie Nicht Gibt. [Values. Why we need them even though they don’t exist.] Stuttgart, Germany: J. B. Metzler, 2016.•I. Sommerville, _Software Engineering_. Harlow, U.K.: Pearson Studium, 2001.•W. Wallach and C. Allen. _Moral Machines: __Teaching Robots Right from Wrong_. New York: Oxford University Press, 2008.•F. T. Winfield, C. Blum, and W. Liu. “Towards an Ethical Robot: Internal Models, Consequences and Ethical Action Selection” in _Advances in Autonomous Robotics Systems, Lecture Notes in Computer Science Volume_, M. Mistry, A. Leonardis, Witkowski, and C. Melhuish, eds. pp. 85–96. Springer, 2014.\"p.175-177","id":"recdlfwstonxtcvgc","dom_id":"item_recdlfwstonxtcvgc"},{"Cases":["recOOmVQviRyJGvea","recmS3zSMbR3ofAR5"],"Principles":["recOHnq45Fq7YWsRO","recxcFmvPG5wrCqpO","recKdujFoPJr4ZAhZ"],"Reference":"Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector. The Alan Turing Institute. https://doi.org/10.5281/ZENODO.3240529","Sources":["recfYC5jjPmpLfSlM"],"Strategies":["rechaQXedBh3OsMjZ","rec81gtnlFS5W2BBF","recgn2UvSD4OhzGI4"],"title":"Decisions with unclear grounding","category":"Challenges","name":"recdmBNNa98cN8Sda","tags":[],"created_at":"2023-05-19T09:30:45.000Z","description":"“Many machine learning models generate their results by operating on high dimensional correlations that are beyond the interpretive capabilities of human scale reasoning. In these cases, the rationale of algorithmically produced outcomes that directly affect decision subjects remains opaque to those subjects. While in some use cases, this lack of explainability may be acceptable, in some applications, where the processed data could harbour traces of discrimination, bias, inequity, or unfairness, the opaqueness of the model may be deeply problematic.” (Leslie, 2019, p. 4-5)","id":"recdmbnna98cn8sda","dom_id":"item_recdmbnna98cn8sda"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"title":"Will 'emotional AI' increase accessibility of AI and what impact might such increased identification with AI have?","category":"Challenges","name":"recebQ2NgeBCmafoX","tags":["synthetic-emotion"],"created_at":"2023-06-05T06:51:34.000Z","description":"Will deployment of synthetic emotions into affective systems increase the accessibility of A/IS? Will increased accessibility prompt unforeseen patterns of identification with A/IS?## BackgroundDeliberately constructed emotions are designed to create empathy between humans and artifacts, which may be useful or even essential for human-A/IS collaboration. Synthetic emotionsare essential for humans to collaborate with theA/IS but can also lead to failure to recognize that synthetic emotions can be compartmentalized and even entirely removed. Potential consequences for humans include differentpatterns of bonding, guilt, and trust, whether between the human and A/IS or between other humans. There is no coherent sense in which A/IS can be made to suffer emotional loss, because any such affect, even if possible, could be avoided at the stage of engineering, or reengineered. As such, it is not possible to allocate moral agency or responsibility in the senses that have been developed for human emotional bonding and thus sociality.## Recommendations1\\.Commercially marketed A/IS should not be persons in a legal sense, nor marketed as persons. Rather their artifactual (authored, designed, and built deliberately) nature should always be made as transparent as possible, at least at point of sale and in available documentation.2\\.Some systems will, due to their application, require opaqueness in some contexts, e.g., emotional therapy. Transparency in such systems should be available to inspection by responsible parties but may be withdrawn for operational needs.## Further Resources•R. C. Arkin, P. Ulam and A. R. Wagner, “Moral Decision-making in Autonomous Systems: Enforcement, Moral Emotions, Dignity, Trust and Deception,” _Proceedings of the IEEE, _vol._ _100, no. 3, pp. 571–589, 2012.•R. Arkin, M. Fujita, T. Takagi and R. Hasegawa. “An Ethological and Emotional Basis for Human-Robot Interaction,” _Robotics and Autonomous Systems, _vol.42, no. 3–4, pp.191–201, 2003.•R. C. Arkin, “Moving up the Food Chain: Motivation and Emotion in Behavior-based Robots,” in _Who Needs Emotions: The Brain Meets the Robot_, J. Fellous and M. Arbib., Eds., New York: Oxford University Press, 2005.•M. Boden, J. Bryson, D. Caldwell, et al. “Principles of Robotics: Regulating Robots in the Real World.” _Connection Science, _vol. 29, no. 2, pp. 124–129, 2017.•J. J Bryson, M. E. Diamantis and T. D. Grant. “Of, For, and By the People: The Legal Lacuna of Synthetic Persons,” _Artificial Intelligence \u0026 Law, _vol._ _25, no. 3, pp. 273–291, Sept. 2017.•J. Novikova, and L. Watts, “Towards Artificial Emotions to Assist Social Coordination in HRI,” _International Journal of Social Robotics, _vol._ _7, no. 1, pp. 77–88, 2015.•M. Scheutz, “The Affect Dilemma for Artificial Agents: Should We Develop Affective Artificial Agents?” _IEEE Transactions on Affective Computing, _vol. 3, no. 4, pp. 424–433, 2012.•A. Sharkey and N. Sharkey. “Children, the Elderly, and Interactive Robots.” _IEEE Robotics \u0026 Automation Magazine, _vol. 18, no. 1, pp. 32–38, 2011.\"p.103-102","id":"recebq2ngebcmafox","dom_id":"item_recebq2ngebcmafox"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recgqEckuFTILlOnz","rec81gtnlFS5W2BBF"],"title":"How do we maintain agency and minimise harms in context of black box models?","category":"Challenges","name":"rececsX8igwNqhhkC","tags":["corporate-ethics"],"created_at":"2023-06-05T10:57:53.000Z","description":"**Issue: **Use of black-box components## BackgroundSoftware developers regularly use “black box” components in their software, the functioning of which they often do not fully understand. “Deep” machine learning processes, which are driving many advancements in autonomous and intelligent systems, are a growing source of black box software. At least for the foreseeable future,A/IS developers will likely be unable to build systems that are guaranteed to operate as intended.## RecommendationWhen systems are built that could impact the safety or well-being of humans, it is not enough to just presume that a system works. Engineers must acknowledge and assess the ethical risks involved with black box software and implement mitigation strategies.Technologists should be able to characterize what their algorithms or systems are going to do via documentation, audits, and transparent and traceable standards. To the degree possible, these characterizations should be predictive, but given the nature of A/IS, they might need to be more retrospective and mitigation-oriented. As such, it is also important to ensure access to remedy adverse impacts.Technologists and corporations must do their ethical due diligence before deploying A/IS technology. Standards for what constitutes ethical due diligence would ideally be generated by an international body such as IEEE or ISO, and barring that, each corporation should work to generate a set of ethical standards by which their processes are evaluated and modified. Similar to a flight data recorder in the field of aviation, algorithmic traceability can provide insights on what computations led to questionable or dangerous behaviors. Even where such processes remain somewhat opaque, technologists should seek indirect means of validating results and detecting harms.## Further resources•M. Ananny and K. Crawford, “[Seeing without Knowing: Limitations of the Transparency Ideal and Its Application to Algorithmic Accountability](http://journals.sagepub.com/doi/abs/10.1177/1461444816676645),” _New Media \u0026 Society_, vol. 20, no. 3, pp. 973-989, Dec. 13, 2016.•D. Reisman, J. Schultz, K. Crawford, and M. Whittaker, “Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability,” AI NOW 2018. [Online]. Available: [https://ainowinstitute.org/ aiareport2018.pdf](https://ainowinstitute.org/aiareport2018.pdf).[Accessed October 28, 2018].•J. A. Kroll “[The Fallacy of Inscrutability](http://rsta.royalsocietypublishing.org/content/376/2133/20180084).” _Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences_, C. Cath, S. Wachter, B. Mittelstadt and L. Floridi, Eds., October 15, 2018 DOI: 10.1098/rsta.2018.0084.","id":"rececsx8igwnqhhkc","dom_id":"item_rececsx8igwnqhhkc"},{"Cases":["recOOmVQviRyJGvea","recmS3zSMbR3ofAR5","recDAvfsflBF0WKpf"],"Principles":["reczVPIH1y2OMpAJH","rec42P8U9usfYCtv9"],"Reference":["IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector. The Alan Turing Institute. https://doi.org/10.5281/ZENODO.3240529"],"Sources":["recpXl48pJdKDhc6f","recfYC5jjPmpLfSlM"],"title":"Reinforcing or creating bias","category":"Challenges","name":"recefglLZ3oJWw2SZ","tags":["AI"],"created_at":"2023-05-18T13:41:32.000Z","description":"New technologies including AI have potential for bias at multiple phases of a project (problem identification, training data, modeling, implementation), and in multiple ways including through use of data that reflects existing societal biases. While reinforcing existing biases based on historic data is typically focal, the potential to create new forms of bias should also be clear “Because they gain their insights from the existing structures and dynamics of the societies they analyse, datadriven technologies can reproduce, reinforce, and amplify the patterns of marginalisation, inequality, and discrimination that exist in these societies. Likewise, because many of the features, metrics, and analytic structures of the models that enable data mining are chosen by their designers, these technologies can potentially replicate their designers’ preconceptions and biases. Finally, the data samples used to train and test algorithmic systems can often be insufficiently representative of the populations from which they are drawing inferences. This creates real possibilities of biased and discriminatory outcomes, because the data being fed into the systems is flawed from the start.” (Leslie, 2019, p. 4)### IEEE report\"**Issue 2: **A/IS can have biases that disadvantage specific groups## BackgroundEven when reflecting the full system of community norms that was identified, A/IS may show operation biases that disadvantage specific groups in the community or instill biases in users by reinforcing group stereotypes. A system’s bias can emerge in perception. For example, a passport application AI rejected an Asian man’s photo because it insisted his eyes were closed (Griffiths 201651). Bias can emerge in information processing. For instance, speech recognition systems are notoriously less accurate for female speakers than for male speakers (Tatman 201652). System bias can affect decisions, such as a criminal risk assessment device which overpredicts recidivism by African Americans (Angwin et al. 201653). The system’s bias can present itself even in its own appearance and presentation: the vast majority of humanoid robots have white “skin” color and use female voices (Riek and Howard 201454).The norm identification process detailed in Section 1 is intended to minimize individual designers’ biases because the community norms are assessed empirically. The identification process also seeks to incorporate norms against prejudice and discrimination. However, biases may still emerge from imperfections in the norm identification process itself, from unrepresentative training sets for machine learning systems, and from programmers’ and designers’ unconscious assumptions. Therefore, unanticipated or undetected biases should be further reduced by including members of diverse social groups in both the planning and evaluation of A/IS and integrating community outreach into the evaluation process, e.g., [DO-IT ](http://www.washington.edu/doit/)program and [RRI](http://www.orbit-rri.org/) framework. Behavioral scientists and members of the target populations will be particularly valuable when devising criterion tasks for system evaluation and assessing the success of evaluating the A/IS performance on those tasks. Such tasks would assess, for example, whether the A/IS apply norms in discriminatory ways to different races, ethnicities, genders, ages, body shapes, or to people who use wheelchairsor prosthetics, and so on.## RecommendationEvaluation of A/IS must carefully assess potential biases in the systems’ performance that disadvantage specific social and demographic groups. The evaluation process should integrate members of potentially disadvantaged groups in efforts to diagnose and correct such biases.## Further Resources•J. Angwin, J. Larson, S. Mattu, and L. Kirchner, “[Machine Bias: There’s Software Used Across](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) [the Country to Predict Future Criminals.](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) [And It’s Biased Against Blacks](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).” ProPublica,May 23, 2016.•J. Griffiths, “[New Zealand Passport Robot ](http://www.cnn.com/2016/12/07/asia/new-zealand-passport-robot-asian-trnd/)[Thinks This Asian Man’s Eyes Are Closed](http://www.cnn.com/2016/12/07/asia/new-zealand-passport-robot-asian-trnd/).” CNN.com, December 9, 2016.•L. D. Riek and D. Howard,. [“A Code of Ethics for the Human-Robot Interaction Profession.](https://ssrn.com/abstract%3D2757805)” _Proceedings of We Robot,_ April 4, 2014.•R. Tatman, “[Google’s Speech Recognition Has a Gender Bias](https://makingnoiseandhearingthings.com/2016/07/12/googles-speech-recognition-has-a-gender-bias/).” _Making Noise and Hearing Things_, July 12, 2016.\"p.184, IEEE, 2019","id":"recefgllz3ojww2sz","dom_id":"item_recefgllz3ojww2sz"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"title":"Will AI adversely affect human psychological and emotional wellbeing?","category":"Challenges","name":"recg7BIZZsvZ57fvV","tags":["human-potential"],"created_at":"2023-06-05T06:51:31.000Z","description":"\"Will use of A/IS adversely affect human psychological and emotional well-being in ways not otherwise foreseen?## BackgroundA/IS may be given unprecedented access to human culture and human spaces—both physical and intellectual. A/IS may communicate via natural language, may move with humanlike form, and may express humanlike identity, but they are not, and should not be regarded as, human. Incorporation of A/IS into daily life may affect human well-being in ways not yet anticipated. Incorporation of A/IS may alter patterns of trust and capability assessment between humans, and between humans and A/IS.## Recommendations1\\.Vigilance and robust, interdisciplinary, on-going research on identifying situations whereA/IS affect human well-being, both positively and negatively, is necessary. Evidence of correlations between the increased use ofA/IS and positive or negative individual or social outcomes must be explored.2\\.  Design restrictions should be placed on the systems themselves to avoid machine decisions that may alter a person’s life in unknown ways. Explanations should be available on demand in systems that may affect human well-being.## Further Resources•K. Kamewari, M. Kato, T. Kanda, H. Ishiguro and K. Hiraki. “Six-and-a-Half-Month-Old Children Positively Attribute Goals to Human Action and to Humanoid-Robot Motion,” _Cognitive Development, _vol._ _20, no. 2, pp. 303–320, 2005.•R.A. Calvo and D. Peters, Positive Computing: Technology for Wellbeing and Human Potential. Cambridge, MA: MIT Press, 2014.\"p.102","id":"recg7bizzsvz57fvv","dom_id":"item_recg7bizzsvz57fvv"},{"Principles":["rec42P8U9usfYCtv9"],"title":"Risks in removal of systems and algorithmic imprints","category":"Challenges","name":"recgZrgA8K7MmIzdF","tags":[],"created_at":"2023-05-18T18:53:45.000Z","description":"Removal of embedded tools can cause harms through: (1) failure to address changes that have been made that rely on the tool; and (2) failure to adequately address the ongoing impacts a tool may have on wider systems. 'Removal' may mean actually taking a tool out of a context, but it could also mean no longer supporting an existing tool, increasing resource needs resulting in a tool becoming unaffordable (financially or otherwise), incompatabilities with new tools, etc. An example of the first kind of harm is e.g. a company no longer supporting a medically implanted device for vision loss patients \u003chttps://spectrum.ieee.org/bionic-eye-obsolete\u003eAn example of the second kind of harm is the ongoing impacts from the UK Exam moderation algorithm in 2020, which had knock-on impacts internationally even after the algorithm itself was removed \u003chttps://dl.acm.org/doi/abs/10.1145/3531146.3533186\u003e ","id":"recgzrga8k7mmizdf","dom_id":"item_recgzrga8k7mmizdf"},{"Principles":["recOHnq45Fq7YWsRO"],"title":"Live A/B testing and validity towards aims","category":"Challenges","name":"rech9r2f3QX8ZvmkP","tags":[],"created_at":"2023-05-18T18:54:44.000Z","id":"rech9r2f3qx8zvmkp","dom_id":"item_rech9r2f3qx8zvmkp"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recsonQhKLmX4D1ao"],"title":"Cross-disciplinary expertise in ethics may be excluded in AI work","category":"Challenges","name":"rech9vLLbQgO3OY0i","tags":["research"],"created_at":"2023-06-05T10:17:28.000Z","description":"Interdisciplinary Collaborations## BackgroundMore institutional resources and incentive structures are necessary to bring A/IS engineers and designers into sustained and constructive contact with ethicists, legal scholars, and social scientists, both in academia and industry. This contact is necessary as it can enable meaningful interdisciplinary collaboration and shape the future of technological innovation. More could be done to develop methods, shared knowledge, and lexicons that would facilitatesuch collaboration.This issue relates, among other things, to funding models as well as the lack of diversity of backgrounds and perspectives in A/IS-related institutions and companies, which limit cross-pollination between disciplines. To help bridge this gap, additional translation work and resource sharing, including websites and Massive Open Online Courses (MOOCs), need to happen among technologists and other relevant experts, e.g., in medicine, architecture, law, philosophy, psychology, and cognitive science. Furthermore, there is a need for more cross-disciplinary conversation and multi-disciplinary research, as is being done, for instance, at the annual ACM Fairness, Accountability, and Transparency (FAT\\*) conference or the work done by the Canadian Institute For Advanced Research (CIFAR), whichis developing Canada’s AI strategy.Funding models and institutional incentive structures should be reviewed and revised to prioritize projects with interdisciplinary ethics components to encourage integration of ethics into projects at all levels.## Further Resources•S. Barocas, Course Material for Ethics and Policy in Data Science, Cornell University, 2017.•L. Floridi, and M. Taddeo. “What Is Data Ethics?” _Philosophical Transactions of the Royal Society, _vol._ _374, no. 2083, 1–4. DOI[10.1098/ rsta.2016.0360,](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5124072/) 2016.•S. Spiekermann, Ethical IT Innovation: A ValueBased System Design Approach. Boca Raton, FL: Auerbach Publications, 2015.•K. Crawford, “[Artificial Intelligence’s White Guy Problem](http://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html?_r=1)”, _New York Times_, July 25, 2016. [Online]. Available: [http://www.nytimes. com/2016/06/26/opinion/sunday/artificialintelligences-white-guy-problem.html?\\_r=1](http://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html?_r=1). [Accessed October 28, 2018].\"p.123-124","id":"rech9vllbqgo3oy0i","dom_id":"item_rech9vllbqgo3oy0i"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["rec1Ha94n5xbeqcOY"],"title":"How can we protect our identities to assure privacy and identity verification?","category":"Challenges","name":"reci0hfcAOIxLVNhB","tags":[],"created_at":"2023-06-05T09:37:23.000Z","description":"\"**Issue: **How can we increase agency by providing individuals access to services allowing them to create a trusted identity to control the safe, specific, and finite exchange of their data?## BackgroundPervasive behavior-tracking adversely affects human agency by recognizing our identity in every action we take on and offline. This is why identity as it relates to individual data is emerging at the forefront of the risks and opportunities related to use of personal information for A/IS. Across the identity landscape there is increasing tension between the requirement for federated identities versus a range of identities. In federated identities, all data are linked to a natural and identified person. When one has a range of identities, or personas, these can be context specific and determined by the use case. New movements, such as “Self-Sovereign Identity”— defined as the right of a person to determine his or her own identity—are emerging alongside legal identities, e.g., those issued by governments, banks, and regulatory authorities, to help put individuals at the center of their data in the algorithmic age.Personas, identities that act as proxies, and pseudonymity are also critical requirements for privacy management and agency. These help individuals select an identity that is appropriate for the context they are in or wish to join. In these settings, trust transactions can still be enabled without giving up the “root” identity of the user. For example, it is possible to validate that a user is over eighteen or is eligible for a service.Attribute verification will play a significant role in enabling individuals to select the identity that provides access without compromising agency. This type of access is especially important in dealing with the myriad of algorithms interacting with narrow segments of our identity data. In these situations, individuals typically are not aware of the context for how their data will be used.## RecommendationIndividuals should have access to trusted identity verification services to validate, prove, and support the context-specific use of their identity. ## Further Resources•Sovrin Foundation, [The Inevitable Rise of SelfSovereign Identity,](https://sovrin.org/wp-content/uploads/2017/06/The-Inevitable-Rise-of-Self-Sovereign-Identity.pdf) Sept. 29, 2016.•T. Ruff, “[Three Models of Digital Identity Relationships](https://medium.com/evernym/the-three-models-of-digital-identity-relationships-ca0727cb5186),” Evernym, Apr. 24, 2018.•C. Pettey, [The Beginner’s Guide to Decentralized Identity.](https://www.gartner.com/smarterwithgartner/the-beginners-guide-to-decentralized-identity/) Gartner, 2018.•C. Allen, [The Path to Self-Sovereign Identity](https://github.com/ChristopherA/self-sovereign-identity/blob/master/ThePathToSelf-SovereignIdentity.md). GitHub, 2017.\"p.112-113 IEEE report","id":"reci0hfcaoixlvnhb","dom_id":"item_reci0hfcaoixlvnhb"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["rec9TbOu2ZXUZG4A5"],"title":"How do we ensure AI nudges are deployed appropriately and address their target outcome?","category":"Challenges","name":"reciH69D8oIGI1p93","tags":["nudge-deception"],"created_at":"2023-06-05T06:13:16.000Z","description":"Will A/IS nudging systems that are not fully relevant to the sociotechnical context in which they are operating cause behaviors with adverse unintended consequences?## BackgroundA well-designed nudging or suggestion system will have sophisticated enough technical capabilities for recognizing the context in which it is applying nudging actions. Assessment of the context requires perception of the scope or impact of the actions to be taken, the consequences of incorrectly or incompletely applied nudges, and acknowledgement of the uncertainties that may stem from long term consequences of a nudge7.## Recommendations1\\.Consideration should be given to the development of a system of technical licensing (“permits”) or other certification from governments or non-governmental organizations (NGOs) that can aid users to understand the nudges from A/IS in their lives.2\\.User autonomy is a key and essential consideration that must be taken into account when addressing whether affective systems should be permitted to nudge human beings.3\\.Design features of an affective system that nudges human beings should include the ability to accurately distinguish between users, including detecting characteristics such as whether the user is an adult or a child.4\\.Affective systems with nudging strategies should incorporate a design system of evaluation, monitoring, and control for unintended consequences.## Further Resources•J. Borenstein and R. Arkin, “[Robotic Nudges: ](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)[Robotic Nudges: The Ethics of Engineering a ](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)[More Socially Just Human Being Just Human Being.](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)” _Science and Engineering Ethics, _vol._ _22, no. 1, pp. 31–46, 2016.•R. C. Arkin, M. Fujita, T. Takagi, and R. Hasegawa, “[An Ethological and Emotional Basis for Human- Robot Interaction.](http://www.sciencedirect.com/science/article/pii/S0921889002003755)” _Robotics and Autonomous Systems, _vol._ _42, no. 3–4 pp.191–201, March 2003.•S. Omohundro “[Autonomous Technology and the Greater Human Good.](http://www.tandfonline.com/doi/abs/10.1080/0952813X.2014.895111?journalCode=teta20)” _Journal of Experimental and Theoretical Artificial Intelligence, vol. _26, no. 3, pp. 303–315, 2014.","id":"recih69d8oigi1p93","dom_id":"item_recih69d8oigi1p93"},{"Principles":["receFm7cGasHwpJZO","recQ9DIFEsOEkCx3O"],"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recZI7HDWKBU5T22v"],"title":"How do we incorporate a human-rights framework into AI design/development/use?","category":"Challenges","name":"reciNc7OeTUmnsLqg","tags":["wellbeing"],"created_at":"2023-06-05T05:47:17.000Z","description":"Human rights law is related to, but distinct from, the pursuit of well-being. Incorporating a human-rights framework as an essential basis for A/IS creators means A/IS creators honor existing law as part of their well-being analysis and implementation.## BackgroundInternational human rights law has been firmly established for decades in order to protect various guarantees and freedoms as enshrined in charters such as the United Nations’[ Universal Declaration of Human Rights ](http://www.un.org/en/universal-declaration-human-rights/)and the Council of Europe’s [Convention on Human Rights](https://www.coe.int/en/web/human-rights-convention). In 2018, the[ Toronto Declaration ](https://www.accessnow.org/the-toronto-declaration-protecting-the-rights-to-equality-and-non-discrimination-in-machine-learning-systems/)on machine learning standards was released, calling on both governments and technology companies to ensure that algorithms respect basic principles of equality and non-discrimination. The Toronto Declaration sets forth an obligation to prevent machine learning systems from discriminating, and in some cases violating, existing human rights law.Well-being initiatives are typically undertaken for the sake of public interest. However, any metric, including well-being metrics, can be misused to justify human rights violations. Encampment and mistreatment of refugees and ethnic cleansing undertaken to preserve a nation’s culture (an aspect of well-being) is one example. Imprisonment or assassination of journalists or researchers to ensure the stability of a government is another. The use of wellbeing metrics to justify human rights violations is an unconscionable perversion of the nature of any well-being metric. It should be noted that these same practices happen today in relation to GDP. For instance, in 2012, according to the [International Labour Organization (](http://www.ilo.org/global/about-the-ilo/newsroom/news/WCMS_181961/lang--en/index.htm)ILO), approximately 21 million people are victims of forced labor (slavery), representing 9% to 56% of GDP income for various countries. These clear human rights violations, from sex trafficking and use of children in armies, to indentured farming or manufacturing labor, can increase a country’s GDP while obviously harming human well-being.## RecommendationsWell-being metrics are designed to measure the efficacy of efforts related to individual and societal flourishing. Well-being as a value complements justice, equality, and freedom. Well-designed application of well-being considerations by A/IS creators should not displace other issues of human rights or ethical methodologies, but rather complement them.A human rights framework should represent the floor, and not the ceiling, for the standards to which A/IS creators must adhere. Developers and users of well-being metrics should be aware these metrics will not always adequately address human rights.## Further Resources•United Nations[ Universal Declaration of Human Rights,](http://www.un.org/en/universal-declaration-human-rights/) 1948.•Council of Europe’s [Convention on Human Rights](https://www.coe.int/en/web/human-rights-convention), 2018.•International Labor Organization (ILO) [Declaration on Fundamental Principles and Rights at Work,](https://www.ilo.org/declaration/lang--en/index.htm) 1998.•The regularly updated [University of Minnesota Human Rights Library ](http://hrlibrary.umn.edu/)provides a wealth of material on human rights laws, its history, and the organizations engaged in promoting them.The [Oxford Human Rights Hub ](http://ohrh.law.ox.ac.uk/why-artificial-intelligence-is-already-a-human-rights-issue/)reports on how and why technologies surrounding artificial intelligence raise human rights issues\"p.76-77","id":"recinc7oetumnslqg","dom_id":"item_recinc7oetumnslqg"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recouZjokdKQz88z1"],"title":"AI could hamper, not foster, progress on SDGs","category":"Challenges","name":"recj64vAVJSm5B2ba","tags":["SDGs"],"created_at":"2023-06-05T10:57:55.000Z","description":"**\"Issue: **Current roadmaps for development and deployment of A/IS are not aligned with or guided by their impact in the most important challenges of humanity, defined in the seventeen United Nations Sustainable Development Goals (SDGs), which collectively aspire to create a more equal world of prosperity, peace, planet protection, and human dignityfor all people.4## Background SDGs promoting prosperity, peace, planet protection, human dignity, and respect for human rights of all, apply to HIC and LMIC alike. Yet ensuring that the benefits of A/IS will accrue to humanity as a whole, leaving “no one behind”, requires an ethical commitment to global citizenship and well-being, and a conscious effort to counter the nature of the tech economy, with its tendency to concentrate wealth within high income populations. Implementation of the SDGs should benefit excluded sectors of society in every country, regardless of A/IS infrastructure.“The Road to Dignity by 2030” document of the UN Secretary General reports on resources and methods for implementing the 2030 Agenda for Sustainable Development and emphasizes the importance of science, technology, and innovation for a sustainable future.5 The UN Secretary General posits that:“A sustainable future will require that we act now to phase out unsustainable technologies and to invest in innovation and in the development of clean and sound technologies for sustainable development. We must ensure that they are fairly priced, broadly disseminated and fairly absorbed, including to and by developing countries.” (para. 120)A/IS are among the technologies that can play an important role in the solution of the deep social problems plaguing our global civilization, contributing to the transformation of society away from an unsustainable, unequal socioeconomic system, towards one that realizes the vision of universal human dignity, peace, and prosperity.However, with all the potential benefits ofA/IS, there are also risks. For example, givenA/IS technology’s immense power needs, without new sources of sustainable energy harnessed to power A/IS in the future, there is a risk that it will increase fossil fuel use and have a negative impact on the environment and the climate.While 45% of the world’s population is not connected to the internet, they are not necessarily excluded from A/IS’ potential benefits: in LMIC mobile networks can provide data for A/IS applications. However, only those connected are likely to benefit from the income-producing potential of internet technologies. In 2017, internet penetration in HIC left behind certain portions of the population often in rural or remote areas; 12% of U.S. residents and 20% of residents across Europe were unable to access the internet. In Asia with its concentration of LMIC, 52% of the population, on average, had no access, a statistic skewed by the large population of China, where internet penetration reached 45% of the population. In numerous other countries in the region, 99% of residents had no access. This nearly total exclusion also exists in several countries in Africa, where the overall internet penetration is only 35%: 2 of every 3 residents in Africa have no access.6 Those with no internet access also do not generate data needed to “train” A/IS, and are thereby excluded from benefits of the technology, the development of which risks systematic discriminatory bias, particularly against people from minority populations, and those living in rural areas, or in low-income countries. As a comparison, one study estimated that “in the US, just one home automation product can generate a data point every six seconds.”7 In Mozambique, where about 90% of the population lack internet access, “the average household generates zero digital data points.”8 With mobile phones generating much of the data needed for developing A/IS applications in LMIC, unequal phone ownership may build in bias. For example, there is a risk of discrimination against women, who across LMIC are 14% less likely than men to own a mobile phone, and in South Asia where 38% are less likely to own a mobile phone.9## RecommendationsThe current range of A/IS applications in sectors crucial to the SDGs, and to excluded populations everywhere, should be studied, with the strengths, weaknesses, and potential of the most significant recent applications analyzed, and the best ones developed at scale. Specific objectives to consider include:•Identifying and experimenting withA/IS technologies relevant to the SDGs,such as: big data for development relevant to, for example, agriculture and medical tele-diagnosis; geographic information systems needed in public service planning, disaster prevention, emergency planning, and disease monitoring; control systems used in, for example, naturalizing intelligent cities through energy and traffic control and management of urban agriculture; applications that promote human empathy focused on diminishing violence and exclusion and increasing well-being.•Promoting the potential role of A/IS in sustainable development by collaboration between national and international government agencies and non-governmental organizations (NGOs) in technology sectors.•Analyzing the cost of and proposing strategies for publicly providing internet access forall, as a means of diminishing the gap inA/IS’ potential benefit to humanity, particularly between urban and rural populations in HIC and LMIC alike.•Investing in the documentation and dissemination of innovative applications ofA/IS that advance the resolution of identified societal issues and the SDGs.•Researching sustainable energy to power A/IS computational capacity.•Investing in the development of transparent monitoring frameworks to track the concrete results of donations by international organizations, corporations, independent agencies, and the State, to ensure efficiency and accountability in applied A/IS.•Developing national legal, policy, and fiscal measures to encourage competition in theA/IS domestic markets and the flourishingof scalable A/IS applications.•Integrating the SDGs into the core of private sector business strategies and adding SDG indicators to companies’ key performance indicators, going beyond corporate social responsibility (CSR).•Applying the well-being indicators10 to evaluate A/IS’ impact from multiple perspectives in HIC and LMIC alike.## Further reading•R. Van Est and J.B.A. Gerritsen, with assistance of L. Kool, Human Rights in the Robot Age: Challenges arising from the use of Robots, Artificial Intelligence and Augmented Reality Expert Report written for the Committee on Culture, Science, Education and Media of the Parliamentary Assembly of the Council of Europe (PACE), The Hague: Rathenau Instituut 2017.•World Economic Forum Global Future Council on Human Rights 2016-18, “White Paper: How to Prevent Discriminatory Outcomes in Machine Learning,” World Economic Forum, March 2018.•United Nations General Assembly, _Transforming Our World: The 2030 Agenda for Sustainable Development_ (A/RES/70/1: 21 October 2015) Preamble. [http://www.un.org/ en/development/desa/population/migration/ generalassembly/docs/globalcompact/ A\\_RES\\_70\\_1\\_E.pdf](http://www.un.org/en/development/desa/population/migration/generalassembly/docs/globalcompact/A_RES_70_1_E.pdf).•United Nations Global Pulse, Big Data for Development: Challenges and Opportunities, 2012.\"p139-140","id":"recj64vavjsm5b2ba","dom_id":"item_recj64vavjsm5b2ba"},{"Principles":["recKdujFoPJr4ZAhZ"],"Reference":"Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector. The Alan Turing Institute. https://doi.org/10.5281/ZENODO.3240529","Sources":["recfYC5jjPmpLfSlM"],"title":"Breakdown in trust in loss of exposure to diverse human-human interactions","category":"Challenges","name":"reckPjhcWwLgjX9p5","tags":[],"created_at":"2023-05-19T09:32:07.000Z","description":"“While the capacity of AI systems to curate individual experiences and to personalise digital services holds the promise of vastly improving consumer life and service delivery, this benefit also comes with potential risks. Excessive automation, for example, might reduce the need for human-to-human interaction, while algorithmically enabled hyper-personalisation, by limiting our exposure to worldviews different from ours, might polarise social relationships. Well-ordered and cohesive societies are built on relations of trust, empathy, and mutual understanding. As AI technologies become more prevalent, it is important that these relations be preserved.” (Leslie, 2019, p. 5)","id":"reckpjhcwwlgjx9p5","dom_id":"item_reckpjhcwwlgjx9p5"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"title":"Will increased access to personal information impact human experience and autonomy?","category":"Challenges","name":"reckyVWsglP8FLeUp","tags":["human-potential"],"created_at":"2023-06-05T06:13:33.000Z","description":"\"Does the increased access to personal information about other members of our society, facilitated by A/IS, alter the human affective experience?Does this access potentially lead to a change in human autonomy?## BackgroundTheoretical biology tells us that we should expect increased communication—which A/IS facilitate— to increase group-level investment8. Extensive use of A/IS could change the expression of individual autonomy and in its place increase group-based identities. Examples of this sortof social alteration may include:1\\.Changes in the scope of monitoring and control of children’s lives by parents.2\\.Decreased willingness to express opinions for fear of surveillance or long-term consequences of past expressions being used in changed temporal contexts.3\\.Utilization of customers or other end users to perform basic corporate business processes such as data entry as a barter for lower prices or access, resulting potentially in reduced tax revenues.4\\.Changes to the expression of individual autonomy could alter the diversity, creativity, and cohesiveness of a society. It may also alter perceptions of privacy and security, and social and legal liability for autonomous expressions.## Recommendations1\\.Organizations, including governments, must put a high value on individuals’ privacy and autonomy, including restricting the amount and age of data held about individuals specifically.2\\.Education in all forms should encourage individuation, the preservation of autonomy, and knowledge of the appropriate uses and limits to A/IS9.## Further Resources•J. J. Bryson, “Artificial Intelligence and Pro-Social Behavior,” in _Collective Agency and Cooperation in Natural and Artificial Systems, _C. Misselhorn, Ed., pp. 281–306, Springer, 2015.•M. Cooke, “A Space of One’s Own: Autonomy, Privacy, Liberty,” _Philosophy \u0026 Social Criticism, _Vol._ _25, no. 1, pp. 22–53, 1999.•D. Peters, R.A. Calvo, R.M. Ryan, “Designing for Motivation, Engagement and Wellbeing in Digital Experience[” _Frontiers in Psychology_](https://www.frontiersin.org/journals/psychology) – _Human Media Interaction_, vol. 9. pp 797, 2018.•J. Roughgarden, M. Oishi and E. Akçay,“Reproductive Social Behavior: Cooperative Games to Replace Sexual Selection.” _Science _311, no. 5763, pp. 965–969, 2006.\"p.101","id":"reckyvwsglp8fleup","dom_id":"item_reckyvwsglp8fleup"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recMgYQ7EqHAyDfi1"],"title":"How do we ensure protection of data during humanitarian emergencies?","category":"Challenges","name":"recmRF2P1OOASMfNF","tags":[],"created_at":"2023-06-05T11:54:11.000Z","description":"**\"Issue: **A/IS are contributing to humanitarian action to save lives, alleviate suffering, and maintain human dignity both during and in the aftermath of man-made crises and natural disasters, as well as to prevent and strengthen preparedness for the occurrence of such situations. However, there are ethical concerns with both the collection and use of data during humanitarian emergencies.## BackgroundThere have been a number of promising A/IS applications that relieve suffering in humanitarian crises, such as extending the reach of the health system by using drones to deliver blood to remote parts of Rwanda,31 locating and removing landmines,32 efforts to use A/IS to track movements and population survival needs following a natural disaster, and to meet the multiple management requirements of refugee camps.33 There are also promising developments using A/IS and robotics to assist people with disabilities to recover mobility, and robots to rescue people trapped in collapsed buildings.34 A/IS are also being used to monitor conflict zones and to enable early warning systems.35 For example, Microsoft has partnered with the UN Human Rights Office of the High Commissioner (OHCHR) to use big data in order to track and analyze human rights violations in conflict zones.36 Machine learning is being used for improved decision-making regarding asylum adjudication and refugee resettlement, with a view to increasing successful integration between refugees and host communities.37 In addition, there is evidence that a recent growth in human empathy has increased well-being while diminishing psychological and physical violence,38 inspiring some researchers to look for ways of harnessing the power of A/IS to introduce more empathy and less violence into society.The design and ethical deployment of these technologies in crisis settings are both essential and challenging. Large volumes of both personally identifiable and demographically identifiable data are collected in fragile environments, where tracking of individuals or groups may compromise their security if data privacy cannot be assured. Consent to data use is also impractical in such environments, yet crucial for the respect of human rights.## RecommendationsThe potential for A/IS to contribute to humanitarian action to save and improve lives should be prioritized for research and development, including by organizing global research challenges, while also building in safeguards to protect the creation, collection, processing, sharing, use, and disposal of information, including data from and about individuals and populations. Specific recommendations include:•Promoting awareness of the vulnerable condition of certain communities around the globe and the need to develop and use A/IS applications for humanitarian purposes.•Elaborating competitions and challenges in high impact conferences and university hackathons to engage both technical and nontechnical communities in the development of A/IS for humanitarian purposes and to address social issues.•Support civil society groups who organize themselves for the purpose of A/IS research and advocacy to develop applications to benefit humanitarian causes.39•Developing and applying ethical standards for the collection, use, sharing, and disposal of data in fragile settings.•Following privacy protection frameworks for pressing humanitarian situations that ensure the most vulnerable are protected.40•Setting up clear ethical frameworks for exceptional use of A/IS technologies in lifesaving humanitarian situations, comparedto \"normal\" situations.41•Stimulating the development of low-costand open source solutions based on A/ISto address specific humanitarian problems.•Training A/IS experts in humanitarian action and norms, and humanitarian practitionersto catalyze collaboration in designing,piloting, developing, and implementingA/IS technologies for humanitarian purposes. Forging public-private A/IS participant alliances that develop crisis scenarios in advance.•Working on cultural and contextual acceptance of any A/IS introduced during emergencies.•Documenting and developing quantifiable metrics for evaluating the outcomes of humanitarian digital projects, and educating the humanitarian ecosystem on the same.## Further resources•E. Prestes et al., \"The 2016 Humanitarian Robotics and Automation Technology Challenge [Competitions],\" in _IEEE Robotics \u0026 Automation Magazine_, vol. 23, no. 3, pp. 23-24, Sept. 2016. [http://ieeexplore.ieee.org/stamp/ stamp.jsp?tp=\u0026arnumber=7565695\u0026isnumber=7565655](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\u0026arnumber=7565695\u0026isnumber=7565655)•L. Marques et al., \"Automation of humanitarian demining: The 2016 Humanitarian Robotics and Automation Technology Challenge,\" _2016 International Conference on Robotics and Automation for Humanitarian Applications (RAHA)_, Kollam, 2016, pp. 1-7. \u003chttp://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\u0026arnumber=7931893\u0026isnumber=7931858\u003e•CYBATHLON 2020 Preliminary Race Task Descriptions [http://www.cybathlon.ethz. ch/cybathlon-2020/preliminary-race-taskdescriptions.html](http://www.cybathlon.ethz.ch/cybathlon-2020/preliminary-race-task-descriptions.html)•CYBATHLON Scientific Publications\u003chttp://www.cybathlon.ethz.ch/\u003e•Immigration Policy Lab (IPL), “Harnessing Big Data to Improve Refugee Resettlement” [https://immigrationlab.org/project/harnessingbig-data-to-improve-refugee-resettlement/](https://immigrationlab.org/project/harnessing-big-data-to-improve-refugee-resettlement/)•Harvard Humanitarian Initiative, _The Signal Code_, [https://signalcode.org](https://signalcode.org/) •J.A. Quinn, et al., “Humanitarian applications of machine learning with remote-sensing data: review and case study in refugee settlement mapping” Philosophical Transactions of the Royal Society A, 376 20170363; DOI:10.1098/rsta.2017.0363. Aug. 6, 2018.•Humanitarian Innovation Guide: [https:// higuide.elrha.org/,](https://higuide.elrha.org/) 2019.•P. Meier, [Digital Humanitarians: How Big Data is Changing the Face of Humanitarian Response](http://cds.cern.ch/record/2123110). Florida: CRC Press, 2015.•“Technology for human rights: UN Human Rights Office announces landmark partnership with Microsoft” [https://www.ohchr.org/ EN/NewsEvents/Pages/DisplayNews. aspx?NewsID=21620\u0026LangID=E](https://www.ohchr.org/EN/NewsEvents/Pages/DisplayNews.aspx?NewsID=21620\u0026LangID=E)•M. Luengo-Oroz, “10 big data science challenges facing humanitarian organizations,” UNHCR, Nov. 22, 2016. [http://www. unhcr.org/innovation/10-big-data-sciencechallenges-facing-humanitarian-organizations/](http://www.unhcr.org/innovation/10-big-data-science-challenges-facing-humanitarian-organizations/)•Optic Technologies, Press Release, Vatican Hack 2018—Results, 18 March 2018, which announced winning AI applications to benefit migrants and refugees as well as social inclusion and interfaith dialogue,[http://optictechnology.org/index.php/en/news-en/151-vhack-2018winners-en ](http://optictechnology.org/index.php/en/news-en/151-vhack-2018winners-en)\"p157-159","id":"recmrf2p1ooasmfnf","dom_id":"item_recmrf2p1ooasmfnf"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recQPwyiQbPcN0G47"],"title":"How do ensure stakeholder experience is considered in AI design?","category":"Challenges","name":"recnYgPiGULdHLunC","tags":["corporate-ethics"],"created_at":"2023-06-05T10:40:09.000Z","description":"**\"Issue:** Stakeholder inclusion## BackgroundThe interface between A/IS and practitioners, as well as other stakeholders, is gaining broader attention in domains such as healthcare diagnostics, and there are many other contexts where there may be different levels of involvement with the technology. We should recognize that, for example, occupational therapists and their assistants may have on-theground expertise in working with a patient, who might be the “end user” of a robot or socialA/IS technology. In order to develop a product that is ethically aligned, stakeholders’ feedback is crucial to design a system that takes ethical and social issues into account. There are successful user experience (UX) design concepts, such as accessibility, that consider human physical disabilities, which should be incorporated into A/IS as they are more widely deployed. It is important to continuously consider the impact of A/IS through unanticipated use and on unforeseen interests.## RecommendationsTo ensure representation of stakeholders, organizations should enact a planned and controlled set of activities to account for the interests of the full range of stakeholders or practitioners who will be working alongsideA/IS and incorporating their insights to build upon, rather than circumvent or ignore, thesocial and practical wisdom of involved practitioners and other stakeholders.## Further Resources•C. Schroeter, et al., “[Realization and User Evaluation of a Companion Robot for People with Mild Cognitive Impairments](http://www.tu-ilmenau.de/fileadmin/media/neurob/publications/conferences_int/2013/Schroeter-ICRA-2013-fin.pdf),” _Proceedings of IEEE International Conference on Robotics and Automation (ICRA 2013)_, Karlsruhe, Germany 2013. pp. 1145–1151.•T. L. [Chen, et al. ](http://ieeexplore.ieee.org/abstract/document/6476704/)“[Robots for Humanity: Using Assistive Robotics to Empower People with Disabilities](http://ieeexplore.ieee.org/document/6476704/),” _IEEE Robotics and Automation Magazine, _vol. 20, no. 1, pp. 30–39, 2013.R. Hartson, and P. S. Pyla. _The UX Book: Process and Guidelines for Ensuring a Quality User Experience_. Waltham, MA: Elsevier, 2012\"p.130-131","id":"recnygpiguldhlunc","dom_id":"item_recnygpiguldhlunc"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recfcXzM3foqFNNGN"],"title":"How do corporations create values-based ethical culture in the context of growth-focused strategy","category":"Challenges","name":"recqpYMjEFJLmyNaN","tags":["corporate-ethics"],"created_at":"2023-06-05T10:25:26.000Z","description":"**\"Issue:** Values-based ethical culture and practices for industry## BackgroundCorporations are built to create profit while competing for market share. This can lead corporations to focus on growth at the expense of avoiding negative ethical consequences. Given the deep ethical implications of widespread deployment of A/IS, in addition to laws and regulations, there is a need to create values-based ethical culture and practices for the development and deployment of those systems. To do so, we need to further identify and refine corporate processes that facilitate values-based design.## RecommendationsThe building blocks of such practices include top-down leadership, bottom-up empowerment, ownership, and responsibility, along with the need to consider system deployment contexts and/or ecosystems. Corporations should identify stages in their processes in which ethical considerations, “ethics filters”, are in place before products are further developed and deployed. For instance, if an ethics review board comes in at the right time during the A/IS creation process, it would help mitigate the likelihood of creating ethically problematic designs. The institution of an ethical A/IS corporate culture would accelerate the adoption of the other recommendations within this section focused on business practices.## Further Resources•[ACM Code of Ethics and Professional Ethics,](https://ethics.acm.org/2018-code-draft-2/) which includes various references to human well-being and human rights, 2018.•Report of UN Special Rapporteur on [Freedom of Expression. _AI and Freedom of Expression_.](http://undocs.org/A/73/348) 2018.•The [website of the Benefit corporations ](https://www.bcorporation.net/)(B-corporations) provides a good overview of a range of companies that personify this type of culture.•R. Sisodia, J. N. Sheth and D. Wolfe, [Firms of ](http://www.firmsofendearment.com/)[Endearment](http://www.firmsofendearment.com/)_, _2nd edition. Upper Saddle River, NJ: FT Press, 2014. This book showcases how companies embracing values and a stakeholder approach outperform their competitors in the long run.\"p.127\"**Issue:** Values-based leadership## BackgroundTechnology leadership should give innovation teams and engineers direction regarding which human values and legal norms should be promoted in the design of A/IS. Cultivating an ethical corporate culture is an essential component of successful leadership in theA/IS domain.## RecommendationsCompanies should create roles for senior-level marketers, engineers, and lawyers who can collectively and pragmatically implement ethically aligned design. There is also a need for more in-house ethicists, or positions that fulfill similar roles. One potential way to ensure values are on the agenda in A/IS development is to have a Chief Values Officer (CVO), a role first suggested by Kay Firth-Butterfield, see “Further Resources”. However, ethical responsibility should not be delegated solely to CVOs. They can support the creation of ethical knowledge in companies, but in the end, all members of an organization will need to act responsibly throughout the design process.Companies need to ensure that their understanding of values-based system innovation is based on _de jure _and _de facto _international human rights standards.•K. Firth-Butterfield, “[How IEEE Aims to Instill Ethics in Artificial Intelligence Design,](http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design)” The Institute. Jan. 19, 2017. [Online]. Available: [http://theinstitute.ieee.org/ieee-roundup/ blogs/blog/how-ieee-aims-to-instill-ethicsin-artificial-intelligence-design](http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design). [Accessed October 28, 2018]. •United Nations, [Guiding Principles on Business and Human Rights: Implementing the United Nations “Protect, Respect and Remedy” Framework,](http://www.ohchr.org/Documents/Publications/GuidingPrinciplesBusinessHR_EN.pdf) New York and Geneva: UN, 2011.•Institute for Human Rights and Business(IHRB), and Shift, ICT [Sector Guide on ](https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf)[Implementing the UN Guiding Principles on Business and Human Rights,](https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf) 2013.•C. Cath, and L. Floridi, “[The Design of the Internet’s Architecture by the Internet ](http://europepmc.org/abstract/med/27255607)[Engineering Task Force (IETF) and Human Rights](http://europepmc.org/abstract/med/27255607).” _Science and Engineering Ethics, _vol._ _23, no. 2, pp. 449–468, Apr. 2017.\"p.128","id":"recqpymjefjlmynan","dom_id":"item_recqpymjefjlmynan"},{"Principles":["recsvi4LnhEEPyQ1h"],"title":"New technologies make reidentification easier","category":"Challenges","name":"recrHBtVR4EOXSJh2","tags":[],"created_at":"2023-05-19T14:21:29.000Z","id":"recrhbtvr4eoxsjh2","dom_id":"item_recrhbtvr4eoxsjh2"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["rec9TbOu2ZXUZG4A5"],"title":"Should affective AI systems ever deceive?","category":"Challenges","name":"recrVvneRZaEutaGw","tags":["nudge-deception"],"created_at":"2023-06-05T06:13:17.000Z","description":"\"When, if ever, andunder which circumstances,is deception performed by affective systems acceptable?## BackgroundDeception is commonplace in everyday human-human interaction. According to Kantian ethics, it is never ethically appropriate to lie, while utilitarian frameworks indicate that it can be acceptable when deception increases overall happiness. Given the diversity of views on ethics and the appropriateness of deception, should affective systems be designed to deceive? Does the non-consensual nature of deception restrict the use of A/IS in contexts in which deception may be required?It is necessary to develop recommendations regarding the acceptability of deception performed by A/IS, specifically with respect to when and under which circumstances, if any,it is appropriate.1\\.In general, deception may be acceptable in an affective agent when it is used for the benefit of the person being deceived, not for the agent itself. For example, deception might be necessary in search and rescue operations or for elder- or child-care.2\\.For deception to be used under any circumstance, a logical and reasonable justification must be provided by the designer, and this rationale should be certified by an external authority, such as a licensing bodyor regulatory agency.## **Further resources**•R. C. Arkin, “Robots That Need to Mislead: Biologically-inspired Machine Deception.” _IEEE Intelligent Systems _27, no. 6, pp. 60–75, 2012.•J. Shim and R. C. Arkin, “Other-Oriented Robot Deception: How Can a Robot’s Deceptive Feedback Help Humans in HRI?” _Eighth International Conference on Social Robotics (ICSR 2016)_, Kansas, MO., November 2016.•J. Shim and R. C. Arkin, “The Benefits of Robot Deception in Search and Rescue: Computational Approach for Deceptive Action Selection via Case-based Reasoning.” _2015 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR 2015)_, West Lafayette, IN, October 2015.•J. Shim and R. C. Arkin, “A Taxonomy of Robot Deception and its Benefits in HRI.” _Proceedings of IEEE Systems, Man and Cybernetics Conference, _Manchester England, October 2013.\"p.102-103","id":"recrvvnerzaeutagw","dom_id":"item_recrvvnerzaeutagw"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recgqEckuFTILlOnz"],"title":"How do we maintain independence of review in AI applications?","category":"Challenges","name":"recrk0Tgfwe5J9xfI","tags":["corporate-ethics"],"created_at":"2023-06-05T10:49:41.000Z","description":"## BackgroundWe need independent, expert opinions that provide guidance to the general public regarding A/IS. Currently, there is a gap between howA/IS are marketed and their actual performance or application. We need to ensure thatA/IS technology is accompanied by best-use recommendations and associated warnings. Additionally, we need to develop a certification scheme for A/IS which ensures that the technologies have been independentlyassessed as being safe and ethically sound.For example, today it is possible for systems to download new self-parking functionality to cars, and no independent reviewer establishes or characterizes boundaries or use. Or, when a companion robot promises to watch your children, there is no organization that can issue an independent seal of approval or limitation on these devices. We need a ratings and approval system ready to serve social/automation technologies that will come online as soon as possible. We also need further government funding for research into how A/IS technologies can best be subjected to review, and howreview organizations can consider bothtraditional health and safety issues, as wellas ethical considerations.## RecommendationAn independent, internationally coordinated body—akin to ISO—should be formed to oversee whether A/IS products actually meet ethical criteria, both when designed, developed, deployed, and when considering their evolution after deployment and during interaction with other products. It should also includea certification process.## Further Resources•A. Tutt, “An FDA for Algorithms,” _Administrative Law Review _69, 83–123, 2016.•M. U. Scherer, “[Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies,](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2609777)” _Harvard Journal of Law and Technology _vol._ _29, no. 2, 354–400, 2016.•D. R. Desai and J. A. Kroll, “[Trust But Verify: A Guide to Algorithms and the Law](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2959472).” _Harvard Journal of Law and Technology_, Forthcoming; Georgia Tech Scheller College of Business Research Paper No. 17-19, 2017.p.133-134\"**Issue 3:** Challenges to evaluation by third parties## BackgroundA/IS should have sufficient transparency to allow evaluation by third parties, including regulators, consumer advocates, ethicists, post-accident investigators, or society at large. However, transparency can be severely limited in some systems, especially in those that rely on machine learning algorithms trained on large data sets. The data sets may not be accessible to evaluators; the algorithms may be proprietary information or mathematically so complex that they defy common-sense explanation; and even fellow software experts may be unable to verify reliability and efficacy of the final system because the system’s specifications are opaque.For less inscrutable systems, numerous techniques are available to evaluate the implementation of the A/IS’ norm conformity. On one side there is formal verification, which provides a mathematical proof that the A/IS will always match specific normative and ethical requirements, typically devised in a top-down approach (see Section 2, Issue 1). This approach requires access to the decision-making process and the reasons for each decision (Fisher, Dennis, and Webster 201355). A simpler alternative, sometimes suitable even for machine learning systems, is to test the A/IS against a set of scenarios and assess how well they matches their normative requirements, e.g., acting in accordance with relevant norms and recognizing other agents’ norm violations. A “red team” may also devise scenarios that try to get the A/ISto break norms so that its vulnerabilities canbe revealed.These different evaluation techniques can be assigned different levels of “strength”: strong ones demonstrate the exhaustive set of theA/IS’ allowable behaviors for a range of criterion scenarios; weaker ones sample from criterion scenarios and illustrate the systems’ behavior for that subsample. In the latter case, confidence in the A/IS’ ability to meet normative requirements is more limited. An evaluation’s concluding judgment must therefore acknowledge the strength of the verification technique used,and the expressed confidence in the evaluation — and in the A/IS themselves—must be qualifiedby this level of strength.Transparency is only a necessary requirement for a more important long-term goal: having systems be accountable to their users and community members. However, this goal raises many questions such as to whom the A/IS are accountable, who has the right to correct the systems, and which kind of A/IS should be subject to accountability requirements.## RecommendationTo maximize effective evaluation by third parties, e.g., regulators and accident investigators, A/IS should be designed, specified, and documented so as to permit the use of strong verification and validation techniques for assessing the system’s safety and norm compliance, in order to achieve accountability to the relevant communities.## Further Resources•M. Fisher, L. A. Dennis, and M. P. Webster. “Verifying Autonomous Systems.” _Communications of the ACM,_ vol. 56, pp.84–93, 2013.•K. Abney, G. A. Bekey, and P. Lin. _[Robot Ethics: The Ethical and Social Implications of Robotics](https://mitpress.mit.edu/books/robot-ethics)_[.](https://mitpress.mit.edu/books/robot-ethics) Cambridge, MA: The MIT Press, 2011.•M. Anderson and S. L. Anderson, eds._[Machine Ethics](http://assets.cambridge.org/97805211/12352/copyright/9780521112352_copyright_info.pdf)._ New York: Cambridge University Press, 2011.•M. Boden, J. Bryson, et al. “Principles of Robotics: Regulating Robots in the Real World.” _Connection Science _29, no. 2, pp. 124–129, 2017.•M. Coeckelbergh, “[Can We Trust Robots?](https://link.springer.com/article/10.1007/s10676-011-9279-1)” _Ethics and Information Technology, _vol.14, pp. 53–60, 2012.•L. A. Dennis, M. Fisher, N. Lincoln, A. Lisitsa, and S. M. Veres, “Practical Verification of Decision-Making in Agent-Based Autonomous Systems.” _Automated Software Engineering, _vol._ _23, no. 3, pp. 305–359, 2016.•M. Fisher, C. List, M. Slavkovik, and A. F. T. Winfield. “Engineering Moral Agents—From Human Morality to Artificial Morality” (Dagstuhl Seminar 16222). _Dagstuhl Reports _6, no. 5, pp. 114–137, 2016.•K. R. Fleischmann, _Information and Human Values_. San Rafael, CA: Morgan and Claypool, 2014.•G. Governatori and A. Rotolo. “How Do Agents Comply with Norms? ” in _Normative Multi-Agent Systems_, G. Boella, P. Noriega, G. Pigozzi, and H. Verhagen, eds., _Dagstuhl Seminar Proceedings_. Dagstuhl, Germany: Schloss Dagstuhl—Leibniz- Zentrum für Informatik, 2009.•B. Higgins, “New York City Task Force to Consider Algorithmic Harm.” _Artificial Intelligence Technology and the Law Blog_, Feb. 7, 2018. [Online]. Available: [http:// aitechnologylaw.com/2018/02/new-york-citytask-force-algorithmic-harm/](http://aitechnologylaw.com/2018/02/new-york-city-task-force-algorithmic-harm/). [Accessed Nov. 1, 2018].•S. L. Jarvenpaa, N. Tractinsky, and L. Saarinen. “[Consumer Trust in an Internet Store: A CrossCultural Validation](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1083-6101.1999.tb00337.x)” _Journal of ComputerMediated Communication, _vol._ _5, no. 2, pp. 1–37, 1999.\"p.185-186, ieee, 2019","id":"recrk0tgfwe5j9xfi","dom_id":"item_recrk0tgfwe5j9xfi"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["reclzrMrQSQls3PI9"],"title":"Smart toys collect data about children with little regulation","category":"Challenges","name":"recroNon39TCBeC88","tags":["education"],"created_at":"2023-06-05T09:46:29.000Z","description":"**Issue: **Intelligent toys## BackgroundChildren will not only be exposed to A/IS at school but also at home, while they play and while they sleep. Toys are already being sold that offer interactive, intelligent opportunities for play. Many of them collect video and audio data which is stored on company servers and either is or could be mined for profiling or marketing data.There is currently little regulatory oversight. In the United States COPPA7 offers some protection for the data of children under 13. Germany has outlawed such toys using legislation banning spying equipment enacted in 1981. Corporate A/IS are being embodied in toys and given to children to play with, to talk to, tell stories to, and to explore all the personal development issues that we learn about in private play as children.## RecommendationsChild data should be held in “escrow” and not used for any commercial purposes until a child reaches the age of majority and is able to authorize use as they choose.Governments and organizations need to educate and inform parents of the mechanisms of A/IS and data collection in toys and the possible impact on children in the future.** **## Further Resources•K. Firth-Butterfield, “What happens when your child’s friend is an AI toy that talks back?” in World Economic Forum: Generation AI, [https://www.weforum.org/agenda/2018/05/ generation-ai-what-happens-when-your-childs-invisible-friend-is-an-ai-toy-that-talks-back/,](https://www.weforum.org/agenda/2018/05/generation-ai-what-happens-when-your-childs-invisible-friend-is-an-ai-toy-that-talks-back/) May 22, 2018.•D. Basulto,“How artificial intelligence is moving from the lab to your kid’s playroom,” Washington Post, Oct. 15, 2015. [Online]. Available: [https://www.washingtonpost. com/news/innovations/wp/2015/10/15/ how-artificial-intelligence-is-moving-fromthe-lab-to-your-kids-playroom/?utm\\_ term=.89a1431a05a7 ](https://www.washingtonpost.com/news/innovations/wp/2015/10/15/how-artificial-intelligence-is-moving-from-the-lab-to-your-kids-playroom/?utm_term=.89a1431a05a7)**[Accessed Dec. 1, 2018].****•S. Chaudron, R. Di Gioia, M. Gemo, D. Holloway, J. Marsh, G. Mascheroni J. Peter, and D. Yamada-Rice , **[http://publications.jrc. ec.europa.eu/repository/handle/JRC105061,](http://publications.jrc.ec.europa.eu/repository/handle/JRC105061) 2016.•S. Chaudron, R. Di Gioia, M. Gemo, D. Holloway, J. Marsh, G. Mascheroni J. Peter, and D. Yamada-Rice , [http://publications.jrc. ec.europa.eu/repository/handle/JRC105061,](http://publications.jrc.ec.europa.eu/repository/handle/JRC105061) 2016.  • S. Chaudron, R. Di Gioia, M. Gemo, D. Holloway, J. Marsh, G. Mascheroni, J. Peter, D. Yamada-Rice [Kaleidoscope on the Internet of Toys - Safety, security, privacy and societal insights](http://publications.jrc.ec.europa.eu/repository/bitstream/JRC105061/jrc105061_final_online.pdf), EUR 28397 EN, doi:10.2788/05383, Luxembourg: Publications Office of the European Union, 2017.Z. Kleinman, “Alexa, are you friends with our kids?” _BBC News,_ July 16, 2018. [Online]. Available: [https://www.bbc.com/news/ technology-44847184.%5b](https://www.bbc.com/news/technology-44847184.%5b). [Accessed Dec. 1, 2018].J. Wakefield, “Germany bans children’s smartwatches.” _BBC News,_ Nov. 17 2017. [Online]. Available: [https://www.bbc.co.uk/ news/technology-42030109](https://www.bbc.co.uk/news/technology-42030109). [Accessed Dec. 2018].p.116-117","id":"recronon39tcbec88","dom_id":"item_recronon39tcbec88"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recfcXzM3foqFNNGN"],"title":"How do we embed values throughout AI workflows?","category":"Challenges","name":"rect310qem9li3HKK","tags":["corporate-ethics"],"created_at":"2023-06-05T10:42:48.000Z","description":"**Issue:** Values-based design## BackgroundEthics are often treated as an impediment to innovation, even among those who ostensibly support ethical design practices. In industries that reward rapid innovation in particular, it is necessary to develop ethical design practices that integrate effectively with existing engineering workflows. Those who advocate for ethical design within a company should be seen as innovators seeking the best outcomes for the company, end users, and society. Leaders can facilitate that mindset by promoting an organizational structure that supports the integration of dialogue about ethics throughout product life cycles.A/IS design processes often present moments where ethical consequences can be highlighted. There are no universally prescribed models for this because organizations vary significantly in structure and culture. In some organizations, design team meetings may be brief and informal. In others, the meetings may be lengthy and structured. The transition points between discovery, prototyping, release, and revisions are natural contexts for conducting such reviews. Iterative review processes are also advisable, in part because changes to risk profiles over time Companies should study design processes to identify situations where engineers and researchers can be encouraged to raise and resolve questions of ethics and foster a proactive environment to realize ethically aligned design. Achieving a distributed responsibility for ethics requires that all people involved in product design are encouraged to notice and respond to ethical concerns. Organizations should consider how they can best encourage and facilitate deliberations among peers. ## RecommendationsOrganizations should identify points for formal review during product development. These reviews can focus on “red flags” that have been identified in advance as indicators of risk. For example, if the datasets involve minors or focus on users from protected classes, then it may require additional justification or alterations to the research or development protocols.can illustrate needs or opportunities for improving the final product.## Further Resources•A. Sinclair, “[Approaches to Organizational Culture and Ethics,](https://doi.org/10.1007/BF01845788)” _Journal of Business Ethics, _vol._ _12, no. 1, pp. 63–73, 1993.•Al Y. S. Chen, R. B. Sawyers, and P. F. Williams. “[Reinforcing Ethical Decision Making Through Corporate Culture,](https://link.springer.com/article/10.1023/A:1017953517947)_” Journal of Business Ethics _16, no. 8, pp. 855–865, 1997. ****• K. Crawford and R. Calo, “[There Is a Blind Spot in AI Research,](http://www.nature.com/news/there-is-a-blind-spot-in-ai-research-1.20805)” _Nature _538, pp. 311–313, 2016.  ","id":"rect310qem9li3hkk","dom_id":"item_rect310qem9li3hkk"},{"Principles":["rec42P8U9usfYCtv9"],"title":"Sustainability","category":"Challenges","name":"recv6cN7XSt5GW32Y","tags":[],"created_at":"2023-05-18T18:55:13.000Z","id":"recv6cn7xst5gw32y","dom_id":"item_recv6cn7xst5gw32y"},{"Cases":["reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5"],"Principles":["recOHnq45Fq7YWsRO"],"Reference":"Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector. The Alan Turing Institute. https://doi.org/10.5281/ZENODO.3240529","Sources":["recfYC5jjPmpLfSlM"],"title":"Concerns with merit of new systems","category":"Challenges","name":"recvQ90DajNCwPiGP","tags":[],"created_at":"2023-05-19T09:32:44.000Z","description":"“Irresponsible data management, negligent design and production processes, and questionable deployment practices can, each in their own ways, lead to the implementation and distribution of AI systems that produce unreliable, unsafe, or poor-quality outcomes. These outcomes can do direct damage to the wellbeing of individual persons and the public welfare. They can also undermine public trust in the responsible use of societally beneficial AI technologies, and they can create harmful inefficiencies by virtue of the dedication of limited public resources to inefficient or even detrimental AI technologies.” (Leslie, 2019, p. 5)","id":"recvq90dajncwpigp","dom_id":"item_recvq90dajncwpigp"},{"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["recwQrzqPp6C7jyJs","recGS9OVQ7qAjvIYE","rec0WScLo6sUnoOQN"],"title":"How do we foster AI that contributes to sustainability in context of short-term growth priorities?","category":"Challenges","name":"recvWM2glArsVhaye","tags":["wellbeing"],"created_at":"2023-06-05T05:43:20.000Z","description":"**\"Issue: **A/IS creators have opportunities to safeguard human well-being by ensuring that A/IS does no harm to earth’s natural systems or that A/IS contributes to realizing sustainable stewardship, preservation, and/or restoration of earth’s natural systems. A/IS creators have opportunities to prevent A/IS from contributing to the degradation of earth’s natural systems and hence losses to human well-being.## BackgroundIt is unwise, and in truth impossible, to separate the well-being of the natural environment of the planet from the well-being of humanity. A range of studies, from the [historic](https://www.clubofrome.org/report/the-limits-to-growth/) to more [recent,](https://www.nationalgeographic.com/environment/2018/10/ipcc-report-climate-change-impacts-forests-emissions/) prove that ecological collapse endangers human existence. Hence, the concept of well-being should encompass planetary wellbeing. Moreover, biodiversity and ecological integrity have intrinsic merit beyond simply their instrumental value to humans.Technology has a long history of contributing to ecological degradation through its role in expanding the scale of resource extraction and environmental pollution, for example, the immense power needs of network computing, which leads to [climate change,](http://www.ipcc.ch/) [water scarcity](http://www.unwater.org/), [soil degradation](https://www.worldwildlife.org/threats/soil-erosion-and-degradation), [species](http://www.iucnredlist.org/) [extinction](http://www.iucnredlist.org/), [deforestation](http://www.wri.org/our-work/topics/forests), [biodiversity loss,](https://www.theguardian.com/news/2018/mar/12/what-is-biodiversity-and-why-does-it-matter-to-us) and destruction of ecosystems which in turn threatens humankind in the long run. These and other costs are often considered externalities and often do not figure into decisions or plans. At the same time, there are many examples, such as photovoltaics and smart grid technology that present potential ways to restore earth’s ecosystems if undertaken within a systems approach aimed at sustainable economic and environmental development.Environmental justice [research ](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5129282/)demonstrates that the negative environmental impacts of technology are commonly concentrated on the middle class and working poor, as well as those suffering from abject poverty, fleeing disaster zones, or otherwise lacking the resources to meet their needs. Ecological impact can thus exacerbate the economic and sociological effects of wealth disparities on human well-being by concentrating environmental injustice onto those who are less well off. Moreover, [well-being research findings](https://www.equalitytrust.org.uk/resources/the-spirit-level) indicate that unfair economic and social inequality has a dampening effect on everyone's well-being, regardless of economic or social class.In these respects, A/IS are no exception; they can be used in ways that either help or harm the ecological integrity of the planet. It may be fair to say that ecological health and human well-being will, increasingly, depend upon A/IS creators. It is imperative that A/IS creators and stakeholders find ways to use A/IS to do no harm and to reduce the environmental degradation associated with economic growth–while simultaneously identifying applications to restore the ecological health of the planet and thereby safeguarding the well-being of humans. For A/IS to reduce environmental degradation and promote wellbeing, it is required that not only A/IS creators act along such lines, but also that a systems approach is taken by all A/IS stakeholders to find solutions that safeguard human well-being with the understanding that human well-being is inextricable from healthy social, economic, and environmental systems.## RecommendationsA/IS creators need to recognize and prioritize the stewardship of the Earth’s natural systems to promote human and ecological well-being. Specifically:_ _•Human well-being should be defined to encompass ecological health, access to nature, safe climate and natural environments, biosystem diversity, and other aspects of a healthy, sustainable natural environment.•A/IS systems should be designed to use, support, and strengthen existing ecological sustainability standards with a certification or similar system, e.g., [LEED,](https://new.usgbc.org/leed) [Energy Star,](https://www.energystar.gov/) or [Forest Stewardship Council.](https://us.fsc.org/en-us) This directs automation and machine intelligence to follow the principle of doing no harm and to safeguard environmental, social, and economic systems.•A/IS creators should prioritize doing no harm to the Earth’s natural systems, both intended and unintended harm.•A committee should be convened to issue findings on ways in which A/IS can be used by business, NGOs, and governmental agencies to promote stewardship and restoration of natural systems while reducing the harmful impact of economic development on ecological sustainability and environmental justice.•D. Austin and M. Macauley. \"[Cutting Through Environmental Issues: Technology as a double-edged sword.](https://www.brookings.edu/articles/cutting-through-environmental-issues-technology-as-a-double-edged-sword/)” The Brookings Institution, Dec. 2001 [Online]. Available: [https://www.brookings.edu/articles/cuttingthrough-environmental-issues-technology-asa-double-edged-sword/.](https://www.brookings.edu/articles/cutting-through-environmental-issues-technology-as-a-double-edged-sword/) [Accessed Dec. 1, 2018].•J. Newton, _[Well-being and the Natural Environment: An Overview of the Evidence](http://resolve.sustainablelifestyles.ac.uk/sites/default/files/JulieNewtonPaper.pdf)_[. ](http://resolve.sustainablelifestyles.ac.uk/sites/default/files/JulieNewtonPaper.pdf)August 20, 2007.•P. Dasgupta, [Human Well-Being and the Natural Environment.](https://books.google.com/books?id=OuMTDAAAQBAJ\u0026amp;dq=wellbeing%2Band%2Bthe%2Bnatural%2Benvironment\u0026amp;lr\u0026amp;source=gbs_navlinks_s) Oxford, U.K.: Oxford University Press, 2001.•R. Haines-Young and M. Potschin. “[The Links ](https://www.pik-potsdam.de/news/public-events/archiv/alter-net/former-ss/2009/10.09.2009/10.9.-haines-young/literature/haines-young-potschin_2009_bes_2.pdf)[Between Biodiversity, Ecosystem Services and Human Well-Being,](https://www.pik-potsdam.de/news/public-events/archiv/alter-net/former-ss/2009/10.09.2009/10.9.-haines-young/literature/haines-young-potschin_2009_bes_2.pdf)” in _Ecosystem Ecology: A New Synthesis_, D. Raffaelli, and C. Frid, Eds. Cambridge, U.K.: Cambridge University Press, 2010.•S. Hart, _[Capitalism at the Crossroads: Next Generation Business Strategies for a PostCrisis World.](https://www.pearson.com/us/higher-education/program/Hart-Capitalism-at-the-Crossroads-Next-Generation-Business-Strategies-for-a-Post-Crisis-World-3rd-Edition/PGM9671.html)_ Upper Saddle River, NJ: Pearson Education, 2010.•United Nations Department of Economic and Social Affairs. “[Call for New Technologies to Avoid Ecological Destruction.](http://www.un.org/en/development/desa/news/policy/wess-2011.html)” Geneva, Switzerland, July 5, 2011.•Pope Francis. [Encyclical Letter Laudato Si’ of the Holy Father Francis On the Care for Our Common Home.](http://w2.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html) May 24, 2015.•“[Environment,](https://www.dalailama.com/messages/environment)” The 14th Dalai Lama. Accessed Dec. 9, 2018. [https://www.dalailama.com/ messages/environment.](https://www.dalailama.com/messages/environment)- Why Islam.org, Environment and Islam, 2018._ \"__p.74-75_","id":"recvwm2glarsvhaye","dom_id":"item_recvwm2glarsvhaye"},{"Cases":["reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5"],"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2","Sources":["recpXl48pJdKDhc6f"],"Strategies":["reccMqFCLOgQwWM5Q"],"title":"How can AI work respect diversity in cross-cultural norms, aims, and practices that vary by location, change over time, and come into conflict?","category":"Challenges","name":"recxjc79LvLdKa4rl","tags":["research"],"created_at":"2023-06-05T10:20:28.000Z","description":"A/IS culture and context## Norms vary: BackgroundA responsible approach to embedding values into A/IS requires that algorithms and systems are created in a way that is sensitive to the variation of ethical practices and beliefs across cultures. The designers of A/IS need to be mindful of cross-cultural ethical variations while also respecting widely held international legal norms.## RecommendationEstablish a leading role for [intercultural information ethics ](http://www.capurro.de/iie.html)(IIE) practitioners in ethics committees informing technologists, policy makers, and engineers. Clearly demonstrate through examples how cultural variation informs not only information flows and information systems, but also algorithmic decision-making and value by design.## Further Resources•D. J. Pauleen, et al. “[Cultural Bias in Information Systems Research and Practice: Are You Coming From the Same Place I Am? ](http://aisel.aisnet.org/cais/vol17/iss1/17/)” _Communications of the Association for Information Systems, _vol._ _17, no. 17, 2006.•J. Bielby, “[Comparative Philosophies in Intercultural Information Ethics](https://scholarworks.iu.edu/iupjournals/index.php/confluence/article/view/540),” _Confluence: Online Journal of World Philosophies _2, no. 1, pp. 233–253, 2016.p.124\"**Issue 1:** Which norms shouldbe identified?## BackgroundIf machines engage in human communities, then those agents will be expected to follow the community’s social and moral norms. A necessary step in enabling machines to do so is to identify these norms. But which norms should be identified? Laws are publicly documented and therefore easy to identify, so they can be incorporated into A/IS as long as they do not violate humanitarian or community moral principles. Social and moral norms are more difficult to ascertain, as they are expressed through behavior, language, customs, cultural symbols, and artifacts. Most important, communities ranging from families to whole nations differ to various degrees in the norms they follow. Therefore, generating a universal set of norms that applies to all A/IS in all contexts is not realistic, but neither is it advisable to completely tailor the A/IS to individual preferences. We suggest that it is feasible to identify broadly observed norms of communities in which a technology is deployed.Furthermore, the difficulty of generating a universal set of norms is not inconsistent with the goal of seeking agreement over Universal Human Rights (see the “General Principles” chapter of _Ethically Aligned Design_). However, these universal rights are not sufficient for devising A/IS that conform to the specific norms of its community. Universal Human Rights must, however, constrain the kinds of norms that are implemented in the A/IS (cf. van de Poel 20168).Embedding norms in A/IS requires a careful understanding of the communities in which the A/IS are to be deployed. Further, even within a particular community, different types of A/IS will demand different sets of norms. The relevant norms for self-driving vehicles, for example,may differ greatly from those for robots usedin healthcare. Thus, we recommend that to develop A/IS capable of following legal, social, and moral norms, the first step is to identify the norms of the specific community in which theA/IS are to be deployed and, in particular, norms relevant to the kinds of tasks and roles for which the A/IS are designed. Even when designating a narrowly defined community, e.g., a nursing home, an apartment complex, or a company, there will be variations in the norms that apply, or in their relative weighting. The norm identification process must heed such variation and ensure that the identified norms are representative, not only of the dominant subgroup in the community but also of vulnerable and underrepresented groups.The most narrowly defined “community” is a single person, and A/IS may well have to adapt to the unique expectations and needs of a given individual, such as the arrangement of a disabled person’s living accommodations. However, unique individual expectations must not violate norms in the larger community. Whereas the arrangement of someone’s kitchen or the frequency with which a care robot checks in with a patient can be personalized without violating any community norms, encouraging the robot to use derogatory language to talk about certain social groups does violate such norms. In the next section, we discuss how A/IS might handle such norm conflicts.Innovation projects and development efforts for A/IS should always rely on empirical research, involving multiple disciplines and multiple methods; to investigate and document both context- and task-specific norms, spoken and unspoken, that typically apply in a particular community. Such a set of empirically identified norms should then guide system design. This process of norm identification and implementation must be iterative and revisable. A/IS with an initial set of implemented norms may betray biases of original assessments (Misra, Zitnick, Mitchell, and Girshick 20169) that can be revealed by interactions with, and feedback from, the relevant community. This leads to a process of norm updating, which is described next in Issue 2.## RecommendationTo develop A/IS capable of following social and moral norms, the first step is to identify the norms of the specific community in which theA/IS are to be deployed and, in particular, norms relevant to the kinds of tasks and roles that the A/IS are designed for. This norm identification process must use appropriate scientific methods and continue through the system's life cycle.## Further Resources•Mack, Ed., “Changing social norms.” _Social Research: An International Quarterly,_ 85, no.1, 1–271, 2018.•I. Misra, C. L. Zitnick, M. Mitchell, and R. Girshick, (2016). Seeing through the human reporting bias: Visual Classifiers from Noisy Human-Centric Labels. In _Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition_ (CVPR), pp. 2930–2939. doi[:10.1109/CVPR.2016.320](https://doi.org/10.1109/CVPR.2016.320)•I. van de Poel, “[An Ethical Framework for Evaluating Experimental Technology,](https://link.springer.com/article/10.1007/s11948-015-9724-3)” _Science and Engineering Ethics_, 22, no. 3,pp. 667686, 2016.\"p.168-169## \"Norms change: BackgroundNorms are not static. They change over time, in response to social progress, political change, new legal measures, or novel opportunities (Mack 201810). Norms can fade away when, for whatever reasons, fewer and fewer people adhere to them. And new norms emerge when technological innovation invites novel behaviors and novel standards, e.g., cell phone use in public.A/IS should be equipped with a starting set of social and legal norms before they are deployed in their intended community (see Issue 1), but this will not suffice for A/IS to behave appropriately over time. A/IS or the designers of A/IS, must be adept at identifying and adding new norms to its starting set, because the initial norm identification process in the community will undoubtedly have missed some norms and because the community’s norms change.Humans rely on numerous capacities to update their knowledge of norms and learn new ones. They observe other community members’ behavior and are sensitive to collective norm change; they explicitly ask about new norms when joining new communities, e.g., entering college or a job in a new town; and they respond to feedback from others when they exhibit uncertainty about norms or have violated a norm.Likewise, A/IS need multiple capacities to improve their own norm knowledge and to adapt to a community’s dynamically changing norms. These capacities include:•Processing behavioral trends by members of the target community and comparing them to trends predicted by the baseline norm system,•Asking for guidance from the community when uncertainty about applicable norms exceeds a critical threshold,•Responding to instruction from the community members who introduce a robot to a previously unknown context or who notice the A/IS’ uncertainty in a familiar context, and•Responding to formal or informal feedback from the community when the A/IS violatea norm.The modification of a normative system can occur at any level of the system: it could involve altering the priority weightings between individual norms, changing the qualitative expression of a norm, or altering the quantitative parameters that enable the norm.We recommend that the system’s norm changes be transparent. That is, the system or its designer should consult with users, designers, and community representatives when adding new norms to its norm system or adjusting the priority or content of existing norms. Allowing a system to learn new norms without public or expert review has detrimental consequences (Green and Hu 201811). The form of consultation and the specific review process will vary by machine sophistication-e.g., linguistic capacity and function/role, or a flexible social companion versus a task-defined medical robot-and best practices will have to be established. In some cases, the system may document its dynamic change, and the user can consult this documentation as desired. In other cases, explicit announcements and requests for discussion with the designer may be appropriate. In yet other cases, the A/IS may propose changes, and the relevant human community, e.g., drawn from a representative crowdsourced panel, will decide whether such changes should be implementedin the system.## RecommendationTo respond to the dynamic change of norms in society A/IS or their designers must be able to amend their norms or add new ones, while being transparent about these changes to users,designers, broader community representatives, and other stakeholders.## Further Resources•B. Green and L. Hu. “The Myth in the Methodology: Towards a Recontextualization of Fairness in ML.” Paper presented at the Debates workshop at the 35th International Conference on Machine Learning, Stockholm, Sweden 2018.• Mack, Ed., “Changing social norms,”_ Social Research: An International Quarterly_, 85(1, Special Issue), 1-271, 2018.**Issue 3: **A/IS will face norm conflicts and need methods to resolve them.## Norms Conflict: BackgroundOften, even within a well-specified context, no action is available that fulfills all obligations and prohibitions. Such situations—often described as moral dilemmas or moral overload (Van den Hoven 201212)—must be computationally tractable by A/IS; they cannot simply stop in their tracks and end on a logical contradiction. Humans resolve such situations by accepting trade-offs between conflicting norms, which constitute priorities of one norm or value over another in a given context. Such priorities may be represented in the norm system as hierarchical relations.Along with identifying the norms within a specific community and task domain, empirical research must identify the ways in which people prioritize competing norms and resolve norm conflicts, and the ways in which people expect A/IS to resolve similar norm conflicts. These more local conflict resolutions will be further constrained by some general principles, such as the “Common Good Principle” (Andre and Velasquez 199213) or local and national laws. For example, a self-driving vehicle’s prioritization of one factor over another in its decision-making will need to reflect the laws and norms of the population in which the A/IS are deployed, e.g., the traffic laws of a U.S. state and the United States as a whole.Some priority orders can be built into a given norm network as hierarchical relations, e.g.,more general prohibitions against harm to humans typically override more specific norms against lying. Other priority orders can stem from the override that norms in the larger communityexert on norms and preferences of an individual user. In the earlier example discussing personalization (see Issue 1), the A/IS of a racist user who demands the A/IS use derogatory language for certain social groups will have to resist such demands because community norms hierarchically override an individual user’s preferences. In many cases, priority orders are not built in as fixed hierarchies because the priorities are themselves context-specific or may arise from net moral costs and benefits of the particular case at hand. A/IS must have learning capacities to track such variations and incorporate user and community input, e.g., about the subtle differences between contexts, so as to refine the system’s norm network (see Issue 2).Tension may sometimes arise between a community’s social and legal norms and the normative considerations of designers or manufacturers. Democratic processes may need to be developed that resolve this tension— processes that cannot be presented in detail in this chapter. Often such resolution will favor the local laws and norms, but in some cases the community may have to be persuaded to accept A/IS favoring international law or broader humanitarian principles over, say, racist or sexist local practices. In general, we recommend that the system’s resolution of norm conflicts be transparent—that is, documented by the system and ready to be made available to users, the relevant community of deployment, and third-party evaluators. Just like people explain to each other why they made decisions, they will expect any A/IS to be able to explain their decisions and be sensitive to user feedback about the appropriateness of the decisions. To do so, design and development of A/IS should specifically identify the relevant groups of humans who may request explanations and evaluate the systems’ behaviors. In the case of a system detecting a norm conflict, the system should consult and offer explanations to representatives from the community, e.g., randomly sampled crowdsourced members or elected officials, as well as to third-party evaluators, with the goal of discussing and resolving the norm conflict.## RecommendationA/IS developers should identify the ways in which people resolve norm conflicts and the ways in which they expect A/IS to resolve similar norm conflicts. A system’s resolution of norm conflicts must be transparent—that is, documented by the system and ready to be made available to users, the relevant community of deployment, and third-party evaluators.## Further resources•M. Velasquez, C. Andre, T. Shanks, S.J., and M. J. Meyer, “[The Common](https://legacy.scu.edu/ethics/publications/iie/v5n1/common.html) [Good](https://legacy.scu.edu/ethics/publications/iie/v5n1/common.html).” _Issues in Ethics_, vol._ _5, no. 1, 1992.•J. Van den Hoven, “Engineering and the Problem of Moral Overload.” _Science and Engineering Ethics, vol. _18, no. 1, pp.143–155, 2012.•D. Abel, J. MacGlashan, and M. L. Littman. “Reinforcement Learning as a Framework for Ethical Decision Making.” _AAAI Workshop AI, Ethics, and Society, Volume WS-16-02 of 13th AAAI Workshops_. Palo Alto, CA: AAAIPress, 2016.•O. Bendel, Die Moral in der Maschine: Beiträge zu Roboter- und Maschinenethik. Hannover, Germany: Heise Medien, 2016. Accessible popular-science contributions to philosophical issues and technical implementations of machine ethics•S. V. Burks, and E. L. Krupka. [“A Multimethod Approach to Identifying Norms and Normative Expectations within a Corporate Hierarchy: ](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1110.1478)[Evidence from the Financial Services Industry.”](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1110.1478) _Management Science, _vol. 58, pp. 203–217, 2012.Illustrates surveys and incentivized coordination games as methods to elicit norms in a large financial services firm•F. Cushman, V. Kumar, and P. Railton, “Moral Learning,” _Cognition_, vol._ _167, pp. 1–282, 2017.•M. Flanagan, D. C. Howe, and H. Nissenbaum, “Embodying Values in Technology: Theory and Practice.” _Information Technology and Moral __Philosophy_, J. van den Hoven and J. Weckert, Eds., Cambridge University Press, 2008, pp. 322–53. Cambridge Core, _Cambridge University Press._ Preprint available at[http://www.nyu.edu/projects/nissenbaum/ papers/Nissenbaum-VID.4-25.pdf](http://www.nyu.edu/projects/nissenbaum/papers/Nissenbaum-VID.4-25.pdf)•B. Friedman, P. H. Kahn, A. Borning, and A. Huldtgren. “Value Sensitive Design and Information Systems,” in _Early Engagement and New Technologies: Opening up the Laboratory, _N. Doorn, Schuurbiers, I. van de Poel, and M. Gorman, Eds., vol. 16, pp. 55–95. Dordrecht: Springer, 2013.A comprehensive introduction into Value Sensitive Design and three sample applications•G. Mackie, F. Moneti, E. Denny, and H. Shakya. “What Are Social Norms? How Are They Measured?” UNICEF Working Pape[r. ](http://dmeforpeace.org/sites/default/files/4%2009%2030%20Whole%20What%20are%20Social%20Norms.pdf)University of California at San Diego: UNICEF, Sept. 2014. [https://dmeforpeace.org/sites/ default/files/4%2009%2030%20Whole%20 What%20are%20Social%20Norms.pdf](https://dmeforpeace.org/sites/default/files/4%2009%2030%20Whole%20What%20are%20Social%20Norms.pdf)A broad survey of conceptual and measurement questions regarding social norms.•J. A. Leydens and J. C. Lucena. Engineering Justice: Transforming Engineering Education and Practice. Hoboken, NJ: John Wiley \u0026 Sons, 2018.Identifies principles of engineering for social justice.•B. F. Malle, “Integrating Robot Ethics and Machine Morality: The Study and Design of Moral Competence in Robots.” _Ethics and Information Technology, _vol._ _18, no. 4, pp. 243–256, 2016.Discusses how a robot’s norm capacity fits in the larger vision of a robot with moral competence.•K. W. Miller, M. J. Wolf, and F. Grodzinsky, “This ‘Ethical Trap’ Is for Roboticists, Not Robots: On the Issue of Artificial Agent Ethical DecisionMaking.” _Science and Engineering Ethics, _vol._ _23, pp. 389–401, 2017.This article raises doubts about the possibility of imbuing artificial agents with morality, or of claiming to have done so.•Open Roboethics Initiative: [www.openroboethics.org](http://www.openroboethics.org/). A series of poll results on differences in human moral decision-making and changes in priority order of values for autonomous systems (e.g., [on care](http://www.openroboethics.org/results-should-a-carebot-bring-an-alcoholic-a-drink-poll-says-it-depends-on-who-owns-the-robot/) [robots)](http://www.openroboethics.org/results-should-a-carebot-bring-an-alcoholic-a-drink-poll-says-it-depends-on-who-owns-the-robot/), 2019.•A. Rizzo and L. L. Swisher, “Comparing the Stewart–Sprinthall Management Survey and the Defining Issues Test-2 as Measures of Moral Reasoning in Public Administration.” _Journal of Public Administration Researchand Theory, _vol._ _14, pp. 335–348, 2004. Describes two assessment instruments of moral reasoning (including norm maintenance) based on Kohlberg’s theoryof moral development.• S. H. Schwartz, “A[n Overview of the Schwartz](https://scholarworks.gvsu.edu/orpc/vol2/iss1/11/)[Theory of Basic Values.” _Online Readings in Psychology and Culture _2, 2012](https://scholarworks.gvsu.edu/orpc/vol2/iss1/11/). •Comprehensive overview of a specific theory of values, understood as motivational orientations toward abstract outcomes (e.g., self-direction, power, security).•S. H. Schwartz and K. Boehnke. “[Evaluating the Structure of Human Values with Confirmatory Factor Analysis.” _Journal of Research in Personality, _vol. 38, ](http://www.sciencedirect.com/science/article/pii/S0092656603000692?via%3Dihub)pp. 230–255, 2004.•Describes an older method of subjective judgments of relations among valued outcomes and a newer, formal method of analyzing these relations.•W. Wallach and C. Allen. _Moral Machines: Teaching Robots Right from Wrong_. New York: Oxford University Press, 2008. This book describes some of the challenges of having a one-size-fits-all approach to embedding human values in autonomous systems. \"p.172-174\"**Issue 1:** Not all norms of a target community apply equally to human and artificial agents## BackgroundAn intuitive criterion for evaluations of norms embedded in A/IS would be that the A/IS norms should mirror the community’s norms—that is, the A/IS should be disposed to behave the same way that people expect each other to behave. However, for a given community and a givenA/IS use context, A/IS and humans are unlikely to have identical sets of norms. People will have some unique expectations for humans than they do not for machines, e.g., norms governing the regulation of negative emotions, assuming that machines do not have such emotions. People may in some cases have unique expectations of A/IS that they do not have for humans, e.g., a robot worker, but not a human worker, is expected to work without regular breaks.## RecommendationThe norm identification process must document the similarities and differences between the norms that humans apply to other humans and the norms they apply to A/IS. Norm implementations should be evaluatedspecifically against the norms that thecommunity expects the A/IS to follow.\"p.183","id":"recxjc79lvldka4rl","dom_id":"item_recxjc79lvldka4rl"},{"Principles":["recKdujFoPJr4ZAhZ","recMGB4iC5oaCtr5x"],"Reference":"Fedoruk, L. (2017b). Ethics in The Scholarship of Teaching and Learning. University of Calgary Taylor Institute for Teaching and Learning. https://taylorinstitute.ucalgary.ca/resources/ethics-scholarship-teaching-and-learning","Sources":["recQzldmBLByP78Uu"],"title":"SoTL secondary use of student data","category":"Challenges","name":"reczg5MObRbgzTeob","tags":[],"created_at":"2023-05-19T13:10:14.000Z","description":"““Reasons to conduct secondary analyses of data include: avoidance of duplication in primary collection and the associated reduction of burdens on participants; corroboration or criticism of the conclusions of the original project; comparison of change in a research sample over time; application of new tests of hypotheses that were not available at the time of original data collection; and confirmation that the data are authentic. Privacy concerns and questions about the need to seek consent arrive, however, when information provided for secondary use in research can be linked to individuals, and when the possibility exists that individuals can be identified in published reports, or through data linkage” (TCPS2, Chapter 5, D. Consent and Secondary Use of Identifiable Information for Research Purposes).” (Fedoruk, 2017, p. 13)““If a researcher satisfies all the conditions in Article 5.5A (a) to (f), the REB may approve the research without requiring consent from the individuals to whom the information relates. a. identifiable information is essential to the research; b. the use of identifiable information without the participants’ consent is unlikely to adversely affect the welfare of individuals to whom the information relates; c. the researchers will take appropriate measures to protect the privacy of individuals, and to safeguard the identifiable information; d. the researchers will comply with any known preferences previously expressed by individuals about any use of their information; e. it is impossible or impracticable to seek consent from individuals to whom the information relates; and f. the researchers have obtained any other necessary permission for secondary use of information for research purposes” (TCPS2, Chapter 5, D. Consent and Secondary Use of Identifiable Information for Research Purposes).” (Fedoruk, 2017, p. 13)““Researchers shall seek REB review, but are not required to seek participant consent, for research that relies exclusively on the secondary use of non-identifiable information” (TCPS2, Chapter 5, D. Consent and Secondary Use of Identifiable Information for Research Purposes).” (Fedoruk, 2017, p. 14)","id":"reczg5mobrbgzteob","dom_id":"item_reczg5mobrbgzteob"},{"Link":"https://ltr.edu.au/resources/Ethicsbooklet01.pdf","Reference":"Thomson, C., Israel, M., \u0026 Allen, G. (2016a). _SoTL Human Research Ethics Resource Manual Booklet 01 Research ethics and the Scholarship of Teaching and Learning_. Office of Learning and Teaching (OLT), Australian Government. \u003chttps://ltr.edu.au/resources/Ethicsbooklet01.pdf\u003e\nThomson, C., Israel, M., \u0026 Allen, G. (2016b). _SoTL Human Research Ethics Resource Manual Booklet 02 Ethics review and grant or fellowship funded research_. Office of Learning and Teaching (OLT), Australian Government. \u003chttps://ltr.edu.au/resources/Ethicsbooklet02.pdf\u003e\nThomson, C., Israel, M., \u0026 Allen, G. (2016c). _SoTL Human Research Ethics Resource Manual Booklet 03 Risks and benefits in SoTL research_. Office of Learning and Teaching (OLT), Australian Government. \u003chttps://ltr.edu.au/resources/Ethicsbooklet03.pdf\u003e\nThomson, C., Israel, M., \u0026 Allen, G. (2016d). _SoTL Human Research Ethics Resource Manual Booklet 04 Recruitment and consent in SoTL research_. Office of Learning and Teaching (OLT), Australian Government. \u003chttps://ltr.edu.au/resources/Ethicsbooklet04.pdf\u003e\nThomson, C., Israel, M., \u0026 Allen, G. (2016e). _SoTL Human Research Ethics Resource Manual Booklet 05 Privacy and confidentiality in SoTL research_. Office of Learning and Teaching (OLT), Australian Government. \u003chttps://ltr.edu.au/resources/Ethicsbooklet05.pdf\u003e\nThomson, C., Israel, M., \u0026 Allen, G. (2016f). _SoTL Human Research Ethics Resource Manual Booklet 06 SoTL Research: Ethical challenges and practical strategies_. Office of Learning and Teaching (OLT), Australian Government. \u003chttps://ltr.edu.au/resources/Ethicsbooklet06.pdf\u003e\n","Rights":["unclear"],"title":"SoTL Human Research Ethics Resource","category":"Sources","name":"rec08iMfls4T3w4KS","tags":["education-research","ethics-guideline","research-ethics"],"created_at":"2023-06-05T14:20:20.000Z","description":"A set of guideline booklets oriented towards research involving Scholarship of Teaching and Learning ","id":"rec08imfls4t3w4ks","dom_id":"item_rec08imfls4t3w4ks"},{"Link":"https://the-sra.org.uk/SRA/SRA/Ethics/Research-Ethics-Guidance.aspx","Reference":"Social Research Association. (2021). Research Ethics Guidance. Social Research Association. \u003chttps://the-sra.org.uk/SRA/SRA/Ethics/Research-Ethics-Guidance.aspx\u003e\n","Rights":["unclear"],"title":"Social Research Association guidelines","category":"Sources","name":"rec26KH3hJZEDfhqG","tags":["social-research","research-ethics","ethics-guideline"],"created_at":"2023-06-05T14:16:45.000Z","description":"“We’ve written this guidance for you to draw on when you face an ethical issue or dilemma in social research. It is intended as a basis for reflection and discussion. Like previous Social Research Association (SRA) guidance on research ethics, it does not offer rigid rules, but illustrates ethical practices to which experienced and respected social researchers generally adhere. Our aim is to encourage you to reflect carefully at all stages of the research process. While the guidance aims to be comprehensive, it does not claim to provide an answer to every ethical dilemma you may face. While it’s important to identify and resolve ethical issues and concerns before research gets underway, it’s not always possible to anticipate these. The guidance is neither exhaustive nor definitive. Rather, we provide clarity about common ethical issues that researchers face, and highlight the importance of reflexivity – that is, checking that your behaviour accords with ethical standards. The guidance reflects ethical norms, policy and law at the time of writing. The SRA will review it as necessary.” (Social Research Association, 2021, p. 1)","id":"rec26kh3hjzedfhqg","dom_id":"item_rec26kh3hjzedfhqg"},{"Link":"https://www.project-sherpa.eu/guidelines/","Reference":"Brey, P., Lundgren, B., Macnish, K., \u0026 Ryan, M. (2020). _Guidelines for the Ethical Development of AI and Big Data Systems: An Ethics by Design approach_. SHERPA project. \u003chttps://doi.org/10.21253/DMU.12301322.v1\u003e\nBrey, P., Lundgren, B., Macnish, K., \u0026 Ryan, M. (2020). _Shaping the ethical dimensions of smart information systems– a European perspective (SHERPA) Guidelines for the Ethical Use of AI and Big Data Systems_. SHERPA project. \u003chttps://doi.org/10.21253/DMU.12301331.v1\u003e\nBrey, P., Lundgren, B., Macnish, K., Ryan, M., Andreou, A., Brooks, L., Tilimbe Jiya, Klar, R., Lanzareth, D., Maas, J., Oluoch, I., \u0026 Stahl, B. (2021). _D3.2 Guidelines for the development and the use of SIS_. SHERPA project. \u003chttps://doi.org/10.21253/DMU.11316833\u003e\nRyan, M., Brey, P., Macnish, K., Tally Hatzakis, King, O., Maas, J., Haasjes, R., Fernandez, A., Martorana, S., Oluoch, I., Eren, S., \u0026 Puil, R. V. D. (2021). _D1.4 Report on Ethical Tensions and Social Impacts_. SHERPA project. \u003chttps://doi.org/10.21253/DMU.8397134\u003e\n\n","Rights":["CC-By"],"title":"Project SHERPA guidelines","category":"Sources","name":"rec488aokfNHPYUqN","tags":["ethics-guideline","not-research-ethics","AI"],"created_at":"2023-06-05T14:03:42.000Z","description":"The SHERPA project provides a set of guidelines for both AI users and developers, oriented towards operationalising ethical processes. They contain a useful expression of underpinning values, and provide e.g. a project-cycle model to outline process. “This report contains ethical guidelines for the technological development of artificial intelligence (AI) and big data systems. It is a Deliverable of the SHERPA project, an EU Horizon 2020 project on the ethical and human rights implications of AI and big data. The guidelines differ from others in that they are directly related to design and development practices. They are intended to be actionable guidelines for systems and software development, rather than abstract principles that have no direct application in practice. We call such guidelines operational, meaning ready for use. Applying these guidelines in development practices would result in more ethical AI and big data products. In constructing Guidelines for the Ethical Development of AI and Big Data Systems: An Ethics by Design approach, we have incorporated input from a wide diversity of stakeholders, SHERPA partners, and insights from other guidelines. In a survey of potential guidelines we found over 70 matching documents, which were reduced to 25 suitable guidelines that we built on. After an introductory section, we devote Section 2 of this report (“High-Level Requirements”) to present and discuss the high-level requirements that form the point of departure for this report. Our requirements are directly based on the guidelines of the EU’s High-Level Expert Group on Artificial Intelligence (HLEG AI), with minor adaptations to improve coherence and fitness for operationalization. This results in the following seven requirements that mirror those of the HLEG AI: human agency, liberty, and dignity; technical robustness and safety; privacy and data governance; transparency; diversity, non-discrimination, and fairness; individual, societal, and environmental wellbeing; and accountability. For each, we specify three to four sub-requirements that constitute a first step towards operationalization.” (Brey et al., 2020, p. 2)“The guidelines that we present in this report are operational in the sense that they are, in our view, ready to be used by ethics officers or managers, who have a responsibility for ensuring the implementation of ethical practices within their organizations. They are perhaps not directly usable by system developers. A further step that is required, but not contained in this report, is the training of developers in this new framework, and the assignment of different roles and responsibilities to them for ensuring that the ethical requirements are met. This may also require the development of training materials and operational guides for professionals with different roles in the development process. We intend to produce further implementation documents in the EU Horizon 2020 SIENNA project (www.sienna-project.eu).” (Brey et al., 2020, p. 3)","id":"rec488aokfnhpyuqn","dom_id":"item_rec488aokfnhpyuqn"},{"Link":"https://ec.europa.eu/research/participants/data/ref/h2020/other/hi/ethics-guide-ethnog-anthrop_en.pdf","Reference":"Iphofen, R. (2012). _Research Ethics in Ethnography/Anthropology_. European Commission. \u003chttps://ec.europa.eu/research/participants/data/ref/h2020/other/hi/ethics-guide-ethnog-anthrop_en.pdf\u003e\n","Rights":["unclear"],"title":"Research Ethics in Ethnography/Anthropology","category":"Sources","name":"rec695ykoVCiLpjlp","tags":["ethnography","anthropology","research-ethics","ethics-guideline"],"created_at":"2023-06-05T14:17:35.000Z","description":"“The primary audience for this Report are ethics review committees or panels who might not be so directly familiar with the methods regularly adopted by ethnographers and anthropologists. There is nothing new here for practitioners of those disciplines, but it is hoped anyone with an interest in ethics review in ethnography/anthropology may also find the information contained here useful. Although there are some fundamental core ethical principles that can be applied to all human subjects research, the operationalisation of those principles varies according to the methodology adopted. A wide variety of research methods can be found within the social sciences and humanities (SSH) – for this reason the contribution that can be made to advancing human knowledge and scientific understanding from the SSH disciplines may be obstructed or undermined if inappropriate review criteria are applied to research proposals. Ethical review should be informed by the underlying theoretical and methodological assumptions of the discipline which frames the research proposal. This requires the provision of a full justification of the research approach from the research proposer, together with a properly constituted and competent review panel and a robust, fair and transparent review process.” (Iphofen, 2012, p. 5)","id":"rec695ykovcilpjlp","dom_id":"item_rec695ykovcilpjlp"},{"Link":"https://www.britsoc.co.uk/ethics","Reference":"BSA. (2017). _Statement of Ethical Practice_. British Sociological Association. \u003chttps://www.britsoc.co.uk/media/24310/bsa_statement_of_ethical_practice.pdf\u003e\nBSA. (2016). _New Methodological Approaches to Research Using Twitter_. British Sociological Association. \u003chttps://www.britsoc.co.uk/media/24897/new-methodological-approaches-to-research-using-twitter.pdf\u003e\nBSA. (2016). _Open Data and Democratic Governance: Policies, Platforms and Practices Ethics Case Study_. British Sociological Association. \u003chttps://www.britsoc.co.uk/media/24932/j000208_open_data_and_democratic_governance_policies_platforms_and_practices_cs6_v2.pdf\u003e\n","Rights":["unclear"],"title":"British Sociological Association ethics guidelines including for digital research and social media","category":"Sources","name":"rec6VgPcUOVnAu0uO","tags":["sociology","digital-data","research-ethics","ethics-guideline"],"created_at":"2023-06-05T14:21:37.000Z","description":"British Sociological Association ethics guidelines including for digital research and social media","id":"rec6vgpcuovnau0uo","dom_id":"item_rec6vgpcuovnau0uo"},{"Link":"https://aoir.org/reports/ethics3.pdf","Reference":"franzke, aline shakti, Bechmann, A., Zimmer, M., Ess, C., \u0026 Association of Internet Researchers (AoIR). (2020). _Internet Research: Ethical Guidelines 3.0 Association of Internet Researchers_. AoIR. \u003chttps://aoir.org/reports/ethics3.pdf\u003e\n","Rights":["CC-By","CC-NC","CC-SA"],"Strategies":["rec3q0xKZABgZf9Dg","recAnT7HDpWYvdJ1V","rec09f7Nm4RTf6WjE","recsAAZKwCyesrHK6","recB7MF7TeUKH3chO","recpvGxQzTNQS1smH","recnFHf9V2VtAVJMx","recvVsXM40mlnmhqP","rec81gtnlFS5W2BBF","rec8dLtyDCwvjMWge","recgNdOlcMTiussUk","rec8cNT8sPSFtMSAc"],"title":"AoIR report 3","category":"Sources","name":"rec6r8OkE2Q2EdiM3","tags":["AI","internet-research","research-ethics","ethics-guideline"],"created_at":"2023-05-19T12:18:53.000Z","description":"“If you are a student, you may find the 2002 guidelines (IRE 2.0) a good starting point of reflection. If you are looking for a draft to obtain informed consent, look into the appendix of the 2002 guidelines (IRE 1.0, https://aoir.org/reports/ethics.pdf; IRE 2.0, https://aoir.org/reports/ethics2.pdf). If you are a researcher, you might want to begin with the 2019 guidelines to see if the provided resources are a helpful starting point. If you are looking for ways to solve the issue of informed consent you might want to have a look into the appendices of the 2002 document. A catalogue of important questions to start with can be found in the 2012 guidelines. For additional information, especially that focuses on recent technological developments, you might find the 2019 document a useful. Especially political and institutional pressure on researcher has gained importance. If you are an IRB, Ethical Review Board, or member of a similar research ethics oversight board, we highly encourage you to take a look into the 3.0 guidelines. Notice that we emphasize deliberative processes of ethical reflection. At the same time, we believe that in times of Big Data, experimental research needs to be done that requires considerations beyond informed consent, but further includes careful reflection on research design, the context of research, and the basic requirement to minimize associated risks and harms. An ongoing ethical reflection might be more helpful and beneficial in the long term for society than now restricting research. If you are a developer, you might find it helpful to have a closer look into the Companion Resources (6.0). These include “AI and Machine Learning: Internet Research Ethics Guidelines” (6.1) and an “Impact Model” (6.4) for ethical reflection which may be helpful” (franzke et al., 2020, p. 2)","id":"rec6r8oke2q2edim3","dom_id":"item_rec6r8oke2q2edim3"},{"Link":"https://ssrn.com/abstract=1478388","Reference":"Schroeder, R., Meyer, E., \u0026 Ziewitz, M. (2009). _Social, Ethical, and Legal Issues in Presence Research and Applications_. Presence Research in Action (peach). \u003chttps://ssrn.com/abstract=1478388\u003e\n","Rights":["unclear"],"title":"Social, Ethical, and Legal Issues in Presence Research and Applications.","category":"Sources","name":"rec7O4POwgYQFt2Xg","tags":["presence-technologies"],"created_at":"2023-05-19T05:11:00.000Z","description":"“This report analyses the social, ethical, and legal issues of Presence technologies and their implications for society. Most generally, Presence is an emerging andinterdisciplinaryieldofresearchandrefers to the cognitive experience of being somewhere.1 Presence thus aims at achieving realistic feelings and experiences of an environment that does not actually exist, using a broad range of technologies.While virtual and augmented realities, CAVEs, head-mounted displays, and high-end cinemas are among the more well-known examples, recent developments have brought about novel and potentially more intrusive technologies like brain-computer interfaces (BCI), which can translate brainwaves into control signals and vice versa.” (Schroeder et al., 2009, p. 5)","id":"rec7o4powgyqft2xg","dom_id":"item_rec7o4powgyqft2xg"},{"Cases":["reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5","recSXcY4cnofb4zTP","recAlOHJhEy5nDwA6"],"Link":"https://education.ec.europa.eu/node/2285","Principles":["recUYS0TFpk2MhVD7","recqdGhz7l1cGnQeU","recaMsksKInFYbnCL","recwjv8IMAZFWfMSr","reco4DUa3rsjP0hyg","recWLcMWDPE9Fd1pE","reczcRriFbQQpn8iX"],"Reference":"European Commission. (2022). _Ethical guidelines on the use of artificial intelligence and data in teaching and learning for educators | European Education Area_. European Commission. \u003chttps://education.ec.europa.eu/node/2285\u003e\n","Rights":["CC-By"],"Strategies":["recegTm800wJXGjO1","rechaQXedBh3OsMjZ","rec35PeHdUmtalypk","rec7daqDHSCuc70yS","recOo7Pmo4FBYcu7P","recTWhZ88TbQLcNaQ","recRTVqtvPcBS6zps"],"title":"EC guidelines on AI in education for educators","category":"Sources","name":"rec9jnxuHOioQn4DC","tags":["ethics-guideline","not-research-ethics","education-research","AI"],"created_at":"2023-05-19T12:59:16.000Z","description":"“These Guidelines are to be used in schools across Europe and we shall actively promote them through the Erasmus+ programme. Collectively or individually, teachers and school leaders will now have a solid basis to venture out and expand their use of these technologies in a considerate, safe and ethical way. These Guidelines, along with their use on the ground, are fundamental to our ongoing efforts to achieve the European Education Area, while supporting the work being carried out by EU Member States. The Guidelines are part of a longer journey, while the EU is negotiating and preparing for a comprehensive and effective regulatory framework for trustworthy AI, to be implemented across all sectors in the EU, including education. And our work does not stop here. As we move forward, we will continue to develop a better understanding of how to apply these technologies, allowing educators to be even more inclusive and pragmatic, especially in primary and secondary education” (European Commission, 2022, p. 6)“Therefore, I would invite all European teachers and educators to take advantage of these guidelines, and to share their feedback on their practical application and experience, as this will support our ongoing efforts regarding the digital transition in education. We shall also strongly benefit from the views and experience of our pupils, their families, and all stakeholders in the field of education about the use and impact of AI in their daily work and how to make it further beneficial while avoiding risks and negative effects to human rights and our fundamental EU values. Our joint work on AI and data in education shows a shared commitment to the education community, to our learners, to their development and well-being. These Guidelines are an important starting point. It is now up to all of us to promote them and put them into practice. I am counting on you.” (European Commission, 2022, p. 7)","id":"rec9jnxuhoioqn4dc","dom_id":"item_rec9jnxuhoioqn4dc"},{"Link":"https://www.dur.ac.uk/resources/beacon/CBPREthicsGuidewebNovember20121.pdf","Reference":"Centre for Social Justice and Community Action \u0026 National Co-ordinating Centre for Public Engagement. (2012). _Community-based participatory research A guide to ethical principles and practice_. Centre for Social Justice and Community Action. \u003chttps://www.dur.ac.uk/resources/beacon/CBPREthicsGuidewebNovember20121.pdf\u003e\n","Rights":["unclear"],"title":"Community-based participatory research A guide to ethical principles and practice.","category":"Sources","name":"recEUvvWsvLXjiGRC","tags":["participatory-or-community-research","ethics-guideline","research-ethics"],"created_at":"2023-06-05T13:58:14.000Z","description":"“One of the main aims of producing this guide is to enhance the awareness of ethical challenges on the part of research partners and their ability to tackle these challenges. Another aim is to encourage research funders, academic and other institutions and research ethics committees to understand the complexities of CBPR and modify some of their existing requirements to fit CBPR contexts. Taking account of the nuances and complexities of CBPR may require institutions to change their ways of working and reconsider their values. The ethical principles underpinning CBPR emphasise democratic participation in the research process. This means it is important that these principles are made explicit, in order to ensure all participants are aware of them, and able to discuss what they mean in their own contexts and work together to interpret, develop and implement them. In summary, the guide to ethical principles and practice has a number of purposes, including to: • raise ethical awareness amongst all research partners and participants • encourage discussion about ethical issues that can arise in CBPR • offer ethical guidance to partners and participants in CBPR” (Centre for Social Justice and Community Action and National Co-ordinating Centre for Public Engagement, 2012, p. 6)“• inform research institutions (including universities), research funders and sponsors about what ethical issues might come up so they can ensure CBPR is conducted according to the highest standards • inform research institutions, research funders and sponsors about the complexities and nuances of CBPR to ensure they do not impose ethical standards that are impractical, patronising to community researchers or partners or inappropriate in other ways • improve ethical practice in CBPR The ethical principles and guidelines are designed not to be too detailed or prescriptive (i.e. they do not take the form of rules) as this removes control and responsibility from the research partners themselves and assumes a fixed model of what counts as good research. The ethical principles and guidelines cannot offer simple solutions to the inevitable dilemmas and challenges that are part of the CBPR process.” (Centre for Social Justice and Community Action and National Co-ordinating Centre for Public Engagement, 2012, p. 7)","id":"receuvvwsvlxjigrc","dom_id":"item_receuvvwsvlxjigrc"},{"Link":"https://doi.org/10.1145/3530019.3531329","PrincipleCollation":["recG3YFWxYITJ7HdM","recS88wQCG2Q3vCmN","recW30jj9GzhNGTuX","recYP5QdyBcP0gruN","rec3rTrXMiXaZjy7s","rec5tUjckYAzHmc7e","recl9ptNqbtslS9s6","recZQWPVfsVKa7q8t","rec3tM5qcjsVoGf8o","recLwscIEEHBzI5Nq","recNFvfNqfB2VwhZ8","recO0PpscH27fFkXA","recgk6VOEM4y4w7gV","rechhdJ1P2uBHmBJx","reci6dYShRhu4H0ad","recsJ2B2umNP3DulI","recQECJl1tr5x63ig","recQEQ1NAj92jByY5","recUb7Jo7VY1Hx5Co","rec2lbN7lCx1fQKnh","recBuQv8QRql0Ow4q"],"Reference":"Khan, A. A., Badshah, S., Liang, P., Waseem, M., Khan, B., Ahmad, A., Fahmideh, M., Niazi, M., \u0026 Akbar, M. A. (2022). Ethics of AI: A Systematic Literature Review of Principles and Challenges. _The International Conference on Evaluation and Assessment in Software Engineering 2022_, 383–392. \u003chttps://doi.org/10.1145/3530019.3531329\u003e\n","title":"Khan-review systematic literature analysis ","category":"Sources","name":"recFglyNv1cgX2zx7","tags":["research-paper"],"created_at":"2023-06-08T15:02:40.000Z","description":"A review piece","id":"recfglynv1cgx2zx7","dom_id":"item_recfglynv1cgx2zx7"},{"Cases":["rec8YSr2IJxVndTKb","recDAvfsflBF0WKpf","recVLyzjb1UfqbrIh","recWNt6W4v0xr3x0Z","recNNFrnCiGxTP4WE"],"Challenges":["recHHr97jsyNDnlsJ","recJjTMjpfE0WHSWu"],"Link":"https://www.accessnow.org/cms/assets/uploads/2018/11/AI-and-Human-Rights.pdf","Reference":"Access Now. (2018). HUMAN RIGHTS IN THE AGE OF ARTIFICIAL INTELLIGENCE. Access Now. \u003chttps://www.accessnow.org/cms/assets/uploads/2018/11/AI-and-Human-Rights.pdf\u003e\n","Rights":["CC-By"],"title":"AccessNow","category":"Sources","name":"recH8KmnURSknCr5y","tags":["AI","legal-focus","not-research-ethics"],"created_at":"2023-05-25T18:27:28.000Z","description":"“Access Now defends and extends the digital rights of users at risk around the world. By combining direct technical support, comprehensive policy engagement, global advocacy, grassroots grantmaking, and convenings such as RightsCon, we fight for human rights in the digital age.” (Access Now, 2018, p. 38)“The first section proposes definitions for key terms and concepts, including “artificial intelligence” and “machine learning.” We next look at how different artificial intelligence systems are used in the world today and ways in which they can both help or harm society. Turning to human rights, we look at the role human rights law can play in the development of artificial intelligence, including the interplay between these fundamental rights and ethics. Then, looking at widely adopted human rights instruments, we highlight the ways current and foreseeable uses of artificial intelligence can interfere with a broad range of human rights. Finally, we offer a list of recommendations for stakeholders to protect those rights.” (Access Now, 2018, p. 7)","id":"rech8kmnurskncr5y","dom_id":"item_rech8kmnurskncr5y"},{"Link":" https://apo.org.au/node/75590","Reference":"Clark, K., Duckham, M., Guillemin, M., Hunter, A., McVernon, J., O’Keefe, C., Pitkin, C., Prawer, S., Sinnott, R. O., Warr, D., \u0026 Waycott, J. (2015). _Guidelines for the ethical use of digital data in human research_. The University of Melbourne. \u003chttps://apo.org.au/node/75590\u003e\nCo-design workshops with stakeholders + interviews for guideline development and refining, outlined in\nClark, K., Duckham, M., Guillemin, M., Hunter, A., McVernon, J., O’Keefe, C., Pitkin, C., Prawer, S., Sinnott, R., Warr, D., \u0026 Waycott, J. (2019). Advancing the ethical use of digital data in human research: Challenges and strategies to promote ethical practice. Ethics and Information Technology, 21(1), 59–73. Scopus. https://doi.org/10.1007/s10676-018-9490-4\n\n","Rights":["unclear"],"title":"ethical use of digital data in human research.","category":"Sources","name":"recK8N5VaHTZwJJQV","tags":["digital-data","research-ethics","ethics-guideline"],"created_at":"2023-06-05T14:07:22.000Z","description":"Structured with:\\-introduction (why need guidelines and method background)\\-ethical issues for researchers (set out key issues) with example cases\\-Practical approaches for research ethics committees – sets out key questions RECs might ask mostly\\-Glossary\\- resources“The guidelines presented here have been developed to assist researchers who are conducting, and ethics committee members who are assessing, research involving digital data. Digital data presents researchers and ethics committees with familiar and novel ethical issues. Accepted strategies for managing issues such as privacy and confidentiality, and informed consent, need rethinking. The qualities of digital data, including its mobility and replicability, present new kinds of ethical issues which emerge in relation to data governance, data security and data management. This document has five parts. Part A discusses key features of digital data and explains how these guidelines were developed. Guidelines for researchers and human research ethics committees are presented in Parts B and C. Part B addresses researchers and discusses five categories of key ethical issues and poses related guiding questions to consider when conducting research involving digital data: • Consent • Privacy and confidentiality • Ownership and authorship • Data governance and custodianship • Data sharing: assessing the social benefits of research Part C addresses members of human research ethics committees and provides guiding questions for reviewing projects involving the use of digital data. The guidelines in Part B and C are formulated as discussions of key issues and arising questions. They are not intended to be prescriptive, but rather to contextualise and focus on key ethical risks in research involving digital data. Part D is a glossary of key terms used in the document. Part E lists resources that have informed the development of these guidelines and others which readers may find useful.” (Clark et al., 2015, p. 3)","id":"reck8n5vahtzwjjqv","dom_id":"item_reck8n5vahtzwjjqv"},{"Link":"https://clok.uclan.ac.uk/10729/1/BVL%20VOLUME%201%20final.pdf","Reference":"Johnson, V., Hart, R., \u0026 Colwell, J. (Eds.). (2014). _Steps for Engaging Young Children in Research Volume 1: The Guide_. Education Research Centre, University of Brighton. \u003chttps://clok.uclan.ac.uk/10729/1/BVL%20VOLUME%201%20final.pdf\u003e\n","Rights":["unclear"],"title":"Steps for Engaging Young Children in Research ","category":"Sources","name":"recPWpuKPvgwavZtB","tags":["young-participants","research-ethics","ethics-guideline"],"created_at":"2023-06-05T14:23:42.000Z","description":"“‘Steps to Engaging Young Children in Research’ is split into a Researcher Resource and a Researcher Toolkit. These have been developed to assist researchers to design research which is ethical, sensitive to the needs of the children, the community in which they live, whilst also being engaging for young children. These steps suggest an initial process of reviewing capabilities, developing ethical protocols and building trust and relationships.This Researcher Resource provides academic background about why young children should be involved in research and provides more guidance on each of the six steps to engaging young children in research. For each of the steps guidance is given about what the researcher might include in their research when considering each step.For each cluster of methods an overview is provided to show the range of the types of methods that could be applied with young children, key strengths and weaknesses of these methods and a consideration of the potential contextual, ethical and capacity issues which may arise through the use of such methods\" (“Steps for Engaging Young Children in Research Volume 1: The Guide”, 2014, p. 8)“Steps to Engaging Young Children in Research Case Studies of Learning from Practice are included in this Researcher Resource to demonstrate research processes have been applied with young children around the world. These demonstrate the adaptation and combination of methods that have been applied to answer particular research questions in different contexts. These examples are not meant to be prescriptive, but to give researchers examples from particular settings and to demonstrate the complexity of engaging young children in research. ...The accompanying Researcher Toolkit encourages researchers planning to work with young children to consider not only the types of methods needed to answer different research questions, but also the context in which the methods are to be applied and the skills that will be required to use them. ...A collection of methods provides a number of examples of methods successfully used in research with young children. This will support researchers to identify and trial different methods in their context to answer their research questions. The methods presented have been divided into six separate, though interlinking, clusters: • Gaining Consent and Developing Trust • Interviews and Discussion • Child-led Tours and in-Situ • Visual Free Expression • Structured Visuals • Drama and Performance • Play and Games This Researcher Toolkit is presented as guidance rather than as a ‘how-to guide’ to be strictly followed. Each research problem is unique, each group of children will have different needs and abilities and as such researchers need access to a variety of methods that can be applied flexibly, modified, and combined in different ways to provide a unique research design. Many questions are raised for researchers to consider as they engage in this creative process of design. Collaboration with other researchers is encouraged. In doing so, if researchers continue to share their progress and extend ideas, then a community of practice of those engaging young children in research can be expanded and strengthened. Detailed descriptions of how methods have been applied in different contexts are included in the Researcher Toolkit. These show how methods have been applied in a range of countries and settings so that researchers can get ideas of innovative tools and how they may be suited to their needs.” (“Steps for Engaging Young Children in Research Volume 1: The Guide”, 2014, p. 9)Step 2 re ethics sets out some clear very specific issues that should (in all cases) or could (depending on the research) be considered, with examples of practical guidance.In considering each _method (step 5) _an “ethics and context” section provides some specific elaboration for that method and examples of research that might be drawn onIn the _case studies (p.67 on)_- which give examples of research conducted - “ethical considerations” are set out briefly.","id":"recpwpukpvgwavztb","dom_id":"item_recpwpukpvgwavztb"},{"Link":"https://aiatsis.gov.au/sites/default/files/2022-02/aiatsis-guide-code-ethics-jan22.pdf","Reference":"Australian Institute of Aboriginal and Torres Strait Islander Studies (AIATSIS). (2020). _A Guide to applying The AIATSIS Code of Ethics for Aboriginal and Torres Strait Islander Research_. Australian Institute of Aboriginal and Torres Strait Islander Studies (AIATSIS). \u003chttps://aiatsis.gov.au/sites/default/files/2022-02/aiatsis-guide-code-ethics-jan22.pdf\u003e\nAustralian Institute of Aboriginal and Torres Strait Islander Studies (AIATSIS). (2020). _AIATSIS Code of Ethics for Aboriginal and Torres Strait Islander Research_. Australian Institute of Aboriginal and Torres Strait Islander Studies (AIATSIS). \u003chttps://aiatsis.gov.au/sites/default/files/2022-02/aiatsis-code-ethics-jan22.pdf\u003e\n","Rights":["All-rights-reserved"],"title":"AIATSIS code of ethics","category":"Sources","name":"recPr1Vbe9mTKyXIc","tags":["indigenous-research","ethics-guideline","research-ethics"],"created_at":"2023-06-05T13:53:20.000Z","description":"“Who should use the AIATSIS Code? The AIATSIS Code of Ethics is intended for use by: • Any person conducting Aboriginal and Torres Strait Islander research. • Any member of an ethical review body or other body reviewing Aboriginal and Torres Strait Islander research, including human research ethics committees (HRECs), grant assessment panels and data governance committees. • Sponsors or commissioners of Aboriginal and Torres Strait Islander research, which includes any person or public or private organisation that is providing financial or other support to the project. • Those involved in research governance and policy relating to research or management of collections, including the development of standards. • Aboriginal and Torres Strait Islander peoples, communities and organisations engaged in or with research.17” (Australian Institute of Aboriginal and Torres Strait Islander Studies (AIATSIS), 2020, p. 6)“Many Aboriginal and Torres Strait Islander communities are actively involved in managing research that concerns them, including through the development of local protocols and template agreements, establishing research priorities, and participating in and leading projects. Nevertheless, there is a positive obligation on researchers to ensure communities with which they engage are aware of their rights and what they should expect of researchers. In addition to the materials on the AIATSIS website, some useful guides have been developed specifically for Indigenous communities and organisations engaging with research, including NHMRC’s Keeping Research on Track II 2018 and the Lowitja Institute’s EthicsHub.18 This Code applies to all Aboriginal and Torres Strait Islander research conducted in Australia. We recognise that institutions involved in this research vary in size, maturity, experience and organisational structure. Similarly, researchers range in experience and skills, including Indigenous researchers, researchers with long-term relationships with particular communities, and those who are new to Aboriginal and Torres Strait Islander research. Accordingly, it is acknowledged that different policies and practices are capable of fulfilling the aim of this Code and attempts have been made to ensure that there are appropriate options for flexibility in its application.” (Australian Institute of Aboriginal and Torres Strait Islander Studies (AIATSIS), 2020, p. 7)The additional material guide... “provides advice for researchers in applying the principles in the AIATSIS Code of Ethics for Aboriginal and Torres Strait Islander Research (the AIATSIS Code of Ethics or this Code). It is also useful for ethics review bodies to identify practical ways in which the principles should be evident in project design. The best ethical research practice occurs in the partnership, design and planning stages of a project. However, ethical practice permeates every stage of the research process and should be revisited regularly as research proceeds. In addition, where a researcher or community encounters a project that is already underway, it is important to take stock of the ethical underpinnings of the project and adjust as required. In acknowledging this, this Guide to Applying the AIATSIS Code of Ethics for Aboriginal and Torres Strait Islander Research provides practical information on how you may apply the principles in four main stages of your research: 1. getting started 2. implementing your project 3. communicating research results 4. post-project. This Guide is designed to encompass a range of different project methodologies: • quantitative methods (surveys, big data, statistics) • qualitative methods (ethnographic, case studies, interview based) • mixed methods (participatory action, focus groups, workshops) • working with documents (historical, archival, discourse analysis). The Guide offers a range of best practices and ideas for any person engaging in ethical research. For more detailed information specific to particular types of research, for example large surveys, please refer to the supporting case studies on the AIATSIS Ethics webpage.2 We plan to expand our range of resources and tools.” (Australian Institute of Aboriginal and Torres Strait Islander Studies (AIATSIS), 2020, p. 3)","id":"recpr1vbe9mtkyxic","dom_id":"item_recpr1vbe9mtkyxic"},{"Link":"https://doi.org/10.1109/TTS.2021.3052127","PrincipleCollation":["recG3YFWxYITJ7HdM","recS88wQCG2Q3vCmN","recW30jj9GzhNGTuX","recYP5QdyBcP0gruN","rec3rTrXMiXaZjy7s","rec5tUjckYAzHmc7e","recl9ptNqbtslS9s6","recZQWPVfsVKa7q8t","rec3tM5qcjsVoGf8o","recMBWhG4UXydd71J","recNFvfNqfB2VwhZ8","recqLh2ycsve2o5QO","recAEIWzHfWw7JYJH","recEhiFWkyd6l5Kim","recZzygIMFc8xIAnW","rec1VlvAnhNs3MkbA","rec2LOHao3FmjpwIv","recbsOi9a2k1nJswY","recH0fKGdRvDSVpQm","recnatLtMuR1D5OXt","recrpsLqCTq9nWSND","recr4fWTm3a7tbJ4G","recCTIw7k9DYnAN2g","recdmyjbklrgjcfc3"],"Reference":"Schiff, D. (2020). _AI Ethics Global Document Collection_ [Data set]. IEEE. \u003chttps://ieee-dataport.org/open-access/ai-ethics-global-document-collection-0\u003e\nSchiff, D., Borenstein, J., Biddle, J., \u0026 Laas, K. (2021). AI Ethics in the Public, Private, and NGO Sectors: A Review of a Global Document Collection. _IEEE Transactions on Technology and Society_, _2_(1), 31–42. \u003chttps://doi.org/10.1109/TTS.2021.3052127\u003e\n","title":"Schiff-review AI Ethics Global Document Collection","category":"Sources","name":"recQXTU23awCZtU2a","tags":["research-paper"],"created_at":"2023-06-08T15:01:40.000Z","description":"A review piece","id":"recqxtu23awcztu2a","dom_id":"item_recqxtu23awcztu2a"},{"Link":"https://aoir.org/reports/ethics2.pdf","Reference":"Markham, A., \u0026 Buchanan, E. (2012). Ethical Decision-Making and Internet Research: Recommendations from the AoIR Ethics Working Committee (Version 2.0). AoIR. \u003chttps://aoir.org/reports/ethics2.pdf\u003e\n","Rights":["CC-By"],"Strategies":["recobVSYWj9jYvbgF","recN2Lw4yXDBWLSYv","recf50wvya0NXxdxz","recpUrzG3GpRiwqnz","recgWFCdfcVaeaPQO","reczG9x5YfScZJdCo","recJQF6QfQGlcSzDm","recY9yr9vYcOAUSEA","recKDgUEDD2f1egyd","recazD3B5XpqgOCGV","recPynxbe7x5wOs5E"],"title":"AoIR report 2","category":"Sources","name":"recQiVQ7CTC72xp6O","tags":["internet-research","ethics-guideline","research-ethics"],"created_at":"2023-05-19T07:13:58.000Z","description":"“This document aligns with and extends the first AoIR document. This document represents a series of considerations designed to support and inform those responsible for making decisions about the ethics of internet research. And, while primarily directed at researchers, it provides a resource for a wide audience of other stakeholders such as review boards, ethicists, and students, by providing a current discussion of important ethical issues and pertinent literature in the field” (Markham and Buchanan, 2012, p. 2)","id":"recqivq7ctc72xp6o","dom_id":"item_recqivq7ctc72xp6o"},{"Challenges":["recCkVZngvMA5I5uv","reczg5MObRbgzTeob"],"Link":"https://taylorinstitute.ucalgary.ca/resources/ethics-scholarship-teaching-and-learning","Reference":"Fedoruk, L. (2017b). _Ethics in The Scholarship of Teaching and Learning_. University of Calgary Taylor Institute for Teaching and Learning. \u003chttps://taylorinstitute.ucalgary.ca/resources/ethics-scholarship-teaching-and-learning\u003e\n","Rights":["CC-By","CC-NC"],"Strategies":["recinajIdAe7hRxjN","rec55h8FhGKyGPNgw","reclplj55gpqdYGTr","recDOGXIsqtXrCuSz","recELo3u36oZuujxN","recI5BV6v0BCK03VN"],"title":"Ethics in The Scholarship of Teaching and Learning","category":"Sources","name":"recQzldmBLByP78Uu","tags":["ethics-guideline","research-ethics","education-research"],"created_at":"2023-05-19T12:58:08.000Z","description":"“This new Taylor Institute Guide takes the researcher through the essentials of the Canadian standards for ethical practice in the scholarship of teaching and learning (SoTL). It began with Lisa Fedoruk’s review of the Tri-Council Policy Statement: Ethical Conduct for Research Involving Humans (TCPS2, 2014), which expanded and clarified the highest level best practices necessary for the scholarship done at Canadian post-secondary institutions across Canada. Because of the unique challenges of SoTL, where the human participants that are the subject of the research are also typically the researcher’s students, this Guide translates the comprehensive TCPS2 (2014) for the researcher conducting SoTL research. Of note, a 2018 summary of revisions to the TCPS2 can be found here. The team at the Taylor Institute wanted to provide a guide that laid bare the potential hegemony and power relationships that are part of instruction in higher education. In addition to a careful extrapolation of the relevant principles from the TCPS2, the author integrated several important findings from the scholarly literature on research ethics and SoTL. Next, input was sought from a Research Ethics Senior Advisor at the Secretariat on Responsible Conduct of Research with the Government of Canada. By including practical strategies for ethical practice in SoTL, the unique challenges that compliance with the TCPS2 poses for SoTL are brought to life. The work was then sent out to a community of Canadian academic researchers for their feedback and contributions (see previous page for the list of contributors). The result is a collaboration between a broad diversity of experts reflecting the insights of ethical researchers and ethics board members and chairs from across the country, to provide a resource to complement researchers’ own ethical practices, training, and sound judgement as they conduct their scholarship of teaching and learning. Research with human participants is complex. Just as the TCPS2 supports researchers in managing that complexity, we hope that this Guide will be helpful to SoTL researchers in their design process so that their research projects will be sound and robust, and the resulting insights can inform and extend our understanding of the processes of learning and of supporting that learning with effective, evidence-based instruction” (Fedoruk, 2017, p. 1)","id":"recqzldmblbyp78uu","dom_id":"item_recqzldmblbyp78uu"},{"Link":"https://www.hrc.govt.nz/resources/te-ara-tika-guidelines-maori-research-ethics-0","Reference":"Hudson, M., Milne, M., Reynolds, P., Russell, K., \u0026 Smith, B. (2019). _Te Ara Tika Guidelines for Maori Research Ethics: A framework for researchers and ethics committee members_. Health Research Council of New Zealand. \u003chttps://www.hrc.govt.nz/resources/te-ara-tika-guidelines-maori-research-ethics-0\u003e\n\nHudson, M., Milne, M., Russell, K., Smith, B., Reynolds, P., \u0026 Atatoa-Carr, P. (2016). The development of guidelines for indigenous research ethics in Aotearoa/New Zealand. In A. L. Drugge (Ed.), _Ethics in Indigenous Research, Past Experiences – Future Challenges_ (pp. 157–174). Vaartoe Centre for Sami Research, Umea University. \u003chttps://researchcommons.waikato.ac.nz/handle/10289/12195\u003e\n","Rights":["unclear"],"title":"Te Ara Tika Guidelines for Maori Research Ethics","category":"Sources","name":"recV9YJfbssdD2lty","tags":["ethics-guideline","research-ethics","indigenous-research"],"created_at":"2023-06-05T14:27:07.000Z","description":"“It draws on a foundation of tikanga Maori (Maori protocols and practices) and will be useful for researchers, ethics committee members and those who engage in consultation or advice about Maori ethical issues from a local, regional, national or international perspective” ([Hudson et al., 2019, p. 7]Identification of key underpinning purpose for the guidelines: “(1) to explain key ethical concepts for Maori; (2) to support decision-making around Maori ethical issues; (3) to identify ways to address Maori ethical concerns; and (4) to clarify the roles of Maori ethics committee members” (Hudson et al., 2016, p. 161)“Structure of the Framework We considered the dynamics of the engagement space between researchers and the community and oriented the framework around four key questions that we thought a Maori community would ask of researchers. • He aha te whakapapa o tenei kaupapa? / How did this project come about? • Kei a wai te mana mo tenei kaupapa? / Who is in charge of the project? • Me pehea e tika ai tenei kaupapa? / Will it produce the intended outcomes? • Ma wai e manaaki tenei kaupapa? Who looks out for the peoples interests?” (Hudson et al., 2016, p. 162)Providing space for cross-cutting concepts drawn particularly from Maori knowledge and Principles of the Treaty of Waitangi.Identified ethical issues from existing policy, articles, and guidelines and aligned these to Maori values.Undertook consultation with stakeholder groups","id":"recv9yjfbssdd2lty","dom_id":"item_recv9yjfbssdd2lty"},{"Cases":["reco7BYWc7reRuaCh","recHAwvOykeiUxFsY"],"Link":"https://www2.ed.gov/documents/ai-report/ai-report.pdf","Reference":"Cardona, M. A., Rodríguez, R. J., \u0026 Ishmael, K. (2023). Artificial Intelligence and the Future of Teaching and Learning: Insights and Recommendations. U.S. Department of Education, Office of Educational Technology. \u003chttps://www2.ed.gov/documents/ai-report/ai-report.pdf\u003e\n","Rights":["public-domain"],"Strategies":["recWm6C6wOuVr6UCX","recX4wcNFSw9YljDD","recqY2lEigz82Xmh5"],"title":"Office of EdTech AI and Future of T\u0026L","category":"Sources","name":"recY4zreDoWrsbsv7","tags":["AI","not-research-ethics","education-research"],"created_at":"2023-05-29T07:59:13.000Z","description":"“We will consider “educational technology” (edtech) to include both (a) technologies specifically designed for educational use, as well as (b) general technologies that are widely used in educational settings.” (Cardona et al., 2023, p. 1)“AI can be defined as “automation based on associations.” When computers automate reasoning based on associations in data (or associations deduced from expert knowledge), two shifts fundamental to AI occur and shift computing beyond conventional edtech: (1) from capturing data to detecting patterns in data and (2) from providing access to instructional resources to automating decisions about instruction and other educational processes. Detecting patterns and automating decisions are leaps in the level of responsibilities that can be delegated to a computer system. The process of developing an AI system may lead to bias in how patterns are detected and unfairness in how decisions are automated. Thus, educational systems must govern their use of AI systems.” (Cardona et al., 2023, p. 1)“Understanding that AI increases automation and allows machines to do some tasks that only people did in the past leads us to a pair of bold, overarching questions: 1. What is our collective vision of a desirable and achievable educational system that leverages automation to advance learning while protecting and centering human agency? 2. How and on what timeline will we be ready with necessary guidelines and guardrails, as well as convincing evidence of positive impacts, so that constituents can ethically and equitably implement this vision widely?” (Cardona et al., 2023, p. 6)“Below we address three additional perspectives on what constitutes AI. Educators will find these different perspectives arise in the marketing of AI functionality and are important to understand when evaluating edtech systems that incorporate AI. One useful glossary of AI for Education terms is the CIRCLS Glossary of Artificial Intelligence Terms for Educators.11 AI is not one thing but an umbrella term for a growing set of modeling capabilities, as visualized in Figure 3. Figure3:Components,types,andsubfieldsofAIbasedonRegonaetal(2022).12” (Cardona et al., 2023, p. 11)(Cardona et al., 2023, p. 11)“12 Regona, Massimo \u0026 Yigitcanlar, Tan \u0026 Xia, Bo \u0026 Li, R.Y.M. (2022). Opportunities and adoption challenges of AI in the construction industry: A PRISMA review. Journal of Open Innovation Technology Market and Complexity, 8(45). https://doi.org/10.3390/joitmc8010045” (Cardona et al., 2023, p. 11)**Human-Like Reasoning:** “The idea of “human-like” is helpful because it can be a shorthand for the idea that computers now have capabilities that are very different from the capabilities of early edtech applications. Educational applications will be able to converse with students and teachers, co-pilot how activities unfold in classrooms, and take actions that impact students and teachers more broadly. There will be both opportunities to do things much better than we do today and risks that must be anticipated and addressed. The “human-like” shorthand is not always useful, however, because AI processes information differently from how people process information. When we gloss over the differences between people and computers, we may frame policies for AI in education that miss the mark.” (Cardona et al., 2023, p. 12)**An Algorithm that Pursues a Goal:**“This second definition emphasizes that AI systems and tools identify patterns and choose actions to achieve a given goal. These pattern recognition capabilities and automated recommendations will be used in ways that impact the educational process, including student learning and teacher instructional decision making. For example, today’s personalized learning systems may recognize signs that a student is struggling and may recommend an alternative instructional sequence.” (Cardona et al., 2023, p. 12)“Although this perspective can be useful, it can be misleading. A human view of agency, pursuing goals, and reasoning includes our human abilities to make sense of multiple contexts. For example, a teacher may see three students each make the same mathematical error but recognize that one student has an Individualized Education Program to address vision issues, another misunderstands a mathematical concept, and a third just experienced a frustrating interaction on the playground; the same instructional decision is therefore not appropriate. However, AI systems often lack data and judgement to appropriately include context as they detect patterns and automate decisions. Further, case studies show that technology has the potential to quickly derail from safe to unsafe or from effective to ineffective when the context shifts even slightly. For this and other reasons, people must be involved in goal setting, pattern analysis, and decision-making.15” (Cardona et al., 2023, p. 13)**Intelligence Augmentation:**““Intelligence Augmentation” (IA)17 centers “intelligence” and “decision making” in humans but recognizes that people sometimes are overburdened and benefit from assistive tools. AI may help teachers make better decisions because computers notice patterns that teachers can miss. For example, when a teacher and student agree that the student needs reminders, an AI system may provide reminders in whatever form a student likes without adding to the teacher’s workload. Intelligence Automation (IA) uses the same basic capabilities of AI, employing associations in data to notice patterns, and, through automation, takes actions based on those patterns. However, IA squarely focuses on helping people in human activities of teaching and learning, whereas AI tends to focus attention on what computers can do.” (Cardona et al., 2023, p. 14)“To develop guidance for edtech, the Department works closely with educational constituents. These constituents include educational leaders—teachers, faculty, support staff, and other educators—researchers; policymakers; advocates and funders; technology developers; community members and organizations; and, above all, learners and their families/caregivers. Recently, through its activities with constituents, the Department noticed a sharp rise in interest and concern about AI. For example, a 2021 field scan found that developers of all kinds of technology systems—for student information, classroom instruction, school logistics, parentteacher communication, and more—expect to add AI capabilities to their systems. Through a series of four listening sessions conducted in June and August 2022 and attended by more than 700 attendees, it became clear that constituents believe that action is required now in order to get ahead of the expected increase of AI in education technology—and they want to roll up their sleeves and start working together. In late 2022 and early 2023, the public became aware of new generative AI chatbots and began to explore how AI could be used to write essays, create lesson plans, produce images, create personalized assignments for students, and more. From public expression in social media, at conferences, and in news media, the Department learned more about risks and benefits of AI-enabled chatbots. And yet this report will not focus on a specific AI tool, service, or announcement, because AI-enabled systems evolve rapidly. Finally, the Department engaged the educational policy expertise available internally and in its relationships with AI policy experts to shape the findings and recommendations in this report.” (Cardona et al., 2023, p. 2)","id":"recy4zredowrsbsv7","dom_id":"item_recy4zredowrsbsv7"},{"Link":"https://edisciplinas.usp.br/pluginfile.php/4434501/mod_resource/content/1/Ethical%20guidelines%20for%20visual%20ethnography.pdf","Reference":"Cox, S., Drew, S., Guillemin, M., Howell, C., Warr, D., \u0026 Waycott, J. (2014). _Guidelines for Ethical Visual Research Methods_. Melbourne School of Population and Global Health, The University of Melbourne. \u003chttps://edisciplinas.usp.br/pluginfile.php/4434501/mod_resource/content/1/Ethical%20guidelines%20for%20visual%20ethnography.pdf\u003e\n","Rights":["All-rights-reserved"],"title":"Ethical Visual Research Methods","category":"Sources","name":"recYWeamyNKERwiEb","tags":["research-ethics","ethics-guideline","visual-methods"],"created_at":"2023-06-05T14:02:36.000Z","description":"“This document presents guidelines to assist researchers and research ethics committees in recognising and responding to ethical issues that arise from the use of visual research methods. The guidelines have been developed in recognition of the growing use of visual methods in research. Visual research methods present both familiar and novel ethical issues that are often amplified by the kinds of data collected in visual research, the processes used for data collection and dissemination, and the sensitive settings in which visual methods are frequently used. This document has four sections. Part A provides an overview of the field of visual research and explains how the guidelines were developed. Part B presents six categories of ethical issues for researchers to consider when using visual methods. Part C presents guidelines for human research ethics committees when considering visual research projects. The guidelines are not intended to be prescriptive; rather they identify critical questions that should be considered when developing and conducting studies using visual research methods. Finally, Part D lists resources that we have drawn on, and that users of these guidelines may find informative.” (Cox et al., 2014, p. 3)","id":"recyweamynkerwieb","dom_id":"item_recyweamynkerwieb"},{"Link":"https://doi.org/10.1038/s42256-019-0088-2","PrincipleCollation":["recG3YFWxYITJ7HdM","recS88wQCG2Q3vCmN","recW30jj9GzhNGTuX","recYP5QdyBcP0gruN","rec3rTrXMiXaZjy7s","rec5tUjckYAzHmc7e","rec3tM5qcjsVoGf8o","recLwscIEEHBzI5Nq","recO0PpscH27fFkXA","rechhdJ1P2uBHmBJx","reci6dYShRhu4H0ad","recqLh2ycsve2o5QO","recsJ2B2umNP3DulI","recAEIWzHfWw7JYJH","recQECJl1tr5x63ig","recQEQ1NAj92jByY5","recUb7Jo7VY1Hx5Co","recZzygIMFc8xIAnW","rec1VlvAnhNs3MkbA","rec2LOHao3FmjpwIv","rec2lbN7lCx1fQKnh","recKt2LyhX48RpbtK","recm9sVI6QmlMdHor","reco1j4ikYZLaYZi6","recRopSKW7iYgFHlz","recUiZ5lcg30F10Ns","recVOZptlmWy2vW4P","recWg8LODvWwEikAo","rec26Igx0Uax18EHh","rec7XT5tn9q11nNbX"],"Reference":"Jobin, A., Ienca, M., \u0026 Vayena, E. (2019a). / Supplementary File The global landscape of AI ethics guidelines. _Nature Machine Intelligence_, _1_(9), 389–399. \u003chttps://doi.org/10.1038/s42256-019-0088-2\u003e\nJobin, A., Ienca, M., \u0026 Vayena, E. (2019b). The global landscape of AI ethics guidelines. _Nature Machine Intelligence_, _1_(9), Article 9. \u003chttps://doi.org/10.1038/s42256-019-0088-2\u003e\n","title":"Jobin-review global landscape of AI ethics guidelines","category":"Sources","name":"recZ82hfuUzG775I3","tags":["research-paper"],"created_at":"2023-06-08T15:02:41.000Z","description":"A review piece","id":"recz82hfuuzg775i3","dom_id":"item_recz82hfuuzg775i3"},{"Link":"https://www.nhmrc.gov.au/about-us/publications/national-statement-ethical-conduct-human-research-2007-updated-2018","Principles":["recLHILkx2JDFsLbX","recKdujFoPJr4ZAhZ","recsvi4LnhEEPyQ1h","recU6u0AZbcNj1ik9","reczVPIH1y2OMpAJH","recmzjcGKv3yNOxbl","rec42P8U9usfYCtv9","rec6O9e1nYBJtQUTj","recSqx6wklVpDzx3s","recMGB4iC5oaCtr5x","recOHnq45Fq7YWsRO","recgDkzdE9dfpTxCK","recy4stJ6Y4e2Fezp","recQ9DIFEsOEkCx3O","recjViPnz3atRIOpD","recheWZC64aZRgmpo","recint2IxoR8aILCp","recPg7Ov0priGGtLm"],"Reference":"Developed jointly by National Health and Medical Research Council Australian Research Council Universities Australia (2018), National Statement on Ethical Conduct in Human Research\n","Rights":["CC-By"],"title":"National Statement on Ethical Conduct in Human Research","category":"Sources","name":"recZRK4rseqW5VsRL","tags":["ethics-guideline","research-ethics"],"created_at":"2023-05-18T13:43:18.000Z","description":"\"The **National Statement** is intended for use by:- any researcher conducting research with human participants- any member of an ethical review body reviewing that research- those involved in research governance- potential research participants.The National Statement is developed jointly by the National Health and Medical Research Council, the Australian Research Council and Universities Australia.The National Statement is subject to rolling review. This means that parts of the National Statement will be updated as needed, rather than reviewing the entire document every 5 years.All human interaction, including the interaction involved in human research, has ethical dimensions. However, 'ethical conduct' is more than simply doing the right thing. It involves acting in the right spirit, out of an abiding respect and concern for one's fellow creatures. This National Statement on 'ethical conduct in human research' is therefore oriented to something more fundamental than ethical 'do's' and 'don'ts' – namely, an ethos that should permeate the way those engaged in human research approach all that they do in their research.Human research is research conducted with or about people, or their data or tissue. It has contributed enormously to human good. Much human research carries little risk and in Australia the vast majority of human research has been carried out in a safe and ethically responsible manner. But human research can involve significant risks and it is possible for things to go wrong. Sometimes risks are realised despite the best of intentions and care in planning and practice. Sometimes they are realised because of technical error or ethical insensitivity, neglect or disregard. On rare occasions the practice of research has even involved the deliberate and appalling violation of human beings – notoriously, the Second World War experiments in detention and concentration camps.This range of possibilities can give rise to important and sometimes difficult ethical questions about research participation. Two considerations give further weight to those questions. First, research participants may enter into a relationship with researchers whom they may not know but need to trust. This trust adds to the ethical responsibility borne by those in whom it is placed. Secondly, many who contribute as participants in human research do so altruistically, for the common good, without thought of recompense for their time and effort.This underscores the importance of protecting research patients.Since earliest times, human societies have pondered the nature of ethics and its requirements and have sought illumination on ethical questions in the writings of philosophers, novelists, poets and sages, in the teaching of religions, and in everyday individual thinking. Reflection on the ethical dimensions of medical research, in particular, has a long history, reaching back to classical Greece and beyond. Practitioners of human research in many other fields have also long reflected upon the ethical questions raised by what they do. There has, however, been increased attention to ethical reflection about human research since the Second World War. The judgment of the Nuremberg military tribunal included ten principles about permissible medical experiments, since referred to as the Nuremberg Code. Discussion of these principles led the World Medical Assembly in 1964 to adopt what came to be known as the Helsinki Declaration, revised several times since then. The various international human rights instruments that have also emerged since the Second World War emphasise the importance of protecting human beings in many spheres of community life. During this period, written ethical guidelines have also been generated in many areas of research practice as an expression of professional responsibility.But what is the justification for ethical research guidelines as extensive as this National Statement, and for its wide-reaching practical authority?The National Statement has been extended to address many issues not discussed in the previous version, or discussed in less detail. This is in response to requests for clearer guidance for those conducting research and those involved in its ethical review. At the same time, without compromising the protection of participants, the revised National Statement provides for greater flexibility in the practice of ethical review, depending on the type and area of research and the degree of risk involved.Research often involves public interaction between people that serves a public good. There is, therefore, a public responsibility for seeing that these interactions are ethically acceptable to the Australian community. That responsibility is acknowledged and given effect in the widereaching authority of this National Statement, which sets out national standards for the ethical design, review and conduct of human research. Its content reflects the outcome of wide consultation with Australian communities who participate in, design, conduct, fund, manage and publish human research.\" (Preamble)","id":"reczrk4rseqw5vsrl","dom_id":"item_reczrk4rseqw5vsrl"},{"Link":"https://www.bera.ac.uk/publication/ethical-guidelines-for-educational-research-2018","Reference":"BERA. (2018). _Ethical Guidelines for Educational Research, fourth edition (2018)_. BERA. \u003chttps://www.bera.ac.uk/publication/ethical-guidelines-for-educational-research-2018\u003e\nPennacchia, J. (Ed.). (2019). _BERA Research Ethics Case Studies: 1. Twitter, data collection \u0026 informed consent._ BERA. \u003chttps://www.bera.ac.uk/publication-series/research-ethics-case-studies\u003e\nWyse, D., Brown, C., Oliver, S., \u0026 Poblete, X. (2018). _CLOSE-TO-PRACTICE EDUCATIONAL RESEARCH_. BERA. \u003chttps://www.bera.ac.uk/wp-content/uploads/2018/11/BERA-Close-to-Practice_statement_Nov2018-1.pdf\u003e\nHennessy, S. (2018). The 2018 BERA Ethical Guidelines: Behind the scenes of the revision. _Research Intelligence_, _136_, 15–16. \u003chttps://www.bera.ac.uk/publication/summer-2018\u003e\n","Rights":["CC-By","CC-NC","CC-ND"],"title":"British Educational Research Association ethics guidelines","category":"Sources","name":"recb4qL0nQ1TSaE0w","tags":["research-ethics","ethics-guideline","education-research","digital-data"],"created_at":"2023-06-05T14:51:13.000Z","description":"“The intended audience for these guidelines is anyone undertaking educational research – be they people whose job description includes research, or others who, for a variety of reasons (including studying for a qualification or with the intention of improving practice), conduct research within the field. This includes both independent researchers and those based in educational institutions of any kind (including but not limited to early years settings, schools, colleges and universities).” (BERA, 2018, p. 1)BERA has also published some case studies including regarding use of Twitter data, and a discussion of close-to-practice research which involves some reflection regarding concerns of power and merit. ","id":"recb4ql0nq1tsae0w","dom_id":"item_recb4ql0nq1tsae0w"},{"Link":"https://guidelines.panelfit.eu/","Reference":"Beriain, I. de M., Schaber, F., Malgieri, G., \u0026 Penedo, A. C. (2021). _Guidelines on Data Protection Ethical and Legal Issues in ICT Research and Innovation. AI: Requirements for developers and innovators_. PANELFIT consortium. \u003chttps://guidelines.panelfit.eu/ai/general-exposition/\u003e\nCampillo, L. P., \u0026 Parrilla, J. A. C. (2022). _Guidelines on Data Protection Ethical and Legal Issues in ICT Research and Innovation. DAY TO DAY ACTIVITIES. 1.1 Organizing a congress or a conference_. PANELFIT consortium. \u003chttps://guidelines.panelfit.eu/wp-content/uploads/2022/06/Guidelines-DayToDay-Activities.pdf\u003e\n","Rights":["CC-By","CC-NC","CC-ND"],"title":"PANELFIT guidelines for data protection","category":"Sources","name":"recctRdjoSccU3vcD","tags":["digital-data","ethics-guideline"],"created_at":"2023-06-05T14:13:41.000Z","description":"The PANELFIT Guidelines (Guidelines on Data Protection ELI in ICT Research and Innovation by the PANELFIT Project) have been conceived as a complex tool aimed at serving as a handbook of information about the regulatory framework on ICT data protection Ethical and Legal issues.Therefore, The Guidelines are mainly focused on the research and innovation community working on the ICT field. They have been conceived to make it easier for researchers to fulfill their legal and ethical obligations in this regard.These materials, which have been selected based on internal consensus, have been produced through a long and complex co-creation process. The first drafts were produced by the PanelfitConsortium members and then sent to more than 25 reviewers. Their comments were integrated in the materials and a second version created. On this basis, a second review was performed. Finally, the updated versions were validated by an additional external expert with expertise on data protection issues at the DPA level.Concrete information about the names of the authors and the concrete reviewers who helped as to create these materials has been included in each section.The Guidelines produced by PANELFIT are mainly aimed at showing the legal issues regarding data processing. An impressive amount of information about ethical issues can be found in the SHERPA project website (\u003chttps://www.project-sherpa.eu/\u003e)","id":"recctrdjosccu3vcd","dom_id":"item_recctrdjosccu3vcd"},{"Challenges":["recefglLZ3oJWw2SZ","recBc3GCNokDL220T","rec8hxSSEudaplJiA","recdmBNNa98cN8Sda","reckPjhcWwLgjX9p5","recvQ90DajNCwPiGP"],"Link":"https://doi.org/10.5281/ZENODO.3240529","Principles":["recImuZ3T4iDiNP2B","rec5216SybPPylYdD","rec6tz9Phzck0hvT8","recGb4WZzr34RXKr0","recv9z5lu6zZspHYA","rectPrYetyr4YIuq8","reclur1ImQFlLyYof","recB9JaNSRmLbD8eE","recoXK4tOGqoh2nE3","recArMNCiaj832hZ1","recvLGtdSLYOV3Azw","recC49LlmCoNq2Svy","rec7n2TGrH9RHYpQj","rechB1SCdOpa940OZ","recy6hrMpKZ7TOn3Q","recL7CrHMutQEBBu7","recZToVrPeFlFq0Aw","recDRQE1qQQNI65Xn","recZbEXiEs1AlDdn3","recgEYKqzkCYqPmXW","recyigniIRfB6DPTu","recduyDUAsQdJ4EMy"],"Reference":"Leslie, D. (2019). _Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector_. The Alan Turing Institute. \u003chttps://doi.org/10.5281/ZENODO.3240529\u003e\n","Rights":["CC-By","CC-SA","CC-NC"],"Strategies":["recmhwo8kmYkBZ7Sy","recgswAsiepwEclOd","recNzXJwCfbwBLmVU","recAhcg8OdusJvk43","rec0GhefNkhJqW0c2","recGS9OVQ7qAjvIYE","recKWkdYO98PeEKDz","recgn2UvSD4OhzGI4","recZPU5yTmrjPXlF2"],"title":"Turing responsible design and implementation of AI systems in the public sector","category":"Sources","name":"recfYC5jjPmpLfSlM","tags":["AI","not-research-ethics","ethics-guideline"],"created_at":"2023-05-19T09:35:56.000Z","description":"\"The Public Policy Programme at The Alan Turing Institute was set up in May 2018 with the aim of developing research, tools, and techniques that help governments innovate with data-intensive technologies and improve the quality of people's lives. We work alongside policy makers to explore how data science and artificial intelligence can inform public policy and improve the provision of public services. We believe that governments can reap the benefits of these technologies only if they make considerations of ethics and safety a first priority. This document provides end-to-end guidance on how to apply principles of AI ethics and safety to the design and implementation of algorithmic systems in the public sector. We will shortly release a workbook to bring the recommendations made in this guide to life. The workbook will contain case studies highlighting how the guidance contained here can be applied to concrete AI projects. It will also contain exercises and practical tools to help strengthen the process-based governance of your AI project. Please note, that this guide is a living document that will evolve and improve with input from users, affected stakeholders, and interested parties. We need your participation.\"p.3","id":"recfyc5jjpmplfslm","dom_id":"item_recfyc5jjpmplfslm"},{"Link":" https://www.britishgerontology.org/about-bsg/bsg-ethical-guidelines","Reference":"British Society of Gerontology. (2012). _BSG Ethical Guidelines_. British Society of Gerontology. \u003chttps://www.britishgerontology.org/about-bsg/bsg-ethical-guidelines\u003e\n","Rights":["All-rights-reserved"],"title":"British Society of Gerontology ethical guidelines","category":"Sources","name":"recl6lvnjffHMH8K1","tags":["older-participants","ethics-guideline"],"created_at":"2023-06-05T14:45:30.000Z","description":"Structure:1. Background sets out need relating to the target topic2. Preliminary questions - two key questions to ask ‘what might it be like to take part in the research’ and ‘as a result of your work might you identify individuals with needs’ (paraphrasing), these are two key questions clearly targeting the specific needs of the topic area and ethical issues arising3. Undertaking ethical research - introduces basic principles through a process description (from inception) and bullet points. Resources relating to methods in teh field (e.g. oralhistory.org.uk) are flagged4. Ethical research practice - discusses issues around research practice e.g. teamwork, interpersonal considerations, piloting of methods5. New ways of working - flags key institutional considerations (ethics committees, etc.).","id":"recl6lvnjffhmh8k1","dom_id":"item_recl6lvnjffhmh8k1"},{"Link":"https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai","Principles":["recRbUmCzD09d6KZS","recRBt2kSianfxfwe","reckb3cgfeDh1EeUP","recScYLR2TNiv7iKf","recTLqwMltPHHQDxH","recQEiU22Qy1E0YuA","reclPiw2VvNOSTzv5","recK5zFiq18A3wHAE","recxcFmvPG5wrCqpO"],"Reference":"High-Level Expert Group on AI. (2019). _Ethics guidelines for trustworthy AI | Shaping Europe’s digital future_. FUTURIUM - European Commission. \u003chttps://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\u003e\n","Rights":["CC-By"],"Strategies":["recHEUOqkzQTNsAMw","rec03QV2RUJkP3dfo","recTr6cDE4OBKV0wv","recspvYTySr0ANH6j","rec4oGONFgYMu3JGf","recNHvwgyFPXERuJ8","recAMa23XxDLEzMn8","recBUdrJ8n1tIJLTz","rec95I69E1YcGg0Sa","recypFGFqJzmgFmWV","reciw4r1bRvx6A6Fq","rec7m69DQyCw3rlFg","recdtW2BdWmY6WSP5","recEHwRaLD44KPk8v","recTjwhqfrJoaRxYo","recq6GeKNegFQdzS9","recQPwyiQbPcN0G47","rec0WScLo6sUnoOQN","recJMrMEZnz9rYlnD","recffbHgJO0d179DG","reczFKqCos9f1opXO","recJyzFLE9ry4YEbb","reccAKmFQRH7IiuJi","recaTTvWN1olqx608","rec81gtnlFS5W2BBF"],"title":"High-level Expert Group on AI","category":"Sources","name":"recnCULdYQ36cpZR7","tags":["AI","ethics-guideline","not-research-ethics"],"created_at":"2023-05-25T18:00:54.000Z","description":"“These guidelines are addressed to all AI stakeholders designing, developing, deploying, implementing, using or being affected by AI, including but not limited to companies, organisations, researchers, public services, government agencies, institutions, civil society organisations, individuals, workers and consumers. Stakeholders committed towards achieving Trustworthy AI can voluntarily opt to use these Guidelines as a method to operationalise their commitment, in particular by using the practical assessment list of Chapter III when developing, deploying or using AI systems. This assessment list can also complement – and hence be incorporated in – existing assessment processes. The Guidelines aim to provide guidance for AI applications in general, building a horizontal foundation to achieve Trustworthy AI. However, different situations raise different challenges. AI music recommendation systems do not” (High-Level Expert Group on AI, 2019, p. 5)“raise the same ethical concerns as AI systems proposing critical medical treatments. Likewise, different opportunities and challenges arise from AI systems used in the context of business-to-consumer, business-tobusiness, employer-to-employee and public-to-citizen relationships, or more generally, in different sectors or use cases. Given the context-specificity of AI systems, the implementation of these Guidelines needs to be adapted to the particular AI-application. Moreover, the necessity of an additional sectorial approach, to complement the more general horizontal framework proposed in this document, should be explored.” (High-Level Expert Group on AI, 2019, p. 6)","id":"recnculdyq36cpzr7","dom_id":"item_recnculdyq36cpzr7"},{"Link":" http://cerna-ethics-allistene.org/digitalAssets/54/54730_cerna_2017_machine_learning.pdf","Reference":"ALLISTENE. (2018). _Research Ethics in Machine Learning_. CERNA Report, Research Ethics Board of Allistene, the Digital Sciences and Technologies Alliance. \u003chttp://cerna-ethics-allistene.org/digitalAssets/54/54730_cerna_2017_machine_learning.pdf\u003e\n","Rights":["unclear"],"title":"Research Ethics in Machine Learning","category":"Sources","name":"recoJTiIbIxoY001E","tags":["AI","research-ethics","ethics-guideline"],"created_at":"2023-06-05T14:18:23.000Z","description":"“The rapid spread of innovation-based IT practices complicates the interaction between technological capacity and societal adoption and reduces the relevance of forecast activities about the consequences of research. However, this relative unpredictability does not free scientists of responsibility, but should instead motivate ethical reflection and the quest for appropriate perspectives and methods. Researchers should be aware that their work de facto contributes to changing society and humanity, and the process is not always predictable. Although the responsibility for this impact should not be borne by them alone, they too have a share of collective responsibility. Against this background, the aim of CERNA is to encourage and support researchers in the exercise of ethical reflection about their work.This document is addressed to IT researchers, developers, and designers. Societal issues are listed but not explored in depth. CERNA considers only scientifically plausible possibilities, avoiding science-fiction scenarios that might become a source of confusion” (ALLISTENE, 2018, p. 4)“the purpose of the present document is to: • Raise awareness and provide “researchers” with food for thought and certain waymarks. For reasons of convenience, the term “researcher” is used here to refer to people—designers, engineers, developers, entrepreneurs—and their communities or institutions; • Contribute to a wider debate on the ethical and societal questions associated with the development of artificial intelligence, so that machine learning develops to the benefit of society. CERNA is therefore addressing two kinds of reader here: on the one hand specialists, and on the other hand anyone interested, whether decisionmakers or ordinary citizens.” (ALLISTENE, 2018, p. 7)","id":"recojtiibixoy001e","dom_id":"item_recojtiibixoy001e"},{"Challenges":["rec1QZHHMARBQcZoo","recvWM2glArsVhaye","reciNc7OeTUmnsLqg","recA7Kh502s4UKWGo","recGRpoF7DA23ODSj","rec5nbWC0ZbVwnmxJ","recdZI38VrUaUKRYf","recL5sNl6bi1vrIzj","recaFyWRROaJ1ZFyY","recIUf0R50ogibwe6","rec2ULcHUjSbbSnHL","rec2ajglpzbFYRivi","reciH69D8oIGI1p93","recrVvneRZaEutaGw","rec3Fm8dyG49YU137","reckyVWsglP8FLeUp","recg7BIZZsvZ57fvV","recebQ2NgeBCmafoX","recO1L6GoFMkA6Lt4","recAoyMGCCLhaSqEb","reci0hfcAOIxLVNhB","recKzhZVabDuYM6rG","recUKAxgCSxylEiDm","recroNon39TCBeC88","recPqsTlFK76oGD9C","rech9vLLbQgO3OY0i","recxjc79LvLdKa4rl","recELObWGfkhXzFG2","recqpYMjEFJLmyNaN","recaBNAcact8Bz6dg","recZ43i8Cnhohwb2x","rec9qgAZS5U8GL137","recnYgPiGULdHLunC","rect310qem9li3HKK","recX6r1O4jcsp0nIM","recrk0Tgfwe5J9xfI","rececsX8igwNqhhkC","rec9Cuz3HWc9lWW23","recj64vAVJSm5B2ba","recOpn4I30te1qiKl","recJV8yTX0MnF3rGD","rec0BAjUqQrSIMAEG","recb50cfuQUWDAHZW","recJfAMUB8HdjVQYD","recmRF2P1OOASMfNF","recefglLZ3oJWw2SZ","recdlFwsToNXtcvGC"],"Link":"https://doi.org/10.1007/978-3-030-12524-0_2","Principles":["recxc8SQN09R985HI","receFm7cGasHwpJZO","recD6uFlCKFiHl414","rec5N7PaVEFRs2o8R","recFVLYl0cMH88Lzv","recWlipA3L9QTydWS","recCFSnZpjvw0PcUE","recKWrfJzX52AXSIf","recSwTdeEk6XbEgXU","rec9uueW1gq31CcBo","recuQpwelm0FwdAib"],"Reference":"IEEEE, Chatila, R., \u0026 Havens, J. C. (2019). The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva Sequeira, G. Singh Virk, M. O. Tokhi, \u0026 E. E. Kadar (Eds.), Robotics and Well-Being (Vol. 95, pp. 11–16). Springer International Publishing. \u003chttps://doi.org/10.1007/978-3-030-12524-0_2\u003e\n","Rights":["CC-By","CC-NC"],"Strategies":["recwWov6KzmwU0FQW","recG86YsfXnKY9kNc","rec1Ha94n5xbeqcOY","reclzrMrQSQls3PI9","reck6xeMUc9jtmLr3","recsonQhKLmX4D1ao","reccMqFCLOgQwWM5Q","recI7WdfNhrQxRt7F","recfcXzM3foqFNNGN","recV8WjuiXlfbVN5d","rec9SSyrfFkmuJkCe","recCN9eqkgKT3KjCB","recQPwyiQbPcN0G47","recJyzFLE9ry4YEbb","recgqEckuFTILlOnz","rec81gtnlFS5W2BBF","recouZjokdKQz88z1","recZI7HDWKBU5T22v","recwQrzqPp6C7jyJs","rec9UfowPkXtPUBC9","recHsUV9w4HKBA1w1","rec5CFheImo8onTcd","rec9TbOu2ZXUZG4A5","recrBZOfrDC2lKpRM","recqJAVTtqOUfypNI","recnLauSoXMQCSK5c","recg2u6ZXSRiUO8uF","rec2679E9TXh1DCL8","recgn2UvSD4OhzGI4","recKWkdYO98PeEKDz","recMgYQ7EqHAyDfi1"],"title":"IEEE","category":"Sources","name":"recpXl48pJdKDhc6f","tags":["AI","ethics-guideline","research-ethics"],"created_at":"2023-06-03T16:02:16.000Z","description":"\"Autonomous and intelligent technical systems are specifically designed to reduce the necessity for human intervention in our day-to-day lives. In so doing, these new systems are also raising concerns about their impact on individuals and societies. Current discussions include advocacy for a positive impact, such as optimization of processes and resource usage, more informed planning and decisions, and recognition of useful patterns in big data. Discussions also include warnings about potential harm to privacy, discrimination, loss of skills, adverse economic impacts, risks to security of critical infrastructure, and possible negative long-term effects on societal well-being.Because of their nature, the full benefit of these technologies will be attained only if they are aligned with society’s defined values and ethical principles.\" p.7","id":"recpxl48pjdkdhc6f","dom_id":"item_recpxl48pjdkdhc6f"},{"Link":"https://nda.ie/publications/ethical-guidance-for-research-with-people-with-disabilities-report","Reference":"National Disability Authority. (2009). Ethical Guidance for Research with People with Disabilities: NDA Report. National Disability Authority. \u003chttps://nda.ie/publications/ethical-guidance-for-research-with-people-with-disabilities-report\u003e\n","Rights":["All-rights-reserved"],"title":"Ethical guidance for Research with People with Disabilities","category":"Sources","name":"recr4dvMvPupPbFfo","tags":["research-with-people-with-disabilities","ethics-guideline","research-ethics"],"created_at":"2023-06-05T13:59:01.000Z","description":"“The purpose of the guidance The purpose is to assist researchers and Research Ethics Committees by offering guidance in relation to good practice in research involving people with disabilities.4 The guidance may also be of interest to research sponsors and funders, those involved in research governance, people with disabilities and disability organisations. Ethical responsibilities apply throughout the research process. Applying ethical guidance in individual research projects is an ongoing matter of judgment and good research practice. Researchers cannot simply consider their ethical responsibilities as fulfilled once ethical approval for a research project has been obtained. Issues arise during fieldwork that still have ethical implications so that ‘permission from an ethics committee to proceed with the research is just the beginning of a process of constant self-monitoring by the researcher’ (Rolph, 1998, p.135, citing Ristock and Pennell, 1996). Wiles et al. (2004) state, that, for instance, in relation to issues of informed consent, there are no simple solutions that can be applied universally to resolve all ethical dilemmas. Rather, researchers need at all stages of the research process to be mindful of the various issues that can arise in the context of their individual research projects. Issues can relate to: 1. the needs of participants 2. ensuring ongoing assent or consent 3. handling relationships that develop during the research process 4. unanticipated, distressing emotions 5. unexpected revelations” (\\[National Disability Authority, 2009, p. 13]\\(zotero://select/groups/4907410/items/LT3RKDKU)) (\\[pdf]\\(zotero://open-pdf/groups/4907410/items/HY72W4CC?page=15))","id":"recr4dvmvpuppbffo","dom_id":"item_recr4dvmvpuppbffo"},{"Link":"https://doi.org/10.2139/ssrn.3518482","PrincipleCollation":["recG3YFWxYITJ7HdM","recS88wQCG2Q3vCmN","recW30jj9GzhNGTuX","recYP5QdyBcP0gruN","rec3rTrXMiXaZjy7s","rec5tUjckYAzHmc7e","recl9ptNqbtslS9s6","recZQWPVfsVKa7q8t","recMBWhG4UXydd71J","recgk6VOEM4y4w7gV","recEhiFWkyd6l5Kim","recbsOi9a2k1nJswY","reck7CXWZJAjaLrDR"],"Reference":"Fjeld, J., Achten, N., Hilligoss, H., Nagy, A., \u0026 Srikumar, M. (2020). Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI (SSRN Scholarly Paper No. 3518482). \u003chttps://doi.org/10.2139/ssrn.3518482\u003e\n","title":"Fjeld-review mapping consensus in AI ethics","category":"Sources","name":"recrDXMNtbEk0mCAY","tags":["research-paper"],"created_at":"2023-06-08T15:02:41.000Z","description":"A review piece","id":"recrdxmntbek0mcay","dom_id":"item_recrdxmntbek0mcay"},{"Link":"https://data.europa.eu/doi/10.2777/18849","Reference":"European Commission. Directorate-General for Research. (2010). _Syllabus on ethics in research: Addendum to the European textbook on ethics in research_. Publications Office. \u003chttps://data.europa.eu/doi/10.2777/18849\u003e\n","Rights":["CC-By"],"title":"EC Syllabus on ethics in research","category":"Sources","name":"recwJEbOurgXcmPiM","tags":["not-research-ethics","ethics-learning"],"created_at":"2023-06-05T14:26:19.000Z","description":"“The syllabus presented here is designed for use in the training of researchers and research ethics committee members throughout the European Union and beyond. It is intended to be accessible to scientific and layreaders, including those with no previous experience of ethical theory and analysis. The syllabus will cover key issues in the ethics of research involving human participants, including the ethical issues associated with new technologies. The scope of the syllabus is the ethics of scientific research involving human beings. The case studies relate to a variety of scientific disciplines including biomedical and human life sciences, and the social sciences. Readers will be introduced to a range of philosophical perspectives and concepts, and these will be used to inform discussion of practical ethical issues, but without any particular approach being promoted. Similarly, reference will be made to major religious views where relevant (e.g. in relation to research involving human embryos), but without endorsing or rejecting any particular view. The syllabus consists of eight ‘units of study’ corresponding to the chapters of the textbook, each of which will include a list of intended learning outcomes for that unit, a brief overview of the topic to be addressed in the unit, one or more case studies each with a set of questions to guide discussion by students, descriptions of the main issues that should be discussed arising out of the case studies, and (where appropriate) identification of further issues for discussion that may not arise directly out of the case studies but are related to the themes of the unit. The intention is that an instructor should be able to use the intended learning outcomes and overview in planning the course and introducing the topics to students; the case studies and associated questions can be used either as a basis for small group discussion within the classroom or as preparatory work; and the descriptions of the main issues can be used by the instructor as a basis for guiding and responding to the discussions.” (\\[European Commission. Directorate-General for Research, 2010, p. 5]\\(zotero://select/groups/4907410/items/JYVSIUCE)) (\\[pdf]\\(zotero://open-pdf/groups/4907410/items/JJP2HGB3?page=7))","id":"recwjebourgxcmpim","dom_id":"item_recwjebourgxcmpim"},{"Link":"https://www.microsoft.com/en-us/research/uploads/prod/2018/11/Bot_Guidelines_Nov_2018.pdf","Reference":"Microsoft. (2018). Responsible bots: 10 guidelines for developers of conversational AI. Microsoft. \u003chttps://www.microsoft.com/en-us/research/uploads/prod/2018/11/Bot_Guidelines_Nov_2018.pdf\u003e\n","Rights":["unclear"],"title":"Responsible bots: 10 guidelines for developers of conversational AI","category":"Sources","name":"reczEXIqduaoX8Jab","tags":["not-research-ethics","AI"],"created_at":"2023-06-05T14:19:02.000Z","description":"“These guidelines are aimed at helping you to design a bot that builds trust in the company and service that the bot represents. These guidelines are not intended as legal advice and you should separately ensure that your bot complies with the fast-paced developments in the law in this area. Also, in designing your bot, you should consider a broad set of responsibilities you have when developing any data-centric AI system, including ethics, privacy, security, safety, inclusion, transparency and accountability.” (Microsoft, 2018, p. 1)","id":"reczexiqduaox8jab","dom_id":"item_reczexiqduaox8jab"},{"Stakeholder-impacted":["reccWWRCdr9ZKJBZE","rec1RzTKH08fWjSbT","recAlOHJhEy5nDwA6","recSXcY4cnofb4zTP","recpFE7h6QhaEpNAQ"],"title":"Parents","category":"Stakeholders","name":"rec0af1wTjdFwMqsw","tags":[],"created_at":"2023-05-18T14:40:34.000Z","id":"rec0af1wtjdfwmqsw","dom_id":"item_rec0af1wtjdfwmqsw"},{"Stakeholder-as-actor":["reccWWRCdr9ZKJBZE","recMalmQowxcka9V6","recKWQBBplr1vXmRc","reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5","recSXcY4cnofb4zTP","recAlOHJhEy5nDwA6","recpFE7h6QhaEpNAQ"],"title":"Technology-providers","category":"Stakeholders","name":"recGRt9lWJvxx5GAE","tags":[],"created_at":"2023-05-18T14:40:23.000Z","id":"recgrt9lwjvxx5gae","dom_id":"item_recgrt9lwjvxx5gae"},{"Stakeholder-as-actor":["recMalmQowxcka9V6","reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5","recSXcY4cnofb4zTP","recAlOHJhEy5nDwA6","recpFE7h6QhaEpNAQ"],"title":"Policy-makers","category":"Stakeholders","name":"recSaPtMj9rxfu4uu","tags":[],"created_at":"2023-05-18T14:40:23.000Z","id":"recsaptmj9rxfu4uu","dom_id":"item_recsaptmj9rxfu4uu"},{"Stakeholder-as-actor":["reccWWRCdr9ZKJBZE","reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5","recSXcY4cnofb4zTP","recAlOHJhEy5nDwA6","recpFE7h6QhaEpNAQ"],"title":"Technology-developers-and-designers","category":"Stakeholders","name":"recTYTKZphOqdzYoL","tags":[],"created_at":"2023-06-05T14:49:01.000Z","id":"rectytkzphoqdzyol","dom_id":"item_rectytkzphoqdzyol"},{"Stakeholder-as-actor":["recKWQBBplr1vXmRc","reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5","recSXcY4cnofb4zTP","recAlOHJhEy5nDwA6","recpFE7h6QhaEpNAQ"],"Stakeholder-impacted":["reccWWRCdr9ZKJBZE","recKWQBBplr1vXmRc","reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5","recSXcY4cnofb4zTP","recAlOHJhEy5nDwA6","recpFE7h6QhaEpNAQ"],"title":"Teachers-of-children","category":"Stakeholders","name":"recVHjstaBpfkQwvp","tags":[],"created_at":"2023-05-18T13:51:22.000Z","id":"recvhjstabpfkqwvp","dom_id":"item_recvhjstabpfkqwvp"},{"Stakeholder-impacted":["recKWQBBplr1vXmRc","reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5","recSXcY4cnofb4zTP","recAlOHJhEy5nDwA6","recpFE7h6QhaEpNAQ"],"title":"Learners-adults","category":"Stakeholders","name":"recdkIzRDcuurhTmM","tags":[],"created_at":"2023-06-05T14:49:46.000Z","id":"recdkizrdcuurhtmm","dom_id":"item_recdkizrdcuurhtmm"},{"Stakeholder-as-actor":["recpFE7h6QhaEpNAQ"],"Stakeholder-impacted":["reccWWRCdr9ZKJBZE","rec1RzTKH08fWjSbT","recKWQBBplr1vXmRc","reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5","recSXcY4cnofb4zTP","recAlOHJhEy5nDwA6","recpFE7h6QhaEpNAQ"],"title":"Learners-children","category":"Stakeholders","name":"reci4jOiaZbCd37eY","tags":[],"created_at":"2023-06-05T14:49:50.000Z","id":"reci4joiazbcd37ey","dom_id":"item_reci4joiazbcd37ey"},{"Stakeholder-as-actor":["recKWQBBplr1vXmRc","reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5","recSXcY4cnofb4zTP","recAlOHJhEy5nDwA6","recpFE7h6QhaEpNAQ"],"Stakeholder-impacted":["recMalmQowxcka9V6","recKWQBBplr1vXmRc","reciNqxyfUgE5XM7t","recrVkbG0XGe2Ca0v","recOOmVQviRyJGvea","recmS3zSMbR3ofAR5","recSXcY4cnofb4zTP","recAlOHJhEy5nDwA6","recpFE7h6QhaEpNAQ"],"title":"Teachers-of-adults","category":"Stakeholders","name":"reckhslKpo5ZlVNpl","tags":[],"created_at":"2023-05-18T13:51:22.000Z","id":"reckhslkpo5zlvnpl","dom_id":"item_reckhslkpo5zlvnpl"},{"Stakeholder-as-actor":["rec1RzTKH08fWjSbT","recSXcY4cnofb4zTP"],"Stakeholder-impacted":["reccWWRCdr9ZKJBZE"],"title":"Institutional-leaders","category":"Stakeholders","name":"recwbivinQr5lOP5r","tags":[],"created_at":"2023-05-18T13:51:22.000Z","id":"recwbivinqr5lop5r","dom_id":"item_recwbivinqr5lop5r"},{"Stakeholder-as-actor":["reccWWRCdr9ZKJBZE"],"title":"Researchers","category":"Stakeholders","name":"recz1WGGiX1qkZTxC","tags":[],"created_at":"2023-06-05T14:48:51.000Z","id":"recz1wggix1qkztxc","dom_id":"item_recz1wggix1qkztxc"}]));
  }
    </script>
  </body>
</html>
