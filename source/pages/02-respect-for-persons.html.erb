---
layout: postmd
permalink: "respect-for-persons"
title: Respect for Persons and Human Dignity
---

### Respect for Persons and Human Dignity <a id="respect-for-persons"></a>

A central tenet of ethics is [respect for the intrinsic value of persons](/recLHILkx2JDFsLbX), or human dignity in all interactions. Respect comprises:

1. [Autonomy](/reckdujfopjr4zahz)
1. [Protection of vulnerable populations](/reczvpih1y2ompajh)

Including:

- Regard for their welfare
- Consideration of the views, norms, and cultural context of individuals involved in research and the communities from which they are recruited (and not recruited)
- Respect for [privacy and confidentiality](/recpg7ov0priggtlm) of participants and their communities
- Consideration of the capacity of humans to make their own decisions, and empowering those who may have diminished capacity to make such decisions, with particular care towards these vulnerable populations

Autonomy is normally taken to comprise:

1. [informed consent](/recsvi4lnheepyq1h) - that participants may choose to freely participate or not, based on an understanding of that participation
1. [agency](/recu6u0azbcnj1ik9) - that people have the opportunity to participate in research, that their views are heard, respected, and fairly represented, both individually and with respect to cultural norms and values.

**Autonomy** is about having freedom from external control, being able to according to one's own values; it is fundamental to informed consent. **Agency** is about one's capacity to be heard and play an active role, recognising participants as active contributors to research.

#### Specific ethical concepts arising in the context of technology

<%= partial 'templates/components/callout', locals: { type: "alert", title: "here to edit", text: "check this, these are now drawn from the same table as the concepts in the introduction and so the thread should be clear." } %>

<!-- first get the keys of principle matching respect for persons -->
<% keys_principles = partial "templates/components/getkeys", locals: { my_filters: "(item[:category] == 'Principles') && (item[:title]&.match?(/respect for persons/i))" } %>

<!-- then get the ChallengeInstances of challenges with those keys, and matching AI -->
<% callout_content = partial "templates/components/includeset", locals: { my_filters: "item[:category] == 'ChallengeInstances' && (item[:tags].include? 'AI') && (item[:OverarchingPrinciples].is_a?(Array) ? item[:OverarchingPrinciples]&.compact&.any? { |x| x&.match?(/#{keys_principles}/i) } : item[:OverarchingPrinciples]&.match?(/#{keys_principles}/i))" } %>

<!-- and display them -->
<%= partial 'templates/components/callout', locals: { type: "information", title: "How is AI research different?", text: "#{callout_content}"} %>

#### And for education

<%= partial 'templates/components/callout', locals: { type: "alert", title: "here to edit", text: "check this, these are now drawn from the same table as the concepts in the introduction and so the thread should be clear" } %>

<!-- then get the ChallengeInstances of challenges with those keys, and matching tag -->
<% callout_content = partial "templates/components/includeset", locals: { my_filters: "item[:category] == 'ChallengeInstances' && (item[:tags].include? 'education') && (item[:OverarchingPrinciples].is_a?(Array) ? item[:OverarchingPrinciples]&.compact&.any? { |x| x&.match?(/#{keys_principles}/i) } : item[:OverarchingPrinciples]&.match?(/#{keys_principles}/i))" } %>

<!-- and display them -->
<%= partial 'templates/components/callout', locals: { type: "information", title: "How is education research different?", text: "#{callout_content}"} %>


Considerations regarding respect for persons may be distinctive in the context of AI. That is for two reasons:

1. **Interactions and distance between researcher-participant are impacted**, because tools may mediate this interaction, or research may be conducted in a way that is distal to human's interactions with the inputs or outputs of the research (e.g., distal to the original capture of the data in the context of gathering training data; or to the use of a tool in the context of training an algorithm to be deployed beyond the research project)

1. **Human control of technology** - through the inception, design, development, implementation, evaluation, and decommissioning - is central to considerations of autonomy, accountability, and human oversight of AI. That is because AI has the potential to diminish autonomy and thus influence democratic participation and human flourishing.

1. **Clarity or accountability regarding the purposes of technology, and its role in decision processes is required.** **Clarity** underpins responsible use of technology, and **informed engagement (and consent)** with it. **Accountability** of AI's use by humans, to humans underpins **respect for persons**. However, achieving clarity and accountability may be challenging where there are different levels of expertise regarding the technology, and understandings of the contexts into which technology may be deployed.

1. **Privacy and confidentiality** expectations are bound up in cultural norms, and other aspects of particular contexts, with emerging challenges regarding potential for **re-identification** of participant data. **Data quality, integrity, and governance** are a consideration of respect for persons. Data quality represents the reliability and validity of the data, key features in respecting persons through how we represent them. Data integrity represents the security of the data from manipulation or corruption, ensuring data can be used for its purpose.

1. **Respect for diversity, non-discrimination, and fairness** respect for persons includes respect for difference, non-discrimination, and fairness in treatment of persons with particular protection for vulnerable populations. This principle is particularly salient in the context of AI because models built on historic data may reflect existing societal biases in a way that makes them opaque or unaccountable. This principle includes that systems should provide for sakeholder participation and use regardless of their characteristics.

1. **Respect for human-human relationships, at individual and collective levels** AI has the potential to impact on how we connect and talk together, and the structure of societies. AI should avoid harms and promote pro-social engagement, recognising that both the benefits and harms of research (and broader engagement with technology) may accrue over groups.

A number of these concepts relate to both **proximal and indirect, long-range, and dual-use impacts**. Immediate or proximal impacts are those experienced in the immediate context, for example in interactions of researcher-participant groups. Indirect, long-range, and dual-use impacts relate to the potential for:

- secondary impacts that are not a direct use of a tool but may relate to system changes that occur as a result;
- that these impacts may occur over a longer period than the immediate context of research, which is ordinarily the context for which ethics approval is provided
- and that some effects may arise from unintended uses of dual-use technologies, for example facial recognition developed for one purpose may be used for surveillance or military purposes.

> "A/IS can be an enormous force for good in society provided they are designed to respect human rights, align with human values, and holistically increase well-being while empowering as many people as possible. They should also be designed to safeguard our environment and natural resources. These values should guide policy makers as well as engineers, designers, and developers. Advances in A/IS should be in the service of all people, rather than benefiting solely small groups, a single nation, or a corporation." [(IEEE, 2019, p.7)](/recswtdeek6xbegxu)

>“Human beings should remain free to make life decisions for themselves. This entails freedom from sovereign intrusion, but also requires intervention from government and non-governmental organisations to ensure that individuals or people at risk of exclusion have equal access to AI’s benefits and opportunities. In an AI context, freedom of the individual for instance requires mitigation of (in)direct illegitimate coercion, threats to mental autonomy and mental health, unjustified surveillance, deception and unfair manipulation. In fact, freedom of the individual means a commitment to enabling individuals to wield even higher control over their lives, including (among other rights) protection of the freedom to conduct a business, the freedom of the arts and science, freedom of expression, the right to private life and privacy, and freedom of assembly and association” [(High-Level Expert Group on AI, 2019, p. 10)](/recrbt2ksianfxfwe)

> “Humans interacting with AI systems must be able to keep full and effective self-determination over themselves, and be able to partake in the democratic process. AI systems should not unjustifiably subordinate, coerce, deceive, manipulate, condition or herd humans. Instead, they should be designed to augment, complement and empower human cognitive, social and cultural skills. The allocation of functions between humans and AI systems should follow human-centric design principles and leave meaningful opportunity for human choice. This means securing human oversight over work processes in AI systems. AI systems may also fundamentally change the work sphere. It should support humans in the working environment, and aim for the creation of meaningful work.” [(High-Level Expert Group on AI, 2019, p. 12)](/recqeiu22qy1e0yua)

> "Ensure their abilities to make free and informed decisions about their own lives. Safeguard their autonomy, their power to express themselves, and their right to be heard. Secure their capacities to make well-considered and independent contributions to the life of the community. Support their abilities to flourish, to fully develop themselves, and to pursue their passions and talents according to their own freely determined life plans” [(Leslie, 2019, p. 10)](/recimuz3t4idinp2b)

> "The programming, output, and purpose of A/IS are often not discernible by the general public. Based on the cultural context, application, and use of A/IS, people and institutions need clarity around the manufacture and deployment of these systems to establish responsibility and accountability, and to avoid potential harm. Additionally, manufacturers of these systems must be accountable in order to address legal issues of culpability. It should, if necessary, be possible to apportion culpability among responsible creators (designers and manufacturers) and operators to avoid confusion or fear within the general public.Accountability and partial accountability are not possible without transparency, thus this principle is closely linked with Principle 5–Transparency." [(IEEE, 2019, p.29-30)](/recwlipa3l9qtydws)

> "Digital consent is a misnomer in its current manifestation. Terms and conditions or privacy policies are largely designed to provide legally accurate information regarding the usage of people’s data to safeguard institutional and corporate interests, while often neglecting the needs of the people whose data they process. “Consent fatigue”, the constant request for agreement to sets of long and unreadable data handling conditions, causes a majority of users to simply click and accept terms in order to access the services they wish to use. General obfuscation regarding privacy policies, and scenarios like the Cambridge Analytica scandal in 2018, demonstrate that even when individuals provide consent, the understanding of the value regarding their data and its safety is out of an individual’s control.This existing model of data exchange has eroded human agency in the algorithmic age. People don’t know how their data is being used at all times or when predictive messaging is honoring their existing preferences or manipulating them to create new behaviors.Regulations like the EU General Data Protection Regulation (GDPR) will help improve this lack of clarity regarding the exchange of personal data. But compliance with existing models of consent is not enough to safeguard people’s agency regarding their personal information. In an era where A/IS are already pervasive in society, governments must recognize that limiting the misuse of personal data is not enough.Society must also recognize that human rights in the digital sphere don’t exist until individuals globally are empowered with means—including tools and policies—that ensure their dignity through some form of sovereignty, agency, symmetry, or control regarding their identity and personal data. These rights rely on individuals being able to make their choices, outside of the potential influence of biased algorithmic messaging or bad actors. Society also needs to be confident that those who are unable to provide legal informed consent, including minors and people with diminished capacity to make informed decisions, do not lose their dignity due to this." [(IEEE, 2019, p.25-26)](/recd6uflckfihl414)

##### Technical procedural ethics addressing respect for persons

<%= partial 'templates/components/callout', locals: { type: "alert", title: "to add here", text: "tbc" } %>

Explainability (all 4)

Transparency (all 4)
Interpretability (only Khan)

Data agency - suporting people to make decisions regarding data


<h5>This section draws on principles discussed in</h5>
<%= partial 'templates/components/cardset', locals: { my_filters: '(item[:title]&.match(/National Statement|Schiff|Khan|Jobin|Fjeld|Turing responsible design and implementation of AI systems|High-level expert group on AI|EC guidelines on AI in education for educators|^IEEE/i) || item[:Description]&.match(/Turing responsible design and implementation of AI systems|High-level expert group on AI|EC guidelines on AI in education for educators|^IEEE/i) ) && item[:category] == "Sources"' } %>


#### And in education

A set of particularly concerns arise in the context of research in education, the use of AI in education research (as a tool or object of research), and in applications of AI in education beyond research contexts.

- reinforcing or inventing bias
- incidental data capture (e.g., non-consenting students in background)
- informed consent with blackb-x models and classroom power relationships
-

“Accountability including auditability, minimisation and reporting of negative impact, trade-offs, and redress. These considerations and requirements can help educators, school leaders and technology providers to adequately assess the impact, address the potential risks, and realise the benefits of an AI system deployed and used in education. As such they guide the development, deployment and use of trustworthy AI systems.” [(European Commission, 2022, p. 19)](/reczcrrifbqqpn8ix)


##### This section draws on

<%= partial 'templates/components/cardset', locals: { my_filters: '(item[:title]&.match(/Turing responsible|national statement|^IEEE|EC guidelines on AI in education for educators|ethics in the scholarship of teaching and learning|markham/i) || item[:Description]&.match(/Turing responsible|national statement|Autonomous and intelligent technical systems are|EC guidelines on AI in education for educators|ethics in the scholarship of teaching and learning|markham/i) ) && item[:category] == "Sources"' } %>

<%= link_to "Previous section", "#respect-for-persons", class: "index-link" %>

<h3>Particular Challenges, Cases, and Strategies</h3>

These are strategies that relate to respect for persons, autonomy, informed consent, agency, and protection of vulnerable persons.

<!-- this gets the keys (name/url) of principles with those titles -->
<% keys_content = partial "templates/components/getkeys", locals: { my_filters: "item[:title]&.match?(/Respect for persons|Autonomy|informed consent|agency|protection of vulnerable persons/i) && item[:category] == 'Principles'" } %>

<!-- this then uses those keys, to show strategies that have that principle assigned -->
<%= partial "templates/components/cardset", locals: { my_filters: "item[:category] == 'Strategies' && (item[:Principles].is_a?(Array) ? item[:Principles]&.compact&.any? { |principle| principle&.match?(/#{keys_content}/i) } : item[:Principles]&.match?(/#{keys_content}/i))" } %>

