- Description: "**\"Issue:** Vastly different power structures among and within countries\
    \ create risk that A/IS deployment accelerates, rather than reduces, inequality\
    \ in the pursuit of a sustainable future. It is unclear how LMIC can best implement\
    \ A/IS via existing resources and take full advantage of the technology\u2019\
    s potential to achieve a sustainable future.\n## Background\nThe potential use\
    \ of A/IS to create sustainable economic growth for LMIC is uniquely powerful.\
    \ Yet, many of the debates surrounding A/IS take place within HIC, among highly\
    \ educated and financially secure individuals. It is imperative that all humans,\
    \ in any condition around the world, are considered in the general development\
    \ and application of these systems to avoid the risk of bias, excessive inequality,\
    \ classism, and general rejection of these technologies. With much of the financial\
    \ and technical resources for A/IS development and deployment residing in HIC,\
    \ not only are A/IS benefits more difficult to access for LMIC populations, but\
    \ those A/IS applications that are_ _deployed outside of HIC realities may not\
    \ be appropriate. This is for reasons of cultural/ethnic bias, language difficulties,\
    \ or simply an inability to adapt to local internet infrastructure constraints.\n\
    Furthermore, technological innovation in LMIC comes up against many potential\
    \ obstacles, which could be considered when undertaking initiatives aimed at enhancing\
    \ LMIC access:\n\u2022\_\_\_\_\_Reluctance to provide open source licensing of\
    \ technological development innovations,\n\u2022\_\_\_\_\_Lack of the human capital\
    \ and knowledge required to adapt HIC-developed technologies to resolving problems\
    \ in the LMIC context, or to develop local technological solutions to these problems,\n\
    \u2022\_\_\_\_\_Retention of A/IS capacity in LMIC due to globally uncompetitive\
    \ salaries,\n\u2022\_\_\_\_\_Lack of infrastructure for deployment, and difficulties\
    \ in taking technological solutions to where they are needed,\n\u2022\_\_\_\_\_\
    Lack of organizational and business models for adapting technologies to the specific\
    \ needs of different regions,\n\u2022\_\_\_\_\_Lack of active participation of\
    \ the target population,\n\u2022\_\_\_\_\_Lack of political will to allow people\
    \ to have access to technological resources,\n\u2022\_\_\_\_\_Existence of oligopolies\
    \ that hinder new technological development,\n\u2022\_\_\_\_\_Lack of inclusive\
    \ and high-quality education at all levels, and\n\u2022\_\_\_\_\_Bureaucratic\
    \ policies ill-adapted to highly dynamic scenarios.\nFor A/IS capacities and benefits\
    \ to become equally available worldwide, training, education, and opportunities\
    \ should be provided particularly for LMIC. Currently, access to products that\
    \ facilitate A/IS research of timely topics is quite limited for researchers in\
    \ LMIC, due to cost considerations.\nIf A/IS capacity and governance problems,\
    \ such as relevant laws, policies, regulations, and anticorruption safeguards,\
    \ are addressed, LMIC could have the ability to use A/IS to transform their economies\
    \ and leapfrog into a new era of inclusive growth. Indeed, A/IS itself can contribute\
    \ to good governance when applied to the detection of corruption in state and\
    \ banking institutions, one of the most serious recognized constraints to investment\
    \ in LMIC. Particular attention, however, must be paid to ensure that the use\
    \ of A/IS is for the common good\u2014especially in the context of LMIC\u2014\
    and does not reinforce existing socioeconomic inequities through systematic discriminatory\
    \ bias in both design and application, or undermine fundamental rights through,\
    \ among other issues, lax data privacy laws and practice.\n## Recommendations\n\
    A/IS benefits should be equally available to populations in HIC and LMIC, in the\
    \ interest of universal human dignity, peace, prosperity, and planet protection.\
    \ Specific measures for LMIC should include:\n\u2022\_\_\_\_\_ Deploying A/IS\
    \ to detect fraud and corruption, to increase the transparency of power structures,\
    \ to contribute to a favorable investment, governance, and innovation environment.\
    \ \u2022\_\_\_\_\_Supporting LMIC in the development of their own A/IS strategies,\
    \ and in the retention or return of their A/IS talent to prevent \u201Cbrain drain\u201D\
    .\n\u2022\_\_\_\_\_Encouraging global standardization/ harmonization and open\
    \ source A/IS software.\n\u2022\_\_\_\_\_Promoting distribution of knowledge and\
    \ wealth generated by the latest A/IS, including through formal public policy\
    \ and financial mechanisms to advance equity worldwide.\n\u2022\_\_\_\_\_Developing\
    \ public datasets to facilitate the access of people from LMIC to data resources\
    \ to facilitate their applied research, while ensuring the protection of personal\
    \ data.\n\u2022\_\_\_\_\_Creating A/IS international research centers in every\
    \ continent, that promote culturally appropriate research, and allow the remote\
    \ access of LMIC's communities to high-end technology.16\n\u2022\_\_\_\_\_Facilitating\
    \ A/IS access in LMIC through online courses in local languages.\n\u2022\_\_\_\
    \_\_Ensuring that, along with the use of A/IS, discussions related to identity,\
    \ platforms, and blockchain are conducted, such that core enabling technologies\
    \ are designed to meet the economic, social, and cultural needs of LMIC.\n\u2022\
    \_\_\_\_\_Diminishing the barriers and increase LMIC access to technological products,\
    \ including the formation of collaborative networks between developers in HIC\
    \ and LMIC, supporting the latter in attending global A/IS conferences.17\n\u2022\
    \_\_\_\_\_Promoting research into A/IS-based technologies, for example, mobile\
    \ lightweight A/IS applications, that are readily available\_in LMIC.\n\u2022\_\
    \_\_\_\_Facilitating A/IS research and development in\nLMIC through investment\
    \ incentives, public-\nprivate partnerships, and/or joint grants, and collaboration\
    \ between international organizations, government bodies, universities, and research\
    \ institutes.\n\u2022\_\_\_\_\_Prioritizing A/IS infrastructure in international\
    \ development assistance, as necessary to improve the quality and standard of\
    \ living and advance progress towards the SDGs in LMIC.\n\u2022\_\_\_\_\_Recognizing\
    \ data issues that may be particular to LMIC contexts, i.e., insufficient sample\
    \ size for machine learning which sometimes results in _de facto_ discrimination,\
    \ and inadequate laws for, and the practice of, data protection. \_\n\u2022\_\_\
    \_\_\_Supporting research on the adaptation of\_A/IS methods to scarce data environments\_\
    and other remedies that facilitate an optimal\_A/IS enabling environment in LMIC.\n\
    ## Further Resources\n\u2022\_\_\_\_\_A. Akubue, \u201CAppropriate Technology\
    \ for Socioeconomic Development in Third World Countries.\u201D _The Journal of\
    \ Technology Studies _26, no. 1, pp. 33\u201343, 2000. \n\u2022\_\_\_\_\_O. Ajakaiye\
    \ and M. S. Kimenyi. \u201CHigher Education and Economic Development in Africa:\
    \ Introduction and Overview.\u201D _Journal of African Economies _20, no. 3, iii3\u2013\
    iii13, 2011.\n\u2022\_\_\_\_\_D. Allison-Hope and M. Hodge, \"Artificial Intelligence:\
    \ A Rights-Based Blueprint for Business,\u201D San Francisco: BSF, Aug. 28, 2018\n\
    \u2022\_\_\_\_\_D. E. Bloom, D. Canning, and K. Chan. _Higher Education and Economic\
    \ Development in Africa _(Vol. 102). Washington, DC: World Bank, 2006.\n\u2022\
    \_\_\_\_\_N. Bloom, \u201CCorporations in the Age of Inequality.\u201D _Harvard\
    \ Business Review, _April 21, 2017.\n\u2022\_\_\_\_\_C. Dahlman, _Technology,\
    \ Globalization, and Competitiveness: Challenges for Developing Countries. Industrialization\
    \ in the 21st Century_. New York: United Nations, 2006.\u2022\_\_\_\_\_M. Fong,\
    \ _Technology_ _Leapfrogging for Developing Countries. Encyclopedia of Information\
    \ Science and Technology_, 2nd ed. Hershey, PA: IGI Global, 2009 (pp. 3707\u2013\
    \ 3713).\n\u2022\_\_\_\_\_C. B. Frey and M. A. Osborne. \u201CThe Future of Employment:\
    \ How Susceptible Are Jobs to Computerisation?\u201D (working paper). Oxford,\
    \ U.K.: Oxford University, 2013.\n\u2022\_\_\_\_\_B. Hazeltine and C. Bull. _Appropriate\
    \ Technology: Tools, Choices, and Implications. _New York: Academic Press, 1999.\n\
    \u2022\_\_\_\_\_McKinsey Global Institute. \u201CDisruptive Technologies: Advances\
    \ That Will Transform Life, Business, and the Global Economy\u201D (report), May\
    \ 2013.\n\u2022\_\_\_\_\_D. Rotman, \u201CHow Technology Is Destroying Jobs.\u201D\
    \ _MIT Technology Review_, June 12, 2013.\n\u2022\_\_\_\_\_R. Sauter and J. Watson.\
    \ \u201CTechnology Leapfrogging: A Review of the Evidence, A Report for DFID.\u201D\
    \ Brighton, England: University of Sussex. October 3, 2008.\n\u2022\_\_\_\_\_\u201C\
    The Rich and the Rest.\u201D _The Economist. _October 13, 2012.\n\u2022\_\_\_\_\
    \_\u201CWealth without Workers, Workers without Wealth.\u201D _The Economist_.\
    \ October 4, 2014.\n\u2022\_\_\_\_\_World Bank. \u201CGlobal Economic Prospects\
    \ 2008: Technology Diffusion in the Developing World.\u201D Washington, DC: World\
    \ Bank, 2008.\n\u2022\_\_\_\_\_World Development Report 2016: Digital Dividends_.\
    \ _Washington, DC: World Bank. doi:10.1596/978-1-4648-0671-1.\n\u2022\_\_\_\_\_\
    World Wide Web Foundation \u201CArtificial Intelligence: The Road ahead in Low\
    \ and Middle-income Countries,\u201D webfoundation.org, June 2017.\"\n\np.145-147\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recnLauSoXMQCSK5c
  airtable_createdTime: '2023-06-05T11:18:00.000Z'
  airtable_id: rec0BAjUqQrSIMAEG
  title: Inequities in access to AI may increase, not tackle, inequality
- Description: "\"Increased awareness and application of well-being metrics by A/IS\
    \ creators can create greater value, safety, and relevance to corporate communities\
    \ and other organizations in the algorithmic age.\" \n\n## \"Background\nWhile\
    \ many organizations in the private and public sectors are increasingly aware\
    \ of the need to incorporate well-being measures as part of their efforts, the\
    \ reality is that bottom line, quarterly-driven shareholder growth remains a dominant\
    \ goal and metric. Short term growth is often the priority in the private sector\
    \ and public sector. As long as organizations exist in a larger societal system\
    \ which prioritizes financial success, these companies will remain under pressure\
    \ to deliver financial results that do not fully incorporate societal and environmental\
    \ impacts, measurements, or priorities.\nRather than focus solely on the negative\
    \ aspects of how A/IS could harm humans and environments, we seek to explore how\
    \ the implementation of well-being metrics can help A/IS to have a measurable,\
    \ positive impact on human well-being as well as on systems and organizations.\
    \ Incorporation of well-being goals and measures beyond what is strictly required\
    \ can benefit both private sector organizations\u2019 brands and public sector\
    \ organizations\u2019 stability and reputation, as well as help realize financial\
    \ savings, innovation, trust, and many other benefits. For instance, a companion\
    \ robot outfitted to support seniors in assisted living situations might traditionally\
    \ be launched with a technology development model that was popularized by Silicon\
    \ Valley known as \u201Cmove fast and break things\u201D. The A/IS creator who\
    \ rushed to bring the robot to market faster than the competition and who was\
    \ unaware of well-being metrics, may have overlooked critical needs of the seniors.\
    \ The robot might actually hurt the senior instead of helping by exacerbating\
    \ isolation or feelings of loneliness and helplessness. While this is a hypothetical\
    \ scenario, it is intended to demonstrate the value of linking A/IS design to\
    \ well-being indicators.\nBy prioritizing largely fiscal metrics of success,\_\
    A/IS devices might fail in the market because of limited adoption and subpar reception.\
    \ However, if during use of the A/IS product, success were measured in terms of\
    \ relevant aspects of wellbeing, developers and researchers could be in a better\
    \ position to attain funding and public support. Depending on the intended use\
    \ of the A/IS product, well-being measures that could be used extend to emotional\
    \ levels of calm or stress; psychological states of thriving or depression; behavioral\
    \ patterns of engagement in community or isolation; eating, exercise and consumption\
    \ habits; and many other aspects of human well-being. The A/IS product could significantly\
    \ improve quality of life guided by metrics from trusted sources, such as the\
    \ [World Health Organization,](http://www.who.int/en/) [European Social Survey,](https://www.europeansocialsurvey.org/)\_\
    and [Sustainable Development Goal Indicators.](https://unstats.un.org/sdgs/)\n\
    Thought leaders in the corporate arena have recognized the multifaceted need to\
    \ utilize metrics beyond fiscal indicators. PricewaterhouseCoopers defines \u201C\
    [total impact\u201D ](https://www.pwc.com/gx/en/services/sustainability/total-impact-measurement-management/measuring-and-managing-total-impact-a-new-language-for-business-decisions.html)as\
    \ a \u201Cholistic view of social, environmental, fiscal and economic dimensions\u2014\
    the big picture\u201D. Other thought-leading organizations in the public sector,\
    \ such as the OECD, demonstrate the desire for business leaders to incorporate\
    \ metrics of success beyond fiscal indicators for their efforts, exemplified in\
    \ their 2017 workshop, [Measuring Business Impacts on People\u2019s WellBeing.](http://www.oecd.org/statistics/Biz4WB-Highlights-OECD.pdf)\
    \ The [B-Corporation movement h](https://www.bcorporation.net/)as created a new\
    \ legal status for \u201Ca new type of company that uses the power of business\
    \ to solve social and environmental problems\u201D. Focusing on increasing stakeholder\
    \ value versus shareholder returns alone, B-Corps are defining their brands by\
    \ provably aligning their efforts with wider measures of well-being.\n**\_**\n\
    ## Recommendations\nA/IS creators should work to better understand and apply well-being\
    \ metrics in the algorithmic age. Specifically:\n\u2022\_\_\_\_\_A/IS creators\
    \ should work directly with experts, researchers, and practitioners in wellbeing\
    \ concepts and metrics to identify existing metrics and combinations of indicators\
    \ that would bring support a \u201Ctriple bottom line\u201D, i.e., accounting\
    \ for economic, social, and environmental impacts, approach to wellbeing. However,\
    \ well-being metrics should only be used with consent, respect for privacy, and\
    \ with strict standards for collection and use of these data.\n\u2022\_\_\_\_\_\
    For A/IS to promote human well-being, the well-being metrics should be chosen\
    \ in collaboration with the populations most affected by those systems\u2014the\
    \ A/IS stakeholders\u2014including both the intended end-users or beneficiaries\
    \ and those groups whose lives might be unintentionally transformed by them. This\
    \ selection process should be iterative and through a learning\_and continually\
    \ improving process. In addition, \u201Cmetrics of well-being\u201D should be\
    \ treated as vehicles for learning and potential mid- course corrections. The\
    \ effects of A/IS on human well-being should be monitored continuously throughout\
    \ their life cycles, by\_A/IS creators and stakeholders, and both A/IS creators\
    \ and stakeholders should be prepared to significantly modify, or even roll back,\
    \ technology that is shown to reduce well-being, as defined by affected populations.\n\
    \u2022\_\_\_\_\_A/IS creators in the business or academic, engineering, or policy\
    \ arenas are advised to review the additional resources on standards development\
    \ models and frameworks at the end of this chapter to familiarize themselves with\
    \ existing indicators relevant to their work.\n## Further Resources\n\u2022\_\_\
    \_\_\_PricewaterhouseCoopers (PwC). [Managing and Measuring Total Impact: A New\
    \ Language for Business Decisions](https://www.pwc.com/gx/en/services/sustainability/total-impact-measurement-management/measuring-and-managing-total-impact-a-new-language-for-business-decisions.html),\
    \ 2017.\n\u2022\_\_\_\_\_World Economic Forum. [The Inclusive Growth and Development\
    \ Report 2017](https://www.weforum.org/reports/the-inclusive-growth-and-development-report-2017),\
    \ Geneva, Switzerland: World Economic Forum, January 16, 2017.\n\u2022\_\_\_\_\
    \_[OECD Guidelines on Measuring Subjective Well-being,](http://www.oecd.org/statistics/oecd-guidelines-on-measuring-subjective-well-being-9789264191655-en.htm)\
    \ 2013.\n\u2022\_\_\_\_\_ National Research Council. [Subjective WellBeing: Measuring\
    \ Happiness, Suffering, and Other Dimensions of Experience. D](https://www.nap.edu/catalog/18548/subjective-well-being-measuring-happiness-suffering-and-other-dimensions-of)C:\
    \ The National Academies Press, 2013.\"\np73-74\n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  - recOHnq45Fq7YWsRO
  Principles:
  - receFm7cGasHwpJZO
  - recint2IxoR8aILCp
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recZI7HDWKBU5T22v
  Tags:
  - wellbeing
  airtable_createdTime: '2023-06-05T05:40:06.000Z'
  airtable_id: rec1QZHHMARBQcZoo
  title: How do we foster AI to support wellbeing in context of short-term growth
    priorities?
- Description: "Are moral and ethical boundaries crossed when the design of affective\
    \ systems allows them to develop intimate relationships with their users?\n##\
    \ Background\n There are many robots in development or production designed to\
    \ focus on intimate care of children, adults, and the elderly2. While robots capable\
    \ of participating fully in intimate relationships are not currently available,\
    \ the potential use of such robots routinely captures the attention of the media.\
    \ It is important that professional communities, policy makers, and the general\
    \ public participate in development of guidelines for appropriate use of A/IS\
    \ in this area. Those guidelines should acknowledge fundamental human rights to\
    \ highlight potential ethical benefits and risks that may emerge, if\_and when\
    \ affective systems interact intimately with users. Among the many areas of concern\
    \ are the representation of care, embodiment of caring\_A/IS, and the sensitivity\
    \ of data generated through intimate and caring relationships with\_A/IS. The\
    \ literature suggests that there are some potential benefits to individuals and\
    \ to society from the incorporation of caring A/IS, along with duly cautionary\
    \ notes concerning the possibility that these systems could negatively impact\
    \ human-to-human intimate relations3.\n## Recommendations\nAs this technology\
    \ develops, it is important to monitor research into the development of intimate\
    \ relationships between A/IS and humans. Research should emphasize any technical\
    \ and normative developments that reflect use of\_A/IS in positive and therapeutic\
    \ ways while also creating appropriate safeguards to mitigate against uses that\
    \ contribute to problematic individual or social relationships:\n1\\.\_\_\_Intimate\
    \ systems must not be designed or deployed in ways that contribute to stereotypes,\
    \ gender or racial inequality,\_or the exacerbation of human misery.\n2\\.\_\_\
    \_Intimate systems must not be designed to explicitly engage in the psychological\
    \ manipulation of the users of these systems unless the user is made aware they\
    \ are being manipulated and consents to this behavior. Any manipulation should\
    \ be governed\_through an opt-in system.\n3\\.\_\_\_Caring A/IS should be designed\
    \ to avoid contributing to user isolation from society.\_\n4\\.\_\_\_Designers\
    \ of affective robotics must publicly acknowledge, for example, within a notice\
    \ associated with the product, that these systems can have side effects, such\
    \ as interfering with the relationship dynamics between human partners, causing\
    \ attachments between the user and the A/IS that are distinct from human partnership.\n\
    5\\.\_\_Commercially marketed A/IS for caring applications should not be presented\
    \ to be a person in a legal sense, nor marketed as a person. Rather its artifactual,\
    \ that is, authored, designed, and built deliberately, nature should always be\
    \ made as transparent as possible, at least at point of sale and in available\
    \ documentation, as noted in Section 4, Systems Supporting Human Potential.6.\_\
    \_\_Existing laws regarding personal imagery need to be reconsidered in light\
    \ of caring A/IS.\_In addition to other ethical considerations, it will also be\
    \ necessary to establish conformance with local laws and mores in the context\
    \ of caring A/IS systems.\n## Further Resources\n\u2022\_\_\_\_\_M. Boden, J.\
    \ Bryson, D. Caldwell, K. Dautenhahn, L. Edwards, S. Kember, P.\nNewman, V. Parry,\
    \ G. Pegman, T. Rodden and T. Sorrell, Principles of robotics: regulating robots\
    \ in the real world. Connection Science, vol. 29, no. 2, pp. 124-129, April 2017.\n\
    \u2022\_\_\_\_\_J. J. Bryson, M. E. Diamantis, and T. D. Grant, \u201COf, For,\
    \ and By the People: The Legal Lacuna of Synthetic Persons.\u201D _Artificial\
    \ Intelligence & Law_, vol. 25, no. 3, pp. 273\u2013291, Sept. 2017.\n\u2022\_\
    \_\_\_\_M. Scheutz, \u201CThe Inherent Dangers of\nUnidirectional Emotional Bonds\
    \ between Humans and Social Robots,\u201D in _Robot Ethics: The Ethical and Social\
    \ Implications of Robotics,_ P. Lin, K. Abney, and G. Bekey, Eds., pp. 205. Cambridge,\
    \ MA: MIT Press, 2011.\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - rec5CFheImo8onTcd
  Tags:
  - affective-computing
  airtable_createdTime: '2023-06-05T06:05:11.000Z'
  airtable_id: rec2ULcHUjSbbSnHL
  title: How might AI deployed in care settings to foster intimate relationships impact
    on relationships among humans?
- Description: "\"Should affective systems be designed to nudge people\_for the user\u2019\
    s personal benefit and/or for the benefit of others?\n## Background\nManipulation\
    \ can be defined as an exercise of influence by one person or group, with the\
    \ intention to attempt to control or modify the actions of another person or group.\
    \ Thaler and Sunstein (2008) call the tactic of subtly modifying behavior a \u201C\
    nudge4\u201D. Nudging mainly operates through the affective elements of a human\
    \ rational system. Making use of a nudge might be considered appropriate in situations\
    \ like teaching children, treating drug dependency, and in some healthcare settings.\
    \ While nudges can be deployed to encourage individuals to express behaviors that\
    \ have community benefits, a nudge could have unanticipated consequences for people\
    \ whose backgrounds were not well considered in the development of the nudging\
    \ system5. Likewise, nudges may encourage behaviors with unanticipated long-term\
    \ effects, whether positive or negative, for the\_individual and/or society. The\
    \ effect of\_A/IS nudging a person, such as potentially eroding or encouraging\
    \ individual liberty, or expressing behaviors that are for the benefit others,\
    \ should be well characterized in the design of A/IS.\n## Recommendations\n1\\\
    .\_\_\_Systematic analyses are needed that examine the ethics and behavioral consequences\
    \ of designing affective systems to nudge human beings prior to deployment.\n\
    2\\.\_\_\_The user should be empowered, through an explicit opt-in system and\
    \ readily available, comprehensible information, to recognize different types\
    \ of A/IS nudges, regardless of whether they seek to promote beneficial social\
    \ manipulation or to enhance consumer acceptance of commercial goals. The user\
    \ should be able to access and check facts behind the nudges and then make a conscious\
    \ decision to accept or reject a nudge. Nudging systems must be transparent, with\
    \ a clear chain of accountability that includes human agents: data logging is\
    \ required so users can know how, why, and by whom they were nudged.\n3\\.\_\_\
    \_A/IS nudging must not become coercive and should always have an opt-in system\
    \ policy with explicit consent.\_\n4\\.\_\_\_Additional protections against unwanted\
    \ nudging must be put in place for vulnerable populations, such as children, or\
    \ when informed consent cannot be obtained. Protections against unwanted nudging\
    \ should be encouraged when nudges alter long-term behavior or when consent alone\
    \ may not be\_a sufficient safeguard against coercion\_or exploitation.\n5\\.\_\
    \_\_Data gathered which could reveal an individual or groups\u2019 susceptibility\
    \ to a nudge or their emotional reaction to a nudge should not be collected or\
    \ distributed without opt-in consent, and should only be retained transparently,\
    \ with access restrictions in compliance with the highest requirements of data\
    \ privacy and law.\n## Further Resources\n\u2022\_\_\_\_\_R. Thaler, and C. R.\
    \ Sunstein, _Nudge: Improving Decision about Health, Wealth and Happiness_, New\
    \ Haven, CT: Yale University Press, 2008.\n\u2022\_\_\_\_\_L. Bovens, \u201CThe\
    \ Ethics of Nudge,\u201D in _Preference change: Approaches from Philosophy, Economics\
    \ and Psychology_, T. Gr\xFCne-Yanoff and S. O. Hansson, Eds., Berlin, Germany:\
    \ Springer, 2008 pp. 207\u2013219.\n\u2022\_\_\_\_\_S. D. Hunt and S. Vitell.\
    \ \"A General Theory of Marketing Ethics.\" Journal of Macromarketing, vol.6,\
    \ no. 1, pp. 5-16, June 1986.\n\u2022\_\_\_\_\_A. McStay, [Empathic Media and\
    \ Advertising: Industry, Policy, Legal and Citizen Perspectives (the Case for\
    \ Intimacy)](http://journals.sagepub.com/doi/pdf/10.1177/2053951716666868), Big\
    \ Data & Society, pp. 1-11, December 2016.\n\u2022\_\_\_\_\_J. de Quintana Medina\
    \ and P. Hermida Justo, \u201CNot All Nudges Are Automatic: Freedom of Choice\
    \ and Informative Nudges.\u201D Working paper presented to the European Consortium\
    \ for Political Research, Joint Session of Workshops, 2016 Behavioral Change and\
    \ Public Policy, Pisa, Italy, 2016.\n\u2022\_\_\_\_\_ M. D. White, _[The Manipulation\
    \ of Choice. Ethics and Libertarian Paternalism. ](http://www.palgraveconnect.com/doifinder/10.1057/9781137313577)_New\
    \ York: Palgrave Macmillan, 2013\n\u2022\_\_\_\_\_C.R. Sunstein, The Ethics of\
    \ Influence: Government in the Age of Behavioral Science. New York: Cambridge,\
    \ 2016\n\u2022\_\_\_\_\_M. Scheutz, \u201C[The Affect Dilemma for Artificial Agents:\
    \ Should We Develop Affective Artificial Agents? ](http://ieeexplore.ieee.org/document/6296668/)\u201D\
    \ _IEEE Transactions on Affective Computing, _vol._ _3, no. 4,pp. 424\u2013433,\_\
    Sept. 2012.\n\u2022\_\_\_\_\_A. Grinbaum, R. Chatila, L. Devillers, J.G. Ganascia,\
    \ C. Tessier and M. Dauchet. \u201C[Ethics in Robotics Research: CERNA Recommendations,](http://ieeexplore.ieee.org/document/7822928/)\u201D\
    \ _IEEE Robotics and Automation Magazine, _vol._ _24, no. 3,pp. 139\u2013145,\
    \ Sept. 2017.\n\u201CDesigning Moral Technologies: Theoretical, Practical, and\
    \ Ethical Issues\u201D Conference July 10\u201315, 2016, Monte Verit\xE0, Switzerland\"\
    \n\np.96-97\n\n\"Governmental entities may potentially use nudging strategies,\
    \ for example to promote the performance of charitable acts. Does the practice\
    \ of nudging for the benefit\_of society, including nudges\_by affective systems,\
    \ raise\_ethical concerns?\n## Background\nA few scholars have noted a potentially\
    \ controversial practice of the future: allowing a robot or another affective\
    \ system to nudge a user for the good of society6. For instance, if it is possible\
    \ that a well-designed robot could effectively encourage humans to perform charitable\
    \ acts, would it be ethically appropriate for the robot to do so? This design\
    \ possibility illustrates just one behavioral outcome that a robot could potentially\
    \ elicit from a user.\nGiven the persuasive power that an affective system may\
    \ have over a user, ethical concerns related to nudging must be examined. This\
    \ includes the significant potential for misuse.\n## Recommendations\n1\\.\_\_\
    \_As more and more computing devices subtly and overtly influence human behavior,\
    \ it is important to draw attention to whether it is ethically appropriate to\
    \ pursue this type of design pathway in the context of governmental actions.\n\
    2\\.\_\_\_ There needs to be transparency regarding who the intended beneficiaries\
    \ are, and whether any form of deception or manipulation is going to be used to\
    \ accomplish the intended goal.\n## Further Resources\n\u2022\_\_\_\_\_J. Borenstein\
    \ and R. Arkin, \u201C[Robotic Nudges: Robotic Nudges: The Ethics of Engineering\
    \ a More Socially Just Human Being Just Human Being.](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)\u201D\
    \ _Science and Engineering Ethics, _vol._ _22, no. 1,pp. 31\u201346, Feb. 2016.\n\
    \u2022\_\_\_\_\_J. Borenstein and R. Arkin. \u201C[Nudging for Good: Robots and\
    \ the Ethical Appropriateness of Nurturing Empathy and Charitable Behavior ](https://link.springer.com/article/10.1007/s00146-016-0684-1?no-access=true).\u201D\
    \ _AI and Society, _vol._ _32, no. 4, pp. 499\u2013507, Nov. 2016.\"\np.97-98\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - rec9TbOu2ZXUZG4A5
  Tags:
  - nudge-deception
  airtable_createdTime: '2023-06-05T06:13:01.000Z'
  airtable_id: rec2ajglpzbFYRivi
  title: Should affective AI nudge users for personal or societal benefit?
- Description: "\"Will extensive use of\_A/IS in society make our organizations more\
    \ brittle by reducing human autonomy within organizations, and by replacing creative,\
    \ affective, empathetic components\_of management chains?\n## Background\nIf human\
    \ workers are replaced by A/IS, the possibility of corporations, governments,\
    \ employees, and customers discovering new equilibria outside the scope of what\
    \ the organizations\u2019 past leadership originally foresaw may be unduly limited.\
    \ A lack of empathy based on shared needs, abilities, and disadvantages between\
    \ organizations and customers causes disequilibria between the individuals and\
    \ corporations and governments that exist to serve them. Opportunities for useful\
    \ innovation may therefore be lost through automation. Collaboration requires\
    \ enough commonality\_of collaborating intelligences to create empathy\u2014 the\
    \ capacity to model the other\u2019s goals based\_on one\u2019s own.\nAccording\
    \ to scientists within several fields, autonomy is a psychological need. Without\
    \ it, humans fail to thrive, create, and innovate.\nEthically aligned design should\
    \ support, not hinder, human autonomy or its expression.\n## Recommendations\n\
    1\\.\_\_\_It is important that human workers\u2019 interaction with other workers\
    \ not always be intermediated by affective systems (or other technology) which\
    \ may filter out autonomy, innovation, and communication.\n2\\.\_\_\_Human points\
    \ of contact should remain available to customers and other organizations when\
    \ using A/IS.\n3\\.\_\_\_Affective systems should be designed to support human\
    \ autonomy, sense of competence, and meaningful relationships as these are necessary\
    \ to support a flourishing life.\n4\\.\_\_\_Even where A/IS are less expensive,\
    \ more predictable, and easier to control than human employees, a core network\
    \ of human employees should be maintained at every level of decision-making in\
    \ order to ensure preservation of human autonomy, communication, and innovation.\n\
    5\\.\_\_\_Management and organizational theorists should consider appropriate\
    \ use of affective and autonomous systems to enhance their business models and\
    \ the efficacy of their workforce within the limits of the preservation of human\
    \ autonomy.\n## Further reading\n\u2022\_\_\_\_\_J. J. Bryson, \u201CArtificial\
    \ Intelligence and Pro-Social Behavior,\u201D in _Collective Agency and Cooperation\
    \ in Natural and Artificial Systems, _C. Misselhorn, Ed., pp. 281\u2013306, Springer,\
    \ 2015.\n\u2022\_\_\_\_\_D. Peters, R.A. Calvo, and R.M. Ryan,\u201C[Designing\
    \ for Motivation, Engagement and  Wellbeing in Digital Experience](https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00797/full),\u201D\
    _ Frontiers in Psychology_\u2013 Human Media Interaction, vol. 9, pp 797, 2018.\"\
    \"\n\np.100-101\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recrBZOfrDC2lKpRM
  Tags:
  - human-potential
  airtable_createdTime: '2023-06-05T06:13:18.000Z'
  airtable_id: rec3Fm8dyG49YU137
  title: Will AI reduce autonomy through reducing creative, affective, and empathetic
    elements of management?
- Cases:
  - reciNqxyfUgE5XM7t
  - recrVkbG0XGe2Ca0v
  - recOOmVQviRyJGvea
  - recmS3zSMbR3ofAR5
  - recpFE7h6QhaEpNAQ
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  Principles:
  - recLHILkx2JDFsLbX
  airtable_createdTime: '2023-05-18T14:51:53.000Z'
  airtable_id: rec5VzvnIZcaNKqDh
  title: Tyranny of averages
- Description: "\"**Issue: **Decision processes for determining relevant well-being\
    \ indicators through stakeholder deliberations need to be established.\n## Background\n\
    A/IS stakeholder involvement is necessary to determine relevant well-being indicators,\
    \ for a number of reasons:\n\u2022\_\_\_\_\_\u201CWell-being\u201D will be defined\
    \ differently by different groups affected by A/IS. The most relevant indicators\
    \ of well-being may vary according to country, with concerns of wealthy nations\
    \ being different than those of low- and middle-income countries. Indicators may\
    \ vary based on geographical region or unique circumstances. The indicators may\
    \ also be different across social groups, including gender, race, ethnicity, and\
    \ disability status.\n\u2022\_\_\_\_\_Common indicators of well-being include\
    \ satisfaction with life, healthy life expectancy, government, social support,\
    \ perceived freedom to make life decisions, income equality, access to education,\
    \ and poverty rates. Applying them in particular settings necessarily requires\
    \ judgment, to ensure that assessments of well-being are in fact meaningful in\
    \ context and reflective of the life circumstances of the diverse groups in question.\_\
    \n\u2022\_\_\_\_\_Not all aspects of well-being are easily quantifiable. The importance\
    \ of hard-to-quantify aspects of well-being is most likely to become apparent\
    \ through interaction with those more directly affected by A/IS in specific settings.\n\
    \u2022\_\_\_\_\_Engineers and corporate employees frequently misunderstand stakeholders\u2019\
    \ needs and expectations, especially when the stakeholders are very different\
    \ from them in terms of educational and cultural background, social location,\
    \ and/or economic status.\nThe processes through which stakeholders become involved\
    \ in determining relevant wellbeing indicators will affect the quality of the\
    \ indicators selected and assessed. Stakeholders should be empowered to define\
    \ well-being, assess the appropriateness of existing indicators and propose new\
    \ ones, and highlight context-specific factors that bear on issues of well-being,\
    \ whether or not the issues have been recognized previously or are amenable to\
    \ measurement. Interactive, open-ended discussions or deliberations among a wide\
    \ variety of stakeholders and system designers are more likely to yield robust,\
    \ widely-shared understandings of well-being and how to measure it in context.\
    \ Closed-ended or over-determined methods for soliciting stakeholder input are\
    \ likely to miss relevant information that system designers have not anticipated.deliberation\
    \ is one model for collective decisionmaking. Parties in such deliberation come\
    \ together as equals. Their goal is to set aside their immediate, personal interests\
    \ in order to think together about the common good. Participants in a stakeholder\
    \ engagement and deliberation learn from one another\u2019s perspectives and experiences.\n\
    **In the real world, stakeholder engagement and deliberation may run into the\
    \ following challenges:**\n\u2022\_\_\_\_\_Individuals with more education, power,\
    \ or higher social status may\u2014intentionally or unintentionally\u2014dominate\
    \ the discussion, undermining their ability to learn from less powerful participants.\n\
    \u2022\_\_\_\_\_Topics may be preemptively ruled \u201Cout of bounds\u201D, to\
    \ the detriment of collective problem-solving. An example would be if, in a deliberation\
    \ on well-being and A/IS, participants were told that worries about the costs\
    \ of health insurance were unrelated to\_A/IS and thus could not be discussed.\n\
    \u2022\_\_\_\_\_Engineers and scientists may claim authority over technical issues\
    \ and be willing to deliberate only on social issues, obscuring the ways that\
    \ technical and social issues are intertwined.\n\u2022\_\_\_\_\_Less powerful\
    \ groups may be unable to keep more powerful ones \u201Cat the table\u201D when\
    \ discussions get contentious, and vice versa.\n\u2022\_\_\_\_\_Participants may\
    \ not agree on who can legitimately be involved in the conversation. For example,\
    \ the consensual spirit of deliberation is often used as a justification for excluding\
    \ activists and others who already hold a position on the issue.\n**Stakeholder\
    \ engagement and deliberative processes can be effective when:**\n\u2022\_\_\_\
    \_\_Their design is guided by experts or practitioners who are experienced in\
    \ deliberation models.\n\u2022\_\_\_\_\_Deliberations are facilitated by individuals\
    \ sensitive to issues of power and are skilled in mediating deliberation sessions.\n\
    \u2022\_\_\_\_\_Less powerful actors participate with the help of allies who can\
    \ amplify their voices.\n\u2022\_\_\_\_\_More powerful actors participate with\
    \ an awareness of their own power and make a commitment to listen with humility,\
    \ curiosity, and open-mindedness.\n\u2022\_\_\_\_\_Deliberations are convened\
    \ by institutions or individuals who are trusted and respected by all parties\
    \ and who hold all actors accountable for participating constructively.\nEthically\
    \ aligned design of A/IS would be furthered by thoughtfully constructed, context-specific\
    \ deliberations on well-being and the best indicators for assessing it.\n## Recommendation\n\
    Appoint a lead team or person, \u201Cleads\u201D, to facilitate stakeholder engagement\
    \ and to serve as a resource for A/IS creators who use stakeholderbased processes\
    \ to establish well-being indicators. Specifically:\n\u2022\_\_\_\_\_Leads should\
    \ solicit and collect lessons learned from specific applications of stakeholder\
    \ engagement and deliberation in order to continually refine its guidance.\n\u2022\
    \_\_\_\_\_ When determining well-being indicators, the leads should enlist the\
    \ help of experts in public participation and deliberation. With expert guidance,\
    \ facilitators can provide guidance for how to: take steps to mitigate the effects\
    \ of unequal power in deliberative processes; incorporate appropriately trained\
    \ facilitators and coaching participants in deliberations; recognize and curb\
    \ disproportionate influence by morepowerful groups; use techniques to maximize\
    \ the voices of less-powerful groups.\u2022\_\_\_\_\_Leads should use their convening\
    \ power to bring together A/IS creators and stakeholders, including critics of\
    \ A/IS, for deliberations on well-being indicators, impacts, and other considerations\
    \ for specific contexts and settings. Leads\u2019 involvement would help bring\
    \ actors to the table with a balance of power and encourage all actors to remain\
    \ in conversation until robust, mutually agreeable definitions\_are found.\n##\
    \ Further Resources\n\u2022\_\_\_\_\_D. E. Booher and J. E. Innes. Planning with\
    \ Complexity: An Introduction to Collaborative Rationality for Public Policy.\
    \ London:\_Routledge, 2010.\n\u2022\_\_\_\_\_J. A. Leydens and J. C. Lucena. Engineering\
    \ Justice: Transforming Engineering Education and Practice_. _Wiley-IEEE Press,\
    \ 2018.\n\u2022\_\_\_\_\_G. Ottinger. [Assessing Community Advisory ](https://www.sciencehistory.org/sites/default/files/studies-in-sustainability-ottinger2009.pdf)\n\
    [Panels: A Case Study from Louisiana\u2019s Industrial Corridor.](https://www.sciencehistory.org/sites/default/files/studies-in-sustainability-ottinger2009.pdf)\
    \ Center for Contemporary History and Policy, 2008.\n\u2022\_\_\_\_\_[Expert and\
    \ Citizen Assessment of Science and ](https://ecastnetwork.org/about/)\n[Technology\
    \ (ECAST) Network ](https://ecastnetwork.org/about/)\"\np.82-83\n"
  OverarchingPrinciples (from Principles):
  - recOHnq45Fq7YWsRO
  Principles:
  - recQ9DIFEsOEkCx3O
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recZI7HDWKBU5T22v
  Tags:
  - wellbeing
  airtable_createdTime: '2023-06-05T05:56:59.000Z'
  airtable_id: rec5nbWC0ZbVwnmxJ
  title: How do we incorporate stakeholder processes to determine key wellbeing considerations
    in AI design/development/use?
- Description: "\u201CWhen citizens are subject to decisions, predictions, or classifications\
    \ produced by AI systems, situations may arise where such individuals are unable\
    \ to hold directly accountable the parties responsible for these outcomes. AI\
    \ systems automate cognitive functions that were previously attributable exclusively\
    \ to accountable human agents. This can complicate the designation of responsibility\
    \ in algorithmically generated outcomes, because the complex and distributed character\
    \ of the design, production, and implementation processes of AI systems may make\
    \ it difficult to pinpoint accountable parties. In cases of injury or negative\
    \ consequence, such an accountability gap may harm the autonomy and violate the\
    \ rights of the affected individuals.\u201D (Leslie, 2019, p. 4)\n\n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  Principles:
  - recKdujFoPJr4ZAhZ
  Reference:
  - 'Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A
    guide for the responsible design and implementation of AI systems in the public
    sector. The Alan Turing Institute. https://doi.org/10.5281/ZENODO.3240529'
  Sources:
  - recfYC5jjPmpLfSlM
  airtable_createdTime: '2023-05-18T19:10:49.000Z'
  airtable_id: rec8hxSSEudaplJiA
  title: Lack of accountability through automated decisions leading to loss of autonomy
- Description: "**\"Issue:** Need for better\_technical documentation \n## Background\n\
    A/IS are often construed as fundamentally opaque and inscrutable. However, lack\
    \ of transparency is often the result of human decision. The problem can be traced\
    \ to a\_variety of sources, including poor documentation that excludes vital information\
    \ about the limitations and assumptions of a system.\_Better documentation combined\
    \ with\_internal and external auditing are crucial to\_understanding a system\u2019\
    s ethical impact.\n## Recommendation\nEngineers should be required to thoroughly\
    \ document the end product and related data flows, performance, limitations, and\
    \ risks of\_A/IS. Behaviors and practices that have been prominent in the engineering\
    \ processes should also be explicitly presented, as well as empirical evidence\
    \ of compliance and methodology used, such as training data used in predictive\
    \ systems, algorithms and components used, and results of behavior monitoring.\
    \ Criteria for such documentation could be: auditability, accessibility, meaningfulness,\
    \ and readability.\nCompanies should make their systems auditable and should explore\
    \ novel methods for external and internal auditing.\_\n## Further reading\n\u2022\
    \_\_\_\_\_S. Wachter, B. Mittelstadt, and L. Floridi. \u201C[Transparent, Explainable,\
    \ and Accountable AI for Robotics](http://robotics.sciencemag.org/content/2/6/eaan6080).\u201D\
    \ _[Science Robotics, v](http://robotics.sciencemag.org/content/2/6/eaan6080)ol.\
    \ _2, no. 6, May 31, 2017. [Online]. Available: DOI: 10.1126/scirobotics.aan6080.\
    \ [Accessed Nov.\n\u2022\_\_\_\_\_S. Barocas, and A. D. Selbst, \u201C[Big Data\u2019\
    s Disparate Impact.](http://www.californialawreview.org/wp-content/uploads/2016/06/2Barocas-Selbst.pdf)\u201D\
    \ _California Law Review_ 104, 671-732, 2016.\n\u2022\_\_\_\_\_J. A. Kroll, J.\
    \ Huey, S. Barocas, E. W. Felten, J. R. Reidenberg, D. G. Robinson, and H. Yu.\
    \ \u201C[Accountable Algorithms.](https://www.pennlawreview.com/print/165-U-Pa-L-Rev-633.pdf)\u201D\
    \ _University of Pennsylvania Law Review _165, no. 1, 633\u2013 705, 2017.\n\u2022\
    \_\_\_\_\_J. M. Balkin, \u201C[Free Speech in the Algorithmic Society: Big Data,\
    \ Private Governance, and New School Speech Regulation.](https://ssrn.com/abstract%3D3038939)\u201D\
    \ _UC Davis Law Review_, 2017.\"\n\np.135\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - rec81gtnlFS5W2BBF
  Tags:
  - corporate-ethics
  airtable_createdTime: '2023-06-05T10:57:55.000Z'
  airtable_id: rec9Cuz3HWc9lWW23
  title: How do we ensure assumptions and limitations are clear in technical documentation?
- Description: "\"Ownership and responsibility\n## Background\nThere is variance within\
    \ the technology community on how it sees its responsibility regarding A/IS. The\
    \ difference in values and behaviors are not necessarily aligned with the broader\
    \ set of social concerns raised by public, legal, and professional communities.\
    \ The current makeup of most organizations has clear delineations among engineering,\
    \ legal, and marketing functions. Thus, technologists will often be incentivized\
    \ in terms of meeting functional requirements, deadline, and financial constraints,\
    \ but for larger social issues may say, \u201CLegal will handle that.\u201D In\
    \ addition, in employment and management technology or work contexts, \u201Cethics\u201D\
    \ typically refers to a code of conduct regarding professional behavior versus\
    \ a values-driven design process mentality.\nAs such, ethics regarding professional\
    \ conduct often implies moral issues such as integrity or the lack thereof, in\
    \ the case of whistleblowing, for instance. However, ethics in A/IS design include\
    \ broader considerations about the consequences of technologies.\n## Recommendations\n\
    Organizations should clarify the relationship between professional ethics and\
    \ applied\_A/IS ethics by helping or enabling designers, engineers, and other\
    \ company representatives to discern the differences between these kinds of ethics\
    \ and where they complement each other.\nCorporate ethical review boards, or comparable\
    \ mechanisms, should be formed to address\_ethical and behavioral concerns in\
    \ relation to\_A/IS design, development and deployment. Such boards should seek\
    \ an appropriately diverse composition and use relevant criteria, including both\
    \ research ethics and product ethics, at the appropriate levels of advancement\
    \ of research and development. These boards should examine justifications of research\
    \ or industrial projects.\n## Further Resources\n\u2022 HH van der Kloot Meijberg\
    \ and RHJ ter Meulen, \u201C[Developing Standards for Institutional](https://jme.bmj.com/content/27/suppl_1/i36.full)\
    \ [Ethics Committees: Lessons from the Netherlands,](https://jme.bmj.com/content/27/suppl_1/i36.full)\u201D\
    \ _Journal of Medical Ethics_ 27 i36-i40, 2001.\"\n\np.130\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recCN9eqkgKT3KjCB
  Tags:
  - corporate-ethics
  airtable_createdTime: '2023-06-05T10:33:11.000Z'
  airtable_id: rec9qgAZS5U8GL137
  title: How do we encourage taking ownership and responsibility for AI impacts?
- Description: "\"How can A/IS creators incorporate well-being into\_their work?\n\
    ## Background\nWithout practical ways of incorporating well-being metrics to guide,\
    \ measure, and monitor impact, A/IS will likely lack fall short of its potential\
    \ to avoid harm and promote well-being. Incorporating well-being thinking into\
    \ typical organizational processes of design, prototyping, marketing, etc., suggests\
    \ a variety of adaptations.\nOrganizations and A/IS creators should consider clearly\
    \ defining the type of A/IS product or service that they are developing, including\
    \ articulating its intended stakeholders and uses. By defining typical uses, possible\
    \ uses, and finally unacceptable uses of the technology, creators will help to\
    \ spell out the context of well-being. This can help to identify possible harms\
    \ and risks given the different possible uses and end users, as well as intended\
    \ and unintended positive consequences.\nAdditionally, internal and external stakeholders\
    \ should be extensively consulted to ensure that impacts are thoroughly considered\
    \ through an iterative and learning stakeholder engagement process. After consultation,\
    \ A/IS creators should select appropriate well-being indicators based on the possible\
    \ scope and impact of their A/IS product or service. These well-being indicators\
    \ can be drawn from mainstream sources and models and adapted as necessary. They\
    \ can be used to engage in pre-assessment of the intended user population, projection\
    \ of possible impacts, and post-assessment. Development of a well-being indicator\
    \ measurement plan and relevant data infrastructure will support a robust integration\
    \ of well-being. A/IS models can also be trained to explicitly include well-being\
    \ indicators as subgoals.\nData and discussions on well-being impacts can be used\
    \ to suggest improvements and modifications to existing A/IS products and services\
    \ throughout their lifecycle. For example, a [team](https://www.aaai.org/Papers/Symposia/Fall/2008/FS-08-02/FS08-02-024.pdf)\
    \ [seeking to increase the well-being o](https://www.aaai.org/Papers/Symposia/Fall/2008/FS-08-02/FS08-02-024.pdf)f\
    \ people using wheelchairs found that when provided the opportunity to use a smart\
    \ wheelchair, some users were delighted with the opportunity for more mobility,\
    \ while others felt it would decrease their opportunities for social contact,\
    \ increase their sense of isolation, and lead to an overall decrease in their\
    \ well-being. Therefore, even though a product modification may increase well-being\
    \ according to one indicator or set of A/IS stakeholders, it does not mean that\
    \ this modification should automatically be adopted.\nFinally, organizational\
    \ processes can be modified to incorporate the above strategies. Appointment of\
    \ an organizational lead person for well-being impacts, e.g., a well-being lead,\
    \ ombudsman,\_or officer can help to facilitate this effort.\n## Recommendation\n\
    A/IS creators should adjust their existing development, marketing, and assessment\
    \ cycles to incorporate well-being concerns throughout their processes. This includes\
    \ identification of an A/IS lead ombudsperson or officer; identification of stakeholders\
    \ and end users; determination of possible uses, harm and risk assessment; robust\
    \ stakeholder engagement; selection of well-being indicators; development of a\
    \ well-being indicator measurement plan; and ongoing improvement of A/IS products\
    \ and services throughout the lifecycle.\n## Further Resources\n\u2022\_\_\_\_\
    \_[Peter Senge and the Learning Organization](http://infed.org/mobi/peter-senge-and-the-learning-organization/)[\
    \ -](https://api.ag.purdue.edu/api/depotws/File.ashx?t=f&i=11736)\n(synopsis)\
    \ Purdue University\n\u2022\_\_\_\_\_Stakeholder Engagement: A Good Practice Handbook\
    \ for Companies Doing Business in Emerging Markets. International Finance Corporation,\
    \ May 2007.\n\u2022\_\_\_\_\_[Global Reporting Initiative](https://www.globalreporting.org/Pages/default.aspx)\n\
    \u2022\_\_\_\_\_ [GNH Certification](http://www.bhutanstudies.org.bt/gnh-certification/),\
    \ Centre for Bhutan\_and GNH Studies, 2018.\u2022\_\_\_\_\_J. Helliwell, R. Layard,\
    \ and J. Sachs, Eds., \u201CThe Objective Benefits of Subjective Well-Being,\u201D\
    \ in [World Happiness Report ](http://worldhappiness.report/ed/2013/)2013. New\
    \ York: UN Sustainable Development Solutions Network, pp. 54-79, 2013.\n\u2022\
    \_\_\_\_\_[Global Happiness and Well-being Policy Report](http://www.happinesscouncil.org/)\
    \ by the Global Happiness Council, 2018.\"\n\np.78-79\n"
  OverarchingPrinciples (from Principles):
  - recOHnq45Fq7YWsRO
  - recLHILkx2JDFsLbX
  Principles:
  - recQ9DIFEsOEkCx3O
  - receFm7cGasHwpJZO
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recZI7HDWKBU5T22v
  - recfcXzM3foqFNNGN
  Tags:
  - wellbeing
  airtable_createdTime: '2023-06-05T05:53:34.000Z'
  airtable_id: recA7Kh502s4UKWGo
  title: How do we incorporate wellbeing considerations into AI impact measurement
    and monitoring?
- Description: "\"**Issue: **What would it mean for a person to have an algorithmic\
    \ agent helping them actively represent and curate their terms and conditions\
    \ at all times?\n## Background\nWhile it\u2019s essential to create your own terms\
    \ and conditions to broadcast your preferences, it\u2019s also important to recognize\
    \ that humans do not operate at an algorithmic speed or level. A significant part\
    \ of retaining your agency in this way involves identifying trusted services that\
    \ can essentially act on your behalf when making decisions about your data.\_\n\
    Part of this logic entails putting you \u201Cat the center of your data\u201D\
    . One of the greatest challenges to user agency is that once you give your data\
    \ away, you do not know how it is being used or by whom. But when all transactions\
    \ about your data go through your A/IS agent honoring your preferences, you have\
    \ better opportunities to control how your information is shared.\nAs an example,\
    \ with medical data\u2014while it is assumed most would share all their medical\
    \ data with their spouse\u2014most would also not wish to share that same amount\
    \ of data with their local gym. This is an issue that extends beyond privacy,\
    \ meaning one\u2019s cultural or individual preferences about what personal information\
    \ to share, to utility and clarity. This type of sharing also benefits users or\
    \ organizations on the receiving end of data from these exchanges. For instance,\
    \ the local gym in the previous example may only need basic heart or general health\
    \ information and would actually not wish to handle or store sensitive cancer\
    \ or other personal health data for reasons of liability.\_\nA precedent for this\
    \ type of patient- or usercentric model comes from Gliimpse, a service described\
    \ by Jordan Crook from _TechCrunch_ in his article, \u201C[Apple acquired Gliimpse,\
    \ a personal health data startup\u201D](https://techcrunch.com/2016/08/22/apple-acquired-gliimpse-a-personal-health-data-startup/):\
    \ \u201CGliimpse works by letting users pull their own medical info into a single\
    \ virtual space, with the ability to add documents and pictures to fill out the\
    \ profile. From there, users can share that data (as a comprehensive picture)\
    \ to whomever they wish.\u201D The fact that Apple acquired the startup points\
    \ to the potential for the successful business model of user-centric data exchange\
    \ and putting individuals at the center of their data. A person\u2019s A/IS agent\
    \ is a proactive algorithmic tool honoring their terms and conditions in the digital,\
    \ virtual, and physical worlds. Any public space where a user may not be aware\
    \ they are under surveillance by facial recognition, biometric, or other tools\
    \ that could track, store, and utilize their data can now provide overt opportunity\
    \ for consent via an A/IS agent platform. Even where an individual is not sure\
    \ they are being tracked, by broadcasting their terms and conditions via digital\
    \ means, they can demonstrate their preferences in the public arena. Via Bluetooth\
    \ or similar technologies, individuals could offer their terms and conditions\
    \ in a ubiquitous and always-on manner. This means even when an individual\u2019\
    s terms and conditions are not honored, people would have the ability to demonstrate\
    \ their desire not to be tracked which could provide a methodology for the democratic\
    \ right to protest in a peaceful manner. And where those terms and conditions\
    \ are recognized\u2015 meaning technically recognized even if they are not honored\u2015\
    one\u2019s opinions could be formally logged via GPS and timestamp data.\nThe\
    \ A/IS agent could serve as an educator and negotiator on behalf of its user by\
    \ suggesting how requested data could be combined with other data that has already\
    \ been provided, inform the user if data are being used in a way that was not\
    \ authorized, or make recommendations to the user based on a personal profile.\
    \ As a negotiator, the agent could broker conditions for sharing data and could\
    \ include payment to the user as a term, or even retract consent for the use of\
    \ data previously authorized, for instance, if a breach of conditions was detected.\n\
    ## Recommendations\nAlgorithmic agents should be developed for individuals to\
    \ curate and share their personal data. Specifically:\n\u2022\_\_\_\_\_For purposes\
    \ of privacy, a person must be able to set up complex permissions that reflect\
    \ a variety of wishes.\n\u2022\_\_\_\_\_The agent should help a person foresee\
    \ and mitigate potential ethical implications of specific machine learning data\
    \ exchanges.\n\u2022\_\_\_\_\_A user should be able to override his/her personal\
    \ agents should he/she decide that the service offered is worth the conditions\
    \ imposed.\n\u2022\_\_\_\_\_An agent should enable machine-to-machine processing\
    \ of information to compare, recommend, and assess offers and services.\n\u2022\
    \_\_\_Institutional systems should ensure support for and respect the ability\
    \ of individuals to bring their own agent to the relationship without constraints\
    \ that would make some guardians inherently incompatible or subject to censorship.\n\
    \u2022\_\_\_\_\_Vulnerable parts of the population will need protection in the\
    \ process of granting access.\n## Further Resources\n\u2022\_\_\_\_\_IEEE P7006\u2122\
    \ - IEEE [Standards Project on Personal Data AI Agent Working Group](https://standards.ieee.org/develop/project/7006.html).\
    \ Designed as a tool to allow any individual to create their own personal \u201C\
    terms and conditions\u201D for their data, the AI Agent will also provide a technological\
    \ tool for individuals to manage and control their identity in the digital and\
    \ virtual world.\n\u2022\_\_\_\_\_Tools allowing an individual to create a form\
    \ of an algorithmic guardian are often labeled as PIMS, or Personal Information\
    \ Management Services. [Nesta in the United Kingdom was one of the funders of\
    \ early research about PIMS ](http://www.nesta.org.uk/publications/personal-information-management-services-analysis-emerging-market)conducted\
    \ by [CtrlShift](https://www.ctrl-shift.co.uk/).\"\np.111-112\n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  Principles:
  - recPg7Ov0priGGtLm
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recG86YsfXnKY9kNc
  airtable_createdTime: '2023-06-05T09:34:56.000Z'
  airtable_id: recAoyMGCCLhaSqEb
  title: How do we know how our data is being used?
- Cases:
  - recSXcY4cnofb4zTP
  Description: "\u201CThreats to privacy are posed by AI systems both as a result\
    \ of their design and development processes, and as a result of their deployment.\
    \ As AI projects are anchored in the structuring and processing of data, the development\
    \ of AI technologies will frequently involve the utilisation of personal data.\
    \ This data is sometimes captured and extracted without gaining the proper consent\
    \ of the data subject or is handled in a way that reveals (or places under risk\
    \ the revelation of) personal information. On the deployment end, AI systems that\
    \ target, profile, or nudge data subjects without their knowledge or consent could\
    \ in some circumstances be interpreted as infringing upon their ability to lead\
    \ a private life in which they are able to intentionally manage the transformative\
    \ effects of the technologies that influence and shape their development. This\
    \ sort of privacy invasion can consequently harm a person\u2019s more basic right\
    \ to pursue their goals and life plans free from unchosen influence.\u201D (Leslie,\
    \ 2019, p. 5) \n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  Principles:
  - recPg7Ov0priGGtLm
  Reference:
  - 'Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A
    guide for the responsible design and implementation of AI systems in the public
    sector. The Alan Turing Institute. https://doi.org/10.5281/ZENODO.3240529'
  Sources:
  - recfYC5jjPmpLfSlM
  airtable_createdTime: '2023-05-18T19:10:38.000Z'
  airtable_id: recBc3GCNokDL220T
  title: Privacy breaches
- Description: "\u201CBecause Instructors typically conduct SoTL in their classrooms\
    \ (current or former), SoTL practitioners frequently find themselves in the dual\
    \ role of instructor and researcher. Ultimately, the instructor-researcher in\
    \ SoTL is an instructor first. As MacLean and Poole (2010, pg. 3) explain, \u201C\
    The teacher\u2019s responsibility to hold students\u2019 educational interests\
    \ paramount provides an important perspective when considering ethical issues\
    \ for research in teaching and learning.\u201D This dual role can raise a set\
    \ of specific ethical dilemmas that require instructor-researchers to plan parts\
    \ of the research carefully and to ask themselves challenging questions. Potential\
    \ ethical dilemmas can arise concerning the following areas of ethical consideration.\
    \ In the table below, we articulate several core principles for ethical practice\
    \ that respond to these potentially dilemmatic areas of consideration and elaborate\
    \ on them in the remainder of the document.\u201D (Fedoruk, 2017, p. 4)\n\n\u201C\
    \u201CDual roles of researchers and their associated obligations (e.g., acting\
    \ as both a researcher and a therapist, health care provider, caregiver, teacher,\
    \ advisor, consultant, supervisor, student or employer) may create conflicts,\
    \ undue influences, power imbalances or coercion that could affect relationships\
    \ with others and affect decision-making procedures (e.g., consent of participants).\
    \ Article 3.2(e) reminds researchers of relevant ethical duties that govern real,\
    \ potential or perceived conflicts of interest as they relate to the consent of\
    \ participants. To preserve and not abuse the trust on which many professional\
    \ relationships rest, researchers should be fully cognizant of conflicts of interest\
    \ that may arise from their dual or multiple roles, their rights and responsibilities,\
    \ and how they can manage the conflict. When acting in dual or multiple roles,\
    \ the researcher shall disclose the nature of the conflict to the participant\
    \ in the consent process\u201D (TCPS2, Chapter 7, D. Researchers and Conflicts\
    \ of Interest).\u201D (Fedoruk, 2017, p. 5)\n\n\u201C\u201CThe approach to recruitment\
    \ is an important element in assuring voluntariness. In particular, how, when\
    \ and where participants are approached, and who recruits them are important elements\
    \ in assuring (or undermining) voluntariness. In considering the voluntariness\
    \ of consent, REBs and researchers should be cognizant of situations where undue\
    \ influence, coercion, or the offer of incentives may undermine the voluntariness\
    \ of a participants\u2019 consent to participate in research\u201D (TCPS2, Chapter\
    \ 3, A. General Principles, \u201CConsent Should Be Given Voluntarily\u201D).\u201D\
    \ (Fedoruk, 2017, p. 7)\n\n\u201C\u201CConsent shall be maintained throughout\
    \ the research project. Researchers have an ongoing duty to provide participants\
    \ with all information relevant to their ongoing consent to participate in the\
    \ research\u201D (TCPS2, Chapter 3, A. General Principles, \u201CConsent Shall\
    \ Be an Ongoing Process\u201D).\u201D (Fedoruk, 2017, p. 8)\n\n\u201C\u201CTaking\
    \ into account the scope and objectives of their research, researchers should\
    \ be inclusive in selecting participants. Researchers shall not exclude individuals\
    \ from the opportunity to participate in research on the basis of attributes such\
    \ as culture, language, religion, race, disability, sexual orientation, ethnicity,\
    \ linguistic proficiency, gender or age, unless there is a valid reason for the\
    \ exclusion. Application ... The focus, objective, nature of research and context\
    \ in which the research is conducted inform the inclusion and exclusion criteria\
    \ for a specific research project... Other examples include research focused on\
    \ specific cultural traditions or languages, or on one age group...Such research\
    \ should not be precluded so long as the selection criteria for those included\
    \ in the research are germane to answering the research question. Researchers\
    \ who plan to actively exclude particular groups should clarify to their REBs\
    \ the grounds for the exclusion\u201D (TCPS2, Chapter 4, A. Appropriate Inclusion).\u201D\
    \ (Fedoruk, 2017, p. 9)\n\n\u201C\u201CResearchers should anticipate, to the best\
    \ of their ability, needs of participants, groups and their communities that might\
    \ arise in any given research project. ... Researchers should consider ways to\
    \ ensure the equitable distribution of any benefits of participation in research\u201D\
    \ (TCPS2, Chapter 4, B. Inappropriate Exclusion, \u201CParticipants\u2019 Vulnerability\
    \ and Research\u201D).\u201D (Fedoruk, 2017, p. 9)\n\n\u201C\u201CResearchers\
    \ should normally provide copies of publications, or other research reports or\
    \ products, arising from the research to the institution or organization \u2013\
    \ normally the host institution \u2013 that is best suited to act as a repository\
    \ and disseminator of the results within the participating communities. This may\
    \ not be necessary for jurisdictions where the results are readily available in\
    \ print or electronically. In general, researchers should ensure that participating\
    \ individuals, groups and communities are informed of how to access the results\
    \ of the research. Results of the research should be made available to them in\
    \ a culturally appropriate and meaningful format, such as reports in plain language\
    \ in addition to technical reports\u201D (TCPS2, Chapter 4, B. Inappropriate Exclusion,\
    \ \u201CEquitable Distribution of Research Benefits\u201D).\u201D (Fedoruk, 2017,\
    \ p. 10)\n\n\u201C\u201CResearchers shall safeguard information entrusted to them\
    \ and not misuse or wrongfully disclose it. Institutions shall support their researchers\
    \ in maintaining promises of confidentiality\u201D (TCPS2, Chapter 5, B. Ethical\
    \ Duty of Confidentiality).\u201D (Fedoruk, 2017, p. 11)\n\n\u201C\u201CResearchers\
    \ shall describe measures for meeting confidentiality obligations and explain\
    \ any reasonably foreseeable disclosure requirements: a. in application materials\
    \ they submit to the REB; and b. during the consent process with prospective participants\u201D\
    \ (TCPS2, Chapter 5, B. Ethical Duty of Confidentiality).\u201D (Fedoruk, 2017,\
    \ p. 11)\n\n\u201C\u201CResearchers shall provide details to the REB regarding\
    \ their proposed measures for safeguarding information, for the full life cycle\
    \ of information: its collection, use, dissemination, retention and/or disposal\u201D\
    \ (TCPS2, Chapter 5, C. Safeguarding Information).\u201D (Fedoruk, 2017, p. 12)\n\
    \n\u201C\u201CInstitutions or organizations where research data are held have\
    \ a responsibility to establish appropriate institutional security safeguards\u201D\
    \ (TCPS2, Chapter 5, C. Safeguarding Information).\u201D (Fedoruk, 2017, p. 12)\n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  Principles:
  - recKdujFoPJr4ZAhZ
  Reference:
  - Fedoruk, L. (2017b). Ethics in The Scholarship of Teaching and Learning. University
    of Calgary Taylor Institute for Teaching and Learning. https://taylorinstitute.ucalgary.ca/resources/ethics-scholarship-teaching-and-learning
  Sources:
  - recQzldmBLByP78Uu
  airtable_createdTime: '2023-05-19T13:05:35.000Z'
  airtable_id: recCkVZngvMA5I5uv
  title: SoTL power
- OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  - recLHILkx2JDFsLbX
  - recmzjcGKv3yNOxbl
  Principles:
  - recU6u0AZbcNj1ik9
  - reczVPIH1y2OMpAJH
  - rec42P8U9usfYCtv9
  Tags:
  - AI
  airtable_createdTime: '2023-05-18T13:41:32.000Z'
  airtable_id: recCpTJXuuowqsqQa
  title: Incidental findings
- Description: "**Issue: **Institutional ethics committees in the A/IS fields\n##\
    \ Background\nIt is unclear how research on the interface of humans and A/IS,\
    \ animals and A/IS, and biological hazards will impact research ethical review\
    \ boards. Norms, institutional controls, and risk metrics appropriate to the technology\
    \ are not well established in the relevant literature and research governance\
    \ infrastructure. Additionally, national and international regulations governing\
    \ review of human-subjects research may explicitly or implicitly exclude A/IS\
    \ research from their purview on the basis of legal technicalities or medical\
    \ ethical concerns, regardless of the potential harms posed by the research.\n\
    Research on A/IS human-machine interaction, when it involves intervention or interaction\
    \ with identifiable human participants or their data, typically falls to the governance\
    \ of research ethics boards, e.g., institutional review boards. The national level\
    \ and institutional resources, e.g., hospitals and universities, necessary to\
    \ govern ethical conduct of Human-Computer Interaction (HCI), particularly within\
    \ the disciplines pertinent to A/IS research, are underdeveloped.\nFirst, there\
    \ is limited international or national guidance to govern this form of research.\
    \ Sections of IEEE standards governing research on A/IS in medical devices address\
    \ some of the issues related to the security of A/ISenabled devices. However,\
    \ the ethics of testing those devices for the purpose of bringing them to market\
    \ are not developed into policies or guidance documents from recognized national\
    \ and international bodies, e.g., U.S. Food and Drug Administration (FDA) and\
    \ EU European Medicines Agency (EMA). Second, the bodies that typically train\
    \ individuals to be gatekeepers for the research ethics bodies are under-resourced\
    \ in terms of expertise for A/IS development, e.g., Public Responsibility in Medicine\
    \ and Research (PRIM&R) and the Society of Clinical Research Associates (SoCRA).\
    \ Third, it is not clear whether there is sufficient attention paid to A/IS ethics\
    \ by research ethics board members or by researchers whose projects involve the\
    \ use of human participants or their identifiable data.\nFor example, research\
    \ pertinent to the ethics-governing research at the interface of animals\_and\
    \ A/IS research is underdeveloped with respect to systematization for implementation\
    \ by the Institutional Animal Care and Use Committee (IACUC) or other relevant\
    \ committees. In institutions without a veterinary school, it is unclear that\
    \ the organization would have the relevant resources necessary to conduct an ethical\
    \ review of such research.\nSimilarly, research pertinent to the intersection\
    \ of radiological, biological, and toxicological research \u2014ordinarily governed\
    \ under institutional biosafety committees\u2014and A/IS research is not often\_\
    found in the literature pertinent to research\_ethics or research governance.\n\
    ## Recommendations\nThe IEEE and other standards-setting bodies should draw upon\
    \ existing standards, empirical research, and expertise to identify priorities\_\
    and develop standards for the governance of\_A/IS research and partner with relevant\
    \ national agencies, and international organizations,\_when possible.\n## Further\
    \ Resources\n\u2022\_\_\_\_\_S. R. Jordan, \u201CThe Innovation Imperative.\u201D\
    \ _Public Management Review _16, no. 1,\_pp. 67\u201389, 2014.\n\u2022\_\_\_\_\
    \_B. Schneiderman, \u201C[The Dangers of Faulty, Biased, or Malicious Algorithms\
    \ Requires Independent Oversight.](http://www.pnas.org/content/113/48/13538.long)\u201D\
    \ _Proceedings of the National Academy of Sciences of the United States of America\
    \ _113, no. 48, 13538\u201313540, 2016.\n\u2022\_\_\_\_\_J. Metcalf and K. Crawford,\
    \ \u201C[Where are Human Subjects in Big Data Research? The Emerging Ethics Divide](http://papers.ssrn.com/abstract%3D2779647).\u201D\
    \ _Big Data & Society,_ May 14, 2016._ _[Online]. Available: SSRN: [https://ssrn.\
    \ com/abstract=2779647](https://ssrn.com/abstract=2779647). [Accessed Nov. 1,\
    \ 2018].\n\u2022\_\_\_\_\_R. Calo, \u201C[Consumer Subject Review Boards: A Thought\
    \ Experiment](https://www.stanfordlawreview.org/online/privacy-and-big-data-consumer-subject-review-boards/).\u201D\
    \ _Stanford Law Review Online _66 97, Sept. 2013. \n\np.125\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recI7WdfNhrQxRt7F
  Tags:
  - research
  airtable_createdTime: '2023-06-05T10:22:04.000Z'
  airtable_id: recELObWGfkhXzFG2
  title: How can institutional ethics review committees provide oversight of AI research?
- OverarchingPrinciples (from Principles):
  - recSqx6wklVpDzx3s
  - recSqx6wklVpDzx3s
  Principles:
  - recSqx6wklVpDzx3s
  - recMGB4iC5oaCtr5x
  airtable_createdTime: '2023-05-18T18:54:06.000Z'
  airtable_id: recENQXCSOQQ3Y8et
  title: Digital divides in burden of data capture
- OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  Principles:
  - recKdujFoPJr4ZAhZ
  airtable_createdTime: '2023-05-18T18:55:13.000Z'
  airtable_id: recGEFZ3EynXjRaa9
  title: Autonomy reduced through power relationships such as teacher-student, administrator-teacher,
    or perception that a tool cannot be questioned
- Description: "\"How can A/IS creators influence A/IS goals to ensure well-being,\
    \ and what can A/IS creators learn or borrow from existing models in the well-being\
    \ and other arenas?\n## Background\nAnother way to incorporate considerations\
    \ of well-being is to include well-being measures\_in the development, goal setting,\
    \ and training\_of the A/IS systems themselves.\nIdentified metrics of well-being\
    \ could be formulated as auxiliary objectives of the A/IS. As these auxiliary\
    \ well-being objectives will be only a subset of the intended goals of the system,\
    \ the architecture will need to balance multiple objectives. Each of these auxiliary\
    \ objectives may be expressed as a goal, set of rules, set of values, or as a\
    \ set of preferences, which can be weighted and combined using established methodologies\
    \ from intelligent systems engineering.\_\nFor example, an educational A/IS tool\
    \ could not only optimize learning outcomes, but also incorporate measures of\
    \ student social and emotional education, learning, and thriving.\nA/IS-related\
    \ data relates both to the individual\u2014 through personalized algorithms, in\
    \ conjunction with affective sensors measuring and influencing emotion, and other\
    \ aspects of individual well-being \u2014and to society as large data sets representing\
    \ aggregate individual subjective and objective data. As the exchange of this\
    \ data becomes more widely available via establishing tracking methodologies,\
    \ the data can be aligned within A/IS products and services to increase human\
    \ well-being. For example, robots like [Pepper](https://www.sbs.com.au/news/dateline/article/2017/04/11/love-intimacy-and-companionship-tale-robots-japan)\
    \ are equipped to share data regarding their usage and interaction with humans\
    \ to the cloud. This allows almost instantaneous innovation, as once an action\
    \ is validated as useful for one Pepper robot, all other Pepper units (and ostensibly\
    \ their owners) benefit as well. As long as this data exchange happens with the\
    \ predetermined consent of the robots\u2019 owners, this innovation in real time\
    \ model can be emulated for the large-scale aggregation of information relating\
    \ to existing well-being metrics.\nA/IS creators can also help to operationalize\
    \ well-being metrics by providing stakeholders with reports on the expected or\
    \ actual outcomes of the A/IS and the values and objectives embedded in the systems.\
    \ This transparency will help creators, users, and third parties assess the state\
    \ of well-being produced by A/IS and make improvements in A/IS. In addition, A/IS\
    \ creators should consider allowing end users to layer on their own preferences,\
    \ such as allowing users to limit their use of an A/IS product if it leads to\
    \ increased sustained stress levels, sustained isolation, development of unhealthy\
    \ habits, or other decreases to well-being.\_\nIncorporating well-being goals\
    \ and metrics into broader organizational values and processes would support the\
    \ use of well-being metrics as there would be institutional support. A key factor\
    \ in industrial, corporate, and societal progress is cross-dissemination of concepts\
    \ and models from one industry or field to another. To date, a number of successful\
    \ concepts and models exist in the fields of sustainability, economics, industrial\
    \ design and manufacturing, architecture and urban development, and governmental\
    \ policy. These concepts and models can provide a foundation\_for building a metrics\
    \ standard and the use of wellbeing metrics by A/IS creators, from conception\
    \ and design to marketing, product updates, and improvements to the user experience.\_\
    \n## Recommendation\nCreate technical standards for representing goals, metrics,\
    \ and evaluation guidelines for well-being metrics and their precursors and components\
    \ within A/IS that include:\n\u2022\_\_\_\_\_[O](https://en.wikipedia.org/wiki/Ontology_(information_science))ntologies\
    \ for representing technological requirements.\n\u2022\_\_\_\_\_A testing framework\
    \ for validating adherence to well-being metrics and ethical principles such as\
    \ [IEEE P7010\u2122 Standards Project for Wellbeing Metric for Autonomous and\
    \ Intelligent Systems](https://standards.ieee.org/project/7010.html).\nabove as\
    \ well as others as a basis for a wellbeing metrics standard for A/IS creators.\
    \ _(See page 191, [Additional Resources: Additional Resources: Standards Development\
    \ Models and Frameworks)](https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/ead1e_standards_development_models_frameworks.pdf)_\n\
    \u2022\_\_\_\_\_The development of a well-being metrics standard for A/IS that\
    \ encompasses an understanding of well-being as holistic and interlinked to social,\
    \ economic, and ecological systems.\n\_\n## Further Resources\n\u2022\_\_\_\_\_\
    A.F.T Winfield, C. Blum, and W. Liu. \u201C[Towards an Ethical Robot: Internal\
    \ Models, Consequences and Ethical Action Selection](https://link.springer.com/chapter/10.1007/978-3-319-10401-0_8),\u201D\
    \ in Advances in Autonomous Robotics Systems. Springer, 2014, pp. 85\u201396\n\
    \u2022\_\_\_\_\_R. A. Calvo, and D. Peters. [Positive Computing: Technology for\
    \ Well-Being and Human Potential](https://mitpress.mit.edu/books/positive-computing)._\
    \ _Cambridge MA: MIT Press, 2014.\n\u2022\_\_\_\_\_Y. Collette, and P. Slarry.\
    \ [Multiobjective Optimization: Principles and Case Studies ](https://link.springer.com/book/10.1007%2F978-3-662-08883-8)\n\
    (Decision Engineering Series). Berlin, Germany:\nSpringer, 2004. doi: 10.1007/978-3-662-08883-8.\n\
    \u2022\_\_\_\_\_J. Greene, et al. \u201C[Embedding Ethical Principles in Collective\
    \ Decision Support Systems](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12457),\u201D\
    \ in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence_,\
    \ _4147\u20134151. Palo Alto, CA: AAAI Press, 2016.\n\u2022\_\_\_\_\_L. Li, I.\
    \ Yevseyeva, V. Basto-Fernandes, H. Trautmann, N. Jing, and M. Emmerich,\u201C\
    [Building and Using an Ontology of Preference-Based Multiobjective Evolutionary\
    \ Algorithms.](https://dl.acm.org/citation.cfm?id=3088704)\u201D In 9th International\
    \ Conference on Evolutionary Multi-Criterion Optimization\u2014Volume 10173 (EMO\
    \ O. Sch\xFCtze, M. Wiecek, Y. Jin, and C. Grimme, Eds., Vol. 10173. Springer-Verlag,\
    \ Berlin, Heidelberg, 406-421, 2017.\n\u2022\_\_\_\_\_[PositiveSocialImpact:](https://invis.io/9874GSJS6)\
    \ Empowering people, organizations and planet with information and knowledge to\
    \ make a positive impact to sustainable development, 2017.\n\u2022\_\_\_\_\_D.K.\
    \ Ura, Bhutan\u2019s [Gross National Happiness Policy Screening Tool](http://www.grossnationalhappiness.com/docs/GNH/PDFs/PoliSTools.pdf).\n\
    \"\np.79-81\n"
  OverarchingPrinciples (from Principles):
  - recOHnq45Fq7YWsRO
  - recLHILkx2JDFsLbX
  Principles:
  - recQ9DIFEsOEkCx3O
  - receFm7cGasHwpJZO
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recZI7HDWKBU5T22v
  Tags:
  - wellbeing
  airtable_createdTime: '2023-06-05T05:54:42.000Z'
  airtable_id: recGRpoF7DA23ODSj
  title: How do we incorporate wellbeing considerations into AI design/development/use?
- Cases:
  - recDAvfsflBF0WKpf
  Description: "\u201CAI can be biased both at the system and the data or input level.\
    \ Bias at the system level involves developers building their own personal biases\
    \ into the parameters they consider or the labels they define. Although this rarely\
    \ occurs intentionally, unintentional bias at the system level is common. This\
    \ often occurs in two ways: \n\u2022 When developers allow systems to conflate\
    \ correlation with causation. Take credit scores as an example. People with a\
    \ low income tend to have lower credit scores, for a variety of reasons. If an\
    \ ML system used to build credit scores includes the credit scores of your Facebook\
    \ friends as a parameter, it will result in lower scores among those with low-income\
    \ backgrounds, even if they have otherwise strong financial indicators, simply\
    \ because of the credit scores of their friends. \n\u2022 When developers choose\
    \ to include parameters that are proxies for known bias. For example, although\
    \ developers of an algorithm may intentionally seek to avoid racial bias by not\
    \ including race as a parameter, the algorithm will still have racially biased\
    \ results if it includes common proxies for race, like income, education, or postal\
    \ code.26 Bias at the data or input level occurs in a number of ways:27 \n\u2022\
    \ The use of historical data that is biased. Because ML systems use an existing\
    \ body of data to identify patterns, any bias in that data is naturally reproduced.\
    \ For example, a system used to recommend admissions at a top university that\
    \ uses the data of previously admitted students to train the model is likely to\
    \ recommend upper class males over women and traditionally underrepresented groups.\
    \ \n\u2022 When the input data are not representative of the target population.\
    \ This is called selection bias, and results in recommendations that favor certain\
    \ groups over another. For example, if a GPS-mapping app used only input data\
    \ from smartphone users to estimate travel times and distances, it could be more\
    \ accurate in wealthier areas of cities that have a higher concentration of smartphone\
    \ users, and less accurate in poorer areas or informal settlements, where smartphone\
    \ penetration is lower and there is sometimes no official mapping. \n\u2022 When\
    \ the input data are poorly selected. In the GPS mapping app example, this could\
    \ involve including only information related to cars, but not public transportation\
    \ schedules or bike paths, resulting in a system that favored cars and was useless\
    \ for buses or biking. \n\u2022 When the data are incomplete, incorrect, or outdated.\
    \ If there is insufficient data to make certain conclusions, or the data are out\
    \ of date, results will naturally be inaccurate. And if a machine learning model\
    \ is not continually updated with new data that reflects current reality, it will\
    \ naturally become less accurate over time. Unfortunately, biased data and biased\
    \ parameters are the rule rather than the exception. Because data are produced\
    \ by humans, the information carries all the natural human bias within it. Researchers\
    \ have begun trying to figure out how to best deal with and mitigate bias, including\
    \ whether it is possible to teach ML systems to learn without bias;28 however,\
    \ this research is still in its nascent stages. For the time being, there is no\
    \ cure for bias in AI systems\u201D (Access Now, 2018, p. 12)\n"
  OverarchingPrinciples (from Principles):
  - recSqx6wklVpDzx3s
  - recLHILkx2JDFsLbX
  - recSqx6wklVpDzx3s
  - recSqx6wklVpDzx3s
  - recLHILkx2JDFsLbX
  - recmzjcGKv3yNOxbl
  Principles:
  - recK5zFiq18A3wHAE
  - reczVPIH1y2OMpAJH
  - recDRQE1qQQNI65Xn
  - recZbEXiEs1AlDdn3
  - recScYLR2TNiv7iKf
  - rec42P8U9usfYCtv9
  Reference:
  - Access Now. (2018). HUMAN RIGHTS IN THE AGE OF ARTIFICIAL INTELLIGENCE. Access
    Now. https://www.accessnow.org/cms/assets/uploads/2018/11/AI-and-Human-Rights.pdf
  Sources:
  - recH8KmnURSknCr5y
  Strategies:
  - recTjwhqfrJoaRxYo
  airtable_createdTime: '2023-05-29T07:14:32.000Z'
  airtable_id: recHHr97jsyNDnlsJ
  title: Bias
- Description: "\"When affective systems are deployed across cultures, they could\
    \ adversely affect the cultural, social, or religious values of the community\
    \ in which they interact.\n## Background\nSome philosophers argue that there are\
    \ no universal ethical principles and that ethical norms vary from society to\
    \ society. Regardless of whether universalism or some form of ethical relativism\
    \ is true, affective systems need to respect the values of the cultures within\
    \ which they are embedded. How systems should effectively reflect the values of\
    \ the designers or the users of affective systems is not a settled discussion.\
    \ There is general agreement that developers of affective systems should acknowledge\
    \ that the systems should reflect the values of those with whom the systems are\
    \ interacting. There is a high likelihood that when spanning different groups,\
    \ the values imbued by the developer will be different from the operator or customer\
    \ of that affective system, and that end-user values should be actively considered.\
    \ Differences between affective systems and societal values may generate conflict\
    \ situations producing undesirable results, e.g., gestures or eye contact being\
    \ misunderstood as rude or threatening. Thus, affective systems should adapt to\
    \ reflect the values of the community and individuals where they will operate\
    \ in order to avoid misunderstanding.\n## Recommendations\nAssuming that well-designed\
    \ affective systems have a minimum subset of configurable norms incorporated in\
    \ their knowledge base:\n1\\.\_\_\_Affective systems should have capabilities\
    \ to identify differences between the values they are designed with and the differing\
    \ values of those with whom the systems are interacting.\n2\\.\_\_\_Where appropriate,\
    \ affective systems will adapt accordingly over time to better fit the norms of\
    \ their users. As societal values change, there needs to be a means to detect\
    \ and accommodate such cultural change in affective systems.\n3\\.\_\_\_Those\
    \ actions undertaken by an affective system that are most likely to generate an\
    \ emotional response should be designed to be easily changed in appropriate ways\
    \ by the user without being easily hacked by actors with malicious intentions.\
    \ Similar to how software today externalizes the language and vocabulary to be\
    \ easily changeable based on location, affective systems should externalize some\_\
    of the core aspects of their actions.\n## Further Resources\_\_\n- J. Bielby,\
    \ \u201CComparative Philosophies in Intercultural Information Ethics.\u201D _Confluence:\
    \ Online Journal of World Philosophies _2, no. 1, pp. 233\u2013253, 2015.\_\_\_\
    \_\_\_\_\_\_\_\n- M. Velasquez, C. Andre, T. Shanks, and M. J. Meyer. \u201C[Ethical\
    \ Relativism.](https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/ethical-relativism/)\u201D\
    \ Markkula Center for Applied Ethics, Santa Clara, CA: Santa Clara University,\
    \ August 1, 1992.\n- Culture reflects the moral values and ethical norms governing\
    \ how people should behave and interact with others. \u201C[Ethics, an Overview](https://courses.lumenlearning.com/boundless-management/chapter/ethics-an-overview/).\u201D\
    \ Boundless Management. \n- T. Donaldson, \u201C[Values in Tension: Ethics Away\
    \ from Home Away from Home](https://hbr.org/1996/09/values-in-tension-ethics-away-from-home).\u201D\
    \ _Harvard Business Review. _September\u2013 October 1996.\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recHsUV9w4HKBA1w1
  Tags:
  - affective-computing
  airtable_createdTime: '2023-06-05T06:05:10.000Z'
  airtable_id: recIUf0R50ogibwe6
  title: How do we assess the impact of AI without cultural sensitivity, on value
    systems of communities in which they are deployed?
- ChallengeInstances:
  - recQWEzfzmzmfstG9
  Description: |
    AI and associated infrastructure has shifted relationships of researchers and participants through greater secondary use of data, online platforms that connect (sometimes invisibly) participants to researchers, heightened risks of re-identification, and tools that may be used in unanticipated contexts
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  - recLHILkx2JDFsLbX
  Principles:
  - recKdujFoPJr4ZAhZ
  - recPg7Ov0priGGtLm
  Tags:
  - AI
  airtable_createdTime: '2023-06-14T12:17:55.000Z'
  airtable_id: recJ6khG2kY2Tx5ae
  title: 'Researcher-participant relationships '
- Description: "\"**Issue:** The right to truthful information is key to a democratic\
    \ society and to achieving sustainable development and a more equal world, but\
    \ A/IS poses risks to this right that must be managed.\n## Background\nSocial\
    \ media have become the dominant technological infrastructure for the dissemination\
    \ of information such as news, opinion, advertising, etc., and are currently in\
    \ the vanguard of the movement toward customized/targeted information based on\
    \ user profiling that involves significant use of A/IS techniques. Analysis of\
    \ opinion polls and trends in social networks, blogs, etc., and of the emotional\
    \ response to news items can be used for the purposes of manipulation, facilitating\
    \ both the selection of news that guides public opinion in the desired direction\
    \ and the practice of sensationalism.\nThe \"personalization of the consumer experience\"\
    , that is, the adaptation of articles to the interests, political vision, cultural\
    \ level, education, and geographic location of the reader, is a new challenge\
    \ for the journalism profession that expands the possibilities of manipulation.\n\
    The information infrastructure is currently lacking in transparency, such that\
    \ it is difficult or impossible to know (except perhaps for the infrastructure\
    \ operator):\n\u2022\_\_\_\_\_what private information is being collected for\n\
    user profiling and by whom,\n\u2022\_\_\_\_\_which groups are targeted and by\
    \ whom,\n\u2022\_\_\_\_\_what information has been received by any given targeted\
    \ group,\n\u2022\_\_\_\_\_who financed the creation and dissemination of this\
    \ information,\n\u2022\_\_\_\_\_the percentage of the information being disseminated\
    \ by bots, and\n\u2022\_\_\_\_\_who is financing these bots.\nMany actors have\
    \ found this opaque infrastructure ideal for spreading politically motivated disinformation,\
    \ which has a negative effect on the creation of a more equal world, democracy,\
    \ and the respect for fundamental rights. This disinformation can have tragic\
    \ consequences. For instance, human rights groups have unearthed evidence that\
    \ the military authorities of Myanmar used Facebook for inciting hatred against\
    \ the Rohingya Muslim minority, hatred which facilitated an ethnic cleansing campaign\
    \ and the murder of up to 50,000 people.14 The UN determined that these actions\
    \ constituted genocide, crimes against humanity, and war crimes.15\n## Recommendations\n\
    To protect democracy, respect fundamental rights, and promote sustainable development,\
    \ governments should implement a legislative agenda which prevents the spread\
    \ of misinformation and hate speech, by:\n\u2022\_\_\_\_\_Ensuring more control\
    \ and transparency in the use of A/IS techniques for user profiling in order to\
    \ protect privacy and prevent user manipulation.\n\u2022\_\_\_\_\_Using A/IS techniques\
    \ to detect untruthful information circulating in the infrastructures, overseen\
    \ by a democratic body to prevent potential censorship.\n\u2022\_\_\_\_\_Obliging\
    \ companies owning A/IS infrastructures to provide more transparency regarding\
    \ their algorithms, sources of funding, services, and clients.\n\u2022\_\_\_\_\
    \_Defining a new legal status somewhere between \"platforms\" and \"content providers\"\
    \ for A/IS infrastructures.\n\u2022\_\_\_\_\_ Reformulating the deontological\
    \ codes of the journalistic profession to take into account the intensive use\
    \ of A/IS techniques foreseen\_in the future. \u2022\_\_\_\_\_Promoting the right\
    \ to information in official documents, and developing A/IS techniques to automate\
    \ journalistic tasks such as verification of sources and checking the accuracy\
    \ of the information in official documents, or in the selection, hierarchy, assessment,\
    \ and development of news, thereby contributing to objectivity and reliability.\n\
    ## Further Resources\n\u2022\_\_\_\_\_M. Broussard, \u201CArtificial Iintelligence\
    \ for Investigative Reporting: Using an expert system to enhance journalists\u2019\
    \ ability to discover original public affairs stories.\u201D Digital Journalism,\
    \ vol. 3, no. 6, pp. 814-831, 2015.\n\u2022\_\_\_\_\_M. Carlson, \u201CThe robotic\
    \ reporter: Automated journalism and the redefinition of labor, compositional\
    \ forms, and journalistic authority.\u201D Digital Journalism, vol. 3, no. 3,\
    \ pp. 416-431, 2015.\n\u2022\_\_\_\_\_A. L\xF3pez Barriuso, F. de la Prieta Pintado,\
    \ \xC1. Lozano Murciego, , D. Hern\xE1ndez de la Iglesia and J. Revuelta Herrero,\
    \ JOUR-MAS: A Multiagent System Approach to Help Journalism Management, vol. 4,\
    \ no. 4, 2015.\n\u2022\_\_\_\_\_P. Mozur, \u201DA Genocide Incited on Facebook\
    \ with Posts from Myanmar\u2019s Military,\u201D _The New York Times,_ Oct. 15\
    \ 2018. [https:// www.nytimes.com/2018/10/15/technology/ myanmar-faceboo.k-genocide.html](https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html)\n\
    \u2022\_\_\_\_\_UK Parliament, House of Commons, Digital, Culture, Media and Sport\
    \ Committee Disinformation and \u2018fake news\u2019: Interim Report, Fifth Report\
    \ of Session 2017\u201319UK Parliament, Published on July 29, 2018\"\np.142-144\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recqJAVTtqOUfypNI
  airtable_createdTime: '2023-06-05T11:18:00.000Z'
  airtable_id: recJV8yTX0MnF3rGD
  title: AI poses risks to the right to truthful information
- Description: "\"**Issue:** Education to prepare the future workforce, in both HIC\
    \ and LMIC, to design ethical A/IS applications or to have a comparative advantage\
    \ in working alongside A/IS, is either lacking or unevenly available, risking\
    \ inequality perpetuated across generations, within and between countries, constraining\
    \ equitable growth, supporting a sustainable future, and achievement of the SDGs.\n\
    ## Background\nMultiple international institutions, in particular educational\
    \ engineering organizations,27 have called on universities to play an active role,\
    \ both locally and globally, in the resolution of the enormous problems that the\
    \ world faces in securing peace, prosperity, planet protection, and universal\
    \ human dignity: armed conflict, social injustice, rapid climate change, abuse\
    \ of human rights, etc. Addressing global social problems is one of the central\
    \ objectives of many universities, transversal to their other functions, including\
    \ research in A/IS. UNESCO points out that universities\u2019 preparation of future\
    \ scientists and engineers for social responsibility is presently very limited,\
    \ in view of the enormous ethical and social problems associated with technology.28\
    \ Enhancing the global dimension of engineering\_in undergraduate and postgraduate\
    \ A/IS education is necessary, so that students can\_be prepared as technical\
    \ professionals, aware\_of the opportunities and risks that A/IS present, and\
    \ ready for work anywhere in the world in\_any sector.\nEngineering studies at\
    \ the university and postgraduate levels is just one dimension of the A/IS education\
    \ challenge. For instance, business, law, public policy, and medical students\
    \ will also need to be prepared for professions where A/IS are a partner, and\
    \ to have internalized ethical principles to guide the deployment of such technologies.\
    \ LMIC need financial and academic support to incorporate global A/IS professional\
    \ curricula in their own universities, and all countries need to develop the pipeline\
    \ by preparing elementary and secondary school students to access such professional\
    \ programs. While the need for curriculum reform is recognized, the impact of\
    \ A/IS on various professions and socioeconomic contexts is, at this time, both\
    \ evolving and largely undocumented. Thus, the overhaul of education systems at\
    \ all levels should be preceded by A/IS research.\nMuch of LMIC education is not\
    \ globally competitive today, so there is a risk that the global advent of A/IS\
    \ could negatively affect the chances of young people in LMIC finding productive\
    \ employment, further fueling global inequality. Education systems worldwide have\
    \ to be reformed and transformed to fit the new demands of the information age,\
    \ in view of the changing mix of skills demanded from the workforce.29 In 21st\
    \ century education, it has been observed that children need less rote knowledge,\
    \ given so much is instantly accessible on the web and more tools to network and\
    \ innovate are available; less memory and more imagination should be developed;\
    \ and fewer physical books and more internet access is required. Young people\
    \ everywhere need to develop their capacities for creativity, human empathy, ethics,\
    \ and systems thinking in order to work productively alongside robots and A/IS\
    \ technologies. Science, Technology, Engineering, Art/design, and Math (STEAM)\
    \ subjects need to be more extensive and more creatively taught.30 In addition,\
    \ research is needed to establish ways that a new subject, empathy, can be added\
    \ to these crucial 21st century subjects in order to educate the future A/IS workforce\
    \ in social skills. Instead, in rich and poor countries alike, children are continuing\
    \ to be educated for an industrial age which has disappeared or never even arrived.\
    \ LMIC education systems, being less entrenched in many countries, may have the\
    \ potential to be more flexible than those in HIC. Perhaps A/IS can be harnessed\
    \ to help educational systems to leapfrog into the 21st century, just as mobile\
    \ phone technology enabled LMIC leapfrog over the phase of wired communication\
    \ infrastructure.\n## Recommendations\nEducation with respect to A/IS must be\
    \ targeted to three sets of students: the general public, present and future professionals\
    \ in A/IS, and present and future policy makers. To prepare the future workforce\
    \ to develop culturally appropriate A/IS, to work productively and ethically alongside\
    \ such technologies, and to advance the UN SDGs, the curricula in HIC and LMIC\
    \ universities and professional schools require innovation. Equally importantly,\
    \ preuniversity education systems, starting with early childhood education, need\
    \ to be reformed to prepare society for the risks and opportunities of the A/IS\
    \ age, rather than the current system which prepares society for work in an industrial\
    \ age that ended with the 20th century. Specific recommendations include:\n\u2022\
    \_\_\_\_\_Preparing future managers, lawyers, engineers, civil servants, and entrepreneurs\
    \ to work productively and ethically as global citizens alongside A/IS, through\
    \ reform of undergraduate and graduate curricula as well as of preschool, primary,\
    \ and secondary school curricula. This will require:\n\u2022\_\_\_\_\_Fomenting\
    \ interaction between universities and other actors such as companies, governments,\
    \ NGOs, etc., with respect to A/IS research through definition of research priorities\
    \ and joint projects, subcontracts to universities, participation in observatories,\
    \ and co-creation of curricula, cooperative teaching, internships/service learning,\
    \ and conferences/seminars/courses.\n\u2022\_\_\_\_\_Establishing and supporting\
    \ more multidisciplinary degrees that include\_A/IS, and adapting university curricula\
    \ to provide a broad, integrated perspective which allows students to understand\
    \ the impact of A/IS in the global, economic, environmental, and sociocultural\
    \ domains and trains them as future policy makers in A/IS fields.\n\u2022\_\_\_\
    \_\_Integrating the teaching of ethics and\_A/IS across the education spectrum,\
    \ from preschool to postgraduate curricula, instead of relegating ethics to a\
    \ standalone module with little direct practical application.\n\u2022\_\_\_\_\_\
    Promoting service learning opportunities that allow A/IS undergraduate and graduate\
    \ students to apply their knowledge to meet the needs of a community.\n\u2022\_\
    \_\_\_\_Creating international exchange programs, through both private and public\
    \ institutions, which expose students to different cultural contexts for A/IS\
    \ applications in both HIC and LMIC.\n\u2022\_\_\_\_\_Creating experimental curricula\
    \ to prepare people for information-based work in the 21st century, from preschool\
    \ through postgraduate education.\n\u2022\_\_\_\_\_Taking into account transversal\
    \ competencies students need to acquire to become ethical global citizens, i.e.,\
    \ critical thinking, empathy, sociocultural awareness, flexibility, and deontological\
    \ reasoning\_in the planning and assessment of\_A/IS curricula.\n\u2022\_\_\_\_\
    \_ Training teachers in teaching methodologies suited to addressing challenges\
    \ imposed in the age of A/IS. \n\u2022\_\_\_\_\_Stimulating STEAM courses in preuniversity\
    \ education.\n\u2022\_\_\_\_\_Encouraging high-quality HIC-LMIC collaborative\
    \ A/IS research in both private and public universities.\n\u2022\_\_\_\_\_Conducting\
    \ research to support innovation in education and business for the A/IS world,\
    \ which could include:\n\u2022\_\_\_\_\_Researching the impact of A/IS on the\
    \ governance and macro/micro strategies of companies and organizations, together\
    \ with those companies, in an interdisciplinary manner which harnesses expertise\
    \ of both social scientists and technology experts.\n\u2022\_\_\_\_\_Researching\
    \ the impact of A/IS on the business model for the development of new products\
    \ and services through the collaborative efforts of management, operations, and\
    \ the technical research and development function.\n\u2022\_\_\_\_\_Researching\
    \ how empathy can be taught and integrated into curricula, starting at the preschool\
    \ level.\n\u2022\_\_\_\_\_Researching how schools and education systems in low-income\
    \ settings of both HIC and LMIC can leverage their lessentrenched interests to\
    \ leapfrog into a 21st century-ready education system.\n\u2022\_\_\_\_\_Establishing\
    \ ethics observatories in universities with the purpose of fostering an informed\
    \ public opinion capable\_of participating in policy decisions\_regarding the\
    \ ethics and social impact\_of A/IS applications.\n\u2022\_\_\_\_\_Creating professional\
    \ continuing education and employment opportunities in A/IS for current professionals,\
    \ including through online and executive education courses.\n\u2022\_\_\_\_\_\
    Creating educative mass media campaigns to elevate society\u2019s ongoing baseline\
    \ level of understanding of A/IS systems, including what it is, if and how it\
    \ can be trusted in various contexts, and what are its limitations.\n## Further\
    \ resources\n\u2022\_\_\_\_\_ABET Computing and Engineering Accreditation Criteria\
    \ 2018. Available at: [http://www.abet.org/accreditation/ accreditation-criteria/](http://www.abet.org/accreditation/accreditation-criteria/)\n\
    \u2022\_\_\_\_\_ABET, 2017 ABET Impact Report, Working Together for a Sustainable\
    \ Future_, _2017.\n\u2022\_\_\_\_emlyon business school, Artificial Intelligence\
    \ in Management (AIM) Institute [http://aim. em-lyon.com](http://aim.em-lyon.com/)\n\
    UNESCO,_ The UN Decade of Education for Sustainable Development, Shaping the Education\
    \ of Tomorrow_. UNESCO 2012.\"\np.153-156\n"
  OverarchingPrinciples (from Principles):
  - recSqx6wklVpDzx3s
  Principles:
  - recSqx6wklVpDzx3s
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - rec2679E9TXh1DCL8
  airtable_createdTime: '2023-06-05T11:54:10.000Z'
  airtable_id: recJfAMUB8HdjVQYD
  title: How do we educate the workforce for design for, or work with, ethical AI?
- Description: "\u201CBecause of their statistical basis, all ML systems have error\
    \ rates. Even though in many cases ML systems are far more accurate than human\
    \ beings, there is danger in assuming that simply because a system\u2019s predictions\
    \ are more accurate than a human\u2019s, the outcome is necessarily better. Even\
    \ if the error rate is close to zero, in a tool with millions of users, thousands\
    \ could be affected by error rates. Consider the example of Google Photos. In\
    \ 2015 Google Photos\u2019 image recognition software was found to have a terribly\
    \ prejudicial and offensive error: it was occasionally labeling photos of black\
    \ people as gorillas. Because the system used a complex ML model, engineers were\
    \ unable to figure out why this was happening. The only \u201Csolution\u201D they\
    \ could work out to this \u201Cracist\u201D ML was merely a band-aid: they removed\
    \ any monkey-related words from the list of image tags. \nNow, imagine a similar\
    \ software system used by U.S Customs and Border Patrol that photographs every\
    \ person who enters and exits the U.S. and cross-references it with a database\
    \ of photos of known or suspected criminals and terrorists. In 2016, an estimated\
    \ 75.9 million people arrived in the United States.31 Even if the facial recognition\
    \ system was 99.9% accurate, the 0.1% error rate would result in 75,900 people\
    \ being misidentified. How many of these people would be falsely identified as\
    \ wanted criminals and detained? And what would the impact be on their lives?\
    \ Conversely, how many known criminals would get away? Even relatively narrow\
    \ error rates in cases such as these can have severe consequences.\u201D (Access\
    \ Now, 2018, p. 13)\n"
  OverarchingPrinciples (from Principles):
  - recOHnq45Fq7YWsRO
  Principles:
  - recOHnq45Fq7YWsRO
  Reference:
  - Access Now. (2018). HUMAN RIGHTS IN THE AGE OF ARTIFICIAL INTELLIGENCE. Access
    Now. https://www.accessnow.org/cms/assets/uploads/2018/11/AI-and-Human-Rights.pdf
  Sources:
  - recH8KmnURSknCr5y
  airtable_createdTime: '2023-05-29T07:17:18.000Z'
  airtable_id: recJjTMjpfE0WHSWu
  title: Accuracy paradox and the risks of false positives
- Description: "\"**Issue: **Mass personalization\_of instruction\n## Background\n\
    The mass personalization of education offers better education for all at very\
    \ low cost through A/IS-enabled computer-based instruction that promises to free\
    \ up teachers to work with kids individually to pursue their passions. These applications\
    \ will rely on the continuous gathering of personal data regarding mood, thought\
    \ processes, private stories, physiological data, and more. The data will be used\
    \ to construct a computational model of each child\u2019s interests, understanding,\
    \ strengths, and weaknesses. The model provides an intimate understanding of how\
    \ they think, what they understand, how they process information, or react to\
    \ new information; all of which can be used to drive instructional content and\
    \ feedback.\nSharing of this data between classes, enabling it to follow students\
    \ through their schooling, will make the models more effective and beneficial\
    \ to children, but it also exposes children and their families to social control.\
    \ If performance data are correlated with social data on a family, it could be\
    \ used by social authorities in decision-making about the family. For example,\
    \ since 2015-2018, well-being digital tests were performed in schools in Denmark.\
    \ Children were asked about everything from bullying, loneliness, and stomach\
    \ aches. Recently it was disclosed that although the collected data was presented\
    \ as anonymous, they were not. Data were stored with social security numbers,\
    \ correlated with other test data, and even used in case management by some Danish\
    \ municipalities.5 \_Commercial profiling and correlation of different sets of\
    \ personal data may further affect these children in future job or educational\
    \ situations.\n## Recommendation\nEducational data offer a unique opportunity\
    \ to model individuals\u2019 thought processes and could be used to predict or\
    \ change individuals\u2019 behavior in many situations. Governments and organizations\
    \ should classify educational data\_as being sensitive and implement special protective\
    \ standards.\nChildren\u2019s data should be held in \u201Cescrow\u201D\_and not\
    \ used for any commercial purposes\_until a child reaches the age of majority\
    \ and is able to authorize use as they choose.\n## Further Resources\n\u2022\_\
    \_\_\_\_The journal of the International Artificial Intelligence in Education\
    \ Society[:\_](http://iaied.org/journal/)\n<http://iaied.org/journal/>\n\u2022\
    \_\_\_\_\_Deeper discussion and bibliography of future trends of AI-based education\
    \ with utopian and dystopian case scenarios: N. Pinkwart, \u201CAnother 25 Years\
    \ of AIED? Challenges and\nOpportunities for Intelligent Educational\nTechnologies\
    \ of the Future,\u201D International Journal of Artificial Intelligence in Education,\
    \ vol. 26, no. 2, pp. 771\u2013783, 2016.[ ](https://doi.org/10.1007/s40593-016-0099-7)[Online].\n\
    Available: [https://doi.org/10.1007/s40593016-0099-7](https://doi.org/10.1007/s40593-016-0099-7)\
    \ [Accessed Dec. 2018].\n\u2022\_\_\_\_\_Information Commissioners Office (ico.),\u201C\
    What if we want to profile children or make automated decisions about them?\u201D\
    \ [https://ico. org.uk/for-organisations/guide-to-the-generaldata-protection-regulation-gdpr/children-andthe-gdpr/what-if-we-want-to-profile-childrenor-make-automated-decisions-about-them/](https://ico.org.uk/for-organisations/guide-to-the-general-data-protection-regulation-gdpr/children-and-the-gdpr/what-if-we-want-to-profile-children-or-make-automated-decisions-about-them/)\n\
    \u2022\_\_\_\_\_K. Firth-Butterfield, \u201CWhat happens when your child\u2019\
    s friend is an AI toy that talks back?\u201D in World Economic Forum: Generation\
    \ AI, [https://www.weforum.org/agenda/2018/05/ generation-ai-what-happens-when-your-childsinvisible-friend-is-an-ai-toy-that-talks-back/,](https://www.weforum.org/agenda/2018/05/generation-ai-what-happens-when-your-childs-invisible-friend-is-an-ai-toy-that-talks-back/)\_\
    May 22, 2018.\"\n\np.114-115\n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  Principles:
  - reczVPIH1y2OMpAJH
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - reclzrMrQSQls3PI9
  Tags:
  - education
  airtable_createdTime: '2023-06-05T09:40:36.000Z'
  airtable_id: recKzhZVabDuYM6rG
  title: Collecting data to support learning without falling into social profiling
    of learners
- Cases:
  - reciNqxyfUgE5XM7t
  - recrVkbG0XGe2Ca0v
  - recOOmVQviRyJGvea
  - recmS3zSMbR3ofAR5
  - recpFE7h6QhaEpNAQ
  OverarchingPrinciples (from Principles):
  - recSqx6wklVpDzx3s
  - recSqx6wklVpDzx3s
  - recSqx6wklVpDzx3s
  - recSqx6wklVpDzx3s
  Principles:
  - recSqx6wklVpDzx3s
  - recMGB4iC5oaCtr5x
  - recK5zFiq18A3wHAE
  - recZbEXiEs1AlDdn3
  airtable_createdTime: '2023-05-18T18:54:16.000Z'
  airtable_id: recL5M2Hye2XHK7zg
  title: Digital divides in quality of experience with technology
- Description: "\"Should affective systems interact using the norms for verbal and\
    \ nonverbal communication consistent with the norms of the society in which they\
    \ are embedded?\n## Background\nIndividuals around the world express intentions\
    \ differently, including the ways that they make eye contact, use gestures, or\
    \ interpret silence. These particularities are part of an individual\u2019s and\
    \ a society's culture and are incorporated into their affective systems in order\
    \ to convey the intended message. To ensure that the emotional systems of autonomous\
    \ and intelligent systems foster effective communication within a specific culture,\
    \ an understanding of the norms/values of the community where the affective system\
    \ will be deployed is essential.\n## Recommendations\n1\\.\_\_\_\_\_\_ A well-designed\
    \ affective system will have a set of essential norms, specific to its intended\
    \ cultural context of use, in its knowledge base. Research has shown that A/IS\
    \ technologies can use at least five types of cues to simulate social interactions.\n\
    2\\.\_\_\_\_\_\_These include: physical cues such as simulated facial expressions,\
    \ psychological cues such as 1simulated humor or other emotions, use of language,\
    \ use of social dynamics like taking turns, and through social roles such as acting\
    \ as a tutor or medical advisor. Further examples are listed below:\na.\_\_\_\_\
    \_\_Well-designed affective systems will use language with affective content carefully\
    \ and within the contemporaneous expectations of the culture. An example is small\
    \ talk. Although small talk is useful for establishing a friendly rapport in many\
    \ communities, some communities see people that use small talk as insincere and\
    \ hypocritical. Other cultures may consider people that do not use small talk\
    \ as unfriendly, uncooperative, rude, arrogant, or ignorant. Additionally, speaking\
    \ with proper vocabulary, grammar, and sentence structure may contrast with the\
    \ typical informal interactions between individuals. For example, the latest trend,\
    \ TV show, or other media may significantly influence what is viewed as appropriate\
    \ vocabulary and interaction style.\nb.\_\_\_\_\_\_Well-designed affective systems\
    \ will recognize that the amount of personal space (proxemics) given by individuals\
    \ in an important part of culturally specific human interaction. People from varying\
    \ cultures maintain, often unknowingly, different spatial distances between themselves\
    \ to establish smooth communication. Crossing these limits may require explicit\
    \ or implicit consent, which A/IS must learn to negotiate to avoid transmitting\
    \ unintended messages.\nc.\_\_\_\_\_\_\_Eye contact is an essential component\
    \ for culturally sensitive social interaction. For some interactions, direct eye\
    \ contact is needed but for others it is not essential and may even generate misunderstandings.\
    \ It is important that A/IS be equipped to recognize the role of eye contact in\
    \ the development of emotional interaction.\nd.\_\_\_\_\_\_Hand gestures and other\
    \ non-verbal communication are very important for social interaction. Communicative\
    \ gestures are culturally specific and thus should be used with caution in cross-cultural\
    \ situations. The specificity of physical communication techniques must be acknowledged\
    \ in the design of functional affective systems. For instance, although a \u201C\
    thumbs-up\u201D sign is commonly used to indicate approval, in some countries\
    \ this gesture can be considered an insult.\ne.\_\_\_\_\_\_ Humans use facial\
    \ expressions to detect emotions and facilitate communication. Facial expressions\
    \ may not be universal across cultures, however, and A/IS trained with a dataset\
    \ from one culture may not be readily usable in another culture. Well-developed\
    \ A/IS will be able to recognize, analyze, and even display facial expressions\
    \ essential for culturally specific social interaction.\n3\\.\_\_\_\_\_\_Engineers\
    \ should consider the need for cross-cultural use of affective systems.\_Well-designed\
    \ systems will have options innate to facilitate flexibility in cultural programming.\
    \ Mechanisms to enable and disable culturally specific \u201Cadd-ons\u201D should\
    \ be considered an essential part of A/IS development.\n## Further Resources\n\
    \u2022\_\_\_\_\_G. Cotton, \u201C[Gestures to Avoid in Cross-Cultural Business:\
    \ In Other Words, \u2018Keep Your Fingers to Yourself!\u2019](http://www.huffingtonpost.com/gayle-cotton/cross-cultural-gestures_b_3437653.html)\u201D\
    \ _Huffington Post, _June 13, 2013.\n\u2022\_\_\_\_\_\u201C[Paralanguage Across\
    \ Cultures,](https://cultureplusconsulting.com/2015/04/16/paralanguage-across-cultures/)\u201D\
    \ Sydney, Australia: Culture Plus Consulting, 2016.\n\u2022\_\_\_\_\_G. Cotton,\
    \ _[Say Anything to Anyone, Say Anything to Anyone, Anywhere: 5 Keys to Successful\
    \ Cross-Cultural Communication](http://www.wiley.com/WileyCDA/WileyTitle/productCd-111842042X.html)_[.](http://www.wiley.com/WileyCDA/WileyTitle/productCd-111842042X.html)\
    \ Hoboken, NJ: Wiley, 2013.\n\u2022\_\_\_\_\_D. Elmer, _[Cross-Cultural Connections:\
    \ ](https://www.ivpress.com/cross-cultural-connections)_\n_[Stepping Out and Fitting\
    \ In Around the World](https://www.ivpress.com/cross-cultural-connections)_[.](https://www.ivpress.com/cross-cultural-connections)\
    \ Westmont, IL: InterVarsity Press, 2002.\n\u2022\_\_\_\_\_B. J. Fogg, [Persuasive\
    \ Technology.](https://dl.acm.org/citation.cfm?id=763957) _Ubiquity_, December\
    \ 2, 2002.\n\u2022\_\_\_\_\_A. McStay, Emotional AI: The Rise of Empathic Media.\
    \ London: Sage, 2018.\nM. Price, \u201C[Facial Expressions\u2014Including Fear\u2014\
    \ May Not Be as Universal as We Thought.](http://www.sciencemag.org/news/2016/10/facial-expressions-including-fear-may-not-be-universal-we-thought)\u201D\
    \ _Science, _October 17, 2016.\"\np.90-91\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - rec9UfowPkXtPUBC9
  Tags:
  - affective-computing
  airtable_createdTime: '2023-06-05T06:03:37.000Z'
  airtable_id: recL5sNl6bi1vrIzj
  title: How do we represent cross-cultural differences in communication through AI
    systems?
- Description: "\"**Issue**: What would it mean for a person to have individually\
    \ controlled terms and conditions for their personal data? \n## Background\nPart\
    \ of providing individually controlled terms and conditions for personal data\
    \ is to help each person consider what their preferences are regarding their data\
    \ versus dictating how they need to share it. While questions along these lines\
    \ are framed in light of a person\u2019s privacy, their preferences also reveal\
    \ larger values for individuals. The ethical issue is whether A/IS act in accordance\
    \ with these values.\nThis process of investigating one\u2019s values to identify\
    \ these preferences is a powerful step towards regaining data agency. The point\
    \ is not only that a person\u2019s data are protected, but also that by curating\
    \ these answers they become educated about how important their information is\
    \ in the context of how it is shared. Most individuals also believe controlling\
    \ their personal data only happens on the sites or social networks to which they\
    \ belong and have no idea of the consequences of how that data may be used by\
    \ others in the future. Agreeing to most standard terms and conditions on these\
    \ sites largely means users consent to give up control of their personal data\
    \ rather than play a meaningful role in defining and curating its downstream use.\_\
    \nThe scope of how long one should or could control the downstream use of their\
    \ data can be difficult to calculate as consent-based models of personal data\
    \ have trained users to release rights on any claims for use of their data which\
    \ are entirely provided to the service, manufacturer, and their partners. However,\
    \ models like YouTube\u2019s [Content ID ](https://www.youtube.com/t/contentid)provide\
    \ a form of precedent for thinking about how an individual\u2019s data could be\
    \ technically protected where it is considered as an asset they could control\
    \ and copyright. Here is language from [YouTube\u2019s site about the service:](https://support.google.com/youtube/answer/2797370?hl=en)\
    \ \u201CCopyright owners can use a system called Content ID to easily identify\
    \ and manage their content on YouTube. Videos uploaded to YouTube are scanned\
    \ against a database of files that have been submitted to us by content owners.\u201D\
    \ In this sense, the question of how long or how far downstream one\u2019s personal\
    \ data should be protected takes on the same logic of how long a corporation\u2019\
    s intellectual property or copyrights could be protected based on initial legal\
    \ terms set.\nOne challenge is how to define use of data that can affect the individual\
    \ directly, versus use of aggregated data. For example, an individual subway user\u2019\
    s travel card, tracking their individual movements, should be protected from uses\
    \ that identify or profile that individual to make inferences about his/her likes\
    \ or location generally. But data provided by a user could be included in an overall\
    \ travel system\u2019s management database, aggregated into patterns for scheduling\
    \ and maintenance as long as the individual-level data are deleted. Where users\
    \ have predetermined via their terms and conditions that they are willing to share\
    \ their data for these travel systems, they can meaningfully articulate how to\
    \ share their information.\nUnder current business models, it is common for people\
    \ to consent to the sharing of discrete data like credit card transaction data,\
    \ answers to test questions, or how many steps they walk. However, once aggregated\
    \ these data and the associated insights may lead to complex and sensitive conclusions\
    \ being drawn about individuals. This end use of the individual\u2019s data may\
    \ not have been part of the initial sharing agreement. This is why models for\
    \ terms and conditions created for user control typically alert people via onscreen\
    \ or other warning methods when their predetermined preferences are\_not being\
    \ honored.\_\n## Recommendation\nIndividuals should be provided tools that produce\
    \ machine-readable terms and conditions that are dynamic in nature and serve to\
    \ protect their data and honor their preferences for its use.\_\n\u2022\_\_\_\_\
    \_Personal data access and consent should be managed by the individual using their\
    \ curated terms and conditions that provide notification and an opportunity for\
    \ consent at the time data are exchanged, versus outside actors being able to\
    \ access personal data without an individual\u2019s awareness or control.\_\_\n\
    \u2022\_\_\_\_\_Terms should be presented in a way that allows a user to easily\
    \ read, interpret, understand, and choose to engage with any A/IS. Consent should\
    \ be both conditional and dynamic, where \u201Cdynamic\u201D means downstream\
    \ uses of a person\u2019s data must be explicitly called out, allowing them to\
    \ cancel a service and potentially rescind or \u201Ckill\u201D any data they have\
    \ shared with a service to date via the use of a \u201CSmart Contract\u201D or\
    \ specific conditions as described in mutual terms and conditions between two\
    \ parties at the time of exchange.\n\u2022\_\_\_\_\_For further information on\
    \ these issues, please see the following section in regard to algorithmic agents\
    \ and their application.\n## Further Resources\n\u2022\_\_\_\_\_IEEE P7012\u2122\
    \ - IEEE [Standards Project for Machine Readable Personal Privacy Terms.](https://development.standards.ieee.org/get-file/P7012.pdf%3Ft=95323600003)\_\
    This approved standardization project (currently in development) directly honors\_\
    the goals laid out in Section One of\_this document.\_\n\u2022\_\_\_\_\_[The Personalized\
    \ Privacy Assistant Project](https://privacyassistant.org/) Carnegie Mellon University.\
    \ [https:// privacyassistant.org,](https://privacyassistant.org/) 2019.\_\\\\\n\
    \u2022\_\_\_\_\_M. Orcutt, \u201C[Personal AI Privacy Watchdog Could Help You\
    \ Regain Control of Your Data\u201D](https://www.technologyreview.com/s/607830/personal-ai-privacy-watchdog-could-help-you-regain-control-of-your-data/)\
    \ MIT Technology Review, May 11, 2017.\n\u2022\_\_\_\_\_M. Hintze, [Privacy Statements:\
    \ Purposes, Requirements, and Best Practices.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2927105)\
    \ Cambridge, U.K.: Cambridge University Press, 2017.\n\u2022\_\_\_\_\_D. J. Solove,\
    \ \u201CPrivacy self-management and the consent dilemma, Harvard Law Review, vol.\
    \ 126, no. 7, pp. 1880\u20131903, May 2013.\n\u2022\_\_\_\_\_N. Sadeh, M. Degeling,\
    \ A. Das, A. S. Zhang, A. Acquisti, L. Bauer, L. Cranor, A. Datta, and D. Smullen,\
    \ A Privacy Assistant for the Internet of Things: [https://www.usenix.org/sites/default/\
    \ files/soups17\\_poster\\_sadeh.pdf](https://www.usenix.org/sites/default/files/soups17_poster_sadeh.pdf)\n\
    \u2022\_\_\_\_\_H. Lee, R. Chow, M. R. Haghighat, H. M. Patterson and A. Kobsa,\
    \ \u201CIoT Service Store: A Web-based System for Privacy-aware IoT Service Discovery\
    \ and Interaction,_\u201D 2018 IEEE International Conference on Pervasive Computing\
    \ and Communications\_Workshops (PerCom Workshops),_\_Athens, pp. 107-112, 2018.\n\
    \u2022\_\_\_\_\_L. Cranor, M. Langheinrich, M. Marchiori, M. Presler-Marshall,\
    \ and J. Reagle, \u201CThe Platform for Privacy Preferences 1.0 (P3P1.0) Specification,\u201D\
    \ W3C Recommendation, [Online]. Available: www.w3.org/TR/P3P/, Apr. 2002.\nL.\
    \ F. Cranor, \u201CPersonal Privacy Assistants in the Age of the Internet of Things,\u201D\
    \ in World Economic Forum Annual Meeting, 2016\"\n\np.108-110\n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  - recLHILkx2JDFsLbX
  Principles:
  - recPg7Ov0priGGtLm
  - recsvi4LnhEEPyQ1h
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recwWov6KzmwU0FQW
  Tags:
  - consent
  airtable_createdTime: '2023-06-05T09:28:22.000Z'
  airtable_id: recO1L6GoFMkA6Lt4
  title: Individuals don't have control over their own data with generic T&C statements
- OverarchingPrinciples (from Principles):
  - recSqx6wklVpDzx3s
  Principles:
  - recMGB4iC5oaCtr5x
  airtable_createdTime: '2023-05-18T19:10:21.000Z'
  airtable_id: recOeanQuUyB6Dx8g
  title: Injustice in intellectual property (e.g., failure to acknowledge participant
    contributions)
- Description: "\"**Issue:** Integration of ethics in\_A/IS-related degree programs\n\
    ## Background\nA/IS engineers and design teams do not always thoroughly explore\
    \ the ethical considerations implicit in their technical work and design choices.\
    \ Moreover, the overall science, technology, engineering, and mathematics (STEM)\
    \ field struggles with the complexity of ethical considerations, which cannot\
    \ be readily articulated and translated into the formal languages of mathematics\
    \ and computer programming associated with algorithms and machine learning.\n\
    Ethical issues can easily be rendered invisible or inappropriately reduced and\
    \ simplified in the context of technical practice. For the dangers of this approach\
    \ see for instance, Lipton and Steinhardt (2018), listed under \u201CFurther Resources\u201D\
    . This problem is further compounded by the fact that many STEM programs do not\
    \ sufficiently integrate applied ethics throughout their curricula. When they\
    \ do, often ethics is relegated to a stand-alone course or module that gives students\
    \ little or no direct experience in ethical decision-making. Ethics education\
    \ should be meaningful, applicable, and incorporate best practices from the broader\
    \ field.\nThe aim of these recommendations is to prepare students for the technical\
    \ training and engineering development methods that incorporate ethics as essential\
    \ so that ethics,\_and relevant principles, like human rights, become naturally\
    \ a part of the design process.\n## Recommendations\n\u2022\_\_\_\_\_Ethics training\
    \ needs to be a core subject\_for all those in the STEM field, beginning at\_\
    the earliest appropriate level and for all advanced degrees.\n\u2022\_\_\_\_\_\
    Effective STEM ethics curricula should be informed by experts outside the STEM\
    \ community_ _from a variety of cultural and educational backgrounds to ensure\
    \ that students acquire sensitivity to a diversity\_of robust perspectives on\
    \ ethics and design.\n\u2022\_\_\_\_\_Such curricula should teach aspiring engineers,\
    \ computer scientists, and statisticians about the relevance and impact of their\
    \ decisions in designing A/IS technologies. Effective ethics education in STEM\
    \ contexts and beyond should span primary, secondary, and postsecondary education,\
    \ and include both universities and vocational training schools.\n\u2022\_\_\_\
    \_\_Relevant accreditation bodies should reinforce this integrated approach as\
    \ outlined above.\n## Further Resources\n\u2022\_\_\_\_\_[IEEE P7000TM Standards\
    \ Project for a Model Process for Addressing Ethical Concerns During System Design.](https://standards.ieee.org/develop/project/7000.html)\
    \ IEEE P7000 aims to enhance corporate IT innovation practices by providing processes\
    \ for embedding a values- and virtue-based thinking, culture, and practice into\
    \ them.\n\u2022\_\_\_\_\_Z. Lipton and J. Steinhardt, [Troubling Trends in Machine\
    \ Learning Scholarship.](https://www.dropbox.com/s/ao7c090p8bg1hk3/Lipton%20and%20Steinhardt%20-%20Troubling%20Trends%20in%20Machine%20Learning%20Scholarship.pdf?dl=0)\
    \ ICML conference paper, July 2018.\n\u2022\_\_\_\_\_J. Holdren, and M. Smith.\
    \ \u201C[Preparing for the Future of Artificial Intelligence.\u201D](https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_future_of_ai.pdf)\
    \ Washington, DC: Executive Office of the President, National Science and Technology\
    \ Council, 2016.\n\u2022\_\_\_\_\_Comparing the UK, EU, and US approaches to AI\
    \ and ethics: C. Cath, S. Wachter, B. Mittelstadt, et al., \u201C[Artificial Intelligence\
    \ and the \u2018Good Society\u2019: The US, EU, and UK Approach.](https://link.springer.com/article/10.1007/s11948-017-9901-7)\u201D\
    \ _Science and Engineering Ethics, _vol._ _24_, _pp. 505-528, 2017.\"\n\np.122\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - reck6xeMUc9jtmLr3
  Tags:
  - research
  airtable_createdTime: '2023-06-05T10:12:55.000Z'
  airtable_id: recPqsTlFK76oGD9C
  title: AI teams may not thoroughly explore the ethical considerations in their work
- Description: "\"**Issue: **Technology choice-making in schools\n## Background\n\
    Children, as minors, have no standing to give or deny consent, or to control the\
    \ use of their personal data. Parents only have limited choices in what are often\
    \ school-wide implementations of educational technology. Examples include the\
    \ use of Google applications, face recognition in security systems, and computer\
    \ driven instruction as described above. In many cases, parents\u2019 only choice\
    \ would be to send their children to a different school, but that choice is seldom\
    \ available.\nHow should schools make these choices? How much input should parents\
    \ have? Should parents be able to demand technology-free teaching?There are many\
    \ gaps in current student data regulation. In June 2018, CLIP, The Center on Law\
    \ and Information Policy at Fordham Law School published, \u201DTransparency and\
    \ the Marketplace for Student Data\u201D.6 This study concluded that \u201Cstudent\
    \ lists are commercially available for purchase on the basis of ethnicity, affluence,\
    \ religion, lifestyle, awkwardness, and even a perceived or predicted need for\
    \ family planning services\u201D. Fordham found that the data market is becoming\
    \ one of the largest and most profitable marketplaces in the United States. Data\
    \ brokers have databases that store billions of data elements on nearly every\
    \ United States consumer. However, information from students in the pursuit of\
    \ an education should not be exploited and commercialized without restraint.\n\
    Fordham researchers found at least 14 data brokers who advertise the sale of student\
    \ information. One sold lists of students as young as two years old. Another sold\
    \ lists of student profiles on the basis of ethnicity, religion, economic factors,\
    \ and even gawkiness.\n## Recommendation\nLocal and national educational authorities\
    \ must work to develop policies surrounding students\u2019 personal data with\
    \ all stakeholders: administrators, teachers, technology providers, students,\
    \ and parents in order to balance the best educational interests of each child\
    \ with the best practices to ensure safety of their personal data. Such efforts\
    \ will raise awareness among all stakeholders of the promise and the compromises\
    \ inherent in new educational technologies.\n## Further reading\n\u2022\_\_\_\_\
    \_Common Sense Media privacy evaluation project: [https://www.commonsense.org/\
    \ education/privacy](https://www.commonsense.org/education/privacy)\n\u2022\_\_\
    \_\_\_D. T. Ritvo, L. Plunkett, and P. Haduong,\u201DPrivacy and Student Data:\
    \ Companion Learning Tools.\u201D Berkman Klein Center for Internet and Society\
    \ at Harvard University, 2017. [Online]. Available: [http://blogs.harvard. edu/youthandmediaalpha/files/2017/03/\
    \ PrivacyStudentData\\_Companion\\_Learning\\_ Tools.pdf ](http://blogs.harvard.edu/youthandmediaalpha/files/2017/03/PrivacyStudentData_Companion_Learning_Tools.pdf)[Accessed\
    \ Dec. 2018][.](http://blogs.harvard.edu/youthandmediaalpha/files/2017/03/PrivacyStudentData_Companion_Learning_Tools.pdf)\n\
    \u2022\_\_\_\_\_F. Alim, N. Cardozo, G. Gebhart, K. Gullo, and A. Kalia, \u201C\
    Spying on Students: School-Issued Devices and Student Privacy,\u201D Electronic\
    \ Frontier Foundation, [https://www.eff.org/wp/ school-issued-devices-and-student-privacy,](https://www.eff.org/wp/school-issued-devices-and-student-privacy)\
    \ April 13, 2017.\n\u2022\_\_\_\_\_N. C. Russell, J. R. Reidenberg, E. Martin,\
    \ and T. Norton, \u201CTransparency and the Marketplace for Student Data,\u201D\
    \ _Virginia Journal of Law and Technology_, Forthcoming. Available at SSRN: [https://ssrn.com/abstract=3191436,](https://ssrn.com/abstract=3191436)\
    \ June 6, 2018.\"\n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  Principles:
  - recU6u0AZbcNj1ik9
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Tags:
  - education
  airtable_createdTime: '2023-06-05T09:43:22.000Z'
  airtable_id: recUKAxgCSxylEiDm
  title: How do we navigate individual choice of technology engagement and school
    or system-wide procurement?
- ChallengeInstances:
  - recuFqYoT6MFNqspL
  - recwl0AdKj2HLIpDB
  - rec3wCdnHy3TpLuya
  - rec4F8A2UI0uvFh3O
  - reccjN6q8gegWbrUE
  - rec16WOKdMVKAiBSJ
  - recUMRucTbQ7tySEW
  - reciceYsMfOtPdaaA
  Description: "AI has potential to have impact beyond just on the participants involved\
    \ into society, and it may not be clear what these impacts will be.\nImmediate\
    \ or proximal impacts are those experienced in the immediate context, for example\
    \ in interactions of researcher-participant groups. Indirect, long-range, and\
    \ dual-use impacts relate to the potential for:\n- secondary impacts that are\
    \ not a direct use of a tool but may relate to system changes that occur as a\
    \ result;\n- that these impacts may occur over a longer period than the immediate\
    \ context of research, which is ordinarily the context for which ethics approval\
    \ is provided\n- and that some effects may arise from unintended uses of dual-use\
    \ technologies, for example facial recognition developed for one purpose may be\
    \ used for surveillance or military purposes.\n\n**\"Issue: **A/IS are often viewed\
    \ only as having impact in market contexts, yet these technologies also have an\
    \ impact on social relations and culture.\nA/IS are expected to have an impact\
    \ beyond market domains and business models, diffusing throughout the global society.\
    \ For instance,\_A/IS have and will impact social relationships\_in a way similar\
    \ to how mobile phones changed our daily lives, reflecting directly on our culture,\
    \ customs, and language. The extent and direction of this impact is not yet clear,\
    \ but documented experience in HIC and high internet-penetration environments\
    \ of trolls, \u201Cfake news,\u201D and cyberbullying on social media offer a\
    \ cautionary tale.11 Depression, social isolation, aggression, and the dissemination\
    \ of violent behavior with damage to human relations, so extreme that, in some\
    \ cases, it has resulted in suicide, are all correlated with the internet.12 As\
    \ an example, the technology for \u201Csmart homes\u201D has been used for inflicting\
    \ domestic violence by remotely locking doors, turning off heat/AC, and otherwise\
    \ harassing a partner. This problem could be easily extended to include elder\
    \ and child abuse.13 Measures need to be developed to prevent A/IS from contributing\
    \ to the emergence or amplification of social disorders.\" (IEEE, 2019, p.141-142)\n"
  OverarchingPrinciples (from Principles):
  - recNB5h9bK4gEE9uc
  Principles:
  - recNB5h9bK4gEE9uc
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recZI7HDWKBU5T22v
  - recK1gdkh8c01WaXx
  - rec7daqDHSCuc70yS
  Tags:
  - AI
  - indirect-impacts
  - wellbeing
  airtable_createdTime: '2023-05-18T18:54:26.000Z'
  airtable_id: recWPbL5kB1Yh5OPf
  title: Indirect, long-range, and dual-use impacts
- Description: "**Issue:** Oversight for algorithms\n## Background\nThe algorithms\
    \ behind A/IS are not subject to consistent oversight. This lack of assessment\
    \ causes concern because end users have no account of how a certain algorithm\
    \ or system came to its conclusions. These recommendations are similar to those\
    \ made in the \u201CGeneral Principles\u201D and \u201CEmbedding Values into Autonomous\
    \ and Intelligent Systems\u201D chapters of _Ethically Aligned Design_, but here\
    \ the recommendations are used as they apply to the narrow scope of this chapter\
    \ .\n## Recommendations\nAccountability: As touched on in the General Principles\
    \ chapter of _Ethically Aligned Design_, algorithmic transparency is an issue\
    \ of concern. It is understood that specifics relating to algorithms or systems\
    \ contain intellectual property that cannot, or will not, be released to the general\
    \ public. Nonetheless, standards providing oversight of the manufacturing process\
    \ of A/IS technologies need to be created to avoid harm and negative consequences.\
    \ We can look to other technical domains, such as biomedical, civil, and aerospace\
    \ engineering, where commercial protections for proprietary technology are routinely\
    \ and effectively balanced with the need for appropriate oversight standards and\
    \ mechanisms to safeguard the public.Human rights and algorithmic impact assessments\
    \ should be explored as a meaningful way to improve the accountability of A/IS.\_\
    These need to be paired with public consultations, and the final impact\_assessments\
    \ must be made public.\n## Further Resources\n\u2022\_\_\_\_\_F. Pasquale, The\
    \ Black Box Society: The Secret Algorithms That Control Money and Information.\
    \ Cambridge, MA: Harvard University Press, 2016.\n\u2022\_\_\_\_\_R. Calo, \u201C\
    Artificial Intelligence Policy: A Primer and Roadmap,\u201D _UC Davis Law Review,_\
    \ 52: pp. 399\u2013435, 2017.\n\u2022\_\_\_\_\_ARTICLE 19. \u201CPrivacy and Freedom\
    \ of Expression in the Age of Artificial Intelligence,\u201D Privacy International,\
    \ April 2018. [Online]. Available: [https://www.article19.org/wpcontent/uploads/2018/04/Privacy-andFreedom-of-Expression-In-the-Age-of-ArtificialIntelligence-1.pdf.](https://www.article19.org/wp-content/uploads/2018/04/Privacy-and-Freedom-of-Expression-In-the-Age-of-Artificial-Intelligence-1.pdf)\
    \ [Accessed October 28, 2018].\n\np.132-133\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recJyzFLE9ry4YEbb
  Tags:
  - corporate-ethics
  airtable_createdTime: '2023-06-05T10:49:39.000Z'
  airtable_id: recX6r1O4jcsp0nIM
  title: How do we provide consistent oversight of AI to ensure they are accountable
    to end-users for any conclusions?
- OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  Principles:
  - recsvi4LnhEEPyQ1h
  Tags:
  - AI
  airtable_createdTime: '2023-05-18T13:41:32.000Z'
  airtable_id: recYHYaeqVHR93Kuo
  title: Incidental data capture (e.g. capturing non-consenting persons in the background
    of video capture, and challenges of disaggregating group-level data)
- Description: "**\"Issue:** Empowerment to raise ethical concerns\n## Background\n\
    Engineers and design teams may encounter obstacles to raising ethical concerns\
    \ regarding their designs or design specifications within their organizations.\
    \ Corporate culture should incentivize technical staff to voice the full range\
    \ of ethical questions to relevant corporate actors throughout the full product\
    \ lifecycle, including the design, development, and deployment phases. Because\
    \ raising ethical concerns can be perceived as slowing or halting a design project,\
    \ organizations need to consider how they can recognize and incentivize values-based\
    \ design as an integral component of product development.\n## Recommendations\n\
    Employees should be empowered and encouraged to raise ethical concerns in\_day-to-day\
    \ professional practice.\nTo be effective in ensuring adoption of ethical considerations\
    \ during product development or internal implementation of A/IS, organizations\
    \ should create a company culture and set of norms that encourage incorporating\
    \ ethical considerations in the design and implementation processes.\nNew categories\
    \ of considerations around these issues need to be accommodated, along with updated\
    \ Codes of Conduct, company value-statements, and other management principles\
    \ so individuals are empowered to share their insights and concerns in an atmosphere\
    \ of trust. Additionally, bottom-up approaches like company \u201Ctown hall meetings\u201D\
    \ should be explored that reward, rather than punish, those who bring up ethical\
    \ concerns.\n## Further Resources\n\u2022\_\_\_\_\_[The British Computer Society\
    \ (BCS),](http://www.bcs.org/category/6030) Code\_of Conduct, 2019.\n\u2022\_\_\
    \_\_\_C. Cath, and L. Floridi, \u201C[The Design of the Internet\u2019s Architecture\
    \ by the Internet  Engineering Task Force (IETF) and Human Rights](http://link.springer.com/article/10.1007%2Fs11948-016-9793-y),\u201D\
    \ _Science and Engineering Ethics, _vol._ _23, no. 2, pp. 449\u2013468, Apr. 2017.\"\
    \n\np.129\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - rec9SSyrfFkmuJkCe
  Tags:
  - corporate-ethics
  airtable_createdTime: '2023-06-05T10:30:49.000Z'
  airtable_id: recZ43i8Cnhohwb2x
  title: How can we incentivise reporting ethical concerns?
- Description: "\"**Issue:** Values-based leadership\n## Background\nTechnology leadership\
    \ should give innovation teams and engineers direction regarding which human values\
    \ and legal norms should be promoted in the design of A/IS. Cultivating an ethical\
    \ corporate culture is an essential component of successful leadership in the\_\
    A/IS domain.\n## Recommendations\nCompanies should create roles for senior-level\
    \ marketers, engineers, and lawyers who can collectively and pragmatically implement\
    \ ethically aligned design. There is also a need for more in-house ethicists,\
    \ or positions that fulfill similar roles. One potential way to ensure values\
    \ are on the agenda in A/IS development is to have a Chief Values Officer (CVO),\
    \ a role first suggested by Kay Firth-Butterfield, see \u201CFurther Resources\u201D\
    . However, ethical responsibility should not be delegated solely to CVOs. They\
    \ can support the creation of ethical knowledge in companies, but in the end,\
    \ all members of an organization will need to act responsibly throughout the design\
    \ process.\nCompanies need to ensure that their understanding of values-based\
    \ system innovation is based on _de jure _and _de facto _international human rights\
    \ standards.\n\u2022\_\_\_\_\_K. Firth-Butterfield, \u201C[How IEEE Aims to Instill\
    \ Ethics in Artificial Intelligence Design,](http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design)\u201D\
    \ The Institute. Jan. 19, 2017. [Online]. Available: [http://theinstitute.ieee.org/ieee-roundup/\
    \ blogs/blog/how-ieee-aims-to-instill-ethicsin-artificial-intelligence-design](http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design).\
    \ [Accessed October 28, 2018]. \u2022\_\_\_\_\_United Nations, [Guiding Principles\
    \ on Business and Human Rights: Implementing the United Nations \u201CProtect,\
    \ Respect and Remedy\u201D Framework,](http://www.ohchr.org/Documents/Publications/GuidingPrinciplesBusinessHR_EN.pdf)\
    \ New York and Geneva: UN, 2011.\n\u2022\_\_\_\_\_Institute for Human Rights and\
    \ Business\n(IHRB), and Shift, ICT [Sector Guide on ](https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf)\n\
    [Implementing the UN Guiding Principles on Business and Human Rights,](https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf)\
    \ 2013.\n\u2022\_\_\_\_\_C. Cath, and L. Floridi, \u201C[The Design of the Internet\u2019\
    s Architecture by the Internet ](http://europepmc.org/abstract/med/27255607)\n\
    [Engineering Task Force (IETF) and Human Rights](http://europepmc.org/abstract/med/27255607).\u201D\
    \ _Science and Engineering Ethics, _vol._ _23, no. 2, pp. 449\u2013468, Apr. 2017.\"\
    \n\np.128\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recV8WjuiXlfbVN5d
  Tags:
  - corporate-ethics
  airtable_createdTime: '2023-06-05T10:28:38.000Z'
  airtable_id: recaBNAcact8Bz6dg
  title: How do leaders provide direction regarding promotion of human values in AI
    design?
- Description: "\"It is presently unknown** **whether long-term interaction with affective\
    \ artifacts that lack cultural sensitivity could alter human social interaction.\n\
    ## Background\nSystems that do not have cultural knowledge incorporated into their\
    \ knowledge base may or may not interact effectively with humans for whom emotion\
    \ and culture are significant. Given that interaction with A/IS may affect individuals\
    \ and societies, it is imperative that we carefully evaluate mechanisms to promote\
    \ beneficial affective interaction between humans and\_A/IS. Humans often use\
    \ mirroring in order to understand and develop their norms for behavior. Certain\
    \ machine learning approaches also address improving A/IS interaction with humans\
    \ through mirroring human behavior. Thus, we must remember that learning via mirroring\
    \ can go in both directions and that interacting with machines has the potential\
    \ to impact individuals\u2019 norms, as well as societal and cultural norms. If\
    \ affective artifacts with enhanced, different, or absent cultural sensitivity\
    \ interact with impressionable humans this could alter their responses to social\
    \ and cultural cues and values. The potential for A/IS to exert cultural influence\_\
    in powerful ways, at scale, is an area of substantial concern.\n## Recommendations\n\
    \_1\\.\_\_\_Collaborative research teams must research the effects of long-term\
    \ interaction of people with affective systems. This should be done using multiple\
    \ protocols, disciplinary approaches, and metrics to measure the modifications\
    \ of habits, norms, and principles as well as careful evaluation of the downstream\
    \ cultural and societal impacts.\n2\\.\_\_\_Parties responsible for deploying\
    \ affective systems into the lives of individuals or communities should be trained\
    \ to detect\_the influence of A/IS, and to utilize mitigation techniques if A/IS\
    \ effects appear to be harmful. It should always be possible to\_shut down harmful\
    \ A/IS.\n## Further Resources\n\u2022\_\_\_\_\_T. Nishida and C. Faucher, Eds.,\
    \ [Modelling Machine Emotions for Realizing Intelligence: Foundations and Applications.](https://www.springer.com/us/book/9783642126031)\
    \ Berlin, Germany: Springer-Verlag, 2010.\n\u2022\_\_\_\_\_D. J. Pauleen, et al.\
    \ \u201CCultural Bias in Information Systems Research and Practice: Are You Coming\
    \ from the Same Place I Am?\u201D _Communications of the Association for Information\
    \ Systems_, vol._ _17,)pp. 1\u201336, 2006. J. Bielby, \u201CComparative Philosophies\
    \ in Intercultural Information Ethics.\u201D _Confluence: Online Journal of World\
    \ Philosophies _2, no. 1, pp. 233\u2013253, 2015.\n\u2022\_\_\_\_\_J. Bryson,\
    \ [\u201CWhy Robot Nannies Probably Won\u2019t Do Much Psychological Damage.\u201D\
    \ ](http://www.cs.bath.ac.uk/~jjb/ftp/Bryson-SharkeyIS09.pdf)A commentary on an\
    \ article by N. Sharkey and A. Sharkey, _The Crying Shame of Robot Nannies. [Interaction\
    \ Studies](http://www.ingentaconnect.com/content/jbp/is)_, vol. 11, no. 2 pp.\
    \ 161\u2013190, July 2010.\n\u2022\_\_\_\_\_A. Sharkey, and N. Sharkey, \u201C\
    Children, the Elderly, and Interactive Robots.\u201D _IEEE Robotics & Automation\
    \ Magazine, _vol.18, no. 1, pp. 32\u201338, March 2011.\"\np.91-92\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recsonQhKLmX4D1ao
  - recZI7HDWKBU5T22v
  Tags:
  - affective-computing
  airtable_createdTime: '2023-06-05T06:05:09.000Z'
  airtable_id: recaFyWRROaJ1ZFyY
  title: How do we assess the impact of AI without cultural sensitivity, on culturally
    situated human social interaction?
- Description: "\"**Issue: **A/IS are changing the nature of work, disrupting employment,\
    \ while technological change is happening too fast for existing methods of (re)training\
    \ the workforce.\n## Background\nThe current pace of technological development\
    \ will heavily influence changes in employment structure. In order to properly\
    \ prepare the workforce for such evolution, actions should be proactive and not\
    \ only reactive. The wave of automation caused by the A/IS revolution will displace\
    \ a very large share of jobs across domains and value chains. The U.S. \u201C\
    automated vehicle\u201D case study analyzed in the White House 2016 report_ Artificial\
    \ Intelligence, Automation, and the Economy _is emblematic of what is at stake:\
    \ \u201C2.2 to 3.1 million existing part- and full-time U.S. jobs are exposed\
    \ over the next two decades, although the timeline remains uncertain.\u201D18\n\
    The risk of unemployment for LMIC is more serious than for developed countries.\
    \ The industry of most LMIC is labor intensive. While labor may be cheap(er) in\
    \ LMIC economies, the ripple effects of A/IS and automation will be felt much\
    \ more than in the HIC economies. The 2016 World Bank Development Report stated\
    \ that the share of occupations susceptible to automation and A/IS is higher in\
    \ LMIC than in HIC, where such jobs have already disappeared. In addition, the\
    \ qualities which made certain jobs easy to outsource to LMIC where wages are\
    \ lower are those that may make them easy to automate.19 An offsetting factor\
    \ is the reality that many LMIC lack the communication, energy, and IT infrastructure\
    \ required to support highly automated industries.20 Notwithstanding this reality,\
    \ the World Bank estimated the automatable share of employment, unadjusted for\
    \ adoption time lag, for LMIC ranges from 85% in Ethiopia to 62% in Argentina,\
    \ compared to the OECD average of 57%.21\nIn the coming decades, the automation\
    \ wave calls for higher investment and the transformation of labor market capacity\
    \ development programs. Innovative and fair ways of funding such an investment\
    \ are required; the solutions should be designed in cooperation with the companies\
    \ benefiting from the increase of profitability, thanks to automation. This should\
    \ be done in a responsible way so that the innovation cycle is not broken, and\
    \ yet workforce capacity does not fall behind the needs of 21st century employment.\
    \ At the same time, A/IS and other digital technologies offer real potential to\
    \ innovate new approaches to job-search assistance, placement, and hiring processes\
    \ in the age of personalized services. The efficiency of matching labor supply\
    \ and demand can be tremendously enhanced by the rise of multisided platforms\
    \ and predictive analytics, provided they do not entrench discrimination.22 The\
    \ case of platforms, such as LinkedIn, for instance, with its 470 million registered\
    \ users, and online job consolidators such as indeed.com and Simply Hired, are\
    \ interesting as an evolution in hiring practices,\_at least for those able to\
    \ access the internet.\nTailored counseling and integrated retraining programs\
    \ also represent promising grounds for innovation. In addition, much will have\
    \ to be done to create fair and effective lifelong skill development/training,\
    \ infrastructures, and mechanisms capable of empowering millions of people to\
    \ viably transition jobs, sectors, and potentially locations, and to address differential\
    \ geographic impacts that exacerbate income and wealth disparities. Effectively\
    \ enabling the workforce to be more mobile\u2014physically, legally, and virtually\u2014\
    will be crucial. This implies systemic policy approaches which encompass housing,\
    \ transportation, licensing, tax incentives, and crucially in the age of A/IS,\
    \ universal broadband access, especially in rural areas of both HIC\_and LMIC.\n\
    ## Recommendations\nTo thrive in the A/IS age, workers must be provided training\
    \ in skills that improve their adaptability to rapid technological changes; programs\
    \ should be available to any worker, with special attention to the low-skilled\
    \ workforce. Those programs can be private, that is, sponsored by the employer,\
    \ or publicly and freely offered through specific public channels and government\
    \ policies, and should be available regardless of whether the worker is in between\
    \ jobs or still employed. Specific measures include:\n\u2022\_\_\_\_\_ Offering\
    \ new technical programs, possibly earlier than high school, to increase the workforce\
    \ capacity to close the skills gap and thrive in employment alongside A/IS. \u2022\
    \_\_\_\_\_Creating opportunities for apprenticeships, pilot programs, and scaling\
    \ up data-driven evidence-based solutions that increase employment and earnings.\n\
    \u2022\_\_\_\_\_Supporting new forms of public-private partnerships involving\
    \ civil society, as well as new outcome-oriented financial mechanisms, e.g., social\
    \ impact bonds, that help scale up successful innovations.\n\u2022\_\_\_\_\_Supporting\
    \ partnerships between universities, innovation labs in corporations, and governments\
    \ to research and incubate startups for A/IS graduates.23\n\u2022\_\_\_\_\_Developing\
    \ regulations to hold corporations responsible for employee retraining necessary\
    \ due to increased automation and other technological applications having impact\_\
    on the workforce.\n\u2022\_\_\_\_\_Facilitating private sector initiatives by\
    \ public policy for co-investment in training and retraining programs through\
    \ tax incentives.\n\u2022\_\_\_\_\_Establishing and resourcing public policies\
    \ that assure the survival and well-being of workers, displaced by A/IS and automation,\
    \ who cannot be retrained.\n\u2022\_\_\_\_\_Researching complementary areas, to\
    \ lay solid foundations for the transformation outlined above.\n\u2022\_\_\_\_\
    \_Requiring more policy research on the dynamics of professional transitions in\
    \ different labor market conditions.\n\u2022\_\_\_\_\_Researching the fairest\
    \ and most efficient public-private options for financing labor force transformation\
    \ due to A/IS.\n\u2022\_\_\_\_\_Developing national and regional future of work\
    \ strategies based on sound research and strategic foresight.\n## Further Resources\n\
    \u2022\_\_\_\_\_V. Cerf and D. Norfors, The People-centered Economy: The New Ecosystem\
    \ for Work. California: IIIJ Foundation, 2018.\n\u2022\_\_\_\_\_Executive Office\
    \ of the President. _Artificial Intelligence, Automation, and the Economy._ December\
    \ 20, 2016.\n\u2022\_\_\_\_\_S. Kilcarr, \u201CDefining the American Dream for\
    \ Trucking ... and the Nation, Too,\u201D _FleetOwner_, April 26, 2016.\n\u2022\
    \_\_\_\_\_M. Mason, \u201CMillions of Californians\u2019 Jobs could be Affected\
    \ by Automation\u2014a Scenario the next Governor has to Address,\u201D_Los Angeles\
    \ Times_, October 14, 2018.\n\u2022\_\_\_\_\_OECD, \u201CLabor Market Programs:\
    \ Expenditure and Participants,\u201D _OECD Employment and Labor Market Statistics\
    \ _(database), 2016.\n\u2022\_\_\_\_\_M. Vivarelli, \u201CInnovation and Employment:\
    \ ASurvey,\u201D Institute for the Study of Labor (IZA) Discussion Paper No. 2621,\
    \ February 2007.\n\"\np.147-149\n\n\"**Issue: **Analysis of the\_A/IS impact on\
    \ employment is too focused on the number and category of jobs affected, whereas\
    \ more attention should be addressed to the complexities of changing the task\
    \ content\_of jobs.\n## Background\nCurrent attention on automation and employment\
    \ tends to focus on the sheer number of jobs lost or gained. It is important to\
    \ focus the analysis on how employment structures will be changed by A/IS, rather\
    \ than solely dwelling on the number of jobs that might be impacted. For example,\
    \ rather than carrying out a task themselves, workers will need to shift to supervision\
    \ of robots performing that task. Other concerns include changes in traditional\
    \ employment structures, with an increase in flexible, contract-based temporary\
    \ jobs, without employee protection, and a shift in task composition away from\
    \ routine/repetitive and toward complex decision-making. This is in addition to\
    \ the enormous need for the aforementioned retraining. Given the extent of disruption,\
    \ workforce trends will need to measure time spent unemployed or underemployed,\
    \ labor force participation rates, and other factors beyond simple unemployment\
    \ numbers.\nThe _Future of Jobs 2018 _report of the World Economic Forum highlights:\_\
    \n\u201C...the potential of new technologies to create as well as disrupt jobs\
    \ and to improve the quality and productivity of the existing work of human employees.\
    \ Our findings indicate that, by 2022, _augmentation _of existing jobs through\
    \ technology may free up workers from the majority of data processing and information\
    \ search tasks\u2014and may also increasingly support them in high-value tasks\
    \ such as reasoning and decision-making as augmentation becomes increasingly common\
    \ over the coming years as a way to supplement and complement human labour.\n\
    The report predicts the shift in skill demand between today and 2022 will be significant\
    \ and that \u201Cproactive, strategic and targeted efforts will be needed to map\
    \ and incentivize workforce redeployment\u2026 [and therefore]... investment decisions\
    \ [on] whether to prioritize automation or augmentation and the question of whether\
    \ or not to invest in workforce reskilling.\n## Recommendations\nWhile there is\
    \ evidence that robots and automation are taking jobs away in various sectors,\
    \ a more balanced, granular, analytical, and objective treatment of A/IS impact\
    \ on the workforce is needed to effectively inform policy making and essential\
    \ workforce reskilling. Specifics to accomplish this include:\n\u2022\_\_\_\_\_\
    Creating an international and independent agency able to properly disseminate\
    \ objective statistics and inform the media, as well as the general public, about\
    \ the impact of robotics and A/IS on jobs, tax revenue, growth,26 and well-being.\n\
    \u2022\_\_\_\_\_Analyzing and disseminating data on how current task content of\
    \ jobs have changed, based on a clear assessment of the automatability of the\
    \ occupational\_description of such jobs.\n\u2022\_\_\_\_\_Promoting automation\
    \ with augmentation, as recommended in the _Future of Jobs Report 2018_ (see chart\
    \ on page 154), to maximize the benefit of A/IS to employment and meaningful work.\n\
    \u2022\_\_\_\_\_Integrating more granulated dynamic mapping of the future jobs,\
    \ tasks, activities, workplace-structures, associated work-habits, and skills\
    \ base spurred by the A/IS revolution, in order to innovate, align, and synchronize\
    \ skill development and training programs with future requirements. This workforce\
    \ mapping is needed at the macro, but also crucially at the micro, levels where\
    \ labor market programs\_are deployed.\n\u2022\_\_\_\_\_Considering both product\
    \ and process innovation, and looking at them from a global perspective in order\
    \ to understand properly the global impact of A/IS on employment.\n\u2022\_\_\_\
    \_\_Proposing mechanisms for redistribution of productivity increases and developing\
    \ an adaptation plan for the evolving labor market.\n## Further Resources\n\u2022\
    \_\_\_\_\_E. Brynjolfsson and A. McAfee. The Second Age of Machine Intelligence:\
    \ Work Progress and Prosperity in a Time of Brilliant Technologies. New York,\
    \ NY: W. W. Norton & Company, 2014.\n\u2022\_\_\_\_\_P.R. Daugherty, and H.J.\
    \ Wilson, Human + Machine: Reimagining Work in the Age of AI_. _Watertown, MA:_\
    \ _Harvard Business Review Press, 2018.\n\u2022\_\_\_\_\_International Federation\
    \ of Robotics. \u201CThe Impact of Robots on Productivity, Employment and Jobs,\u201D\
    \ A positioning paper by the International Federation of Robotics, April 2017.\n\
    \u2022\_\_\_\_\_RockEU. \u201CRobotics Coordination Action for Europe Report on\
    \ Robotics and Employment,\u201D Deliverable D3.4.1, June 30, 2016.\n\u2022\_\_\
    \_\_\_World Economic Forum, Centre for the New Economy and Society, _The Future\
    \ of Jobs 2018_, Geneva: WEF 2018.\"\n150-152\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recg2u6ZXSRiUO8uF
  airtable_createdTime: '2023-06-05T11:48:28.000Z'
  airtable_id: recb50cfuQUWDAHZW
  title: AI poses threats to labour
- Description: "**\"Issue 1:** Many approaches to norm implementation are currently\
    \ available, and it is\_not yet settled which ones\_are most suitable.\n## Background\n\
    The prospect of developing A/IS that are sensitive to human norms and factor them\
    \ into morally or legally significant decisions has intrigued science fiction\
    \ writers, philosophers, and computer scientists alike. Modest efforts to realize\
    \ this worthy goal in limited or bounded contexts are already underway. This emerging\
    \ field of research appears under many names, including: machine morality, machine\
    \ ethics, moral machines, value alignment, computational ethics, artificial morality,\
    \ safe AI, and friendly AI.\nThere are a number of different implementation routes\
    \ for implementing ethics into autonomous and intelligent systems. Following Wallach\
    \ and Allen (2008)14, we might begin to categorize these as either:\nA.\_\_\_\
    Top-down approaches, where the system,\ne. g., a software agent, has some symbolic\
    \ representation of its activity, and so can identify specific states, plans,\
    \ or actions as ethical or unethical with respect to particular ethical requirements\
    \ (Dennis, Fisher, Slavkovik, Webster 201615; Pereira and Saptawijaya 201616;\
    \ R\xF6tzer, 201617; Scheutz, Malle, and Briggs 201518); or\nB.\_\_\_Bottom-up\
    \ approaches, where the system,\ne. g., a learning component, builds up, through\
    \ experience of what is to be considered ethical and unethical in certain situations,\
    \ an implicit notion of ethical behavior (Anderson and Anderson 201419; Riedl\
    \ and Harrison 201620).\nRelevant examples of these two are: (A) symbolic agents\
    \ that have explicit representations of plans, actions, goals, etc.; and (B) machine\
    \ learning systems that train subsymbolic mechanisms with acceptable ethical behavior.\
    \ For more detailed discussion, see Charisi et al. 201721.\nMany of the existing\
    \ experimental approaches to building moral machines are top-down, in the sense\
    \ that norms, rules, principles, or procedures are used by the system to evaluate\
    \ the acceptability of differing courses of action, or as moral standards or goals\
    \ to be realized. Increasingly, however, A/IS will encounter situations that initially\
    \ programmed norms do not clearly address, requiring algorithmic procedures to\
    \ select the better of two or more novel courses of action. Recent breakthroughs\
    \ in machine learning and perception enable researchers to explore bottom-up approaches\
    \ in which the \_A/IS learn about their context and about human norms, similar\
    \ to the manner in which a child slowly learns which forms of behavior are safe\
    \ and acceptable. Of course, unlike current\_A/IS, children can feel pain and\
    \ pleasure, and empathize with others. Still, A/IS can learn to detect and take\
    \ into account others\u2019 pain and pleasure, thus at least achieving some of\
    \ the positive effects of empathy. As research on A/IS progresses, engineers will\
    \ explore new ways to improve these capabilities.Each of the first two options\
    \ has obvious limitations, such as option A\u2019s inability to learn and adapt\
    \ and option B\u2019s unconstrained learning behavior. A third option tries to\
    \ address these limitations:\nC.\_\_\_Hybrid approaches, combining (A) and (B).\n\
    For example, the selection of action might be carried out by a subsymbolic system,\
    \ but this action must be checked by a symbolic \u201Cgateway\u201D agent before\
    \ being invoked. This is a typical approach for \u201CEthical Governors\u201D\
    \ (Arkin, 200822; Winfield, Blum, and Liu 201423) or \u201CGuardians\u201D (Etzioni\
    \ 201624) that monitor, restrict, and even adapt certain unacceptable behaviors\
    \ proposed by the system (see Issue 3). Alternatively, action selection in light\
    \ of norms could be done in a verifiable logical format, while many of the norms\
    \ constraining those actions can be learned through bottom-up learning mechanisms\
    \ (Arnold, Kasenberg, and Scheutz 201725).\nThese three architectures do not cover\
    \ all possible techniques for implementing norms into A/IS. For example, some\
    \ contributors to the multi-agent systems literature have integrated norms into\
    \ their agent specifications (Andrighetto et al. 201326), and even though these\
    \ agents live in societal simulations and are too underspecified to be translated\
    \ into individual A/IS such as robots, the emerging work can inform cognitive\
    \ architectures of such A/IS that fully integrate norms. Of course, none of these\
    \ experimental systems should be deployed outside of the laboratory before testing\
    \ or before certain criteria are met, which we outline in the remainder of this\
    \ section and in Section 3.\n## Recommendation\nIn light of the multiple possible\
    \ approaches to computationally implement norms, diverse research efforts should\
    \ be pursued, especially collaborative research between scientists from different\
    \ schools of thought and different disciplines.\n## Further Resources\n\u2022\_\
    \_\_\_\_M. Anderson, and S. L. Anderson, \u201CGenEth: A General Ethical Dilemma\
    \ Analyzer,\u201D\n_Proceedings of the Twenty-Eighth AAAI _\n_Conference on Artificial\
    \ Intelligence_, Qu\xE9bec City, Qu\xE9bec, Canada, July 27 \u201331, 2014, pp.\n\
    253\u2013261, Palo Alto, CA, The AAAI Press, 2014.\n\u2022\_\_\_\_\_G. Andrighetto,\
    \ G. Governatori, P. Noriega, and L. W. N. van der Torre, eds. Normative Multi-Agent\
    \ Systems. Saarbr\xFCcken/Wadern, Germany: Dagstuhl Publishing, 2013.\n\u2022\_\
    \_\_\_\_R. Arkin, \u201CGoverning Lethal Behavior:\nEmbedding Ethics in a Hybrid\
    \ Deliberative/ Reactive Robot Architecture.\u201D _Proceedings of the 2008 3rd\
    \ ACM/IEEE International Conference on Human-Robot Interaction_ _(HRI)_, Amsterdam,\
    \ Netherlands, March 12 -15, 2008, IEEE, pp. 121\u2013128, 2008.\n\u2022\_\_\_\
    \_\_T. Arnold, D. Kasenberg, and M. Scheutz.\n\u201CValue Alignment or Misalignment\u2014\
    What Will Keep Systems Accountable_?\u201D The Workshops of the Thirty-First AAAI\
    \ Conference on Artificial Intelligence: Technical Reports_, WS-17-02: AI, Ethics,\
    \ and Society, pp. 81\u201388. Palo Alto, CA: The AAAI Press, 2017.\n\u2022\_\_\
    \_\_\_V. Charisi, L. Dennis, M. Fisher, et al. \u201C[Towards Moral Autonomous\
    \ Systems](https://arxiv.org/abs/1703.04741),\u201D 2017.\n\u2022\_\_\_\_\_A.\
    \ Conn, \u201C[How Do We Align Artificial Intelligence with Human Values?](https://futureoflife.org/2017/02/03/align-artificial-intelligence-with-human-values/)\u201D\
    \ _Future\_of Life Institute_, Feb. 3, 2017.\n\u2022\_\_\_\_\_L. Dennis, M. Fisher,\
    \ M. Slavkovik, and M. Webster, \u201CFormal Verification of Ethical Choices in\
    \ Autonomous Systems.\u201D _Robotics and Autonomous Systems, _vol. 77, pp. 1\u2013\
    14, 2016.\n\u2022\_\_\_\_\_A. Etzioni and O.** **Etzioni, \u201CDesigning AI\n\
    Systems That Obey Our Laws and Values.\u201D _Communications_ _of the ACM_, vol._\
    \ _59, no. 9, pp. 29\u201331, Sept. 2016.\n\u2022\_\_\_\_\_L. M. Pereira and A.\
    \ Saptawijaya, Programming Machine Ethics. Cham, Switzerland: Springer International,\
    \ 2016.\n\u2022\_\_\_\_\_M. O. Riedl and B. Harrison. \u201CUsing Stories to\n\
    Teach Human Values to Artificial Agents.\u201D _AAAI Workshops 2016_. Phoenix,\
    \ Arizona, February 12\u201313, 2016.\n\u2022\_\_\_\_\_F. R\xF6tzer, ed. Programmierte\
    \ Ethik: Brauchen Roboter Regeln oder Moral? Hannover, Germany: Heise Medien,\
    \ 2016.\n\u2022\_\_\_\_\_M. Scheutz, B. F. Malle, and G. Briggs.\n\u201CTowards\
    \ Morally Sensitive Action Selection for Autonomous Social Robots.\u201D _Proceedings\
    \ of the 24th International_ _Symposium on Robot and Human Interactive Communication,\_\
    RO-MAN 2015 _(2015): 492\u2013497.\n\u2022\_\_\_\_\_U. Sommer, Werte: Warum Man\
    \ Sie Braucht, Obwohl es Sie Nicht Gibt. [Values. Why we need them even though\
    \ they don\u2019t exist.] Stuttgart, Germany: J. B. Metzler, 2016.\n\u2022\_\_\
    \_\_\_I. Sommerville, _Software Engineering_. Harlow, U.K.: Pearson Studium, 2001.\n\
    \u2022\_\_\_\_\_W. Wallach and C. Allen. _Moral Machines: _\n_Teaching Robots\
    \ Right from Wrong_. New York: Oxford University Press, 2008.\n\u2022\_\_\_\_\_\
    F. T. Winfield, C. Blum, and W. Liu. \u201CTowards an Ethical Robot: Internal\
    \ Models, Consequences and Ethical Action Selection\u201D in _Advances in Autonomous\
    \ Robotics Systems, Lecture Notes in Computer Science Volume_, M. Mistry, A. Leonardis,\
    \ Witkowski, and C. Melhuish, eds. pp. 85\u201396. Springer, 2014.\"\np.175-177\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recsonQhKLmX4D1ao
  airtable_createdTime: '2023-06-05T12:09:07.000Z'
  airtable_id: recdlFwsToNXtcvGC
  title: How can AI model norms to govern its action
- Cases:
  - recOOmVQviRyJGvea
  - recmS3zSMbR3ofAR5
  - recpFE7h6QhaEpNAQ
  Description: "\u201CMany machine learning models generate their results by operating\
    \ on high dimensional correlations that are beyond the interpretive capabilities\
    \ of human scale reasoning. In these cases, the rationale of algorithmically produced\
    \ outcomes that directly affect decision subjects remains opaque to those subjects.\
    \ While in some use cases, this lack of explainability may be acceptable, in some\
    \ applications, where the processed data could harbour traces of discrimination,\
    \ bias, inequity, or unfairness, the opaqueness of the model may be deeply problematic.\u201D\
    \ (Leslie, 2019, p. 4-5)\n"
  OverarchingPrinciples (from Principles):
  - recOHnq45Fq7YWsRO
  - recOHnq45Fq7YWsRO
  - recLHILkx2JDFsLbX
  Principles:
  - recOHnq45Fq7YWsRO
  - recxcFmvPG5wrCqpO
  - recKdujFoPJr4ZAhZ
  Reference:
  - 'Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A
    guide for the responsible design and implementation of AI systems in the public
    sector. The Alan Turing Institute. https://doi.org/10.5281/ZENODO.3240529'
  Sources:
  - recfYC5jjPmpLfSlM
  Strategies:
  - rechaQXedBh3OsMjZ
  - rec81gtnlFS5W2BBF
  - recgn2UvSD4OhzGI4
  airtable_createdTime: '2023-05-19T09:30:45.000Z'
  airtable_id: recdmBNNa98cN8Sda
  title: Decisions with unclear grounding
- Description: "Will deployment of synthetic emotions into affective systems increase\
    \ the accessibility of A/IS? Will increased accessibility prompt unforeseen patterns\
    \ of identification with A/IS?\n## Background\nDeliberately constructed emotions\
    \ are designed to create empathy between humans and artifacts, which may be useful\
    \ or even essential for human-A/IS collaboration. Synthetic emotions\_are essential\
    \ for humans to collaborate with the\_A/IS but can also lead to failure to recognize\
    \ that synthetic emotions can be compartmentalized and even entirely removed.\
    \ Potential consequences for humans include different\npatterns of bonding, guilt,\
    \ and trust, whether between the human and A/IS or between other humans. There\
    \ is no coherent sense in which A/IS can be made to suffer emotional loss, because\
    \ any such affect, even if possible, could be avoided at the stage of engineering,\
    \ or reengineered. As such, it is not possible to allocate moral agency or responsibility\
    \ in the senses that have been developed for human emotional bonding and thus\
    \ sociality.\n## Recommendations\n1\\.\_\_\_Commercially marketed A/IS should\
    \ not be persons in a legal sense, nor marketed as persons. Rather their artifactual\
    \ (authored, designed, and built deliberately) nature should always be made as\
    \ transparent as possible, at least at point of sale and in available documentation.\n\
    2\\.\_\_\_Some systems will, due to their application, require opaqueness in some\
    \ contexts, e.g., emotional therapy. Transparency in such systems should be available\
    \ to inspection by responsible parties but may be withdrawn for operational needs.\n\
    ## Further Resources\n\u2022\_\_\_\_\_R. C. Arkin, P. Ulam and A. R. Wagner, \u201C\
    Moral Decision-making in Autonomous Systems: Enforcement, Moral Emotions, Dignity,\
    \ Trust and Deception,\u201D _Proceedings of the IEEE, _vol._ _100, no. 3, pp.\
    \ 571\u2013589, 2012.\n\u2022\_\_\_\_\_R. Arkin, M. Fujita, T. Takagi and R. Hasegawa.\
    \ \u201CAn Ethological and Emotional Basis for Human-Robot Interaction,\u201D\
    \ _Robotics and Autonomous Systems, _vol.42, no. 3\u20134, pp.191\u2013201, 2003.\n\
    \u2022\_\_\_\_\_R. C. Arkin, \u201CMoving up the Food Chain: Motivation and Emotion\
    \ in Behavior-based Robots,\u201D in _Who Needs Emotions: The Brain Meets the\
    \ Robot_, J. Fellous and M. Arbib., Eds., New York: Oxford University Press, 2005.\n\
    \u2022\_\_\_\_\_M. Boden, J. Bryson, D. Caldwell, et al. \u201CPrinciples of Robotics:\
    \ Regulating Robots in the Real World.\u201D _Connection Science, _vol. 29, no.\
    \ 2, pp. 124\u2013129, 2017.\n\u2022\_\_\_\_\_J. J Bryson, M. E. Diamantis and\
    \ T. D. Grant. \u201COf, For, and By the People: The Legal Lacuna of Synthetic\
    \ Persons,\u201D _Artificial Intelligence & Law, _vol._ _25, no. 3, pp. 273\u2013\
    291, Sept. 2017.\n\u2022\_\_\_\_\_J. Novikova, and L. Watts, \u201CTowards Artificial\
    \ Emotions to Assist Social Coordination in HRI,\u201D _International Journal\
    \ of Social Robotics, _vol._ _7, no. 1, pp. 77\u201388, 2015.\u2022\_\_\_\_\_\
    M. Scheutz, \u201CThe Affect Dilemma for Artificial Agents: Should We Develop\
    \ Affective Artificial Agents?\u201D _IEEE Transactions on Affective Computing,\
    \ _vol. 3, no. 4, pp. 424\u2013433, 2012.\n\u2022\_\_\_\_\_A. Sharkey and N. Sharkey.\
    \ \u201CChildren, the Elderly, and Interactive Robots.\u201D _IEEE Robotics &\
    \ Automation Magazine, _vol. 18, no. 1, pp. 32\u201338, 2011.\"\n\np.103-102\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Tags:
  - synthetic-emotion
  airtable_createdTime: '2023-06-05T06:51:34.000Z'
  airtable_id: recebQ2NgeBCmafoX
  title: Will 'emotional AI' increase accessibility of AI and what impact might such
    increased identification with AI have?
- Description: "**Issue: **Use of black-box components\n## Background\nSoftware developers\
    \ regularly use \u201Cblack box\u201D components in their software, the functioning\
    \ of which they often do not fully understand. \u201CDeep\u201D machine learning\
    \ processes, which are driving many advancements in autonomous and intelligent\
    \ systems, are a growing source of black box software. At least for the foreseeable\
    \ future,\_A/IS developers will likely be unable to build systems that are guaranteed\
    \ to operate as intended.\n## Recommendation\nWhen systems are built that could\
    \ impact the safety or well-being of humans, it is not enough to just presume\
    \ that a system works. Engineers must acknowledge and assess the ethical risks\
    \ involved with black box software and implement mitigation strategies.\nTechnologists\
    \ should be able to characterize what their algorithms or systems are going to\
    \ do via documentation, audits, and transparent and traceable standards. To the\
    \ degree possible, these characterizations should be predictive, but given the\
    \ nature of A/IS, they might need to be more retrospective and mitigation-oriented.\
    \ As such, it is also important to ensure access to remedy adverse impacts.\n\
    Technologists and corporations must do their ethical due diligence before deploying\
    \ A/IS technology. Standards for what constitutes ethical due diligence would\
    \ ideally be generated by an international body such as IEEE or ISO, and barring\
    \ that, each corporation should work to generate a set of ethical standards by\
    \ which their processes are evaluated and modified. Similar to a flight data recorder\
    \ in the field of aviation, algorithmic traceability can provide insights on what\
    \ computations led to questionable or dangerous behaviors. Even where such processes\
    \ remain somewhat opaque, technologists should seek indirect means of validating\
    \ results and detecting harms.\n## Further resources\n\_\u2022\_\_\_\_\_M. Ananny\
    \ and K. Crawford, \u201C[Seeing without Knowing: Limitations of the Transparency\
    \ Ideal and Its Application to Algorithmic Accountability](http://journals.sagepub.com/doi/abs/10.1177/1461444816676645),\u201D\
    \ _New Media & Society_, vol. 20, no. 3, pp. 973-989, Dec. 13, 2016.\n\u2022\_\
    \_\_\_\_D. Reisman, J. Schultz, K. Crawford, and M. Whittaker, \u201CAlgorithmic\
    \ Impact Assessments: A Practical Framework for Public Agency Accountability,\u201D\
    \ AI NOW 2018. [Online]. Available: [https://ainowinstitute.org/ aiareport2018.pdf](https://ainowinstitute.org/aiareport2018.pdf).\_\
    [Accessed October 28, 2018].\n\u2022\_\_\_\_\_J. A. Kroll \u201C[The Fallacy of\
    \ Inscrutability](http://rsta.royalsocietypublishing.org/content/376/2133/20180084).\u201D\
    \ _Philosophical Transactions of the Royal Society A: Mathematical, Physical and\
    \ Engineering Sciences_, C. Cath, S. Wachter, B. Mittelstadt and L. Floridi, Eds.,\
    \ October 15, 2018 DOI: 10.1098/rsta.2018.0084.\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recgqEckuFTILlOnz
  - rec81gtnlFS5W2BBF
  Tags:
  - corporate-ethics
  airtable_createdTime: '2023-06-05T10:57:53.000Z'
  airtable_id: rececsX8igwNqhhkC
  title: How do we maintain agency and minimise harms in context of black box models?
- Cases:
  - recOOmVQviRyJGvea
  - recmS3zSMbR3ofAR5
  - recDAvfsflBF0WKpf
  Description: "New technologies including AI have potential for bias at multiple\
    \ phases of a project (problem identification, training data, modeling, implementation),\
    \ and in multiple ways including through use of data that reflects existing societal\
    \ biases. While reinforcing existing biases based on historic data is typically\
    \ focal, the potential to create new forms of bias should also be clear \n\u201C\
    Because they gain their insights from the existing structures and dynamics of\
    \ the societies they analyse, datadriven technologies can reproduce, reinforce,\
    \ and amplify the patterns of marginalisation, inequality, and discrimination\
    \ that exist in these societies. Likewise, because many of the features, metrics,\
    \ and analytic structures of the models that enable data mining are chosen by\
    \ their designers, these technologies can potentially replicate their designers\u2019\
    \ preconceptions and biases. Finally, the data samples used to train and test\
    \ algorithmic systems can often be insufficiently representative of the populations\
    \ from which they are drawing inferences. This creates real possibilities of biased\
    \ and discriminatory outcomes, because the data being fed into the systems is\
    \ flawed from the start.\u201D (Leslie, 2019, p. 4)\n\n### IEEE report\n\"**Issue\
    \ 2: **A/IS can have biases that disadvantage specific groups\n## Background\n\
    Even when reflecting the full system of community norms that was identified, A/IS\
    \ may show operation biases that disadvantage specific groups in the community\
    \ or instill biases in users by reinforcing group stereotypes. A system\u2019\
    s bias can emerge in perception. For example, a passport application AI rejected\
    \ an Asian man\u2019s photo because it insisted his eyes were closed (Griffiths\
    \ 201651). Bias can emerge in information processing. For instance, speech recognition\
    \ systems are notoriously less accurate for female speakers than for male speakers\
    \ (Tatman 201652). System bias can affect decisions, such as a criminal risk assessment\
    \ device which overpredicts recidivism by African Americans (Angwin et al. 201653).\
    \ The system\u2019s bias can present itself even in its own appearance and presentation:\
    \ the vast majority of humanoid robots have white \u201Cskin\u201D color and use\
    \ female voices (Riek and Howard 201454).\nThe norm identification process detailed\
    \ in Section 1 is intended to minimize individual designers\u2019 biases because\
    \ the community norms are assessed empirically. The identification process also\
    \ seeks to incorporate norms against prejudice and discrimination. However, biases\
    \ may still emerge from imperfections in the norm identification process itself,\
    \ from unrepresentative training sets for machine learning systems, and from programmers\u2019\
    \ and designers\u2019 unconscious assumptions. Therefore, unanticipated or undetected\
    \ biases should be further reduced by including members of diverse social groups\
    \ in both the planning and evaluation of A/IS and integrating community outreach\
    \ into the evaluation process, e.g., [DO-IT ](http://www.washington.edu/doit/)program\
    \ and [RRI](http://www.orbit-rri.org/) framework. Behavioral scientists and members\
    \ of the target populations will be particularly valuable when devising criterion\
    \ tasks for system evaluation and assessing the success of evaluating the A/IS\
    \ performance on those tasks. Such tasks would assess, for example, whether the\
    \ A/IS apply norms in discriminatory ways to different races, ethnicities, genders,\
    \ ages, body shapes, or to people who use wheelchairs\_or prosthetics, and so\
    \ on.\n## Recommendation\nEvaluation of A/IS must carefully assess potential biases\
    \ in the systems\u2019 performance that disadvantage specific social and demographic\
    \ groups. The evaluation process should integrate members of potentially disadvantaged\
    \ groups in efforts to diagnose and correct such biases.\n## Further Resources\n\
    \u2022\_\_\_\_\_J. Angwin, J. Larson, S. Mattu, and L. Kirchner, \u201C[Machine\
    \ Bias: There\u2019s Software Used Across](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\
    \ [the Country to Predict Future Criminals.](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\
    \ [And It\u2019s Biased Against Blacks](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).\u201D\
    \ ProPublica,\_May 23, 2016.\n\u2022\_\_\_\_\_J. Griffiths, \u201C[New Zealand\
    \ Passport Robot ](http://www.cnn.com/2016/12/07/asia/new-zealand-passport-robot-asian-trnd/)\_\
    [Thinks This Asian Man\u2019s Eyes Are Closed](http://www.cnn.com/2016/12/07/asia/new-zealand-passport-robot-asian-trnd/).\u201D\
    \ CNN.com, December 9, 2016.\n\u2022\_\_\_\_\_L. D. Riek and D. Howard,. [\u201C\
    A Code of Ethics \_for the Human-Robot Interaction Profession.](https://ssrn.com/abstract%3D2757805)\u201D\
    \ _Proceedings of We Robot,_ April 4, 2014.\n\u2022\_\_\_\_\_R. Tatman, \u201C\
    [Google\u2019s Speech Recognition Has a Gender Bias](https://makingnoiseandhearingthings.com/2016/07/12/googles-speech-recognition-has-a-gender-bias/).\u201D\
    \ _Making Noise and Hearing Things_, July 12, 2016.\"\np.184, IEEE, 2019\n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  - recmzjcGKv3yNOxbl
  Principles:
  - reczVPIH1y2OMpAJH
  - rec42P8U9usfYCtv9
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  - 'Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A
    guide for the responsible design and implementation of AI systems in the public
    sector. The Alan Turing Institute. https://doi.org/10.5281/ZENODO.3240529'
  Sources:
  - recpXl48pJdKDhc6f
  - recfYC5jjPmpLfSlM
  Tags:
  - AI
  airtable_createdTime: '2023-05-18T13:41:32.000Z'
  airtable_id: recefglLZ3oJWw2SZ
  title: Reinforcing or creating bias
- Description: "\"Will use of A/IS adversely affect human psychological and emotional\
    \ well-being in ways not otherwise foreseen?\n## Background\nA/IS may be given\
    \ unprecedented access to human culture and human spaces\u2014both physical and\
    \ intellectual. A/IS may communicate via natural language, may move with humanlike\
    \ form, and may express humanlike identity, but they are not, and should not be\
    \ regarded as, human. Incorporation of A/IS into daily life may affect human well-being\
    \ in ways not yet anticipated. Incorporation of A/IS may alter patterns of trust\
    \ and capability assessment between humans, and between humans and A/IS.\_\n##\
    \ Recommendations\n1\\.\_\_\_Vigilance and robust, interdisciplinary, on-going\
    \ research on identifying situations where\_A/IS affect human well-being, both\
    \ positively and negatively, is necessary. Evidence of correlations between the\
    \ increased use of\_A/IS and positive or negative individual or social outcomes\
    \ must be explored.\_\n2\\.\_\_\_  Design restrictions should be placed on the\
    \ systems themselves to avoid machine decisions that may alter a person\u2019\
    s life in unknown ways. Explanations should be available on demand in systems\
    \ that may affect human well-being.\n## Further Resources\n\u2022\_\_\_\_\_K.\
    \ Kamewari, M. Kato, T. Kanda, H. Ishiguro and K. Hiraki. \u201CSix-and-a-Half-Month-Old\
    \ Children Positively Attribute Goals to Human Action and to Humanoid-Robot Motion,\u201D\
    \ _Cognitive Development, _vol._ _20, no. 2, pp. 303\u2013320, 2005.\n\u2022\_\
    \_\_\_\_R.A. Calvo and D. Peters, Positive Computing: Technology for Wellbeing\
    \ and Human Potential. Cambridge, MA: MIT Press, 2014.\"\np.102\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Tags:
  - human-potential
  airtable_createdTime: '2023-06-05T06:51:31.000Z'
  airtable_id: recg7BIZZsvZ57fvV
  title: Will AI adversely affect human psychological and emotional wellbeing?
- Description: "Removal of embedded tools can cause harms through: (1) failure to\
    \ address changes that have been made that rely on the tool; and (2) failure to\
    \ adequately address the ongoing impacts a tool may have on wider systems. \n\n\
    'Removal' may mean actually taking a tool out of a context, but it could also\
    \ mean no longer supporting an existing tool, increasing resource needs resulting\
    \ in a tool becoming unaffordable (financially or otherwise), incompatabilities\
    \ with new tools, etc. \n\nAn example of the first kind of harm is e.g. a company\
    \ no longer supporting a medically implanted device for vision loss patients <https://spectrum.ieee.org/bionic-eye-obsolete>\n\
    \nAn example of the second kind of harm is the ongoing impacts from the UK Exam\
    \ moderation algorithm in 2020, which had knock-on impacts internationally even\
    \ after the algorithm itself was removed <https://dl.acm.org/doi/abs/10.1145/3531146.3533186>\
    \ \n\n"
  OverarchingPrinciples (from Principles):
  - recmzjcGKv3yNOxbl
  Principles:
  - rec42P8U9usfYCtv9
  airtable_createdTime: '2023-05-18T18:53:45.000Z'
  airtable_id: recgZrgA8K7MmIzdF
  title: Risks in removal of systems and algorithmic imprints
- OverarchingPrinciples (from Principles):
  - recOHnq45Fq7YWsRO
  Principles:
  - recOHnq45Fq7YWsRO
  airtable_createdTime: '2023-05-18T18:54:44.000Z'
  airtable_id: rech9r2f3QX8ZvmkP
  title: Live A/B testing and validity towards aims
- Description: "Interdisciplinary Collaborations\n## Background\nMore institutional\
    \ resources and incentive structures are necessary to bring A/IS engineers and\
    \ designers into sustained and constructive contact with ethicists, legal scholars,\
    \ and social scientists, both in academia and industry. This contact is necessary\
    \ as it can enable meaningful interdisciplinary collaboration and shape the future\
    \ of technological innovation. More could be done to develop methods, shared knowledge,\
    \ and lexicons that would facilitate\_such collaboration.\nThis issue relates,\
    \ among other things, to funding models as well as the lack of diversity of backgrounds\
    \ and perspectives in A/IS-related institutions and companies, which limit cross-pollination\
    \ between disciplines. To help bridge this gap, additional translation work and\
    \ resource sharing, including websites and Massive Open Online Courses (MOOCs),\
    \ need to happen among technologists and other relevant experts, e.g., in medicine,\
    \ architecture, law, philosophy, psychology, and cognitive science. Furthermore,\
    \ there is a need for more cross-disciplinary conversation and multi-disciplinary\
    \ research, as is being done, for instance, at the annual ACM Fairness, Accountability,\
    \ and Transparency (FAT\\*) conference or the work done by the Canadian Institute\
    \ For Advanced Research (CIFAR), which\_is developing Canada\u2019s AI strategy.\n\
    Funding models and institutional incentive structures should be reviewed and revised\
    \ to prioritize projects with interdisciplinary ethics components to encourage\
    \ integration of ethics into projects at all levels.\n## Further Resources\n\u2022\
    \_\_S. Barocas, Course Material for Ethics and Policy in Data Science, Cornell\
    \ University, 2017.\n\u2022\_\_L. Floridi, and M. Taddeo. \u201CWhat Is Data Ethics?\u201D\
    \ _Philosophical Transactions of the Royal Society, _vol._ _374, no. 2083, 1\u2013\
    4. DOI[10.1098/ rsta.2016.0360,](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5124072/)\
    \ 2016.\n\u2022\_\_S. Spiekermann, Ethical IT Innovation: A ValueBased System\
    \ Design Approach. Boca Raton, FL: Auerbach Publications, 2015.\n\u2022\_\_K.\
    \ Crawford, \u201C[Artificial Intelligence\u2019s White Guy Problem](http://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html?_r=1)\u201D\
    , _New York Times_, July 25, 2016. [Online]. Available: [http://www.nytimes. com/2016/06/26/opinion/sunday/artificialintelligences-white-guy-problem.html?\\\
    _r=1](http://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html?_r=1).\
    \ [Accessed October 28, 2018].\"\n\np.123-124\n\_\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recsonQhKLmX4D1ao
  Tags:
  - research
  airtable_createdTime: '2023-06-05T10:17:28.000Z'
  airtable_id: rech9vLLbQgO3OY0i
  title: Cross-disciplinary expertise in ethics may be excluded in AI work
- Description: "\"**Issue: **How can we increase agency by providing individuals access\
    \ to services allowing them to create a trusted identity to control the safe,\
    \ specific, and finite exchange of their data?\n## Background\nPervasive behavior-tracking\
    \ adversely affects human agency by recognizing our identity in every action we\
    \ take on and offline. This is why identity as it relates to individual data is\
    \ emerging at the forefront of the risks and opportunities related to use of personal\
    \ information for A/IS. Across the identity landscape there is increasing tension\
    \ between the requirement for federated identities versus a range of identities.\
    \ In federated identities, all data are linked to a natural and identified person.\
    \ When one has a range of identities, or personas, these can be context specific\
    \ and determined by the use case. New movements, such as \u201CSelf-Sovereign\
    \ Identity\u201D\u2014 defined as the right of a person to determine his or her\
    \ own identity\u2014are emerging alongside legal identities, e.g., those issued\
    \ by governments, banks, and regulatory authorities, to help put individuals at\
    \ the center of their data in the algorithmic age.Personas, identities that act\
    \ as proxies, and pseudonymity are also critical requirements for privacy management\
    \ and agency. These help individuals select an identity that is appropriate for\
    \ the context they are in or wish to join. In these settings, trust transactions\
    \ can still be enabled without giving up the \u201Croot\u201D identity of the\
    \ user. For example, it is possible to validate that a user is over eighteen or\
    \ is eligible for a service.\nAttribute verification will play a significant role\
    \ in enabling individuals to select the identity that provides access without\
    \ compromising agency. This type of access is especially important in dealing\
    \ with the myriad of algorithms interacting with narrow segments of our identity\
    \ data. In these situations, individuals typically are not aware of the context\
    \ for how their data will be used.\n## Recommendation\nIndividuals should have\
    \ access to trusted identity verification services to validate, prove, and support\
    \ the context-specific use of their identity. \_\n## Further Resources\n\u2022\
    \_\_\_\_\_Sovrin Foundation, [The Inevitable Rise of SelfSovereign Identity,](https://sovrin.org/wp-content/uploads/2017/06/The-Inevitable-Rise-of-Self-Sovereign-Identity.pdf)\
    \ Sept. 29, 2016.\n\u2022\_\_\_\_\_T. Ruff, \u201C[Three Models of Digital Identity\
    \ Relationships](https://medium.com/evernym/the-three-models-of-digital-identity-relationships-ca0727cb5186),\u201D\
    \ Evernym, Apr. 24, 2018.\n\u2022\_\_\_\_\_C. Pettey, [The Beginner\u2019s Guide\
    \ to Decentralized Identity.](https://www.gartner.com/smarterwithgartner/the-beginners-guide-to-decentralized-identity/)\
    \ Gartner, 2018.\n\u2022\_\_\_\_\_C. Allen, [The Path to Self-Sovereign Identity](https://github.com/ChristopherA/self-sovereign-identity/blob/master/ThePathToSelf-SovereignIdentity.md).\
    \ GitHub, 2017.\"\n\np.112-113 IEEE report\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - rec1Ha94n5xbeqcOY
  airtable_createdTime: '2023-06-05T09:37:23.000Z'
  airtable_id: reci0hfcAOIxLVNhB
  title: How can we protect our identities to assure privacy and identity verification?
- Description: "Will A/IS nudging systems that are not fully relevant to the sociotechnical\
    \ context in which they are operating cause behaviors with adverse unintended\
    \ consequences?\n## Background\nA well-designed nudging or suggestion system will\
    \ have sophisticated enough technical capabilities for recognizing the context\
    \ in which it is applying nudging actions. Assessment of the context requires\
    \ perception of the scope or impact of the actions to be taken, the consequences\
    \ of incorrectly or incompletely applied nudges, and acknowledgement of the uncertainties\
    \ that may stem from long term consequences of a nudge7.\n## Recommendations\n\
    1\\.\_\_\_Consideration should be given to the development of a system of technical\
    \ licensing (\u201Cpermits\u201D) or other certification from governments or non-governmental\
    \ organizations (NGOs) that can aid users to understand the nudges from A/IS in\
    \ their lives.\n2\\.\_\_\_User autonomy is a key and essential consideration that\
    \ must be taken into account when addressing whether affective systems should\
    \ be permitted to nudge human beings.\n3\\.\_\_\_Design features of an affective\
    \ system that nudges human beings should include the ability to accurately distinguish\
    \ between users, including detecting characteristics such as whether the user\
    \ is an adult or a child.\n4\\.\_\_\_Affective systems with nudging strategies\
    \ should incorporate a design system of evaluation, monitoring, and control for\
    \ unintended consequences.\n## Further Resources\n\u2022\_\_\_\_\_J. Borenstein\
    \ and R. Arkin, \u201C[Robotic Nudges: ](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)\n\
    [Robotic Nudges: The Ethics of Engineering a ](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)\n\
    [More Socially Just Human Being Just Human Being.](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)\u201D\
    \ _Science and Engineering Ethics, _vol._ _22, no. 1, pp. 31\u201346, 2016.\n\u2022\
    \_\_\_\_\_R. C. Arkin, M. Fujita, T. Takagi, and R. Hasegawa, \u201C[An Ethological\
    \ and Emotional Basis for Human- Robot Interaction.](http://www.sciencedirect.com/science/article/pii/S0921889002003755)\u201D\
    \ _Robotics and Autonomous Systems, _vol._ _42, no. 3\u20134 pp.191\u2013201,\
    \ March 2003.\n\u2022\_\_\_\_\_S. Omohundro \u201C[Autonomous Technology and the\
    \ Greater Human Good.](http://www.tandfonline.com/doi/abs/10.1080/0952813X.2014.895111?journalCode=teta20)\u201D\
    \ _Journal of Experimental and Theoretical Artificial Intelligence, vol. _26,\
    \ no. 3, pp. 303\u2013315, 2014.\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - rec9TbOu2ZXUZG4A5
  Tags:
  - nudge-deception
  airtable_createdTime: '2023-06-05T06:13:16.000Z'
  airtable_id: reciH69D8oIGI1p93
  title: How do we ensure AI nudges are deployed appropriately and address their target
    outcome?
- Description: "Human rights law is related to, but distinct from, the pursuit of\
    \ well-being. Incorporating a human-rights framework as an essential basis for\
    \ A/IS creators means A/IS creators honor existing law as part of their well-being\
    \ analysis and implementation.\n## Background\nInternational human rights law\
    \ has been firmly established for decades in order to protect various guarantees\
    \ and freedoms as enshrined in charters such as the United Nations\u2019[ Universal\
    \ Declaration of Human Rights ](http://www.un.org/en/universal-declaration-human-rights/)and\
    \ the Council of Europe\u2019s [Convention on Human Rights](https://www.coe.int/en/web/human-rights-convention).\
    \ In 2018, the[ Toronto Declaration ](https://www.accessnow.org/the-toronto-declaration-protecting-the-rights-to-equality-and-non-discrimination-in-machine-learning-systems/)on\
    \ machine learning standards was released, calling on both governments and technology\
    \ companies to ensure that algorithms respect basic principles of equality and\
    \ non-discrimination. The Toronto Declaration sets forth an obligation to prevent\
    \ machine learning systems from discriminating, and in some cases violating, existing\
    \ human rights law.\nWell-being initiatives are typically undertaken for the sake\
    \ of public interest. However, any metric, including well-being metrics, can be\
    \ misused to justify human rights violations. Encampment and mistreatment of refugees\
    \ and ethnic cleansing undertaken to preserve a nation\u2019s culture (an aspect\
    \ of well-being) is one example. Imprisonment or assassination of journalists\
    \ or researchers to ensure the stability of a government is another. The use of\
    \ wellbeing metrics to justify human rights violations is an unconscionable perversion\
    \ of the nature of any well-being metric. It should be noted that these same practices\
    \ happen today in relation to GDP. For instance, in 2012, according to the [International\
    \ Labour Organization (](http://www.ilo.org/global/about-the-ilo/newsroom/news/WCMS_181961/lang--en/index.htm)ILO),\
    \ approximately 21 million people are victims of forced labor (slavery), representing\
    \ 9% to 56% of GDP income for various countries. These clear human rights violations,\
    \ from sex trafficking and use of children in armies, to indentured farming or\
    \ manufacturing labor, can increase a country\u2019s GDP while obviously harming\
    \ human well-being.\n## Recommendations\nWell-being metrics are designed to measure\
    \ the efficacy of efforts related to individual and societal flourishing. Well-being\
    \ as a value complements justice, equality, and freedom. Well-designed application\
    \ of well-being considerations by A/IS creators should not displace other issues\
    \ of human rights or ethical methodologies, but rather complement them.\nA human\
    \ rights framework should represent the floor, and not the ceiling, for the standards\
    \ to which A/IS creators must adhere. Developers and users of well-being metrics\
    \ should be aware these metrics will not always adequately address human rights.\n\
    ## Further Resources\n\u2022\_\_\_\_\_United Nations[ Universal Declaration of\
    \ Human Rights,](http://www.un.org/en/universal-declaration-human-rights/) 1948.\n\
    \u2022\_\_\_\_\_Council of Europe\u2019s [Convention on Human Rights](https://www.coe.int/en/web/human-rights-convention),\
    \ 2018.\n\u2022\_\_\_\_\_International Labor Organization (ILO) [Declaration on\
    \ Fundamental Principles and Rights at Work,](https://www.ilo.org/declaration/lang--en/index.htm)\
    \ 1998.\n\u2022\_\_\_\_\_The regularly updated [University of Minnesota Human\
    \ Rights Library ](http://hrlibrary.umn.edu/)provides a wealth of material on\
    \ human rights laws, its history, and the organizations engaged in promoting them.\n\
    The [Oxford Human Rights Hub ](http://ohrh.law.ox.ac.uk/why-artificial-intelligence-is-already-a-human-rights-issue/)reports\
    \ on how and why technologies surrounding artificial intelligence raise human\
    \ rights issues\"\np.76-77\n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  - recOHnq45Fq7YWsRO
  Principles:
  - receFm7cGasHwpJZO
  - recQ9DIFEsOEkCx3O
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recZI7HDWKBU5T22v
  Tags:
  - wellbeing
  airtable_createdTime: '2023-06-05T05:47:17.000Z'
  airtable_id: reciNc7OeTUmnsLqg
  title: How do we incorporate a human-rights framework into AI design/development/use?
- Description: "**\"Issue: **Current roadmaps for development and deployment of A/IS\
    \ are not aligned with or guided by their impact in the most important challenges\
    \ of humanity, defined in the seventeen United Nations Sustainable Development\
    \ Goals (SDGs), which collectively aspire to create a more equal world of prosperity,\
    \ peace, planet protection, and human dignity\_for all people.4\n## Background\n\
    \ SDGs promoting prosperity, peace, planet protection, human dignity, and respect\
    \ for human rights of all, apply to HIC and LMIC alike. Yet ensuring that the\
    \ benefits of A/IS will accrue to humanity as a whole, leaving \u201Cno one behind\u201D\
    , requires an ethical commitment to global citizenship and well-being, and a conscious\
    \ effort to counter the nature of the tech economy, with its tendency to concentrate\
    \ wealth within high income populations. Implementation of the SDGs should benefit\
    \ excluded sectors of society in every country, regardless of A/IS infrastructure.\u201C\
    The Road to Dignity by 2030\u201D document of the UN Secretary General reports\
    \ on resources and methods for implementing the 2030 Agenda for Sustainable Development\
    \ and emphasizes the importance of science, technology, and innovation for a sustainable\
    \ future.5 The UN Secretary General posits that:\n\u201CA sustainable future will\
    \ require that we act now to phase out unsustainable technologies and to invest\
    \ in innovation and in the development of clean and sound technologies for sustainable\
    \ development. We must ensure that they are fairly priced, broadly disseminated\
    \ and fairly absorbed, including to and by developing countries.\u201D (para.\
    \ 120)\nA/IS are among the technologies that can play an important role in the\
    \ solution of the deep social problems plaguing our global civilization, contributing\
    \ to the transformation of society away from an unsustainable, unequal socioeconomic\
    \ system, towards one that realizes the vision of universal human dignity, peace,\
    \ and prosperity.\nHowever, with all the potential benefits of\_A/IS, there are\
    \ also risks. For example, given\_A/IS technology\u2019s immense power needs,\
    \ without new sources of sustainable energy harnessed to power A/IS in the future,\
    \ there is a risk that it will increase fossil fuel use and have a negative impact\
    \ on the environment and the climate.\nWhile 45% of the world\u2019s population\
    \ is not connected to the internet, they are not necessarily excluded from A/IS\u2019\
    \ potential benefits: in LMIC mobile networks can provide data for A/IS applications.\
    \ However, only those connected are likely to benefit from the income-producing\
    \ potential of internet technologies. In 2017, internet penetration in HIC left\
    \ behind certain portions of the population often in rural or remote areas; 12%\
    \ of U.S. residents and 20% of residents across Europe were unable to access the\
    \ internet. In Asia with its concentration of LMIC, 52% of the population, on\
    \ average, had no access, a statistic skewed by the large population of China,\
    \ where internet penetration reached 45% of the population. In numerous other\
    \ countries in the region, 99% of residents had no access. This nearly total exclusion\
    \ also exists in several countries in Africa, where the overall internet penetration\
    \ is only 35%: 2 of every 3 residents in Africa have no access.6 Those with no\
    \ internet access also do not generate data needed to \u201Ctrain\u201D A/IS,\
    \ and are thereby excluded from benefits of the technology, the development of\
    \ which risks systematic discriminatory bias, particularly against people from\
    \ minority populations, and those living in rural areas, or in low-income countries.\
    \ As a comparison, one study estimated that \u201Cin the US, just one home automation\
    \ product can generate a data point every six seconds.\u201D7 In Mozambique, where\
    \ about 90% of the population lack internet access, \u201Cthe average household\
    \ generates zero digital data points.\u201D8 With mobile phones generating much\
    \ of the data needed for developing A/IS applications in LMIC, unequal phone ownership\
    \ may build in bias. For example, there is a risk of discrimination against women,\
    \ who across LMIC are 14% less likely than men to own a mobile phone, and in South\
    \ Asia where 38% are less likely to own a mobile phone.9\n## Recommendations\n\
    The current range of A/IS applications in sectors crucial to the SDGs, and to\
    \ excluded populations everywhere, should be studied, with the strengths, weaknesses,\
    \ and potential of the most significant recent applications analyzed, and the\
    \ best ones developed at scale. Specific objectives to consider include:\n\u2022\
    \_\_\_\_\_Identifying and experimenting with\_A/IS technologies relevant to the\
    \ SDGs,\_such as: big data for development relevant to, for example, agriculture\
    \ and medical tele-diagnosis; geographic information systems needed in public\
    \ service planning, disaster prevention, emergency planning, and disease monitoring;\
    \ control systems used in, for example, naturalizing intelligent cities through\
    \ energy and traffic control and management of urban agriculture; applications\
    \ that promote human empathy focused on diminishing violence and exclusion and\
    \ increasing well-being.\n\u2022\_\_\_\_\_Promoting the potential role of A/IS\
    \ in sustainable development by collaboration between national and international\
    \ government agencies and non-governmental organizations (NGOs) in technology\
    \ sectors.\n\u2022\_\_\_\_\_Analyzing the cost of and proposing strategies for\
    \ publicly providing internet access for\_all, as a means of diminishing the gap\
    \ in\_A/IS\u2019 potential benefit to humanity, particularly between urban and\
    \ rural populations in HIC and LMIC alike.\n\u2022\_\_\_\_\_Investing in the documentation\
    \ and dissemination of innovative applications of\_A/IS that advance the resolution\
    \ of identified societal issues and the SDGs.\n\u2022\_\_\_\_\_Researching sustainable\
    \ energy to power A/IS computational capacity.\n\u2022\_\_\_\_\_Investing in the\
    \ development of transparent monitoring frameworks to track the concrete results\
    \ of donations by international organizations, corporations, independent agencies,\
    \ and the State, to ensure efficiency and accountability in applied A/IS.\n\u2022\
    \_\_\_\_\_Developing national legal, policy, and fiscal measures to encourage\
    \ competition in the\_A/IS domestic markets and the flourishing\_of scalable A/IS\
    \ applications.\n\u2022\_\_\_\_\_Integrating the SDGs into the core of private\
    \ sector business strategies and adding SDG indicators to companies\u2019 key\
    \ performance indicators, going beyond corporate social responsibility (CSR).\n\
    \u2022\_\_\_\_\_Applying the well-being indicators10 to evaluate A/IS\u2019 impact\
    \ from multiple perspectives in HIC and LMIC alike.\n## Further reading\n\u2022\
    \_\_\_\_\_R. Van Est and J.B.A. Gerritsen, with assistance of L. Kool, Human Rights\
    \ in the Robot Age: Challenges arising from the use of Robots, Artificial Intelligence\
    \ and Augmented Reality Expert Report written for the Committee on Culture, Science,\
    \ Education and Media of the Parliamentary Assembly of the Council of Europe (PACE),\
    \ The Hague: Rathenau Instituut 2017.\n\u2022\_\_\_\_\_World Economic Forum Global\
    \ Future Council on Human Rights 2016-18, \u201CWhite Paper: How to Prevent Discriminatory\
    \ Outcomes in Machine Learning,\u201D World Economic Forum, March 2018.\n\u2022\
    \_\_\_\_\_United Nations General Assembly, _Transforming Our World: The 2030 Agenda\
    \ for Sustainable Development_ (A/RES/70/1: 21 October 2015) Preamble. [http://www.un.org/\
    \ en/development/desa/population/migration/ generalassembly/docs/globalcompact/\
    \ A\\_RES\\_70\\_1\\_E.pdf](http://www.un.org/en/development/desa/population/migration/generalassembly/docs/globalcompact/A_RES_70_1_E.pdf).\n\
    \u2022\_\_\_\_\_United Nations Global Pulse, Big Data for Development: Challenges\
    \ and Opportunities, 2012.\"\n\np139-140\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recouZjokdKQz88z1
  Tags:
  - SDGs
  airtable_createdTime: '2023-06-05T10:57:55.000Z'
  airtable_id: recj64vAVJSm5B2ba
  title: AI could hamper, not foster, progress on SDGs
- Description: "\u201CWhile the capacity of AI systems to curate individual experiences\
    \ and to personalise digital services holds the promise of vastly improving consumer\
    \ life and service delivery, this benefit also comes with potential risks. Excessive\
    \ automation, for example, might reduce the need for human-to-human interaction,\
    \ while algorithmically enabled hyper-personalisation, by limiting our exposure\
    \ to worldviews different from ours, might polarise social relationships. Well-ordered\
    \ and cohesive societies are built on relations of trust, empathy, and mutual\
    \ understanding. As AI technologies become more prevalent, it is important that\
    \ these relations be preserved.\u201D (Leslie, 2019, p. 5)\n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  Principles:
  - recKdujFoPJr4ZAhZ
  Reference:
  - 'Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A
    guide for the responsible design and implementation of AI systems in the public
    sector. The Alan Turing Institute. https://doi.org/10.5281/ZENODO.3240529'
  Sources:
  - recfYC5jjPmpLfSlM
  airtable_createdTime: '2023-05-19T09:32:07.000Z'
  airtable_id: reckPjhcWwLgjX9p5
  title: Breakdown in trust in loss of exposure to diverse human-human interactions
- Description: "\"Does the increased access to personal information about other members\
    \ of our society, facilitated by A/IS, alter the human affective experience?\_\
    Does this access potentially lead to a change in human autonomy?\n## Background\n\
    Theoretical biology tells us that we should expect increased communication\u2014\
    which A/IS facilitate\u2014 to increase group-level investment8. Extensive use\
    \ of A/IS could change the expression of individual autonomy and in its place\
    \ increase group-based identities. Examples of this sort\_of social alteration\
    \ may include:\n1\\.\_\_\_Changes in the scope of monitoring and control of children\u2019\
    s lives by parents.\n2\\.\_\_\_Decreased willingness to express opinions for fear\
    \ of surveillance or long-term consequences of past expressions being used in\
    \ changed temporal contexts.\n3\\.\_\_\_Utilization of customers or other end\
    \ users to perform basic corporate business processes such as data entry as a\
    \ barter for lower prices or access, resulting potentially in reduced tax revenues.\n\
    4\\.\_\_\_Changes to the expression of individual autonomy could alter the diversity,\
    \ creativity, and cohesiveness of a society. It may also alter perceptions of\
    \ privacy and security, and social and legal liability for autonomous expressions.\n\
    ## Recommendations\n1\\.\_\_\_Organizations, including governments, must put a\
    \ high value on individuals\u2019 privacy and autonomy, including restricting\
    \ the amount and age of data held about individuals specifically.\n2\\.\_\_\_\
    Education in all forms should encourage individuation, the preservation of autonomy,\
    \ and knowledge of the appropriate uses and limits to A/IS9.\n## Further Resources\n\
    \u2022\_\_\_\_\_J. J. Bryson, \u201CArtificial Intelligence and Pro-Social Behavior,\u201D\
    \ in _Collective Agency and Cooperation in Natural and Artificial Systems, _C.\
    \ Misselhorn, Ed., pp. 281\u2013306, Springer, 2015.\n\u2022\_\_\_\_\_M. Cooke,\
    \ \u201CA Space of One\u2019s Own: Autonomy, Privacy, Liberty,\u201D _Philosophy\
    \ & Social Criticism, _Vol._ _25, no. 1, pp. 22\u201353, 1999.\n\u2022\_\_\_\_\
    \_D. Peters, R.A. Calvo, R.M. Ryan, \u201CDesigning for Motivation, Engagement\
    \ and Wellbeing in Digital Experience[\u201D _Frontiers in Psychology_](https://www.frontiersin.org/journals/psychology)\
    \ \u2013 _Human Media Interaction_, vol. 9. pp 797, 2018.\n\u2022\_\_\_\_\_J.\
    \ Roughgarden, M. Oishi and E. Ak\xE7ay,\n\u201CReproductive Social Behavior:\
    \ Cooperative Games to Replace Sexual Selection.\u201D _Science _311, no. 5763,\
    \ pp. 965\u2013969, 2006.\"\n\np.101\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Tags:
  - human-potential
  airtable_createdTime: '2023-06-05T06:13:33.000Z'
  airtable_id: reckyVWsglP8FLeUp
  title: Will increased access to personal information impact human experience and
    autonomy?
- Description: "**\"Issue: **A/IS are contributing to humanitarian action to save\
    \ lives, alleviate suffering, and maintain human dignity both during and in the\
    \ aftermath of man-made crises and natural disasters, as well as to prevent and\
    \ strengthen preparedness for the occurrence of such situations. However, there\
    \ are ethical concerns with both the collection and use of data during humanitarian\
    \ emergencies.\n## Background\nThere have been a number of promising A/IS applications\
    \ that relieve suffering in humanitarian crises, such as extending the reach of\
    \ the health system by using drones to deliver blood to remote parts of Rwanda,31\
    \ locating and removing landmines,32 efforts to use A/IS to track movements and\
    \ population survival needs following a natural disaster, and to meet the multiple\
    \ management requirements of refugee camps.33 There are also promising developments\
    \ using A/IS and robotics to assist people with disabilities to recover mobility,\
    \ and robots to rescue people trapped in collapsed buildings.34 A/IS are also\
    \ being used to monitor conflict zones and to enable early warning systems.35\
    \ For example, Microsoft has partnered with the UN Human Rights Office of the\
    \ High Commissioner (OHCHR) to use big data in order to track and analyze human\
    \ rights violations in conflict zones.36 Machine learning is being used for improved\
    \ decision-making regarding asylum adjudication and refugee resettlement, with\
    \ a view to increasing successful integration between refugees and host communities.37\
    \ In addition, there is evidence that a recent growth in human empathy has increased\
    \ well-being while diminishing psychological and physical violence,38 inspiring\
    \ some researchers to look for ways of harnessing the power of A/IS to introduce\
    \ more empathy and less violence into society.\nThe design and ethical deployment\
    \ of these technologies in crisis settings are both essential and challenging.\
    \ Large volumes of both personally identifiable and demographically identifiable\
    \ data are collected in fragile environments, where tracking of individuals or\
    \ groups may compromise their security if data privacy cannot be assured. Consent\
    \ to data use is also impractical in such environments, yet crucial for the respect\
    \ of human rights.\n## Recommendations\nThe potential for A/IS to contribute to\
    \ humanitarian action to save and improve lives should be prioritized for research\
    \ and development, including by organizing global research challenges, while also\
    \ building in safeguards to protect the creation, collection, processing, sharing,\
    \ use, and disposal of information, including data from and about individuals\
    \ and populations. Specific recommendations include:\n\u2022\_\_\_\_\_Promoting\
    \ awareness of the vulnerable condition of certain communities around the globe\
    \ and the need to develop and use A/IS applications for humanitarian purposes.\n\
    \u2022\_\_\_\_\_Elaborating competitions and challenges in high impact conferences\
    \ and university hackathons to engage both technical and nontechnical communities\
    \ in the development of A/IS for humanitarian purposes and to address social issues.\n\
    \u2022\_\_\_\_\_Support civil society groups who organize themselves for the purpose\
    \ of A/IS research and advocacy to develop applications to benefit humanitarian\
    \ causes.39\n\u2022\_\_\_\_\_Developing and applying ethical standards for the\
    \ collection, use, sharing, and disposal of data in fragile settings.\n\u2022\_\
    \_\_\_\_Following privacy protection frameworks for pressing humanitarian situations\
    \ that ensure the most vulnerable are protected.40\_\n\u2022\_\_\_\_\_Setting\
    \ up clear ethical frameworks for exceptional use of A/IS technologies in lifesaving\
    \ humanitarian situations, compared\_to \"normal\" situations.41\n\u2022\_\_\_\
    \_\_Stimulating the development of low-cost\_and open source solutions based on\
    \ A/IS\_to address specific humanitarian problems.\n\u2022\_\_\_\_\_Training A/IS\
    \ experts in humanitarian action and norms, and humanitarian practitioners\_to\
    \ catalyze collaboration in designing,\_piloting, developing, and implementing\_\
    A/IS technologies for humanitarian purposes. Forging public-private A/IS participant\
    \ alliances that develop crisis scenarios in advance.\n\u2022\_\_\_\_\_Working\
    \ on cultural and contextual acceptance of any A/IS introduced during emergencies.\n\
    \u2022\_\_\_\_\_Documenting and developing quantifiable metrics for evaluating\
    \ the outcomes of humanitarian digital projects, and educating the humanitarian\
    \ ecosystem on the same.\n## Further resources\n\u2022\_\_\_\_\_E. Prestes et\
    \ al., \"The 2016 Humanitarian Robotics and Automation Technology Challenge [Competitions],\"\
    \ in _IEEE Robotics & Automation Magazine_, vol. 23, no. 3, pp. 23-24, Sept. 2016.\
    \ [http://ieeexplore.ieee.org/stamp/ stamp.jsp?tp=&arnumber=7565695&isnumber=7565655](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7565695&isnumber=7565655)\n\
    \u2022\_\_\_\_\_L. Marques et al., \"Automation of humanitarian demining: The\
    \ 2016 Humanitarian Robotics and Automation Technology Challenge,\" _2016 International\
    \ Conference on Robotics and Automation for Humanitarian Applications (RAHA)_,\
    \ Kollam, 2016, pp. 1-7. <http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7931893&isnumber=7931858>\n\
    \u2022\_\_\_\_\_CYBATHLON 2020 Preliminary Race Task Descriptions [http://www.cybathlon.ethz.\
    \ ch/cybathlon-2020/preliminary-race-taskdescriptions.html](http://www.cybathlon.ethz.ch/cybathlon-2020/preliminary-race-task-descriptions.html)\n\
    \u2022\_\_\_\_\_CYBATHLON Scientific Publications\_<http://www.cybathlon.ethz.ch/>\n\
    \u2022\_\_\_\_\_Immigration Policy Lab (IPL), \u201CHarnessing Big Data to Improve\
    \ Refugee Resettlement\u201D [https://immigrationlab.org/project/harnessingbig-data-to-improve-refugee-resettlement/](https://immigrationlab.org/project/harnessing-big-data-to-improve-refugee-resettlement/)\n\
    \u2022\_\_\_\_\_Harvard Humanitarian Initiative, _The Signal Code_, [https://signalcode.org](https://signalcode.org/)\
    \ \n\_\u2022\_\_\_\_\_J.A. Quinn, et al., \u201CHumanitarian applications of machine\
    \ learning with remote-sensing data: review and case study in refugee settlement\
    \ mapping\u201D Philosophical Transactions of the Royal Society A, 376 20170363;\
    \ DOI:\n10.1098/rsta.2017.0363. Aug. 6, 2018.\n\u2022\_\_\_\_\_Humanitarian Innovation\
    \ Guide: [https:// higuide.elrha.org/,](https://higuide.elrha.org/) 2019.\n\u2022\
    \_\_\_\_\_P. Meier, [Digital Humanitarians: How Big Data is Changing the Face\
    \ of Humanitarian Response](http://cds.cern.ch/record/2123110). Florida: CRC Press,\
    \ 2015.\n\u2022\_\_\_\_\_\u201CTechnology for human rights: UN Human Rights Office\
    \ announces landmark partnership with Microsoft\u201D [https://www.ohchr.org/\
    \ EN/NewsEvents/Pages/DisplayNews. aspx?NewsID=21620&LangID=E](https://www.ohchr.org/EN/NewsEvents/Pages/DisplayNews.aspx?NewsID=21620&LangID=E)\n\
    \u2022\_\_\_\_\_M. Luengo-Oroz, \u201C10 big data science challenges facing humanitarian\
    \ organizations,\u201D UNHCR, Nov. 22, 2016. [http://www. unhcr.org/innovation/10-big-data-sciencechallenges-facing-humanitarian-organizations/](http://www.unhcr.org/innovation/10-big-data-science-challenges-facing-humanitarian-organizations/)\n\
    \u2022\_\_\_\_\_Optic Technologies, Press Release, Vatican Hack 2018\u2014Results,\
    \ 18 March 2018, which announced winning AI applications to benefit migrants and\
    \ refugees as well as social inclusion and interfaith dialogue,\_[http://optictechnology.org/index.php/en/news-en/151-vhack-2018winners-en\
    \ ](http://optictechnology.org/index.php/en/news-en/151-vhack-2018winners-en)\"\
    \n\np157-159\n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  Principles:
  - reczVPIH1y2OMpAJH
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recMgYQ7EqHAyDfi1
  airtable_createdTime: '2023-06-05T11:54:11.000Z'
  airtable_id: recmRF2P1OOASMfNF
  title: How do we ensure protection of data during humanitarian emergencies?
- Description: "**\"Issue:** Stakeholder inclusion\n## Background\nThe interface between\
    \ A/IS and practitioners, as well as other stakeholders, is gaining broader attention\
    \ in domains such as healthcare diagnostics, and there are many other contexts\
    \ where there may be different levels of involvement with the technology. We should\
    \ recognize that, for example, occupational therapists and their assistants may\
    \ have on-theground expertise in working with a patient, who might be the \u201C\
    end user\u201D of a robot or social\_A/IS technology. In order to develop a product\
    \ that is ethically aligned, stakeholders\u2019 feedback is crucial to design\
    \ a system that takes ethical and social issues into account. There are successful\
    \ user experience (UX) design concepts, such as accessibility, that consider human\
    \ physical disabilities, which should be incorporated into A/IS as they are more\
    \ widely deployed. It is important to continuously consider the impact of A/IS\
    \ through unanticipated use and on unforeseen interests.\n## Recommendations\n\
    To ensure representation of stakeholders, organizations should enact a planned\
    \ and controlled set of activities to account for the interests of the full range\
    \ of stakeholders or practitioners who will be working alongside\_A/IS and incorporating\
    \ their insights to build upon, rather than circumvent or ignore, the\_social\
    \ and practical wisdom of involved practitioners and other stakeholders.\n## Further\
    \ Resources\n\u2022\_\_\_\_\_C. Schroeter, et al., \u201C[Realization and User\
    \ Evaluation of a Companion Robot for People with Mild Cognitive Impairments](http://www.tu-ilmenau.de/fileadmin/media/neurob/publications/conferences_int/2013/Schroeter-ICRA-2013-fin.pdf),\u201D\
    \ _Proceedings of IEEE International Conference on Robotics and Automation (ICRA\
    \ 2013)_, Karlsruhe, Germany 2013. pp. 1145\u20131151.\n\u2022\_\_\_\_\_T. L.\
    \ [Chen, et al. ](http://ieeexplore.ieee.org/abstract/document/6476704/)\u201C\
    [Robots for Humanity: Using Assistive Robotics to Empower People with Disabilities](http://ieeexplore.ieee.org/document/6476704/),\u201D\
    \ _IEEE Robotics and Automation Magazine, _vol. 20, no. 1, pp. 30\u201339, 2013.\n\
    R. Hartson, and P. S. Pyla. _The UX Book: Process and Guidelines for Ensuring\
    \ a Quality User Experience_. Waltham, MA: Elsevier, 2012\"\n\np.130-131\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recQPwyiQbPcN0G47
  Tags:
  - corporate-ethics
  airtable_createdTime: '2023-06-05T10:40:09.000Z'
  airtable_id: recnYgPiGULdHLunC
  title: How do ensure stakeholder experience is considered in AI design?
- Description: "**\"Issue:** Values-based ethical culture and practices for industry\n\
    ## Background\nCorporations are built to create profit while competing for market\
    \ share. This can lead corporations to focus on growth at the expense of avoiding\
    \ negative ethical consequences. Given the deep ethical implications of widespread\
    \ deployment of A/IS, in addition to laws and regulations, there is a need to\
    \ create values-based ethical culture and practices for the development and deployment\
    \ of those systems. To do so, we need to further identify and refine corporate\
    \ processes that facilitate values-based design.\n## Recommendations\nThe building\
    \ blocks of such practices include top-down leadership, bottom-up empowerment,\
    \ ownership, and responsibility, along with the need to consider system deployment\
    \ contexts and/or ecosystems. Corporations should identify stages in their processes\
    \ in which ethical considerations, \u201Cethics filters\u201D, are in place before\
    \ products are further developed and deployed. For instance, if an ethics review\
    \ board comes in at the right time during the A/IS creation process, it would\
    \ help mitigate the likelihood of creating ethically problematic designs. The\
    \ institution of an ethical A/IS corporate culture would accelerate the adoption\
    \ of the other recommendations within this section focused on business practices.\n\
    ## Further Resources\n\u2022\_\_\_\_\_[ACM Code of Ethics and Professional Ethics,](https://ethics.acm.org/2018-code-draft-2/)\
    \ which includes various references to human well-being and human rights, 2018.\n\
    \u2022\_\_\_\_\_Report of UN Special Rapporteur on [Freedom of Expression. _AI\
    \ and Freedom of Expression_.](http://undocs.org/A/73/348) 2018.\n\u2022\_\_\_\
    \_\_The [website of the Benefit corporations ](https://www.bcorporation.net/)(B-corporations)\
    \ provides a good overview of a range of companies that personify this type of\
    \ culture.\n\u2022\_\_\_\_\_R. Sisodia, J. N. Sheth and D. Wolfe, [Firms of ](http://www.firmsofendearment.com/)\n\
    [Endearment](http://www.firmsofendearment.com/)_, _2nd edition. Upper Saddle River,\
    \ NJ: FT Press, 2014. This book showcases how companies embracing values and a\
    \ stakeholder approach outperform their competitors in the long run.\"\np.127\n\
    \n\"**Issue:** Values-based leadership\n## Background\nTechnology leadership should\
    \ give innovation teams and engineers direction regarding which human values and\
    \ legal norms should be promoted in the design of A/IS. Cultivating an ethical\
    \ corporate culture is an essential component of successful leadership in the\_\
    A/IS domain.\n## Recommendations\nCompanies should create roles for senior-level\
    \ marketers, engineers, and lawyers who can collectively and pragmatically implement\
    \ ethically aligned design. There is also a need for more in-house ethicists,\
    \ or positions that fulfill similar roles. One potential way to ensure values\
    \ are on the agenda in A/IS development is to have a Chief Values Officer (CVO),\
    \ a role first suggested by Kay Firth-Butterfield, see \u201CFurther Resources\u201D\
    . However, ethical responsibility should not be delegated solely to CVOs. They\
    \ can support the creation of ethical knowledge in companies, but in the end,\
    \ all members of an organization will need to act responsibly throughout the design\
    \ process.\nCompanies need to ensure that their understanding of values-based\
    \ system innovation is based on _de jure _and _de facto _international human rights\
    \ standards.\n\u2022\_\_\_\_\_K. Firth-Butterfield, \u201C[How IEEE Aims to Instill\
    \ Ethics in Artificial Intelligence Design,](http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design)\u201D\
    \ The Institute. Jan. 19, 2017. [Online]. Available: [http://theinstitute.ieee.org/ieee-roundup/\
    \ blogs/blog/how-ieee-aims-to-instill-ethicsin-artificial-intelligence-design](http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design).\
    \ [Accessed October 28, 2018]. \u2022\_\_\_\_\_United Nations, [Guiding Principles\
    \ on Business and Human Rights: Implementing the United Nations \u201CProtect,\
    \ Respect and Remedy\u201D Framework,](http://www.ohchr.org/Documents/Publications/GuidingPrinciplesBusinessHR_EN.pdf)\
    \ New York and Geneva: UN, 2011.\n\u2022\_\_\_\_\_Institute for Human Rights and\
    \ Business\n(IHRB), and Shift, ICT [Sector Guide on ](https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf)\n\
    [Implementing the UN Guiding Principles on Business and Human Rights,](https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf)\
    \ 2013.\n\u2022\_\_\_\_\_C. Cath, and L. Floridi, \u201C[The Design of the Internet\u2019\
    s Architecture by the Internet ](http://europepmc.org/abstract/med/27255607)\n\
    [Engineering Task Force (IETF) and Human Rights](http://europepmc.org/abstract/med/27255607).\u201D\
    \ _Science and Engineering Ethics, _vol._ _23, no. 2, pp. 449\u2013468, Apr. 2017.\"\
    \n\np.128\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recfcXzM3foqFNNGN
  Tags:
  - corporate-ethics
  airtable_createdTime: '2023-06-05T10:25:26.000Z'
  airtable_id: recqpYMjEFJLmyNaN
  title: How do corporations create values-based ethical culture in the context of
    growth-focused strategy
- OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  Principles:
  - recsvi4LnhEEPyQ1h
  airtable_createdTime: '2023-05-19T14:21:29.000Z'
  airtable_id: recrHBtVR4EOXSJh2
  title: New technologies make reidentification easier
- Description: "\"When, if ever, and\_under which circumstances,\_is deception performed\
    \ by affective systems acceptable?\n## Background\nDeception is commonplace in\
    \ everyday human-human interaction. According to Kantian ethics, it is never ethically\
    \ appropriate to lie, while utilitarian frameworks indicate that it can be acceptable\
    \ when deception increases overall happiness. Given the diversity of views on\
    \ ethics and the appropriateness of deception, should affective systems be designed\
    \ to deceive? Does the non-consensual nature of deception restrict the use of\
    \ A/IS in contexts in which deception may be required?\nIt is necessary to develop\
    \ recommendations regarding the acceptability of deception performed by A/IS,\
    \ specifically with respect to when and under which circumstances, if any,\_it\
    \ is appropriate.\n1\\.\_\_\_In general, deception may be acceptable in an affective\
    \ agent when it is used for the benefit of the person being deceived, not for\
    \ the agent itself. For example, deception might be necessary in search and rescue\
    \ operations or for elder- or child-care.\_\n2\\.\_\_\_For deception to be used\
    \ under any circumstance, a logical and reasonable justification must be provided\
    \ by the designer, and this rationale should be certified by an external authority,\
    \ such as a licensing body\_or regulatory agency.\n## **Further resources**\n\_\
    \u2022\_\_\_\_\_R. C. Arkin, \u201CRobots That Need to Mislead: Biologically-inspired\
    \ Machine Deception.\u201D _IEEE Intelligent Systems _27, no. 6, pp. 60\u2013\
    75, 2012.\n\u2022\_\_\_\_\_J. Shim and R. C. Arkin, \u201COther-Oriented Robot\
    \ Deception: How Can a Robot\u2019s Deceptive Feedback Help Humans in HRI?\u201D\
    \ _Eighth International Conference on Social Robotics (ICSR 2016)_, Kansas, MO.,\
    \ November 2016.\n\u2022\_\_\_\_\_J. Shim and R. C. Arkin, \u201CThe Benefits\
    \ of Robot Deception in Search and Rescue: Computational Approach for Deceptive\
    \ Action Selection via Case-based Reasoning.\u201D _2015 IEEE International Symposium\
    \ on Safety, Security, and Rescue Robotics (SSRR 2015)_, West Lafayette, IN, October\
    \ 2015.\n\u2022\_\_\_\_\_J. Shim and R. C. Arkin, \u201CA Taxonomy of Robot Deception\
    \ and its Benefits in HRI.\u201D _Proceedings of IEEE Systems, Man and Cybernetics\
    \ Conference, _Manchester England, October 2013.\"\np.102-103\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - rec9TbOu2ZXUZG4A5
  Tags:
  - nudge-deception
  airtable_createdTime: '2023-06-05T06:13:17.000Z'
  airtable_id: recrVvneRZaEutaGw
  title: Should affective AI systems ever deceive?
- Description: "## Background\nWe need independent, expert opinions that provide guidance\
    \ to the general public regarding A/IS. Currently, there is a gap between how\_\
    A/IS are marketed and their actual performance or application. We need to ensure\
    \ that\_A/IS technology is accompanied by best-use recommendations and associated\
    \ warnings. Additionally, we need to develop a certification scheme for A/IS which\
    \ ensures that the technologies have been independently\_assessed as being safe\
    \ and ethically sound.\nFor example, today it is possible for systems to download\
    \ new self-parking functionality to cars, and no independent reviewer establishes\
    \ or characterizes boundaries or use. Or, when a companion robot promises to watch\
    \ your children, there is no organization that can issue an independent seal of\
    \ approval or limitation on these devices. We need a ratings and approval system\
    \ ready to serve social/automation technologies that will come online as soon\
    \ as possible. We also need further government funding for research into how A/IS\
    \ technologies can best be subjected to review, and how\_review organizations\
    \ can consider both\_traditional health and safety issues, as well\_as ethical\
    \ considerations.\n## Recommendation\nAn independent, internationally coordinated\
    \ body\u2014akin to ISO\u2014should be formed to oversee whether A/IS products\
    \ actually meet ethical criteria, both when designed, developed, deployed, and\
    \ when considering their evolution after deployment and during interaction with\
    \ other products. It should also include\_a certification process.\n## Further\
    \ Resources\n\u2022\_\_\_\_\_A. Tutt, \u201CAn FDA for Algorithms,\u201D _Administrative\
    \ Law Review _69, 83\u2013123, 2016.\n\u2022\_\_\_\_\_M. U. Scherer, \u201C[Regulating\
    \ Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies,](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2609777)\u201D\
    \ _Harvard Journal of Law and Technology _vol._ _29, no. 2, 354\u2013400, 2016.\n\
    \u2022\_\_\_\_\_D. R. Desai and J. A. Kroll, \u201C[Trust But Verify: A Guide\
    \ to Algorithms and the Law](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2959472).\u201D\
    \ _Harvard Journal of Law and Technology_, Forthcoming; Georgia Tech Scheller\
    \ College of Business Research Paper No. 17-19, 2017.\n\np.133-134\n\n\"**Issue\
    \ 3:** Challenges to evaluation by third parties\n## Background\nA/IS should have\
    \ sufficient transparency to allow evaluation by third parties, including regulators,\
    \ consumer advocates, ethicists, post-accident investigators, or society at large.\
    \ However, transparency can be severely limited in some systems, especially in\
    \ those that rely on machine learning algorithms trained on large data sets. The\
    \ data sets may not be accessible to evaluators; the algorithms may be proprietary\
    \ information or mathematically so complex that they defy common-sense explanation;\
    \ and even fellow software experts may be unable to verify reliability and efficacy\
    \ of the final system because the system\u2019s specifications are opaque.\nFor\
    \ less inscrutable systems, numerous techniques are available to evaluate the\
    \ implementation of the A/IS\u2019 norm conformity. On one side there is formal\
    \ verification, which provides a mathematical proof that the A/IS will always\
    \ match specific normative and ethical requirements, typically devised in a top-down\
    \ approach (see Section 2, Issue 1). This approach requires access to the decision-making\
    \ process and the reasons for each decision (Fisher, Dennis, and Webster 201355).\
    \ A simpler alternative, sometimes suitable even for machine learning systems,\
    \ is to test the A/IS against a set of scenarios and assess how well they matches\
    \ their normative requirements, e.g., acting in accordance with relevant norms\
    \ and recognizing other agents\u2019 norm violations. A \u201Cred team\u201D may\
    \ also devise scenarios that try to get the A/IS\_to break norms so that its vulnerabilities\
    \ can\_be revealed.These different evaluation techniques can be assigned different\
    \ levels of \u201Cstrength\u201D: strong ones demonstrate the exhaustive set of\
    \ the\_A/IS\u2019 allowable behaviors for a range of criterion scenarios; weaker\
    \ ones sample from criterion scenarios and illustrate the systems\u2019 behavior\
    \ for that subsample. In the latter case, confidence in the A/IS\u2019 ability\
    \ to meet normative requirements is more limited. An evaluation\u2019s concluding\
    \ judgment must therefore acknowledge the strength of the verification technique\
    \ used,\_and the expressed confidence in the evaluation \u2014 and in the A/IS\
    \ themselves\u2014must be qualified\_by this level of strength.\nTransparency\
    \ is only a necessary requirement for a more important long-term goal: having\
    \ systems be accountable to their users and community members. However, this goal\
    \ raises many questions such as to whom the A/IS are accountable, who has the\
    \ right to correct the systems, and which kind of A/IS should be subject to accountability\
    \ requirements.\n## Recommendation\nTo maximize effective evaluation by third\
    \ parties, e.g., regulators and accident investigators, A/IS should be designed,\
    \ specified, and documented so as to permit the use of strong verification and\
    \ validation techniques for assessing the system\u2019s safety and norm compliance,\
    \ in order to achieve accountability to the relevant communities.\n## Further\
    \ Resources\n\u2022\_\_\_\_\_M. Fisher, L. A. Dennis, and M. P. Webster. \u201C\
    Verifying Autonomous Systems.\u201D _Communications of the ACM,_ vol. 56, pp.\_\
    84\u201393, 2013.\n\u2022\_\_\_\_\_K. Abney, G. A. Bekey, and P. Lin. _[Robot\
    \ Ethics: The Ethical and Social Implications of Robotics](https://mitpress.mit.edu/books/robot-ethics)_[.](https://mitpress.mit.edu/books/robot-ethics)\
    \ Cambridge, MA: The MIT Press, 2011.\n\u2022\_\_\_\_\_M. Anderson and S. L. Anderson,\
    \ eds.\__[Machine Ethics](http://assets.cambridge.org/97805211/12352/copyright/9780521112352_copyright_info.pdf)._\
    \ New York: Cambridge University Press, 2011.\n\u2022\_\_\_\_\_M. Boden, J. Bryson,\
    \ et al. \u201CPrinciples of Robotics: Regulating Robots in the Real World.\u201D\
    \ _Connection Science _29, no. 2, pp. 124\u2013129, 2017.\n\u2022\_\_\_\_\_M.\
    \ Coeckelbergh, \u201C[Can We Trust Robots?](https://link.springer.com/article/10.1007/s10676-011-9279-1)\u201D\
    \ _Ethics and Information Technology, _vol.14, pp. 53\u201360, 2012.\n\u2022\_\
    \_\_\_\_L. A. Dennis, M. Fisher, N. Lincoln, A. Lisitsa, and S. M. Veres, \u201C\
    Practical Verification of Decision-Making in Agent-Based Autonomous Systems.\u201D\
    \ _Automated Software Engineering, _vol._ _23, no. 3, pp. 305\u2013359, 2016.\n\
    \u2022\_\_\_\_\_M. Fisher, C. List, M. Slavkovik, and A. F. T. Winfield. \u201C\
    Engineering Moral Agents\u2014From Human Morality to Artificial Morality\u201D\
    \ (Dagstuhl Seminar 16222). _Dagstuhl Reports _6, no. 5, pp. 114\u2013137, 2016.\n\
    \u2022\_\_\_\_\_K. R. Fleischmann, _Information and Human Values_. San Rafael,\
    \ CA: Morgan and Claypool, 2014.\n\u2022\_\_\_\_\_G. Governatori and A. Rotolo.\
    \ \u201CHow Do Agents Comply with Norms? \u201D in _Normative Multi-Agent Systems_,\
    \ G. Boella, P. Noriega, G. Pigozzi, and H. Verhagen, eds., _Dagstuhl Seminar\
    \ Proceedings_. Dagstuhl, Germany: Schloss Dagstuhl\u2014Leibniz- Zentrum f\xFC\
    r Informatik, 2009.\n\u2022\_\_\_\_\_B. Higgins, \u201CNew York City Task Force\
    \ to Consider Algorithmic Harm.\u201D _Artificial Intelligence Technology and\
    \ the Law Blog_, Feb. 7, 2018. [Online]. Available: [http:// aitechnologylaw.com/2018/02/new-york-citytask-force-algorithmic-harm/](http://aitechnologylaw.com/2018/02/new-york-city-task-force-algorithmic-harm/).\
    \ [Accessed Nov. 1, 2018].\n\u2022\_\_\_\_\_S. L. Jarvenpaa, N. Tractinsky, and\
    \ L. Saarinen. \u201C[Consumer Trust in an Internet Store: A CrossCultural Validation](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1083-6101.1999.tb00337.x)\u201D\
    \ _Journal of ComputerMediated Communication, _vol._ _5, no. 2, pp. 1\u201337,\
    \ 1999.\"\np.185-186, ieee, 2019\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recgqEckuFTILlOnz
  Tags:
  - corporate-ethics
  airtable_createdTime: '2023-06-05T10:49:41.000Z'
  airtable_id: recrk0Tgfwe5J9xfI
  title: How do we maintain independence of review in AI applications?
- Description: "**Issue: **Intelligent toys\n## Background\nChildren will not only\
    \ be exposed to A/IS at school but also at home, while they play and while they\
    \ sleep. Toys are already being sold that offer interactive, intelligent opportunities\
    \ for play. Many of them collect video and audio data which is stored on company\
    \ servers and either is or could be mined for profiling or marketing data.\nThere\
    \ is currently little regulatory oversight. In the United States COPPA7 offers\
    \ some protection for the data of children under 13. Germany has outlawed such\
    \ toys using legislation banning spying equipment enacted in 1981. Corporate A/IS\
    \ are being embodied in toys and given to children to play with, to talk to, tell\
    \ stories to, and to explore all the personal development issues that we learn\
    \ about in private play as children.\n## Recommendations\nChild data should be\
    \ held in \u201Cescrow\u201D and not used for any commercial purposes until a\
    \ child reaches the age of majority and is able to authorize use as they choose.\n\
    Governments and organizations need to educate and inform parents of the mechanisms\
    \ of A/IS and data collection in toys and the possible impact on children in the\
    \ future.** **\n## Further Resources\n\u2022\_\_\_\_\_K. Firth-Butterfield, \u201C\
    What happens when your child\u2019s friend is an AI toy that talks back?\u201D\
    \ in World Economic Forum: Generation AI, [https://www.weforum.org/agenda/2018/05/\
    \ generation-ai-what-happens-when-your-childs-invisible-friend-is-an-ai-toy-that-talks-back/,](https://www.weforum.org/agenda/2018/05/generation-ai-what-happens-when-your-childs-invisible-friend-is-an-ai-toy-that-talks-back/)\
    \ May 22, 2018.\n\u2022\_\_\_\_\_D. Basulto,\u201CHow artificial intelligence\
    \ is moving from the lab to your kid\u2019s playroom,\u201D Washington Post, Oct.\
    \ 15, 2015. [Online]. Available: [https://www.washingtonpost. com/news/innovations/wp/2015/10/15/\
    \ how-artificial-intelligence-is-moving-fromthe-lab-to-your-kids-playroom/?utm\\\
    _ term=.89a1431a05a7 ](https://www.washingtonpost.com/news/innovations/wp/2015/10/15/how-artificial-intelligence-is-moving-from-the-lab-to-your-kids-playroom/?utm_term=.89a1431a05a7)**[Accessed\
    \ Dec. 1, 2018].**\n**\u2022\_\_\_\_\_S. Chaudron, R. Di Gioia, M. Gemo, D. Holloway,\
    \ J. Marsh, G. Mascheroni J. Peter, and D. Yamada-Rice , **[http://publications.jrc.\
    \ ec.europa.eu/repository/handle/JRC105061,](http://publications.jrc.ec.europa.eu/repository/handle/JRC105061)\
    \ 2016.\n\u2022\_\_\_\_S. Chaudron, R. Di Gioia, M. Gemo, D. Holloway, J. Marsh,\
    \ G. Mascheroni J. Peter, and D. Yamada-Rice , [http://publications.jrc. ec.europa.eu/repository/handle/JRC105061,](http://publications.jrc.ec.europa.eu/repository/handle/JRC105061)\
    \ 2016.  \_\_\_\_\_\_\_\n\_\u2022 \_S. Chaudron, R. Di Gioia, M. Gemo, D. Holloway,\
    \ J. Marsh, G. Mascheroni, J. Peter, D. Yamada-Rice [Kaleidoscope on the Internet\
    \ of Toys - Safety, security, privacy and societal insights](http://publications.jrc.ec.europa.eu/repository/bitstream/JRC105061/jrc105061_final_online.pdf),\
    \ EUR 28397 EN, doi:10.2788/05383, Luxembourg: Publications Office of the European\
    \ Union, 2017.\nZ. Kleinman, \u201CAlexa, are you friends with our kids?\u201D\
    \ _BBC News,_ July 16, 2018. [Online]. Available: [https://www.bbc.com/news/ technology-44847184.%5b](https://www.bbc.com/news/technology-44847184.%5b).\
    \ [Accessed Dec. 1, 2018].\nJ. Wakefield, \u201CGermany bans children\u2019s smartwatches.\u201D\
    \ _BBC News,_ Nov. 17 2017. [Online]. Available: [https://www.bbc.co.uk/ news/technology-42030109](https://www.bbc.co.uk/news/technology-42030109).\
    \ [Accessed Dec. 2018].\n\np.116-117\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - reclzrMrQSQls3PI9
  Tags:
  - education
  airtable_createdTime: '2023-06-05T09:46:29.000Z'
  airtable_id: recroNon39TCBeC88
  title: Smart toys collect data about children with little regulation
- Description: "**Issue:** Values-based design\n## Background\nEthics are often treated\
    \ as an impediment to innovation, even among those who ostensibly support ethical\
    \ design practices. In industries that reward rapid innovation in particular,\
    \ it is necessary to develop ethical design practices that integrate effectively\
    \ with existing engineering workflows. Those who advocate for ethical design within\
    \ a company should be seen as innovators seeking the best outcomes for the company,\
    \ end users, and society. Leaders can facilitate that mindset by promoting an\
    \ organizational structure that supports the integration of dialogue about ethics\
    \ throughout product life cycles.\nA/IS design processes often present moments\
    \ where ethical consequences can be highlighted. There are no universally prescribed\
    \ models for this because organizations vary significantly in structure and culture.\
    \ In some organizations, design team meetings may be brief and informal. In others,\
    \ the meetings may be lengthy and structured. The transition points between discovery,\
    \ prototyping, release, and revisions are natural contexts for conducting such\
    \ reviews. Iterative review processes are also advisable, in part because changes\
    \ to risk profiles over time Companies should study design processes to identify\
    \ situations where engineers and researchers can be encouraged to raise and resolve\
    \ questions of ethics and foster a proactive environment to realize ethically\
    \ aligned design. Achieving a distributed responsibility for ethics requires that\
    \ all people involved in product design are encouraged to notice and respond to\
    \ ethical concerns. Organizations should consider how they can best encourage\
    \ and facilitate deliberations among peers. \n## Recommendations\nOrganizations\
    \ should identify points for formal review during product development. These reviews\
    \ can focus on \u201Cred flags\u201D that have been identified in advance as indicators\
    \ of risk. For example, if the datasets involve minors or focus on users from\
    \ protected classes, then it may require additional justification or alterations\
    \ to the research or development protocols.\ncan illustrate needs or opportunities\
    \ for improving the final product.\n## Further Resources\n\u2022\_\_\_\_\_A. Sinclair,\
    \ \u201C[Approaches to Organizational Culture and Ethics,](https://doi.org/10.1007/BF01845788)\u201D\
    \ _Journal of Business Ethics, _vol._ _12, no. 1, pp. 63\u201373, 1993.\n\u2022\
    \_\_\_\_\_Al Y. S. Chen, R. B. Sawyers, and P. F. Williams. \u201C[Reinforcing\
    \ Ethical Decision Making Through Corporate Culture,](https://link.springer.com/article/10.1023/A:1017953517947)_\u201D\
    \ Journal of Business Ethics _16, no. 8, pp. 855\u2013865, 1997. \n**\_**\u2022\
    \ K. Crawford and R. Calo, \u201C[There Is a Blind Spot in AI Research,](http://www.nature.com/news/there-is-a-blind-spot-in-ai-research-1.20805)\u201D\
    \ _Nature _538, pp. 311\u2013313, 2016.\n  \n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recfcXzM3foqFNNGN
  Tags:
  - corporate-ethics
  airtable_createdTime: '2023-06-05T10:42:48.000Z'
  airtable_id: rect310qem9li3HKK
  title: How do we embed values throughout AI workflows?
- OverarchingPrinciples (from Principles):
  - recmzjcGKv3yNOxbl
  Principles:
  - rec42P8U9usfYCtv9
  airtable_createdTime: '2023-05-18T18:55:13.000Z'
  airtable_id: recv6cN7XSt5GW32Y
  title: Sustainability
- Cases:
  - reciNqxyfUgE5XM7t
  - recrVkbG0XGe2Ca0v
  - recOOmVQviRyJGvea
  - recmS3zSMbR3ofAR5
  Description: "\u201CIrresponsible data management, negligent design and production\
    \ processes, and questionable deployment practices can, each in their own ways,\
    \ lead to the implementation and distribution of AI systems that produce unreliable,\
    \ unsafe, or poor-quality outcomes. These outcomes can do direct damage to the\
    \ wellbeing of individual persons and the public welfare. They can also undermine\
    \ public trust in the responsible use of societally beneficial AI technologies,\
    \ and they can create harmful inefficiencies by virtue of the dedication of limited\
    \ public resources to inefficient or even detrimental AI technologies.\u201D (Leslie,\
    \ 2019, p. 5)\n"
  OverarchingPrinciples (from Principles):
  - recOHnq45Fq7YWsRO
  Principles:
  - recOHnq45Fq7YWsRO
  Reference:
  - 'Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A
    guide for the responsible design and implementation of AI systems in the public
    sector. The Alan Turing Institute. https://doi.org/10.5281/ZENODO.3240529'
  Sources:
  - recfYC5jjPmpLfSlM
  airtable_createdTime: '2023-05-19T09:32:44.000Z'
  airtable_id: recvQ90DajNCwPiGP
  title: Concerns with merit of new systems
- Description: "**\"Issue: **A/IS creators have opportunities to safeguard human well-being\
    \ by ensuring that A/IS does no harm to earth\u2019s natural systems or that A/IS\
    \ contributes to realizing sustainable stewardship, preservation, and/or restoration\
    \ of earth\u2019s natural systems. A/IS creators have opportunities to prevent\
    \ A/IS from contributing to the degradation of earth\u2019s natural systems and\
    \ hence losses to human well-being.\n## Background\nIt is unwise, and in truth\
    \ impossible, to separate the well-being of the natural environment of the planet\
    \ from the well-being of humanity. A range of studies, from the [historic](https://www.clubofrome.org/report/the-limits-to-growth/)\
    \ to more [recent,](https://www.nationalgeographic.com/environment/2018/10/ipcc-report-climate-change-impacts-forests-emissions/)\
    \ prove that ecological collapse endangers human existence. Hence, the concept\
    \ of well-being should encompass planetary wellbeing. Moreover, biodiversity and\
    \ ecological integrity have intrinsic merit beyond simply their instrumental value\
    \ to humans.\nTechnology has a long history of contributing to ecological degradation\
    \ through its role in expanding the scale of resource extraction and environmental\
    \ pollution, for example, the immense power needs of network computing, which\
    \ leads to [climate change,](http://www.ipcc.ch/) [water scarcity](http://www.unwater.org/),\
    \ [soil degradation](https://www.worldwildlife.org/threats/soil-erosion-and-degradation),\
    \ [species](http://www.iucnredlist.org/) [extinction](http://www.iucnredlist.org/),\
    \ [deforestation](http://www.wri.org/our-work/topics/forests), [biodiversity loss,](https://www.theguardian.com/news/2018/mar/12/what-is-biodiversity-and-why-does-it-matter-to-us)\
    \ and destruction of ecosystems which in turn threatens humankind in the long\
    \ run. These and other costs are often considered externalities and often do not\
    \ figure into decisions or plans. At the same time, there are many examples, such\
    \ as photovoltaics and smart grid technology that present potential ways to restore\
    \ earth\u2019s ecosystems if undertaken within a systems approach aimed at sustainable\
    \ economic and environmental development.\nEnvironmental justice [research ](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5129282/)demonstrates\
    \ that the negative environmental impacts of technology are commonly concentrated\
    \ on the middle class and working poor, as well as those suffering from abject\
    \ poverty, fleeing disaster zones, or otherwise lacking the resources to meet\
    \ their needs. Ecological impact can thus exacerbate the economic and sociological\
    \ effects of wealth disparities on human well-being by concentrating environmental\
    \ injustice onto those who are less well off. Moreover, [well-being research findings](https://www.equalitytrust.org.uk/resources/the-spirit-level)\
    \ indicate that unfair economic and social inequality has a dampening effect on\
    \ everyone's well-being, regardless of economic or social class.\nIn these respects,\
    \ A/IS are no exception; they can be used in ways that either help or harm the\
    \ ecological integrity of the planet. It may be fair to say that ecological health\
    \ and human well-being will, increasingly, depend upon A/IS creators. It is imperative\
    \ that A/IS creators and stakeholders find ways to use A/IS to do no harm and\
    \ to reduce the environmental degradation associated with economic growth\u2013\
    while simultaneously identifying applications to restore the ecological health\
    \ of the planet and thereby safeguarding the well-being of humans. For A/IS to\
    \ reduce environmental degradation and promote wellbeing, it is required that\
    \ not only A/IS creators act along such lines, but also that a systems approach\
    \ is taken by all A/IS stakeholders to find solutions that safeguard human well-being\
    \ with the understanding that human well-being is inextricable from healthy social,\
    \ economic, and environmental systems.\n## Recommendations\nA/IS creators need\
    \ to recognize and prioritize the stewardship of the Earth\u2019s natural systems\
    \ to promote human and ecological well-being. Specifically:_ _\n\u2022\_\_\_\_\
    \_Human well-being should be defined to encompass ecological health, access to\
    \ nature, safe climate and natural environments, biosystem diversity, and other\
    \ aspects of a healthy, sustainable natural environment.\n\u2022\_\_\_\_\_A/IS\
    \ systems should be designed to use, support, and strengthen existing ecological\
    \ sustainability standards with a certification or similar system, e.g., [LEED,](https://new.usgbc.org/leed)\
    \ [Energy Star,](https://www.energystar.gov/) or [Forest Stewardship Council.](https://us.fsc.org/en-us)\
    \ This directs automation and machine intelligence to follow the principle of\
    \ doing no harm and to safeguard environmental, social, and economic systems.\n\
    \u2022\_\_\_\_\_A/IS creators should prioritize doing no harm to the Earth\u2019\
    s natural systems, both intended and unintended harm.\_\n\u2022\_\_\_\_\_A committee\
    \ should be convened to issue findings on ways in which A/IS can be used by business,\
    \ NGOs, and governmental agencies to promote stewardship and restoration of natural\
    \ systems while reducing the harmful impact of economic development on ecological\
    \ sustainability and environmental justice.\n\u2022\_\_\_\_\_D. Austin and M.\
    \ Macauley. \"[Cutting Through Environmental Issues: Technology as a double-edged\
    \ sword.](https://www.brookings.edu/articles/cutting-through-environmental-issues-technology-as-a-double-edged-sword/)\u201D\
    \ The Brookings Institution, Dec. 2001 [Online]. Available: [https://www.brookings.edu/articles/cuttingthrough-environmental-issues-technology-asa-double-edged-sword/.](https://www.brookings.edu/articles/cutting-through-environmental-issues-technology-as-a-double-edged-sword/)\
    \ [Accessed Dec. 1, 2018].\n\u2022\_\_\_\_\_J. Newton, _[Well-being and the Natural\
    \ Environment: An Overview of the Evidence](http://resolve.sustainablelifestyles.ac.uk/sites/default/files/JulieNewtonPaper.pdf)_[.\
    \ ](http://resolve.sustainablelifestyles.ac.uk/sites/default/files/JulieNewtonPaper.pdf)August\
    \ 20, 2007.\n\u2022\_\_\_\_\_P. Dasgupta, [Human Well-Being and the Natural Environment.](https://books.google.com/books?id=OuMTDAAAQBAJ&amp;dq=wellbeing%2Band%2Bthe%2Bnatural%2Benvironment&amp;lr&amp;source=gbs_navlinks_s)\
    \ Oxford, U.K.: Oxford University Press, 2001.\n\u2022\_\_\_\_\_R. Haines-Young\
    \ and M. Potschin. \u201C[The Links ](https://www.pik-potsdam.de/news/public-events/archiv/alter-net/former-ss/2009/10.09.2009/10.9.-haines-young/literature/haines-young-potschin_2009_bes_2.pdf)\n\
    [Between Biodiversity, Ecosystem Services and Human Well-Being,](https://www.pik-potsdam.de/news/public-events/archiv/alter-net/former-ss/2009/10.09.2009/10.9.-haines-young/literature/haines-young-potschin_2009_bes_2.pdf)\u201D\
    \ in _Ecosystem Ecology: A New Synthesis_, D. Raffaelli, and C. Frid, Eds. Cambridge,\
    \ U.K.: Cambridge University Press, 2010.\n\u2022\_\_\_\_\_S. Hart, _[Capitalism\
    \ at the Crossroads: Next Generation Business Strategies for a PostCrisis World.](https://www.pearson.com/us/higher-education/program/Hart-Capitalism-at-the-Crossroads-Next-Generation-Business-Strategies-for-a-Post-Crisis-World-3rd-Edition/PGM9671.html)_\
    \ Upper Saddle River, NJ: Pearson Education, 2010.\n\u2022\_\_\_\_\_United Nations\
    \ Department of Economic and Social Affairs. \u201C[Call for New Technologies\
    \ to Avoid Ecological Destruction.](http://www.un.org/en/development/desa/news/policy/wess-2011.html)\u201D\
    \ Geneva, Switzerland, July 5, 2011.\n\u2022\_\_\_\_\_Pope Francis. [Encyclical\
    \ Letter Laudato Si\u2019 of the Holy Father Francis On the Care for Our Common\
    \ Home.](http://w2.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html)\
    \ May 24, 2015.\n\u2022\_\_\_\_\_\u201C[Environment,](https://www.dalailama.com/messages/environment)\u201D\
    \ The 14th Dalai Lama. Accessed Dec. 9, 2018. [https://www.dalailama.com/ messages/environment.](https://www.dalailama.com/messages/environment)\n\
    - Why Islam.org, Environment and Islam, 2018._ \"_\n\n_p.74-75_\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - recwQrzqPp6C7jyJs
  - recGS9OVQ7qAjvIYE
  - rec0WScLo6sUnoOQN
  Tags:
  - wellbeing
  airtable_createdTime: '2023-06-05T05:43:20.000Z'
  airtable_id: recvWM2glArsVhaye
  title: How do we foster AI that contributes to sustainability in context of short-term
    growth priorities?
- airtable_createdTime: '2023-06-14T12:19:37.000Z'
  airtable_id: recwuiKe3bhSLw4xv
  title: Commercial interests - while significant AI work is derived from publicly
    funded research, commercial actors have significant interests and this may impact
    relationships between funders-technology providers-researchers-public with blured
    lines between these groups, and complex power dynamics
- Cases:
  - reciNqxyfUgE5XM7t
  - recrVkbG0XGe2Ca0v
  - recOOmVQviRyJGvea
  - recmS3zSMbR3ofAR5
  Description: "A/IS culture and context\n## Norms vary: Background\nA responsible\
    \ approach to embedding values into A/IS requires that algorithms and systems\
    \ are created in a way that is sensitive to the variation of ethical practices\
    \ and beliefs across cultures. The designers of A/IS need to be mindful of cross-cultural\
    \ ethical variations while also respecting widely held international legal norms.\n\
    ## Recommendation\nEstablish a leading role for [intercultural information ethics\
    \ ](http://www.capurro.de/iie.html)(IIE) practitioners in ethics committees informing\
    \ technologists, policy makers, and engineers. Clearly demonstrate through examples\
    \ how cultural variation informs not only information flows and information systems,\
    \ but also algorithmic decision-making and value by design.\n## Further Resources\n\
    \u2022\_\_\_\_\_D. J. Pauleen, et al. \u201C[Cultural Bias in Information Systems\
    \ Research and Practice: Are You Coming From the Same Place I Am? ](http://aisel.aisnet.org/cais/vol17/iss1/17/)\u201D\
    \ _Communications of the Association for Information Systems, _vol._ _17, no.\
    \ 17, 2006.\n\u2022\_\_\_\_\_J. Bielby, \u201C[Comparative Philosophies in Intercultural\
    \ Information Ethics](https://scholarworks.iu.edu/iupjournals/index.php/confluence/article/view/540),\u201D\
    \ _Confluence: Online Journal of World Philosophies _2, no. 1, pp. 233\u2013253,\
    \ 2016.\n\_\np.124\n\n\"**Issue 1:** Which norms should\_be identified?\n## Background\n\
    If machines engage in human communities, then those agents will be expected to\
    \ follow the community\u2019s social and moral norms. A necessary step in enabling\
    \ machines to do so is to identify these norms. But which norms should be identified?\
    \ Laws are publicly documented and therefore easy to identify, so they can be\
    \ incorporated into A/IS as long as they do not violate humanitarian or community\
    \ moral principles. Social and moral norms are more difficult to ascertain, as\
    \ they are expressed through behavior, language, customs, cultural symbols, and\
    \ artifacts. Most important, communities ranging from families to whole nations\
    \ differ to various degrees in the norms they follow. Therefore, generating a\
    \ universal set of norms that applies to all A/IS in all contexts is not realistic,\
    \ but neither is it advisable to completely tailor the A/IS to individual preferences.\
    \ We suggest that it is feasible to identify broadly observed norms of communities\
    \ in which a technology is deployed.\nFurthermore, the difficulty of generating\
    \ a universal set of norms is not inconsistent with the goal of seeking agreement\
    \ over Universal Human Rights (see the \u201CGeneral Principles\u201D chapter\
    \ of _Ethically Aligned Design_). However, these universal rights are not sufficient\
    \ for devising A/IS that conform to the specific norms of its community. Universal\
    \ Human Rights must, however, constrain the kinds of norms that are implemented\
    \ in the A/IS (cf. van de Poel 20168).\nEmbedding norms in A/IS requires a careful\
    \ understanding of the communities in which the A/IS are to be deployed. Further,\
    \ even within a particular community, different types of A/IS will demand different\
    \ sets of norms. The relevant norms for self-driving vehicles, for example,\_\
    may differ greatly from those for robots used\_in healthcare. Thus, we recommend\
    \ that to develop A/IS capable of following legal, social, and moral norms, the\
    \ first step is to identify the norms of the specific community in which the\_\
    A/IS are to be deployed and, in particular, norms relevant to the kinds of tasks\
    \ and roles for which the A/IS are designed. Even when designating a narrowly\
    \ defined community, e.g., a nursing home, an apartment complex, or a company,\
    \ there will be variations in the norms that apply, or in their relative weighting.\
    \ The norm identification process must heed such variation and ensure that the\
    \ identified norms are representative, not only of the dominant subgroup in the\
    \ community but also of vulnerable and underrepresented groups.\nThe most narrowly\
    \ defined \u201Ccommunity\u201D is a single person, and A/IS may well have to\
    \ adapt to the unique expectations and needs of a given individual, such as the\
    \ arrangement of a disabled person\u2019s living accommodations. However, unique\
    \ individual expectations must not violate norms in the larger community. Whereas\
    \ the arrangement of someone\u2019s kitchen or the frequency with which a care\
    \ robot checks in with a patient can be personalized without violating any community\
    \ norms, encouraging the robot to use derogatory language to talk about certain\
    \ social groups does violate such norms. In the next section, we discuss how A/IS\
    \ might handle such norm conflicts.\nInnovation projects and development efforts\
    \ for A/IS should always rely on empirical research, involving multiple disciplines\
    \ and multiple methods; to investigate and document both context- and task-specific\
    \ norms, spoken and unspoken, that typically apply in a particular community.\
    \ Such a set of empirically identified norms should then guide system design.\
    \ This process of norm identification and implementation must be iterative and\
    \ revisable. A/IS with an initial set of implemented norms may betray biases of\
    \ original assessments (Misra, Zitnick, Mitchell, and Girshick 20169) that can\
    \ be revealed by interactions with, and feedback from, the relevant community.\
    \ This leads to a process of norm updating, which is described next in Issue 2.\n\
    ## Recommendation\nTo develop A/IS capable of following social and moral norms,\
    \ the first step is to identify the norms of the specific community in which the\_\
    A/IS are to be deployed and, in particular, norms relevant to the kinds of tasks\
    \ and roles that the A/IS are designed for. This norm identification process must\
    \ use appropriate scientific methods and continue through the system's life cycle.\n\
    ## Further Resources\n\u2022\_\_\_\_\_Mack, Ed., \u201CChanging social norms.\u201D\
    \ _Social Research: An International Quarterly,_ 85, no.1, 1\u2013271, 2018.\n\
    \u2022\_\_\_\_\_I. Misra, C. L. Zitnick, M. Mitchell, and R. Girshick, (2016).\
    \ Seeing through the human reporting bias: Visual Classifiers from Noisy Human-Centric\
    \ Labels. In _Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern\
    \ Recognition_ (CVPR), pp. 2930\u20132939. doi[:10.1109/CVPR.2016.320](https://doi.org/10.1109/CVPR.2016.320)\n\
    \u2022\_\_\_\_\_I. van de Poel, \u201C[An Ethical Framework for Evaluating Experimental\
    \ Technology,](https://link.springer.com/article/10.1007/s11948-015-9724-3)\u201D\
    \ _Science and Engineering Ethics_, 22, no. 3,pp. 667686, 2016.\"\np.168-169\n\
    \n## \"Norms change: Background\nNorms are not static. They change over time,\
    \ in response to social progress, political change, new legal measures, or novel\
    \ opportunities (Mack 201810). Norms can fade away when, for whatever reasons,\
    \ fewer and fewer people adhere to them. And new norms emerge when technological\
    \ innovation invites novel behaviors and novel standards, e.g., cell phone use\
    \ in public.\nA/IS should be equipped with a starting set of social and legal\
    \ norms before they are deployed in their intended community (see Issue 1), but\
    \ this will not suffice for A/IS to behave appropriately over time. A/IS or the\
    \ designers of A/IS, must be adept at identifying and adding new norms to its\
    \ starting set, because the initial norm identification process in the community\
    \ will undoubtedly have missed some norms and because the community\u2019s norms\
    \ change.\nHumans rely on numerous capacities to update their knowledge of norms\
    \ and learn new ones. They observe other community members\u2019 behavior and\
    \ are sensitive to collective norm change; they explicitly ask about new norms\
    \ when joining new communities, e.g., entering college or a job in a new town;\
    \ and they respond to feedback from others when they exhibit uncertainty about\
    \ norms or have violated a norm.\nLikewise, A/IS need multiple capacities to improve\
    \ their own norm knowledge and to adapt to a community\u2019s dynamically changing\
    \ norms. These capacities include:\n\u2022\_\_\_\_\_Processing behavioral trends\
    \ by members of the target community and comparing them to trends predicted by\
    \ the baseline norm system,\n\u2022\_\_\_\_\_Asking for guidance from the community\
    \ when uncertainty about applicable norms exceeds a critical threshold,\n\u2022\
    \_\_\_\_\_Responding to instruction from the community members who introduce a\
    \ robot to a previously unknown context or who notice the A/IS\u2019 uncertainty\
    \ in a familiar context, and\n\u2022\_\_\_\_\_Responding to formal or informal\
    \ feedback from the community when the A/IS violate\_a norm.\nThe modification\
    \ of a normative system can occur at any level of the system: it could involve\
    \ altering the priority weightings between individual norms, changing the qualitative\
    \ expression of a norm, or altering the quantitative parameters that enable the\
    \ norm.\nWe recommend that the system\u2019s norm changes be transparent. That\
    \ is, the system or its designer should consult with users, designers, and community\
    \ representatives when adding new norms to its norm system or adjusting the priority\
    \ or content of existing norms. Allowing a system to learn new norms without public\
    \ or expert review has detrimental consequences (Green and Hu 201811). The form\
    \ of consultation and the specific review process will vary by machine sophistication\u2015\
    e.g., linguistic capacity and function/role, or a flexible social companion versus\
    \ a task-defined medical robot\u2015and best practices will have to be established.\
    \ In some cases, the system may document its dynamic change, and the user can\
    \ consult this documentation as desired. In other cases, explicit announcements\
    \ and requests for discussion with the designer may be appropriate. In yet other\
    \ cases, the A/IS may propose changes, and the relevant human community, e.g.,\
    \ drawn from a representative crowdsourced panel, will decide whether such changes\
    \ should be implemented\_in the system.\_\n## Recommendation\nTo respond to the\
    \ dynamic change of norms in society A/IS or their designers must be able to amend\
    \ their norms or add new ones, while being transparent about these changes to\
    \ users,\_designers, broader community representatives, and other stakeholders.\n\
    ## Further Resources\n\u2022\_\_\_\_\_B. Green and L. Hu. \u201CThe Myth in the\
    \ Methodology: Towards a Recontextualization of Fairness in ML.\u201D Paper presented\
    \ at the Debates workshop at the 35th International Conference on Machine Learning,\
    \ Stockholm, Sweden 2018.\n\u2022\_\_\_\_\_ Mack, Ed., \u201CChanging social norms,\u201D\
    _ Social Research: An International Quarterly_, 85\_(1, Special Issue), 1-271,\
    \ 2018.**Issue 3: **A/IS will face norm conflicts and need methods to resolve\
    \ them.\n\_\n## Norms Conflict: Background\nOften, even within a well-specified\
    \ context, no action is available that fulfills all obligations and prohibitions.\
    \ Such situations\u2014often described as moral dilemmas or moral overload (Van\
    \ den Hoven 201212)\u2014must be computationally tractable by A/IS; they cannot\
    \ simply stop in their tracks and end on a logical contradiction. Humans resolve\
    \ such situations by accepting trade-offs between conflicting norms, which constitute\
    \ priorities of one norm or value over another in a given context. Such priorities\
    \ may be represented in the norm system as hierarchical relations.\nAlong with\
    \ identifying the norms within a specific community and task domain, empirical\
    \ research must identify the ways in which people prioritize competing norms and\
    \ resolve norm conflicts, and the ways in which people expect A/IS to resolve\
    \ similar norm conflicts. These more local conflict resolutions will be further\
    \ constrained by some general principles, such as the \u201CCommon Good Principle\u201D\
    \ (Andre and Velasquez 199213) or local and national laws. For example, a self-driving\
    \ vehicle\u2019s prioritization of one factor over another in its decision-making\
    \ will need to reflect the laws and norms of the population in which the A/IS\
    \ are deployed, e.g., the traffic laws of a U.S. state and the United States as\
    \ a whole.\nSome priority orders can be built into a given norm network as hierarchical\
    \ relations, e.g.,\_more general prohibitions against harm to humans typically\
    \ override more specific norms against lying. Other priority orders can stem from\
    \ the override that norms in the larger community\_exert on norms and preferences\
    \ of an individual user. In the earlier example discussing personalization (see\
    \ Issue 1), the A/IS of a racist user who demands the A/IS use derogatory language\
    \ for certain social groups will have to resist such demands because community\
    \ norms hierarchically override an individual user\u2019s preferences. In many\
    \ cases, priority orders are not built in as fixed hierarchies because the priorities\
    \ are themselves context-specific or may arise from net moral costs and benefits\
    \ of the particular case at hand. A/IS must have learning capacities to track\
    \ such variations and incorporate user and community input, e.g., about the subtle\
    \ differences between contexts, so as to refine the system\u2019s norm network\
    \ (see Issue 2).\nTension may sometimes arise between a community\u2019s social\
    \ and legal norms and the normative considerations of designers or manufacturers.\
    \ Democratic processes may need to be developed that resolve this tension\u2014\
    \ processes that cannot be presented in detail in this chapter. Often such resolution\
    \ will favor the local laws and norms, but in some cases the community may have\
    \ to be persuaded to accept A/IS favoring international law or broader humanitarian\
    \ principles over, say, racist or sexist local practices.\n In general, we recommend\
    \ that the system\u2019s resolution of norm conflicts be transparent\u2014that\
    \ is, documented by the system and ready to be made available to users, the relevant\
    \ community of deployment, and third-party evaluators. Just like people explain\
    \ to each other why they made decisions, they will expect any A/IS to be able\
    \ to explain their decisions and be sensitive to user feedback about the appropriateness\
    \ of the decisions. To do so, design and development of A/IS should specifically\
    \ identify the relevant groups of humans who may request explanations and evaluate\
    \ the systems\u2019 behaviors. In the case of a system detecting a norm conflict,\
    \ the system should consult and offer explanations to representatives from the\
    \ community, e.g., randomly sampled crowdsourced members or elected officials,\
    \ as well as to third-party evaluators, with the goal of discussing and resolving\
    \ the norm conflict.\n## Recommendation\nA/IS developers should identify the ways\
    \ in which people resolve norm conflicts and the ways in which they expect A/IS\
    \ to resolve similar norm conflicts. A system\u2019s resolution of norm conflicts\
    \ must be transparent\u2014that is, documented by the system and ready to be made\
    \ available to users, the relevant community of deployment, and third-party evaluators.\n\
    ## Further resources\n\u2022\_\_\_\_\_M. Velasquez, C. Andre, T. Shanks, S.J.,\
    \ and M. J. Meyer, \u201C[The Common](https://legacy.scu.edu/ethics/publications/iie/v5n1/common.html)\
    \ [Good](https://legacy.scu.edu/ethics/publications/iie/v5n1/common.html).\u201D\
    \ _Issues in Ethics_, vol._ _5, no. 1, 1992.\n\u2022\_\_\_\_\_J. Van den Hoven,\
    \ \u201CEngineering and the Problem of Moral Overload.\u201D _Science and Engineering\
    \ Ethics, vol. _18, no. 1, pp.\_143\u2013155, 2012.\n\u2022\_\_\_\_\_D. Abel,\
    \ J. MacGlashan, and M. L. Littman. \u201CReinforcement Learning as a Framework\
    \ for Ethical Decision Making.\u201D _AAAI Workshop AI, Ethics, and Society, Volume\
    \ WS-16-02 of 13th AAAI Workshops_. Palo Alto, CA: AAAI\_Press, 2016.\n\u2022\_\
    \_\_\_\_O. Bendel, Die Moral in der Maschine: Beitr\xE4ge zu Roboter- und Maschinenethik.\
    \ Hannover, Germany: Heise Medien, 2016. \_\_\_Accessible popular-science contributions\
    \ to philosophical issues and technical implementations of machine ethics\n\u2022\
    \_\_\_\_\_S. V. Burks, and E. L. Krupka. [\u201CA Multimethod Approach to Identifying\
    \ Norms and Normative Expectations within a Corporate Hierarchy: ](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1110.1478)\n\
    [Evidence from the Financial Services Industry.\u201D](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1110.1478)\
    \ _Management Science, _vol. 58, pp. 203\u2013217, 2012.\_\_\_\_Illustrates surveys\
    \ and incentivized coordination games as methods to elicit norms in a large financial\
    \ services firm\n\u2022\_\_\_\_\_F. Cushman, V. Kumar, and P. Railton, \u201C\
    Moral Learning,\u201D _Cognition_, vol._ _167, pp. 1\u2013282, 2017.\n\u2022\_\
    \_\_\_\_M. Flanagan, D. C. Howe, and H. Nissenbaum, \u201CEmbodying Values in\
    \ Technology: Theory and Practice.\u201D _Information Technology and Moral _\n\
    _Philosophy_, J. van den Hoven and J. Weckert, Eds., Cambridge University Press,\
    \ 2008, pp. 322\u201353. Cambridge Core, _Cambridge University Press._ Preprint\
    \ available at\_[http://www.nyu.edu/projects/nissenbaum/ papers/Nissenbaum-VID.4-25.pdf](http://www.nyu.edu/projects/nissenbaum/papers/Nissenbaum-VID.4-25.pdf)\n\
    \u2022\_\_\_\_\_B. Friedman, P. H. Kahn, A. Borning, and A. Huldtgren. \u201C\
    Value Sensitive Design and Information Systems,\u201D in _Early Engagement and\
    \ New Technologies: Opening up the Laboratory, _N. Doorn, Schuurbiers, I. van\
    \ de Poel, and M. Gorman, Eds., vol. 16, pp. 55\u201395. Dordrecht: Springer,\
    \ 2013.\_\_\_A comprehensive introduction into Value Sensitive Design and three\
    \ sample applications\n\u2022\_\_\_\_\_G. Mackie, F. Moneti, E. Denny, and H.\
    \ Shakya. \u201CWhat Are Social Norms? How Are They Measured?\u201D UNICEF Working\
    \ Pape[r. ](http://dmeforpeace.org/sites/default/files/4%2009%2030%20Whole%20What%20are%20Social%20Norms.pdf)University\
    \ of California at San Diego: UNICEF, Sept. 2014. [https://dmeforpeace.org/sites/\
    \ default/files/4%2009%2030%20Whole%20 What%20are%20Social%20Norms.pdf](https://dmeforpeace.org/sites/default/files/4%2009%2030%20Whole%20What%20are%20Social%20Norms.pdf)\_\
    \_A broad survey of conceptual and measurement questions regarding social norms.\n\
    \u2022\_\_\_\_\_J. A. Leydens and J. C. Lucena. Engineering Justice: Transforming\
    \ Engineering Education and Practice. Hoboken, NJ: John Wiley & Sons, 2018.\_\_\
    Identifies principles of engineering for social justice.\n\u2022\_\_\_\_\_B. F.\
    \ Malle, \u201CIntegrating Robot Ethics and Machine Morality: The Study and Design\
    \ of Moral Competence in Robots.\u201D _Ethics and Information Technology, _vol._\
    \ _18, no. 4, pp. 243\u2013256, 2016.\_\_Discusses how a robot\u2019s norm capacity\
    \ fits in the larger vision of a robot with moral competence.\n\u2022\_\_\_\_\_\
    K. W. Miller, M. J. Wolf, and F. Grodzinsky, \u201CThis \u2018Ethical Trap\u2019\
    \ Is for Roboticists, Not Robots: On the Issue of Artificial Agent Ethical DecisionMaking.\u201D\
    \ _Science and Engineering Ethics, _vol._ _23, pp. 389\u2013401, 2017.\_\_\_This\
    \ article raises doubts about the possibility of imbuing artificial agents with\
    \ morality, or of claiming to have done so.\n\u2022\_\_\_\_\_Open Roboethics Initiative:\
    \ [www.openroboethics.org](http://www.openroboethics.org/). A series of poll results\
    \ on differences in human moral decision-making and changes in priority order\
    \ of values for autonomous systems (e.g., [on care](http://www.openroboethics.org/results-should-a-carebot-bring-an-alcoholic-a-drink-poll-says-it-depends-on-who-owns-the-robot/)\
    \ [robots)](http://www.openroboethics.org/results-should-a-carebot-bring-an-alcoholic-a-drink-poll-says-it-depends-on-who-owns-the-robot/),\
    \ 2019.\n\u2022\_\_\_\_\_A. Rizzo and L. L. Swisher, \u201CComparing the Stewart\u2013\
    Sprinthall Management Survey and the Defining Issues Test-2 as Measures of Moral\
    \ Reasoning in Public Administration.\u201D _Journal of Public Administration\
    \ Research\_and Theory, _vol._ _14, pp. 335\u2013348, 2004. Describes two assessment\
    \ instruments of moral reasoning (including norm maintenance) based on Kohlberg\u2019\
    s theory\_of moral development.\n\u2022\_\_\_\_ S. H. Schwartz, \u201CA[n Overview\
    \ of the Schwartz](https://scholarworks.gvsu.edu/orpc/vol2/iss1/11/)\_[Theory\
    \ of Basic Values.\u201D _Online Readings in Psychology and Culture _2, 2012](https://scholarworks.gvsu.edu/orpc/vol2/iss1/11/).\
    \ \u2022\_\_\_\_\_Comprehensive overview of a specific theory of values, understood\
    \ as motivational orientations toward abstract outcomes (e.g., self-direction,\
    \ power, security).\n\u2022\_\_\_\_\_S. H. Schwartz and K. Boehnke. \u201C[Evaluating\
    \ the Structure of Human Values with Confirmatory Factor Analysis.\u201D _Journal\
    \ of Research in Personality, _vol. 38, ](http://www.sciencedirect.com/science/article/pii/S0092656603000692?via%3Dihub)pp.\
    \ 230\u2013255, 2004.\n\u2022\_\_\_\_\_Describes an older method of subjective\
    \ judgments of relations among valued outcomes and a newer, formal method of analyzing\
    \ these relations.\n\u2022\_\_\_\_\_W. Wallach and C. Allen. _Moral Machines:\
    \ Teaching Robots Right from Wrong_. New York: Oxford University Press, 2008.\
    \ This book describes some of the challenges of having a one-size-fits-all approach\
    \ to embedding human values in autonomous systems. \"\np.172-174\n\n\"**Issue\
    \ 1:** Not all norms of a target community apply equally to human and artificial\
    \ agents\n## Background\nAn intuitive criterion for evaluations of norms embedded\
    \ in A/IS would be that the A/IS norms should mirror the community\u2019s norms\u2014\
    that is, the A/IS should be disposed to behave the same way that people expect\
    \ each other to behave. However, for a given community and a given\_A/IS use context,\
    \ A/IS and humans are unlikely to have identical sets of norms. People will have\
    \ some unique expectations for humans than they do not for machines, e.g., norms\
    \ governing the regulation of negative emotions, assuming that machines do not\
    \ have such emotions. People may in some cases have unique expectations of A/IS\
    \ that they do not have for humans, e.g., a robot worker, but not a human worker,\
    \ is expected to work without regular breaks.\n## Recommendation\nThe norm identification\
    \ process must document the similarities and differences between the norms that\
    \ humans apply to other humans and the norms they apply to A/IS. Norm implementations\
    \ should be evaluated\_specifically against the norms that the\_community expects\
    \ the A/IS to follow.\"\np.183\n"
  Reference:
  - "IEEEE, Chatila, R., & Havens, J. C. (2019). The IEEE Global Initiative on Ethics\
    \ of Autonomous and Intelligent Systems. In M. I. Aldinhas Ferreira, J. Silva\
    \ Sequeira, G. Singh Virk, M. O. Tokhi, & E. E. Kadar (Eds.), Robotics and Well-Being\
    \ (Vol. 95, pp. 11\u201316). Springer International Publishing. https://doi.org/10.1007/978-3-030-12524-0_2"
  Sources:
  - recpXl48pJdKDhc6f
  Strategies:
  - reccMqFCLOgQwWM5Q
  Tags:
  - research
  airtable_createdTime: '2023-06-05T10:20:28.000Z'
  airtable_id: recxjc79LvLdKa4rl
  title: How can AI work respect diversity in cross-cultural norms, aims, and practices
    that vary by location, change over time, and come into conflict?
- Description: "\u201C\u201CReasons to conduct secondary analyses of data include:\
    \ avoidance of duplication in primary collection and the associated reduction\
    \ of burdens on participants; corroboration or criticism of the conclusions of\
    \ the original project; comparison of change in a research sample over time; application\
    \ of new tests of hypotheses that were not available at the time of original data\
    \ collection; and confirmation that the data are authentic. Privacy concerns and\
    \ questions about the need to seek consent arrive, however, when information provided\
    \ for secondary use in research can be linked to individuals, and when the possibility\
    \ exists that individuals can be identified in published reports, or through data\
    \ linkage\u201D (TCPS2, Chapter 5, D. Consent and Secondary Use of Identifiable\
    \ Information for Research Purposes).\u201D (Fedoruk, 2017, p. 13)\n\n\u201C\u201C\
    If a researcher satisfies all the conditions in Article 5.5A (a) to (f), the REB\
    \ may approve the research without requiring consent from the individuals to whom\
    \ the information relates. a. identifiable information is essential to the research;\
    \ b. the use of identifiable information without the participants\u2019 consent\
    \ is unlikely to adversely affect the welfare of individuals to whom the information\
    \ relates; c. the researchers will take appropriate measures to protect the privacy\
    \ of individuals, and to safeguard the identifiable information; d. the researchers\
    \ will comply with any known preferences previously expressed by individuals about\
    \ any use of their information; e. it is impossible or impracticable to seek consent\
    \ from individuals to whom the information relates; and f. the researchers have\
    \ obtained any other necessary permission for secondary use of information for\
    \ research purposes\u201D (TCPS2, Chapter 5, D. Consent and Secondary Use of Identifiable\
    \ Information for Research Purposes).\u201D (Fedoruk, 2017, p. 13)\n\n\u201C\u201C\
    Researchers shall seek REB review, but are not required to seek participant consent,\
    \ for research that relies exclusively on the secondary use of non-identifiable\
    \ information\u201D (TCPS2, Chapter 5, D. Consent and Secondary Use of Identifiable\
    \ Information for Research Purposes).\u201D (Fedoruk, 2017, p. 14)\n"
  OverarchingPrinciples (from Principles):
  - recLHILkx2JDFsLbX
  - recSqx6wklVpDzx3s
  Principles:
  - recKdujFoPJr4ZAhZ
  - recMGB4iC5oaCtr5x
  Reference:
  - Fedoruk, L. (2017b). Ethics in The Scholarship of Teaching and Learning. University
    of Calgary Taylor Institute for Teaching and Learning. https://taylorinstitute.ucalgary.ca/resources/ethics-scholarship-teaching-and-learning
  Sources:
  - recQzldmBLByP78Uu
  airtable_createdTime: '2023-05-19T13:10:14.000Z'
  airtable_id: reczg5MObRbgzTeob
  title: SoTL secondary use of student data
