- Attachments:
  - filename: p17HLEGAI.png
    height: 1490
    id: att1W8lqIJBfmDo6Z
    size: 732931
    thumbnails:
      full:
        height: 3000
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/vVTrhI9IaaMjyQ-fmcnwJQ/8QQtDaxp37_OmDXweaqHqdkFZRk-h_d0ybFyDLgFQUMrAoWh7DUrIrAHyHqZ5sj7cDw1lg4K694LnS5KJunGjQ/Tm_Kr_QsG4sbw1R7-HY1qoqlOqN6hzRbbUGy_Yd0urw
        width: 3000
      large:
        height: 512
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/DVOGfY4LKmUM1NArI8S5KA/u7h47c6gKEYwAGixR6ep0KpuymlNn4xn983W8Z7NNZk-ZGLi48kAPefa5eFBNL7F1UkWDFT0Rd_nAN_Y_vxBdA/kz3RmHpkk39Mncx-ym9WKikaRfoOjXBhFSNEM_Z9lww
        width: 610
      small:
        height: 36
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/G3UwwE1g_lpe-lGN0MTjIQ/cTVUhihVfv9lbgrTKI54wl5ghhVXRFMH0sV_-mc8aes5rsw_xxPRGVIup5yQxO8FtpHqmvfkEzyP0wG-tNiLWw/dBxgpmQLtuvwABYFsykeFCQR19R_Q8X7ierOxAz6sKw
        width: 43
    type: image/png
    url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/gbeN1uMM0GE_pKqZse_cfw/C5lggF83KF2Ksqyv-n7Cc5RRAB107hUDlTNBMmIEpRToifnDdVOyrJ_Jk2BkbWJqhxBAZ3Chqj5jZYz5vCJH5Hp6q8Nyx1zXzVea2EZ3Vcw/1_YWjcvRF78SRjwOKxEwYmFNM-SGIhOyGV0rQZyyQ_I
    width: 1776
  - filename: p22HLEG.png
    height: 664
    id: attAbTSJmsmpp2Woj
    size: 283112
    thumbnails:
      full:
        height: 3000
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/mbkt69bvrrftqkRQFTimPg/dRFdhxyxFH9ioxVKXA4o5GN6c_B6mHAN_j6iUfEgVcxMQVRlTryYWI5D7ym2pI-8hZVKEmqMSvXGfBhjUodpkw/YMLexu2rtzFDKTGvuClTKm1-zgGZZLRCbq8nELXCG-g
        width: 3000
      large:
        height: 512
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/eN2gefyfR9X6s5Dj9HiB1A/SNDLOYnxuUlwRIu5eOkFrUpHtWNvgtw1CfkZKA9faE-7NvdN-kFFnJgt5-mxz91UW56jMQb-AS8rgNcn3t021A/y-AzHUtoKeyTvTVQXctNeThjKey1rUM2PGHkqxSRI3o
        width: 1606
      small:
        height: 36
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/9tKcrwLh6I0bFhEVuqOm6g/peukx2Jeu2cHgwOYNwLZKnPQDq0MpfnXt78GgAgddEPekTO3QpivRttuyE7R5YOZGt_k-TOOFbAzNQ5bldMWSg/32i8FchUxK2IKlDys5g-NhujfSYckRzyNNaC6ZwOJaI
        width: 113
    type: image/png
    url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/O2rU-y8tHFCMZqbIrNztfQ/YKBxdLc2nw2IkZ6_SfJ5BKr8AZwQijxY-M11mW8Uy3WG1njBsoDpG37JmKwGHSSv0x-9FiJMA9GrtoLFsUz3eFvvsKbUHDtnnXcQgf_Gntw/QOvu49KCZOdl6Wn1x72mWoqwkfx2cA0qvE6SeOpuurE
    width: 2083
  Description: "\u201CRequirements of Trustworthy AI The principles outlined in Chapter\
    \ I must be translated into concrete requirements to achieve Trustworthy AI. These\
    \ requirements are applicable to different stakeholders partaking in AI systems\u2019\
    \ life cycle: developers, deployers and end-users, as well as the broader society.\
    \ By developers, we refer to those who research, design and/or develop AI systems.\
    \ By deployers, we refer to public or private organisations that use AI systems\
    \ within their business processes and to offer products and services to others.\
    \ End-users are those engaging with the AI system, directly or indirectly. Finally,\
    \ the broader society encompasses all others that are directly or indirectly affected\
    \ by AI systems. Different groups of stakeholders have different roles to play\
    \ in ensuring that the requirements are met: a. Developers should implement and\
    \ apply the requirements to design and development processes; b. Deployers should\
    \ ensure that the systems they use and the products and services they offer meet\
    \ the requirements; c. End-users and the broader society should be informed about\
    \ these requirements and able to request that they are upheld. The below list\
    \ of requirements is non-exhaustive.35 It includes systemic, individual and societal\
    \ aspects: 1 Human agency and oversight Including fundamental rights, human agency\
    \ and human oversight 2 Technical robustness and safety Including resilience to\
    \ attack and security, fall back plan and general safety, accuracy, reliability\
    \ and reproducibility 3 Privacy and data governance Including respect for privacy,\
    \ quality and integrity of data, and access to data 4 Transparency Including traceability,\
    \ explainability and communication 5 Diversity, non-discrimination and fairness\
    \ Including the avoidance of unfair bias, accessibility and universal design,\
    \ and stakeholder participation 6 Societal and environmental wellbeing Including\
    \ sustainability and environmental friendliness, social impact, society and democracy\
    \ 7 Accountability Including auditability, minimisation and reporting of negative\
    \ impact, trade-offs and redress.\u201D (High-Level Expert Group on AI, 2019,\
    \ p. 14)\n\n\u201CWhile all requirements are of equal importance, context and\
    \ potential tensions between them will need to be taken into account when applying\
    \ them across different domains and industries. Implementation of these requirements\
    \ should occur throughout an AI system\u2019s entire life cycle and depends on\
    \ the specific application. While most requirements apply to all AI systems, special\
    \ attention is given to those directly or indirectly affecting individuals. Therefore,\
    \ for some applications (for instance in industrial settings), they may be of\
    \ lesser relevance. The above requirements include elements that are in some cases\
    \ already reflected in existing laws. We reiterate that \u2013 in line with Trustworthy\
    \ AI\u2019s first component \u2013 it is the responsibility of AI practitioners\
    \ to ensure that they comply with their legal obligations, both as regards horizontally\
    \ applicable rules as well as domain-specific regulation\u201D (High-Level Expert\
    \ Group on AI, 2019, p. 15)\n\nThe full document expands on each in detail.\n\n\
    \u201CTo implement the above requirements, both technical and non-technical methods\
    \ can be employed. These encompass all stages of an AI system\u2019s life cycle.\
    \ An evaluation of the methods employed to implement the requirements, as well\
    \ as reporting and justifying51 changes to the implementation processes, should\
    \ occur on an ongoing basis. AI systems are continuously evolving and acting in\
    \ a dynamic environment.\u201D (High-Level Expert Group on AI, 2019, p. 20) \n"
  Linked Principles:
  - recU6u0AZbcNj1ik9
  - recOHnq45Fq7YWsRO
  - recxcFmvPG5wrCqpO
  - reckb3cgfeDh1EeUP
  Linked Sources:
  - recnCULdYQ36cpZR7
  airtable_createdTime: '2023-05-28T19:04:35.000Z'
  airtable_id: rec03QV2RUJkP3dfo
  title: Overview of considerations in transparency
- Description: "\u201CConducting research, especially with complex and often opaque\
    \ ML models, confers a degree of power in the researcher. The researcher decides\
    \ which technical features will be used, and which data flows will be enacted.\
    \ This paternalistic approach raises questions about the extent to which the stakeholders\
    \ or data subjects are aware and in agreement with the experiments or data collections\
    \ conducted in their social domains.26 Additional to questions from IRE 3.0, other\
    \ questions are therefore relevant to highlight: \n- How does the researcher justify\
    \ the foreseen intervention in a social, economic, or legal context by technical\
    \ means? \n- Is the researcher able to understand and explain in an accessible\
    \ manner how AI is to be used in the research?\u201D \n- Is the researcher able\
    \ to explain why the approach by technical means and the use of AI are better\
    \ suited than any alternative methodology? \n- Have the persons who will be affected\
    \ by the AI system requested the research? \n- If not, have they been informed\
    \ and have they agreed? \n- If gaining the informed consent of all data subjects\
    \ is infeasible, can the researcher obtain proxy consent from a representative\
    \ or institutional ethics board on their behalf? \n- How has the balance between\
    \ advantageous ends and individual freedom been struck? \n- Which values did the\
    \ organization decide to promote, and how?\"\n(franzke et al., 2020, p. 37-38)\n"
  Linked Principles:
  - recsvi4LnhEEPyQ1h
  - recU6u0AZbcNj1ik9
  Linked Sources:
  - rec6r8OkE2Q2EdiM3
  Tags:
  - reflection-questions
  - project-design
  airtable_createdTime: '2023-05-19T12:22:50.000Z'
  airtable_id: rec09f7Nm4RTf6WjE
  title: Questions to consider in key stages of AI and machine learning based research,
    regarding legitmacy and power in consent
- Description: "\"When considering the role of accountability in the AI project delivery\
    \ lifecycle, it is important first to make sure that you are taking a \u2018best\
    \ practices\u2019 approach to data processing that is aligned with Principle 6\
    \ of the Data Ethics Framework. Beyond following this general guidance, however,\
    \ you should pay special attention to the new and unique challenges posed to public\
    \ sector accountability by the design and implementation of AI systems.\n\nResponsible\
    \ AI project delivery requires that two related challenges to public sector accountability\
    \ be confronted directly:\n\n1\\.\tAccountability gap: As mentioned above, automated\
    \ decisions are not self-justifiable. Whereas human agents can be called to account\
    \ for their judgements and decisions in instances where those judgments and decisions\
    \ affect the interests of others, the statistical models and underlying hardware\
    \ that compose AI systems are not responsible in the same morally relevant sense.\
    \ This creates an accountability gap that must be addressed so that clear and\
    \ imputable sources of human answerability can be attached to decisions assisted\
    \ or produced by an AI system.\n\n2\\.\tComplexity of AI production processes:\
    \ Establishing human answerability is not a simple matter when it comes to the\
    \ design and deployment of AI systems. This is due to the complexity and multi-agent\
    \ character of the development and use of these systems. Typically, AI project\
    \ delivery workflows include department and delivery leads, technical\n \nexperts,\
    \ data procurement and preparation personnel, policy and domain experts, implementers,\
    \ and others. Due to this production complexity, it may become difficult to answer\
    \ the question of who among these parties involved in the production of AI systems\
    \ should bear responsibility if these systems\u2019 uses have negative consequences\
    \ and impacts.\n\nMeeting the special requirements of accountability, which are\
    \ born out of these two challenges, call for a sufficiently fine-grained concept\
    \ of what would make an AI project properly accountable. This concept can be broken\
    \ down into two subcomponents of accountability: answerability and auditability:\n\
    \n\u2022\tAnswerability: The principle of accountability demands that the onus\
    \ of justifying algorithmically supported decisions be placed on the shoulders\
    \ of the human creators and users of those AI systems. This means that it is essential\
    \ to establish a continuous chain of human responsibility across the whole AI\
    \ project delivery workflow. Making sure that accountability is effective from\
    \ end to end necessitates that no gaps be permitted in the answerability of responsible\
    \ human authorities from first steps of the design of an AI system to its algorithmically\
    \ steered outcomes.\n\nAnswerability also demands that explanations and justifications\
    \ of both the content of algorithmically supported decisions and the processes\
    \ behind their production be offered by competent human authorities in plain,\
    \ understandable, and coherent language. These explanations and justifications\
    \ should be based upon sincere, consistent, sound, and impartial reasons that\
    \ are accessible to non-technical hearers.\n\n\u2022\tAuditability: Whereas the\
    \ notion of answerability responds to the question of who is accountable for an\
    \ automation supported outcome, the notion of auditability answers the question\
    \ of how the designers and implementers of AI systems are to be held accountable.\
    \ This aspect of accountability has to do with demonstrating both the responsibility\
    \ of design and use practices and the justifiability of outcomes.\n\nYour project\
    \ team must ensure that every step of the process of designing and implementing\
    \ your AI project is accessible for audit, oversight, and review. Successful audit\
    \ requires builders and implementers of algorithmic systems to keep records and\
    \ to make accessible information that enables monitoring of the soundness and\
    \ diligence of the innovation processes that produced the AI system.\n\nAuditability\
    \ also requires that your project team keep records and make accessible information\
    \ that enables monitoring of data provenance and analysis from the stages of collection,\
    \ pre-processing, and modelling to training, testing, and deploying. This is the\
    \ purpose of the previously mentioned Dataset Factsheet.\n\nMoreover, it requires\
    \ your team to enable peers and overseers to probe and to critically review the\
    \ dynamic operation of the system in order to ensure that the procedures and operations\
    \ which are producing the model\u2019s behaviour are safe, ethical, and fair.\
    \ Practically transparent algorithmic models must be built for auditability, reproducible,\
    \ and equipped for end-to-end recording and monitoring of their data processing.\n\
    \ \nThe deliberate incorporation of both of these elements of accountability (answerability\
    \ and auditability) into the AI project lifecycle may be called Accountability-by-Design:\n\
    \n\n\nAccountability deserves consideration across the entire design and implementation\
    \ workflow\n\nAs a best practice, you should actively consider the different demands\
    \ that accountability by design places on you before and after the roll out of\
    \ your AI project. We will refer to the process of ensuring accountability during\
    \ the design and development stages of your AI project as \u2018anticipatory accountability.\u2019\
    \ This is because you are anticipating your AI project\u2019s accountability needs\
    \ prior to it being completed. Following a similar logic, we will refer to the\
    \ process of addressing accountability after the start of the deployment of your\
    \ AI project as \u2018remedial accountability.\u2019 This is because after the\
    \ initial implementation of your system, you are remedying any of the issues that\
    \ may be raised by its effects and potential externalities. These two subtypes\
    \ of accountability are sometimes referred to as ex-ante (or before-the-event)\
    \ accountability and ex-post (after-the-event) accountability respectively.\n\n\
    \u2022\tAnticipatory Accountability: Treating accountability as an anticipatory\
    \ principle entails that you take as of primary importance the decisions made\
    \ and actions taken by your project delivery team prior to the outcome of an algorithmically\
    \ supported decision process.\n\nThis kind of ex ante accountability should be\
    \ prioritised over remedial accountability, which focuses instead on the corrective\
    \ or justificatory measures that can be taken after that automation supported\
    \ process had been completed.\n\nBy ensuring the AI project delivery processes\
    \ are accountable prior to the actual application of the system in the world,\
    \ you will bolster the soundness of design and implementation processes and thereby\
    \ more effectively pre-empt possible harms to individual wellbeing and public\
    \ welfare.\n\nLikewise, by establishing strong regimes of anticipatory accountability\
    \ and by making the design and delivery process as open and publicly accessible\
    \ as possible, you will put affected stakeholders in a position to make better\
    \ informed and more knowledgeable decisions about their involvement with these\
    \ systems in advance of potentially harmful impacts. In doing so, you will also\
    \ strengthen the public narrative and help to safeguard the project from reputational\
    \ harm.\n\n\u2022\tRemedial Accountability: While remedial accountability should\
    \ be seen, along these lines, as a necessary fallback rather than as a first resort\
    \ for imputing responsibility in the design and deployment of AI systems, strong\
    \ regimes of remedial accountability are no less important in\n \nproviding necessary\
    \ justifications for the bearing these systems have on the lives of affected stakeholders.\n\
    \nPutting in place comprehensive auditability regimes as part of your accountability\
    \ framework and establishing transparent design and use practices, which are methodically\
    \ logged throughout the AI project delivery lifecycle, are essential components\
    \ for this sort of remedial accountability.\n\nOne aspect of remedial accountability\
    \ that you must pay close attention to is the need to provide explanations to\
    \ affected stakeholders for algorithmically supported decisions. This aspect of\
    \ accountable and transparent design and use practices will be called explicability,\
    \ which literally means the ability to make explicit the meaning of the algorithmic\
    \ model\u2019s result.\n\nOffering explanations for the results of algorithmically\
    \ supported decision-making involves furnishing decision subjects and other interested\
    \ parties with an understandable account of the rationale behind the specific\
    \ outcome of interest. It also involves furnishing the decision subject and other\
    \ interested parties with an explanation of the ethical permissibility, the fairness,\
    \ and the safety of the use of the AI system. These tasks of content clarification\
    \ and practical justification will be explored in more detail below as part of\
    \ the section on transparency.\"\n(Leslie, 2019, p.22-23)\n"
  Linked Principles:
  - recmzjcGKv3yNOxbl
  - recOHnq45Fq7YWsRO
  - rec6tz9Phzck0hvT8
  - reclur1ImQFlLyYof
  Linked Sources:
  - recfYC5jjPmpLfSlM
  Tags:
  - reflection-discussion
  airtable_createdTime: '2023-05-19T11:35:49.000Z'
  airtable_id: rec0GhefNkhJqW0c2
  title: Principle of Accountability, key considerations
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CSocietal and environmental well-being Sustainable and environmentally\
    \ friendly AI:\n\uF0FC Did you establish mechanisms to measure the environmental\
    \ impact of the AI system\u2019s development, deployment and use (for example\
    \ the type of energy used by the data centres)? \n\uF0FC Did you ensure measures\
    \ to reduce the environmental impact of your AI system\u2019s life cycle?\u201D\
    \ (High-Level Expert Group on AI, 2019, p. 30)\n"
  Linked Challenges:
  - recvWM2glArsVhaye
  Linked Principles:
  - recmzjcGKv3yNOxbl
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:44:36.000Z'
  airtable_id: rec0WScLo6sUnoOQN
  title: Considerations in assessing trustworthy AI - Sustainable and environmentally
    friendly AI
- Description: "\"**Issue: **How can we increase agency by providing individuals access\
    \ to services allowing them to create a trusted identity to control the safe,\
    \ specific, and finite exchange of their data?\n## Recommendation\nIndividuals\
    \ should have access to trusted identity verification services to validate, prove,\
    \ and support the context-specific use of their identity.\_\n## Further Resources\n\
    \u2022\_\_\_\_\_Sovrin Foundation, [The Inevitable Rise of SelfSovereign Identity,](https://sovrin.org/wp-content/uploads/2017/06/The-Inevitable-Rise-of-Self-Sovereign-Identity.pdf)\
    \ Sept. 29, 2016.\n\u2022\_\_\_\_\_T. Ruff, \u201C[Three Models of Digital Identity\
    \ Relationships](https://medium.com/evernym/the-three-models-of-digital-identity-relationships-ca0727cb5186),\u201D\
    \ Evernym, Apr. 24, 2018.\n\u2022\_\_\_\_\_C. Pettey, [The Beginner\u2019s Guide\
    \ to Decentralized Identity.](https://www.gartner.com/smarterwithgartner/the-beginners-guide-to-decentralized-identity/)\
    \ Gartner, 2018.\n\u2022\_\_\_\_\_C. Allen, [The Path to Self-Sovereign Identity](https://github.com/ChristopherA/self-sovereign-identity/blob/master/ThePathToSelf-SovereignIdentity.md).\
    \ GitHub, 2017.\"\n\np.112-113 IEEE report\n"
  Linked Challenges:
  - reci0hfcAOIxLVNhB
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T09:39:00.000Z'
  airtable_id: rec1Ha94n5xbeqcOY
  title: Trust identity verification services to validate and protect identity
- Description: "## \"Recommendations\nEducation with respect to A/IS must be targeted\
    \ to three sets of students: the general public, present and future professionals\
    \ in A/IS, and present and future policy makers. To prepare the future workforce\
    \ to develop culturally appropriate A/IS, to work productively and ethically alongside\
    \ such technologies, and to advance the UN SDGs, the curricula in HIC and LMIC\
    \ universities and professional schools require innovation. Equally importantly,\
    \ preuniversity education systems, starting with early childhood education, need\
    \ to be reformed to prepare society for the risks and opportunities of the A/IS\
    \ age, rather than the current system which prepares society for work in an industrial\
    \ age that ended with the 20th century. Specific recommendations include:\n\u2022\
    \_\_\_\_\_Preparing future managers, lawyers, engineers, civil servants, and entrepreneurs\
    \ to work productively and ethically as global citizens alongside A/IS, through\
    \ reform of undergraduate and graduate curricula as well as of preschool, primary,\
    \ and secondary school curricula. This will require:\n\u2022\_\_\_\_\_Fomenting\
    \ interaction between universities and other actors such as companies, governments,\
    \ NGOs, etc., with respect to A/IS research through definition of research priorities\
    \ and joint projects, subcontracts to universities, participation in observatories,\
    \ and co-creation of curricula, cooperative teaching, internships/service learning,\
    \ and conferences/seminars/courses.\n\u2022\_\_\_\_\_Establishing and supporting\
    \ more multidisciplinary degrees that include\_A/IS, and adapting university curricula\
    \ to provide a broad, integrated perspective which allows students to understand\
    \ the impact of A/IS in the global, economic, environmental, and sociocultural\
    \ domains and trains them as future policy makers in A/IS fields.\n\u2022\_\_\_\
    \_\_Integrating the teaching of ethics and\_A/IS across the education spectrum,\
    \ from preschool to postgraduate curricula, instead of relegating ethics to a\
    \ standalone module with little direct practical application.\n\u2022\_\_\_\_\_\
    Promoting service learning opportunities that allow A/IS undergraduate and graduate\
    \ students to apply their knowledge to meet the needs of a community.\n\u2022\_\
    \_\_\_\_Creating international exchange programs, through both private and public\
    \ institutions, which expose students to different cultural contexts for A/IS\
    \ applications in both HIC and LMIC.\n\u2022\_\_\_\_\_Creating experimental curricula\
    \ to prepare people for information-based work in the 21st century, from preschool\
    \ through postgraduate education.\n\u2022\_\_\_\_\_Taking into account transversal\
    \ competencies students need to acquire to become ethical global citizens, i.e.,\
    \ critical thinking, empathy, sociocultural awareness, flexibility, and deontological\
    \ reasoning\_in the planning and assessment of\_A/IS curricula.\n\u2022\_\_\_\_\
    \_Training teachers in teaching methodologies suited to addressing challenges\
    \ imposed in the age of A/IS.\n\u2022\_\_\_\_\_Stimulating STEAM courses in preuniversity\
    \ education.\n\u2022\_\_\_\_\_Encouraging high-quality HIC-LMIC collaborative\
    \ A/IS research in both private and public universities.\n\u2022\_\_\_\_\_Conducting\
    \ research to support innovation in education and business for the A/IS world,\
    \ which could include:\n\u2022\_\_\_\_\_Researching the impact of A/IS on the\
    \ governance and macro/micro strategies of companies and organizations, together\
    \ with those companies, in an interdisciplinary manner which harnesses expertise\
    \ of both social scientists and technology experts.\n\u2022\_\_\_\_\_Researching\
    \ the impact of A/IS on the business model for the development of new products\
    \ and services through the collaborative efforts of management, operations, and\
    \ the technical research and development function.\n\u2022\_\_\_\_\_Researching\
    \ how empathy can be taught and integrated into curricula, starting at the preschool\
    \ level.\n\u2022\_\_\_\_\_Researching how schools and education systems in low-income\
    \ settings of both HIC and LMIC can leverage their lessentrenched interests to\
    \ leapfrog into a 21st century-ready education system.\n\u2022\_\_\_\_\_Establishing\
    \ ethics observatories in universities with the purpose of fostering an informed\
    \ public opinion capable\_of participating in policy decisions\_regarding the\
    \ ethics and social impact\_of A/IS applications.\n\u2022\_\_\_\_\_Creating professional\
    \ continuing education and employment opportunities in A/IS for current professionals,\
    \ including through online and executive education courses.\n\u2022\_\_\_\_\_\
    Creating educative mass media campaigns to elevate society\u2019s ongoing baseline\
    \ level of understanding of A/IS systems, including what it is, if and how it\
    \ can be trusted in various contexts, and what are its limitations.\n## Further\
    \ resources\n\u2022\_\_\_\_\_ABET Computing and Engineering Accreditation Criteria\
    \ 2018. Available at: [http://www.abet.org/accreditation/ accreditation-criteria/](http://www.abet.org/accreditation/accreditation-criteria/)\n\
    \u2022\_\_\_\_\_ABET, 2017 ABET Impact Report, Working Together for a Sustainable\
    \ Future_, _2017.\n\u2022\_\_\_\_emlyon business school, Artificial Intelligence\
    \ in Management (AIM) Institute [http://aim. em-lyon.com](http://aim.em-lyon.com/)\n\
    UNESCO,_ The UN Decade of Education for Sustainable Development, Shaping the Education\
    \ of Tomorrow_. UNESCO 2012.\"\np.153-156\n"
  Linked Challenges:
  - recJfAMUB8HdjVQYD
  Linked Sources:
  - recpXl48pJdKDhc6f
  Tags:
  - education
  airtable_createdTime: '2023-06-05T12:00:13.000Z'
  airtable_id: rec2679E9TXh1DCL8
  title: Educate the learners in formal education, the public, professionals, and
    policy makers for designing, and working alongside, AI to advance the SDGs.
- Description: "\u201C\u2022 Is the system accessible by everyone in the same way\
    \ without any barriers? \u2022 Does the system provide appropriate interaction\
    \ modes for learners with disabilities or special education needs? Is the AI system\
    \ designed to treat learners respectfully adapting to their individual needs?\
    \ \u2022 Is the user interface appropriate and accessible for the age level of\
    \ the learners? Has the usability and user-experience been tested for the target\
    \ age group? \u2022 Are there procedures in place to ensure that AI use will not\
    \ lead to discrimination or unfair behaviour for all users? \u2022 Does the AI\
    \ system documentation or its training process provide insight into potential\
    \ bias in the data? \u2022 Are procedures in place to detect and deal with bias\
    \ or perceived inequalities that may arise?\u201D (European Commission, 2022,\
    \ p. 20)\n"
  Linked Cases:
  - reciNqxyfUgE5XM7t
  Linked Principles:
  - reczVPIH1y2OMpAJH
  Linked Sources:
  - rec9jnxuHOioQn4DC
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T13:36:43.000Z'
  airtable_id: rec35PeHdUmtalypk
  title: Guiding questions for educators regarding Diversity, non-Discrimination,
    and Fairness of AI in Education
- Description: "\u201CThe first step is to create an understanding of the AI, its\
    \ operation, and social impact based on relevant contextual factors. The researcher\
    \ must scrutinize the social, political, and economic context within which a technology\
    \ operates, as much as the technology itself. This is particularly pertinent when\
    \ dealing with AI as the learning models may show patterns that are unexpected.\
    \ AI can also amplify already existing social hierarchies. The influence of stakeholders\
    \ (e.g. developers in different communities and organizations inside and outside\
    \ academia) may be strong when for instance cleaning data, using pretrained models\
    \ or changing the models. \n- How would you characterize the social context within\
    \ which or about which the research is conducted? \n- Who are the data subjects\
    \ and affected stakeholders involved in this project (directly or indirectly)?\
    \ \n- How would the researcher characterize the norms (e.g. privacy, social hierarchy)\
    \ and sensitivities in this social context?\u201D \n- How have these norms and\
    \ sensitivities influenced the application of AI in gathering data for this research\
    \ project? \n- How will the implementation of the AI system or use of the model\
    \ affect the norms and sensitivities?\n(franzke et al., 2020, p. 36-37)\n\n"
  Linked Principles:
  - recU6u0AZbcNj1ik9
  Linked Sources:
  - rec6r8OkE2Q2EdiM3
  Tags:
  - reflection-questions
  - project-inception
  airtable_createdTime: '2023-05-19T12:17:03.000Z'
  airtable_id: rec3q0xKZABgZf9Dg
  title: Questions to consider in key stages of AI and machine learning based research,
    regarding sociotechnical context
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION)\n \u201CTechnical robustness and safety Resilience to attack and security:\
    \ \n\uF0FC Did you assess potential forms of attacks to which the AI \nsystem\
    \ could be vulnerable? \n\uF0A7 Did you consider different types and natures of\
    \ vulnerabilities, such as data pollution, physical infrastructure, cyber-attacks?\
    \ \n\uF0FC Did you put measures or systems in place to ensure the integrity and\
    \ resilience of the AI system against potential attacks? \n\uF0FC Did you verify\
    \ how your system behaves in unexpected situations and environments? \n\uF0FC\
    \ Did you consider to what degree your system could be dual-use? If so, did you\
    \ take suitable preventative measures against this case (including for instance\
    \ not publishing the research or deploying the system)?\u201D (High-Level Expert\
    \ Group on AI, 2019, p. 27)\n"
  Linked Principles:
  - rec42P8U9usfYCtv9
  - reclPiw2VvNOSTzv5
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:19:25.000Z'
  airtable_id: rec4oGONFgYMu3JGf
  title: Considerations in assessing trustworthy AI - Resilience to attack and security
- Description: "Methods:\n- What methods will you use in your investigation? \n- What\
    \ type of data will you gather? Will this include data that goes beyond normal\
    \ classroom activities and assessments? How much class time will additional data\
    \ collection activities take? \n- How can your investigation be made educationally\
    \ valuable for students? Might students be involved, for instance, by gathering\
    \ and analyzing data?\n-  Will your data collection choices (e.g., video recording,\
    \ use of personal writing, use of data from whole-class discussion) affect your\
    \ ability to protect students\u2019 privacy?\n(Fedoruk, 2017, p. 2)\n"
  Linked Principles:
  - recMGB4iC5oaCtr5x
  - rec42P8U9usfYCtv9
  - rec6O9e1nYBJtQUTj
  Linked Sources:
  - recQzldmBLByP78Uu
  Tags:
  - reflection-questions
  - education-research
  airtable_createdTime: '2023-05-19T12:58:52.000Z'
  airtable_id: rec55h8FhGKyGPNgw
  title: Questions to consider in SoTL research method selection
- Description: "## \"Recommendations\nAs this technology develops, it is important\
    \ to monitor research into the development of intimate relationships between A/IS\
    \ and humans. Research should emphasize any technical and normative developments\
    \ that reflect use of\_A/IS in positive and therapeutic ways while also creating\
    \ appropriate safeguards to mitigate against uses that contribute to problematic\
    \ individual or social relationships:\n1\\.\_\_\_Intimate systems must not be\
    \ designed or deployed in ways that contribute to stereotypes, gender or racial\
    \ inequality,\_or the exacerbation of human misery.\n2\\.\_\_\_Intimate systems\
    \ must not be designed to explicitly engage in the psychological manipulation\
    \ of the users of these systems unless the user is made aware they are being manipulated\
    \ and consents to this behavior. Any manipulation should be governed\_through\
    \ an opt-in system.\n3\\.\_\_\_Caring A/IS should be designed to avoid contributing\
    \ to user isolation from society.\_\n4\\.\_\_\_Designers of affective robotics\
    \ must publicly acknowledge, for example, within a notice associated with the\
    \ product, that these systems can have side effects, such as interfering with\
    \ the relationship dynamics between human partners, causing attachments between\
    \ the user and the A/IS that are distinct from human partnership.\n5\\.\_\_Commercially\
    \ marketed A/IS for caring applications should not be presented to be a person\
    \ in a legal sense, nor marketed as a person. Rather its artifactual, that is,\
    \ authored, designed, and built deliberately, nature should always be made as\
    \ transparent as possible, at least at point of sale and in available documentation,\
    \ as noted in Section 4, Systems Supporting Human Potential.6.\_\_\_Existing laws\
    \ regarding personal imagery need to be reconsidered in light of caring A/IS.\_\
    In addition to other ethical considerations, it will also be necessary to establish\
    \ conformance with local laws and mores in the context of caring A/IS systems.\n\
    ## Further Resources\n\u2022\_\_\_\_\_M. Boden, J. Bryson, D. Caldwell, K. Dautenhahn,\
    \ L. Edwards, S. Kember, P.\nNewman, V. Parry, G. Pegman, T. Rodden and T. Sorrell,\
    \ Principles of robotics: regulating robots in the real world. Connection Science,\
    \ vol. 29, no. 2, pp. 124-129, April 2017.\n\u2022\_\_\_\_\_J. J. Bryson, M. E.\
    \ Diamantis, and T. D. Grant, \u201COf, For, and By the People: The Legal Lacuna\
    \ of Synthetic Persons.\u201D _Artificial Intelligence & Law_, vol. 25, no. 3,\
    \ pp. 273\u2013291, Sept. 2017.\n\u2022\_\_\_\_\_M. Scheutz, \u201CThe Inherent\
    \ Dangers of\nUnidirectional Emotional Bonds between Humans and Social Robots,\u201D\
    \ in _Robot Ethics: The Ethical and Social Implications of Robotics,_ P. Lin,\
    \ K. Abney, and G. Bekey, Eds., pp. 205. Cambridge, MA: MIT Press, 2011.\n"
  Linked Challenges:
  - rec2ULcHUjSbbSnHL
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T11:35:18.000Z'
  airtable_id: rec5CFheImo8onTcd
  title: Monitor impact of affective AI in care on human-human relationships and implement
    safeguards
- Description: "## Recommendations\nTo ensure representation of stakeholders, organizations\
    \ should enact a planned and controlled set of activities to account for the interests\
    \ of the full range of stakeholders or practitioners who will be working alongside\_\
    A/IS and incorporating their insights to build upon, rather than circumvent or\
    \ ignore, the\_social and practical wisdom of involved practitioners and other\
    \ stakeholders.\n## Further Resources\n\u2022\_\_\_\_\_C. Schroeter, et al., \u201C\
    [Realization and User Evaluation of a Companion Robot for People with Mild Cognitive\
    \ Impairments](http://www.tu-ilmenau.de/fileadmin/media/neurob/publications/conferences_int/2013/Schroeter-ICRA-2013-fin.pdf),\u201D\
    \ _Proceedings of IEEE International Conference on Robotics and Automation (ICRA\
    \ 2013)_, Karlsruhe, Germany 2013. pp. 1145\u20131151.\n\u2022\_\_\_\_\_T. L.\
    \ [Chen, et al. ](http://ieeexplore.ieee.org/abstract/document/6476704/)\u201C\
    [Robots for Humanity: Using Assistive Robotics to Empower People with Disabilities](http://ieeexplore.ieee.org/document/6476704/),\u201D\
    \ _IEEE Robotics and Automation Magazine, _vol. 20, no. 1, pp. 30\u201339, 2013.\n\
    R. Hartson, and P. S. Pyla. _The UX Book: Process and Guidelines for Ensuring\
    \ a Quality User Experience_. Waltham, MA: Elsevier, 2012\"\n\np.130-131\n"
  airtable_createdTime: '2023-06-05T10:40:35.000Z'
  airtable_id: rec7RfHxpQolMR5v9
- Description: "\u201C\u2022 How does the AI system affect the social and emotional\
    \ wellbeing of learners and teachers? \u2022 Does the AI system clearly signal\
    \ that its social interaction is simulated and that it has no capacities of feeling\
    \ or empathy? \u2022 Are students or their parents involved in the decision to\
    \ use the AI system and support it? \u2022 Is data used to support teachers and\
    \ school leaders to evaluate student wellbeing and if so, how is this being monitored?\
    \ \u2022 Does use of the system create any harm or fear for individuals or for\
    \ society?\u201D (European Commission, 2022, p. 20)\n"
  Linked Cases:
  - recrVkbG0XGe2Ca0v
  - recAlOHJhEy5nDwA6
  Linked Principles:
  - recmzjcGKv3yNOxbl
  Linked Sources:
  - rec9jnxuHOioQn4DC
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T13:36:44.000Z'
  airtable_id: rec7daqDHSCuc70yS
  title: Guiding questions for educators regarding Societal and Environmental Wellbeing
    of AI in Education
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CPrivacy and data governance Respect for privacy and data Protection:\
    \ \n\u201CAccess to data:\n\uF0FC What protocols, processes and procedures did\
    \ you follow to manage and ensure proper data governance? \n\uF0A7 Did you assess\
    \ who can access users\u2019 data, and under what circumstances? \n\uF0A7 Did\
    \ you ensure that these persons are qualified and required to access the data,\
    \ and that they have the necessary competences to understand the details of data\
    \ protection policy? \n\uF0A7 Did you ensure an oversight mechanism to log when,\
    \ where, how, by whom and for what purpose data was accessed?\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 28)\n"
  Linked Principles:
  - recPg7Ov0priGGtLm
  - recOHnq45Fq7YWsRO
  - reczVPIH1y2OMpAJH
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:32:51.000Z'
  airtable_id: rec7m69DQyCw3rlFg
  title: Considerations in assessing trustworthy AI - Access to data
- Description: "The scientific method requires a high degree of transparency and explainability\
    \ of how research findings were derived. This conflicts to some extent with the\
    \ complex nature of AI technologies (Ananny & Crawford, 2018; Calo, 2017; Danaher,\
    \ 2016; Wachter, Mittelstadt & Floridi, 2017; Weller, 2017). Indeed, it can be\
    \ too complex for a researcher to precisely state how research data was created\
    \ given the way a neural network with potentially hidden layers operates. However,\
    \ the concepts of transparency and explainability do not mean strictly understanding\
    \ the highly specialized technical code and data of the trained neural networks.\
    \ For example, a richer notion of transparency asks researchers to explain the\
    \ ends, means, and thought processes that went into developing the code, the model,\
    \ and how the resulting research data was shaped. Similarly, explainability does\
    \ not need to be exact, but can learn from interpretations in philosophy, cognitive\
    \ psychology or cognitive science, and social psychology (Miller, 2017). \n\_\n\
    \u25CF\_\_\_\_\_Can the researcher give an account of how the model operates and\
    \ how research data was generated? \n\u25CF\_\_\_\_\_Has the researcher explained\
    \ how the model works to an institutional ethics board, and have they understood\
    \ the reasons and methods of data processing? \n\u25CF\_\_\_\_\_What roles have\
    \ the developers and researchers played and what choices have they made in constructing\
    \ the model, choosing and cleaning the training data and how has this affected\
    \ the results and prediction? \n\u25CF\_\_\_\_\_What kind of negotiations have\
    \ taken place in the decision-making around model selection, adjustments and data\
    \ modelling in the research process that can affect the result and prediction?\
    \ \n(franzke, 2020, p.45)\n\n### IEEE Recommendation\n\"When systems are built\
    \ that could impact the safety or well-being of humans, it is not enough to just\
    \ presume that a system works. Engineers must acknowledge and assess the ethical\
    \ risks involved with black box software and implement mitigation strategies.\n\
    Technologists should be able to characterize what their algorithms or systems\
    \ are going to do via documentation, audits, and transparent and traceable standards.\
    \ To the degree possible, these characterizations should be predictive, but given\
    \ the nature of A/IS, they might need to be more retrospective and mitigation-oriented.\
    \ As such, it is also important to ensure access to remedy adverse impacts.\n\
    Technologists and corporations must do their ethical due diligence before deploying\
    \ A/IS technology. Standards for what constitutes ethical due diligence would\
    \ ideally be generated by an international body such as IEEE or ISO, and barring\
    \ that, each corporation should work to generate a set of ethical standards by\
    \ which their processes are evaluated and modified. Similar to a flight data recorder\
    \ in the field of aviation, algorithmic traceability can provide insights on what\
    \ computations led to questionable or dangerous behaviors. Even where such processes\
    \ remain somewhat opaque, technologists should seek indirect means of validating\
    \ results and detecting harms.\n## Further resources\n\_\u2022\_\_\_\_\_M. Ananny\
    \ and K. Crawford, \u201C[Seeing without Knowing: Limitations of the Transparency\
    \ Ideal and Its Application to Algorithmic Accountability](http://journals.sagepub.com/doi/abs/10.1177/1461444816676645),\u201D\
    \ _New Media & Society_, vol. 20, no. 3, pp. 973-989, Dec. 13, 2016.\n\u2022\_\
    \_\_\_\_D. Reisman, J. Schultz, K. Crawford, and M. Whittaker, \u201CAlgorithmic\
    \ Impact Assessments: A Practical Framework for Public Agency Accountability,\u201D\
    \ AI NOW 2018. [Online]. Available: [https://ainowinstitute.org/ aiareport2018.pdf](https://ainowinstitute.org/aiareport2018.pdf).\_\
    [Accessed October 28, 2018].\n\u2022\_\_\_\_\_J. A. Kroll \u201C[The Fallacy of\
    \ Inscrutability](http://rsta.royalsocietypublishing.org/content/376/2133/20180084).\u201D\
    \ _Philosophical Transactions of the Royal Society A: Mathematical, Physical and\
    \ Engineering Sciences_, C. Cath, S. Wachter, B. Mittelstadt and L. Floridi, Eds.,\
    \ October 15, 2018 DOI: 10.1098/rsta.2018.0084.\"\np.139\n\nAnd recommendation\
    \ regarding documentation:\n\"Engineers should be required to thoroughly document\
    \ the end product and related data flows, performance, limitations, and risks\
    \ of\_A/IS. Behaviors and practices that have been prominent in the engineering\
    \ processes should also be explicitly presented, as well as empirical evidence\
    \ of compliance and methodology used, such as training data used in predictive\
    \ systems, algorithms and components used, and results of behavior monitoring.\
    \ Criteria for such documentation could be: auditability, accessibility, meaningfulness,\
    \ and readability.\nCompanies should make their systems auditable and should explore\
    \ novel methods for external and internal auditing.\_\n## Further reading\n\u2022\
    \_\_\_\_\_S. Wachter, B. Mittelstadt, and L. Floridi. \u201C[Transparent, Explainable,\
    \ and Accountable AI for Robotics](http://robotics.sciencemag.org/content/2/6/eaan6080).\u201D\
    \ _[Science Robotics, v](http://robotics.sciencemag.org/content/2/6/eaan6080)ol.\
    \ _2, no. 6, May 31, 2017. [Online]. Available: DOI: 10.1126/scirobotics.aan6080.\
    \ [Accessed Nov.\n\u2022\_\_\_\_\_S. Barocas, and A. D. Selbst, \u201C[Big Data\u2019\
    s Disparate Impact.](http://www.californialawreview.org/wp-content/uploads/2016/06/2Barocas-Selbst.pdf)\u201D\
    \ _California Law Review_ 104, 671-732, 2016.\n\u2022\_\_\_\_\_J. A. Kroll, J.\
    \ Huey, S. Barocas, E. W. Felten, J. R. Reidenberg, D. G. Robinson, and H. Yu.\
    \ \u201C[Accountable Algorithms.](https://www.pennlawreview.com/print/165-U-Pa-L-Rev-633.pdf)\u201D\
    \ _University of Pennsylvania Law Review _165, no. 1, 633\u2013 705, 2017.\n\u2022\
    \_\_\_\_\_J. M. Balkin, \u201C[Free Speech in the Algorithmic Society: Big Data,\
    \ Private Governance, and New School Speech Regulation.](https://ssrn.com/abstract%3D3038939)\u201D\
    \ _UC Davis Law Review_, 2017.\"\n\np.135\n\n## High-level expert group\n\u201C\
    Compliance with this assessment list is not evidence of legal compliance, nor\
    \ is it intended as guidance to ensure compliance with applicable law. Given the\
    \ application-specificity of AI systems, the assessment list will need to be tailored\
    \ to the specific use case and context in which the system operates. In addition,\
    \ this chapter offers a general recommendation on how to implement the assessment\
    \ list for Trustworthy AI though a governance structure embracing both operational\
    \ and management level.\u201D (High-Level Expert Group on AI, 2019, p. 24)\n\n\
    \u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION)\n\u201CTransparency\n\u201C\
    Explainability:\n\uF0FC Did you assess:\n\uF0A7 to what extent the decisions and\
    \ hence the outcome made by the AI system can be understood?\n\uF0A7 to what degree\
    \ the system\u2019s decision influences the organisation\u2019s decision-making\
    \ processes?\n\uF0A7 why this particular system was deployed in this specific\
    \ area?\n\uF0A7 what the system\u2019s business model is (for example, how does\
    \ it create value for the organisation)?\n\uF0FC Did you ensure an explanation\
    \ as to why the system took a certain choice resulting in a certain outcome that\
    \ all users can understand?\n\uF0FC Did you design the AI system with interpretability\
    \ in mind from the start?\n\uF0A7 Did you research and try to use the simplest\
    \ and most interpretable model possible for the application in question?\n\uF0A7\
    \ Did you assess whether you can analyse your training and testing data? Can you\
    \ change and update this over time?\n\uF0A7 Did you assess whether you can examine\
    \ interpretability after the model\u2019s training and development, or whether\
    \ you have access to the internal workflow of the model?\u201D (High-Level Expert\
    \ Group on AI, 2019, p. 29)\n"
  Linked Cases:
  - recmS3zSMbR3ofAR5
  Linked Challenges:
  - recdmBNNa98cN8Sda
  - rececsX8igwNqhhkC
  - rec9Cuz3HWc9lWW23
  - recOpn4I30te1qiKl
  Linked Principles:
  - recOHnq45Fq7YWsRO
  - recQEiU22Qy1E0YuA
  - recxcFmvPG5wrCqpO
  Linked Sources:
  - rec6r8OkE2Q2EdiM3
  - recpXl48pJdKDhc6f
  - recnCULdYQ36cpZR7
  Tags:
  - reflection-questions
  - project-dissemination
  - governance-question
  airtable_createdTime: '2023-05-19T12:42:14.000Z'
  airtable_id: rec81gtnlFS5W2BBF
  title: Questions to consider in key stages of AI and machine learning based research,
    regarding transparency and explainability
- Description: "\"[Questions to] help researchers in identifying the ethical issues\
    \ raised in the cooperation between academy and corporations or while working\
    \ on corporate data:\_\n\u2022\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_Who are the stakeholders\
    \ involved? \n\u2022\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_Who is the subject that is providing\
    \ financial support?\_\n\u2022\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_What are the goals\
    \ of the research? Are the researchers answering only to the financer\u2019s needs\
    \ and/or do they have also their own research questions to pursue? \n\u2022\_\_\
    \_\_\_\_\_\_\_\_\_\_\_\_\_Which is the relationship between the funder(s) and\
    \ the researchers? And between the different stakeholders involved? Is it a relationship\
    \ on equal terms or are there some power imbalances? \n\u2022\_\_\_\_\_\_\_\_\_\
    \_\_\_\_\_\_Who decides the methodological approach(es)? \n\u2022\_\_\_\_\_\_\_\
    \_\_\_\_\_\_\_\_What is / are the methodological approach(es)? \n\u2022\_\_\_\_\
    \_\_\_\_\_\_\_\_\_\_\_Who is the owner of the data obtained during the research\
    \ process and where do the data come from (i.e. originally produced/retrieved\
    \ for the research and/or already extant corporate data provided by the funder)?\
    \ \n\u2022\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_What are the forms of dissemination of\
    \ the research (i.e. academic, corporate, educational)?\_\n\u2022\_\_\_\_\_\_\_\
    \_\_\_\_\_\_\_\_Are there risks for the researchers or for the academic institution\
    \ in carrying on such research (i.e. reputation, data management, ethics)?\_\n\
    \u2022\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_Do the corporate data involve any human subject\
    \ data? Where are the data stored? Which are the terms of use of the platform\
    \ studied? \n\n(franzke, 2020, p.59-60)\n"
  Linked Principles:
  - recOHnq45Fq7YWsRO
  Linked Sources:
  - rec6r8OkE2Q2EdiM3
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T12:49:28.000Z'
  airtable_id: rec8cNT8sPSFtMSAc
  title: Questions to consider in key stages of AI and machine learning based research,
    regarding corporate-academic research interaction
- Description: "Models can be used in a variety of ways, or they may influence others\
    \ to create similar models for other ends. Research ethics frameworks, however,\
    \ typically require the review process to limit itself to the immediate impact\
    \ on research stakeholders and not necessarily assess the potential long-term\
    \ impacts of the research outputs (Zevenbergen, Brown, Wright, & Erdos, 2013).\_\
    \n\_\_\_\_\_\_\_\_\_\_\_This may be problematic for omni-use technologies such\
    \ as AI models. Innovations in AI technologies and their inferences on social\
    \ and human dynamics may be used for a multitude of purposes for instance tailoring\
    \ microtargeted communication and thus potentially undermining democracy (e.g.\
    \ issues of fair election & voting discrimination). Models or datasets that were\
    \ designed to produce positive social ends can be used towards malicious and destructive\
    \ ends (e.g. facial recognition to clamp down on political gatherings/dissent).\
    \ Even if the research aims are beneficial for a wide group of stakeholders, research\
    \ methods and models may need to be published along with the research outcomes\
    \ and thus set a standard or precedent and initiate function creep and unintended\
    \ consequences.\_\n\_\_\_\_\_\_\_\_\_\_\_Once an AI system has left the hands\
    \ of the original researchers, they may not have any control over how their models\
    \ are used by others. The same is true for the generated research data: once it\
    \ has been freely published, it will be difficult to contain its further uses.\_\
    \_\_\_\_\_Legal and constitutional checks and balances on the exertion of power\
    \ or the use of data may differ around the world (Knapp & VandeCreek, 2007). While\
    \ it is beyond the scope of an ethics review to assess the political governance\
    \ in countries around the world, it is useful for researchers to be mindful that\
    \ their data and models may contain personal and sensitive data that could be\
    \ used directly or indirectly against individuals in other countries or political\
    \ systems.\_\n\_\_\_\_\_\_\_\_\_\_\_Researchers should thus engage actively with\
    \ the reality that their methods and models may be misused by others, and find\
    \ ways to mitigate risks and harms. It is ultimately the responsibility of researchers\
    \ \u2013 in dialogue with ethics boards and other stakeholders in a specific project\
    \ \u2013 to agree on the limitations based on a thorough understanding of the\
    \ project weighed heavily against the knowledge production it produces. \n\_\n\
    \u25CF\_\_\_\_\_\_What could be the downstream consequence for data subjects for\
    \ erroneous identifications, labelling, or categorization?\_\n\u25CF\_\_\_\_\_\
    \_To what extent is the researcher sensitive to the local norms, values, and legal\
    \ mechanisms that could adversely affect the subjects of their research? \n\u25CF\
    \_\_\_\_\_\_To what extent can the researcher foresee how the data created through\
    \ research project inferences may be used in further, third-party systems that\
    \ make decisions about people?\_\n\u25CF\_\_\_\_\_\_Is it foreseeable that the\
    \ methodologies, actions, and resulting knowledge may be used for malicious ends\
    \ in another context than research and to what extent can this be mitigated?\_\
    \n\u25CF\_\_\_\_\_\_Which actors will likely be interested to use the methodology\
    \ for malevolent purposes, and how?\_\n\u25CF\_\_\_\_\_\_Can the inferred data\
    \ be directly useful to authoritarian governments who may target or justify crackdowns\
    \ on minorities or special interest groups based on (potentially erroneously inferred)\
    \ collected data? Can this be mitigated without destroying the findings for the\
    \ specific research project? \n\u25CF\_\_\_\_\_\_Is it possible to contain the\
    \ potential malevolent future uses by design? \n\u25CF\_\_\_\_\_\_Up to which\
    \ hypothetical data reuse moment is it appropriate to hold the researcher responsible?\
    \ \n\n(franazke, 2020, p45-46)\n"
  Linked Principles:
  - recOHnq45Fq7YWsRO
  - recmzjcGKv3yNOxbl
  Linked Sources:
  - rec6r8OkE2Q2EdiM3
  Tags:
  - reflection-questions
  - project-dissemination
  - internet-research
  airtable_createdTime: '2023-05-19T12:42:55.000Z'
  airtable_id: rec8dLtyDCwvjMWge
  title: Questions to consider in key stages of AI and machine learning based research,
    regarding downstream responsibility
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \nTechnical robustness and safety\n\u201CReliability and reproducibility\n\
    \u201C\uF0FC Did you put in place a strategy to monitor and test if the AI system\
    \ is meeting the goals, purposes and intended applications? \n\uF0A7 Did you test\
    \ whether specific contexts or particular conditions need to be taken into account\
    \ to ensure reproducibility? \n\uF0A7 Did you put in place verification methods\
    \ to measure and ensure different aspects of the system's reliability and reproducibility?\
    \ \n\uF0A7 Did you put in place processes to describe when an AI system fails\
    \ in certain types of settings? \n\uF0A7 Did you clearly document and operationalise\
    \ these processes for the testing and verification of the reliability of AI systems?\
    \ \n\uF0A7 Did you establish mechanisms of communication to assure (end-)users\
    \ of the system\u2019s reliability?\u201D (High-Level Expert Group on AI, 2019,\
    \ p. 28)\n"
  Linked Principles:
  - rec42P8U9usfYCtv9
  - reclPiw2VvNOSTzv5
  - recOHnq45Fq7YWsRO
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:29:14.000Z'
  airtable_id: rec95I69E1YcGg0Sa
  title: Considerations in assessing trustworthy AI - Reliability and reproducibility
- Description: "## \"Recommendations\nEmployees should be empowered and encouraged\
    \ to raise ethical concerns in\_day-to-day professional practice.\nTo be effective\
    \ in ensuring adoption of ethical considerations during product development or\
    \ internal implementation of A/IS, organizations should create a company culture\
    \ and set of norms that encourage incorporating ethical considerations in the\
    \ design and implementation processes.\nNew categories of considerations around\
    \ these issues need to be accommodated, along with updated Codes of Conduct, company\
    \ value-statements, and other management principles so individuals are empowered\
    \ to share their insights and concerns in an atmosphere of trust. Additionally,\
    \ bottom-up approaches like company \u201Ctown hall meetings\u201D should be explored\
    \ that reward, rather than punish, those who bring up ethical concerns.\n## Further\
    \ Resources\n\u2022\_\_\_\_\_[The British Computer Society (BCS),](http://www.bcs.org/category/6030)\
    \ Code\_of Conduct, 2019.\n\u2022\_\_\_\_\_C. Cath, and L. Floridi, \u201C[The\
    \ Design of the Internet\u2019s Architecture by the Internet Engineering Task\
    \ Force (IETF) and Human Rights](http://link.springer.com/article/10.1007%2Fs11948-016-9793-y),\u201D\
    \ _Science and Engineering Ethics, _vol._ _23, no. 2, pp. 449\u2013468, Apr. 2017.\"\
    \n\np.129\n"
  Linked Challenges:
  - recZ43i8Cnhohwb2x
  - recOpn4I30te1qiKl
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T10:31:54.000Z'
  airtable_id: rec9SSyrfFkmuJkCe
  title: Company culture, values, codes, and power structures should enable speaking
    to ethical concerns
- Description: "## \"Recommendations\n1\\.\_\_\_Systematic analyses are needed that\
    \ examine the ethics and behavioral consequences of designing affective systems\
    \ to nudge human beings prior to deployment.\n2\\.\_\_\_The user should be empowered,\
    \ through an explicit opt-in system and readily available, comprehensible information,\
    \ to recognize different types of A/IS nudges, regardless of whether they seek\
    \ to promote beneficial social manipulation or to enhance consumer acceptance\
    \ of commercial goals. The user should be able to access and check facts behind\
    \ the nudges and then make a conscious decision to accept or reject a nudge. Nudging\
    \ systems must be transparent, with a clear chain of accountability that includes\
    \ human agents: data logging is required so users can know how, why, and by whom\
    \ they were nudged.\n3\\.\_\_\_A/IS nudging must not become coercive and should\
    \ always have an opt-in system policy with explicit consent.\_\n4\\.\_\_\_Additional\
    \ protections against unwanted nudging must be put in place for vulnerable populations,\
    \ such as children, or when informed consent cannot be obtained. Protections against\
    \ unwanted nudging should be encouraged when nudges alter long-term behavior or\
    \ when consent alone may not be\_a sufficient safeguard against coercion\_or exploitation.\n\
    5\\.\_\_\_Data gathered which could reveal an individual or groups\u2019 susceptibility\
    \ to a nudge or their emotional reaction to a nudge should not be collected or\
    \ distributed without opt-in consent, and should only be retained transparently,\
    \ with access restrictions in compliance with the highest requirements of data\
    \ privacy and law.\n## Further Resources\n\u2022\_\_\_\_\_R. Thaler, and C. R.\
    \ Sunstein, _Nudge: Improving Decision about Health, Wealth and Happiness_, New\
    \ Haven, CT: Yale University Press, 2008.\n\u2022\_\_\_\_\_L. Bovens, \u201CThe\
    \ Ethics of Nudge,\u201D in _Preference change: Approaches from Philosophy, Economics\
    \ and Psychology_, T. Gr\xFCne-Yanoff and S. O. Hansson, Eds., Berlin, Germany:\
    \ Springer, 2008 pp. 207\u2013219.\n\u2022\_\_\_\_\_S. D. Hunt and S. Vitell.\
    \ \"A General Theory of Marketing Ethics.\" Journal of Macromarketing, vol.6,\
    \ no. 1, pp. 5-16, June 1986.\n\u2022\_\_\_\_\_A. McStay, [Empathic Media and\
    \ Advertising: Industry, Policy, Legal and Citizen Perspectives (the Case for\
    \ Intimacy)](http://journals.sagepub.com/doi/pdf/10.1177/2053951716666868), Big\
    \ Data & Society, pp. 1-11, December 2016.\n\u2022\_\_\_\_\_J. de Quintana Medina\
    \ and P. Hermida Justo, \u201CNot All Nudges Are Automatic: Freedom of Choice\
    \ and Informative Nudges.\u201D Working paper presented to the European Consortium\
    \ for Political Research, Joint Session of Workshops, 2016 Behavioral Change and\
    \ Public Policy, Pisa, Italy, 2016.\n\u2022\_\_\_\_\_M. D. White, _[The Manipulation\
    \ of Choice. Ethics and Libertarian Paternalism. ](http://www.palgraveconnect.com/doifinder/10.1057/9781137313577)_New\
    \ York: Palgrave Macmillan, 2013\n\u2022\_\_\_\_\_C.R. Sunstein, The Ethics of\
    \ Influence: Government in the Age of Behavioral Science. New York: Cambridge,\
    \ 2016\n\u2022\_\_\_\_\_M. Scheutz, \u201C[The Affect Dilemma for Artificial Agents:\
    \ Should We Develop Affective Artificial Agents? ](http://ieeexplore.ieee.org/document/6296668/)\u201D\
    \ _IEEE Transactions on Affective Computing, _vol._ _3, no. 4,pp. 424\u2013433,\_\
    Sept. 2012.\n\u2022\_\_\_\_\_A. Grinbaum, R. Chatila, L. Devillers, J.G. Ganascia,\
    \ C. Tessier and M. Dauchet. \u201C[Ethics in Robotics Research: CERNA Recommendations,](http://ieeexplore.ieee.org/document/7822928/)\u201D\
    \ _IEEE Robotics and Automation Magazine, _vol._ _24, no. 3,pp. 139\u2013145,\
    \ Sept. 2017.\n\u201CDesigning Moral Technologies: Theoretical, Practical, and\
    \ Ethical Issues\u201D Conference July 10\u201315, 2016, Monte Verit\xE0, Switzerland\"\
    \n\np.96-97\n\n## \"Recommendations\n1\\.\_\_\_As more and more computing devices\
    \ subtly and overtly influence human behavior, it is important to draw attention\
    \ to whether it is ethically appropriate to pursue this type of design pathway\
    \ in the context of governmental actions.\n2\\.\_\_\_There needs to be transparency\
    \ regarding who the intended beneficiaries are, and whether any form of deception\
    \ or manipulation is going to be used to accomplish the intended goal.\n## Further\
    \ Resources\n\u2022\_\_\_\_\_J. Borenstein and R. Arkin, \u201C[Robotic Nudges:\
    \ Robotic Nudges: The Ethics of Engineering a More Socially Just Human Being Just\
    \ Human Being.](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)\u201D\
    \ _Science and Engineering Ethics, _vol._ _22, no. 1,pp. 31\u201346, Feb. 2016.\n\
    \u2022\_\_\_\_\_J. Borenstein and R. Arkin. \u201C[Nudging for Good: Robots and\
    \ the Ethical Appropriateness of Nurturing Empathy and Charitable Behavior ](https://link.springer.com/article/10.1007/s00146-016-0684-1?no-access=true).\u201D\
    \ _AI and Society, _vol._ _32, no. 4, pp. 499\u2013507, Nov. 2016.\"\np.97-98\n\
    \n\"1.\_\_\_Consideration should be given to the development of a system of technical\
    \ licensing (\u201Cpermits\u201D) or other certification from governments or non-governmental\
    \ organizations (NGOs) that can aid users to understand the nudges from A/IS in\
    \ their lives.\n2\\.\_\_\_User autonomy is a key and essential consideration that\
    \ must be taken into account when addressing whether affective systems should\
    \ be permitted to nudge human beings.\n3\\.\_\_\_Design features of an affective\
    \ system that nudges human beings should include the ability to accurately distinguish\
    \ between users, including detecting characteristics such as whether the user\
    \ is an adult or a child.\n4\\.\_\_\_Affective systems with nudging strategies\
    \ should incorporate a design system of evaluation, monitoring, and control for\
    \ unintended consequences.\n## Further Resources\n\u2022\_\_\_\_\_J. Borenstein\
    \ and R. Arkin, \u201C[Robotic Nudges: ](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)\n\
    [Robotic Nudges: The Ethics of Engineering a ](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)\n\
    [More Socially Just Human Being Just Human Being.](https://link.springer.com/article/10.1007/s11948-015-9636-2?no-access=true)\u201D\
    \ _Science and Engineering Ethics, _vol._ _22, no. 1, pp. 31\u201346, 2016.\n\u2022\
    \_\_\_\_\_R. C. Arkin, M. Fujita, T. Takagi, and R. Hasegawa, \u201C[An Ethological\
    \ and Emotional Basis for Human- Robot Interaction.](http://www.sciencedirect.com/science/article/pii/S0921889002003755)\u201D\
    \ _Robotics and Autonomous Systems, _vol._ _42, no. 3\u20134 pp.191\u2013201,\
    \ March 2003.\n\u2022\_\_\_\_\_S. Omohundro \u201C[Autonomous Technology and the\
    \ Greater Human Good.](http://www.tandfonline.com/doi/abs/10.1080/0952813X.2014.895111?journalCode=teta20)\u201D\
    \ _Journal of Experimental and Theoretical Artificial Intelligence, vol. _26,\
    \ no. 3, pp. 303\u2013315, 2014.\"\np.102\n\n\"Deception is commonplace in everyday\
    \ human-human interaction. According to Kantian ethics, it is never ethically\
    \ appropriate to lie, while utilitarian frameworks indicate that it can be acceptable\
    \ when deception increases overall happiness. Given the diversity of views on\
    \ ethics and the appropriateness of deception, should affective systems be designed\
    \ to deceive? Does the non-consensual nature of deception restrict the use of\
    \ A/IS in contexts in which deception may be required?\nIt is necessary to develop\
    \ recommendations regarding the acceptability of deception performed by A/IS,\
    \ specifically with respect to when and under which circumstances, if any,\_it\
    \ is appropriate.\n1\\.\_\_\_In general, deception may be acceptable in an affective\
    \ agent when it is used for the benefit of the person being deceived, not for\
    \ the agent itself. For example, deception might be necessary in search and rescue\
    \ operations or for elder- or child-care.\_\n2\\.\_\_\_For deception to be used\
    \ under any circumstance, a logical and reasonable justification must be provided\
    \ by the designer, and this rationale should be certified by an external authority,\
    \ such as a licensing body\_or regulatory agency.\n## **Further resources**\n\_\
    \u2022\_\_\_\_\_R. C. Arkin, \u201CRobots That Need to Mislead: Biologically-inspired\
    \ Machine Deception.\u201D _IEEE Intelligent Systems _27, no. 6, pp. 60\u2013\
    75, 2012.\n\u2022\_\_\_\_\_J. Shim and R. C. Arkin, \u201COther-Oriented Robot\
    \ Deception: How Can a Robot\u2019s Deceptive Feedback Help Humans in HRI?\u201D\
    \ _Eighth International Conference on Social Robotics (ICSR 2016)_, Kansas, MO.,\
    \ November 2016.\n\u2022\_\_\_\_\_J. Shim and R. C. Arkin, \u201CThe Benefits\
    \ of Robot Deception in Search and Rescue: Computational Approach for Deceptive\
    \ Action Selection via Case-based Reasoning.\u201D _2015 IEEE International Symposium\
    \ on Safety, Security, and Rescue Robotics (SSRR 2015)_, West Lafayette, IN, October\
    \ 2015.\n\u2022\_\_\_\_\_J. Shim and R. C. Arkin, \u201CA Taxonomy of Robot Deception\
    \ and its Benefits in HRI.\u201D _Proceedings of IEEE Systems, Man and Cybernetics\
    \ Conference, _Manchester England, October 2013.\"\np.103\n"
  Linked Challenges:
  - rec2ajglpzbFYRivi
  - reciH69D8oIGI1p93
  - recrVvneRZaEutaGw
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T11:36:59.000Z'
  airtable_id: rec9TbOu2ZXUZG4A5
  title: Systematic analysis of consequences, transparency, user agency, and safeguards
    are central to implementation of AI nudges
- Description: "## \"Recommendations\n1\\.\_\_\_\_\_\_A well-designed affective system\
    \ will have a set of essential norms, specific to its intended cultural context\
    \ of use, in its knowledge base. Research has shown that A/IS technologies can\
    \ use at least five types of cues to simulate social interactions.\n2\\.\_\_\_\
    \_\_\_These include: physical cues such as simulated facial expressions, psychological\
    \ cues such as 1simulated humor or other emotions, use of language, use of social\
    \ dynamics like taking turns, and through social roles such as acting as a tutor\
    \ or medical advisor. Further examples are listed below:\na.\_\_\_\_\_\_Well-designed\
    \ affective systems will use language with affective content carefully and within\
    \ the contemporaneous expectations of the culture. An example is small talk. Although\
    \ small talk is useful for establishing a friendly rapport in many communities,\
    \ some communities see people that use small talk as insincere and hypocritical.\
    \ Other cultures may consider people that do not use small talk as unfriendly,\
    \ uncooperative, rude, arrogant, or ignorant. Additionally, speaking with proper\
    \ vocabulary, grammar, and sentence structure may contrast with the typical informal\
    \ interactions between individuals. For example, the latest trend, TV show, or\
    \ other media may significantly influence what is viewed as appropriate vocabulary\
    \ and interaction style.\nb.\_\_\_\_\_\_Well-designed affective systems will recognize\
    \ that the amount of personal space (proxemics) given by individuals in an important\
    \ part of culturally specific human interaction. People from varying cultures\
    \ maintain, often unknowingly, different spatial distances between themselves\
    \ to establish smooth communication. Crossing these limits may require explicit\
    \ or implicit consent, which A/IS must learn to negotiate to avoid transmitting\
    \ unintended messages.\nc.\_\_\_\_\_\_\_Eye contact is an essential component\
    \ for culturally sensitive social interaction. For some interactions, direct eye\
    \ contact is needed but for others it is not essential and may even generate misunderstandings.\
    \ It is important that A/IS be equipped to recognize the role of eye contact in\
    \ the development of emotional interaction.\nd.\_\_\_\_\_\_Hand gestures and other\
    \ non-verbal communication are very important for social interaction. Communicative\
    \ gestures are culturally specific and thus should be used with caution in cross-cultural\
    \ situations. The specificity of physical communication techniques must be acknowledged\
    \ in the design of functional affective systems. For instance, although a \u201C\
    thumbs-up\u201D sign is commonly used to indicate approval, in some countries\
    \ this gesture can be considered an insult.\ne.\_\_\_\_\_\_Humans use facial expressions\
    \ to detect emotions and facilitate communication. Facial expressions may not\
    \ be universal across cultures, however, and A/IS trained with a dataset from\
    \ one culture may not be readily usable in another culture. Well-developed A/IS\
    \ will be able to recognize, analyze, and even display facial expressions essential\
    \ for culturally specific social interaction.\n3\\.\_\_\_\_\_\_Engineers should\
    \ consider the need for cross-cultural use of affective systems.\_Well-designed\
    \ systems will have options innate to facilitate flexibility in cultural programming.\
    \ Mechanisms to enable and disable culturally specific \u201Cadd-ons\u201D should\
    \ be considered an essential part of A/IS development.\n## Further Resources\n\
    \u2022\_\_\_\_\_G. Cotton, \u201C[Gestures to Avoid in Cross-Cultural Business:\
    \ In Other Words, \u2018Keep Your Fingers to Yourself!\u2019](http://www.huffingtonpost.com/gayle-cotton/cross-cultural-gestures_b_3437653.html)\u201D\
    \ _Huffington Post, _June 13, 2013.\n\u2022\_\_\_\_\_\u201C[Paralanguage Across\
    \ Cultures,](https://cultureplusconsulting.com/2015/04/16/paralanguage-across-cultures/)\u201D\
    \ Sydney, Australia: Culture Plus Consulting, 2016.\n\u2022\_\_\_\_\_G. Cotton,\
    \ _[Say Anything to Anyone, Say Anything to Anyone, Anywhere: 5 Keys to Successful\
    \ Cross-Cultural Communication](http://www.wiley.com/WileyCDA/WileyTitle/productCd-111842042X.html)_[.](http://www.wiley.com/WileyCDA/WileyTitle/productCd-111842042X.html)\
    \ Hoboken, NJ: Wiley, 2013.\n\u2022\_\_\_\_\_D. Elmer, _[Cross-Cultural Connections:\
    \ ](https://www.ivpress.com/cross-cultural-connections)_\n_[Stepping Out and Fitting\
    \ In Around the World](https://www.ivpress.com/cross-cultural-connections)_[.](https://www.ivpress.com/cross-cultural-connections)\
    \ Westmont, IL: InterVarsity Press, 2002.\n\u2022\_\_\_\_\_B. J. Fogg, [Persuasive\
    \ Technology.](https://dl.acm.org/citation.cfm?id=763957) _Ubiquity_, December\
    \ 2, 2002.\n\u2022\_\_\_\_\_A. McStay, Emotional AI: The Rise of Empathic Media.\
    \ London: Sage, 2018.\nM. Price, \u201C[Facial Expressions\u2014Including Fear\u2014\
    \ May Not Be as Universal as We Thought.](http://www.sciencemag.org/news/2016/10/facial-expressions-including-fear-may-not-be-universal-we-thought)\u201D\
    \ _Science, _October 17, 2016.\"\np.90-91\n"
  Linked Challenges:
  - recL5sNl6bi1vrIzj
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T11:29:43.000Z'
  airtable_id: rec9UfowPkXtPUBC9
  title: Design recommendations for affective systems
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \nTechnical robustness and safety\n\u201CFallback plan and general\
    \ safety: \n\uF0FC Did you ensure that your system has a sufficient fallback plan\
    \ if it encounters adversarial attacks or other unexpected situations (for example\
    \ technical switching procedures or asking for a human operator before proceeding)?\
    \ \n\uF0FC Did you consider the level of risk raised by the AI system in this\
    \ specific use case? \n\uF0A7 Did you put any process in place to measure and\
    \ assess risks and safety? \n\uF0A7 Did you provide the necessary information\
    \ in case of a risk for human physical integrity? \n\uF0A7 Did you consider an\
    \ insurance policy to deal with potential damage from the AI system? \n\uF0A7\
    \ Did you identify potential safety risks of (other) foreseeable uses of the technology,\
    \ including accidental or malicious misuse? Is there a plan to mitigate or manage\
    \ these risks? \n\uF0FC Did you assess whether there is a probable chance that\
    \ the AI system may cause damage or harm to users or third parties? Did you assess\
    \ the likelihood, potential damage, impacted audience and severity? \n\uF0A7 Did\
    \ you consider the liability and consumer protection rules, and take them into\
    \ account?\n \uF0A7 Did you consider the potential impact or safety risk to the\
    \ environment or to animals? \n\uF0A7 Did your risk analysis include whether security\
    \ or network problems such as cybersecurity hazards could pose safety risks or\
    \ damage due to unintentional behaviour of the AI system? \n\uF0FC Did you estimate\
    \ the likely impact of a failure of your AI system when it provides wrong results,\
    \ becomes unavailable, or provides societally unacceptable results (for example\
    \ discrimination)? \n\uF0A7 Did you define thresholds and did you put governance\
    \ procedures in place to trigger alternative/fallback plans? \n\uF0A7 Did you\
    \ define and test fallback plans?\u201D (High-Level Expert Group on AI, 2019,\
    \ p. 27)\n"
  Linked Principles:
  - rec42P8U9usfYCtv9
  - reclPiw2VvNOSTzv5
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:21:35.000Z'
  airtable_id: recAMa23XxDLEzMn8
  title: Considerations in assessing trustworthy AI - Fallback plan and general safety
- Description: "\"Implementation fairness When your project team is approaching the\
    \ beta stage, you should begin to build out your plan for implementation training\
    \ and support. This plan should include adequate preparation for the responsible\
    \ and unbiased deployment of the AI system by its on-the-ground users. Automated\
    \ decision-support systems present novel risks of bias and misapplication at the\
    \ point of delivery, so special attention should be paid to preventing harmful\
    \ or discriminatory outcomes at this critical juncture of the AI project lifecycle.\
    \ \n\nIn order to design an optimal regime of implementer training and support,\
    \ you should pay special attention to the unique pitfalls of bias-in-use to which\
    \ the deployment of AI technologies give rise. These can be loosely classified\
    \ as decision-automation bias (more commonly just \u2018automation bias\u2019\
    ) and automation-distrust bias: \n- Decision-Automation Bias/The Technological\
    \ Halo Effect: Users of automated decision-support systems may tend to become\
    \ hampered in their critical judgment, rational agency, and situational awareness\
    \ as a result of their faith in the perceived objectivity, neutrality, certainty,\
    \ or superiority of the AI system. This may lead to over-reliance or errors of\
    \ omission, where implementers lose the capacity to identify and respond to the\
    \ faults, errors, or deficiencies, which might arise over the course of the use\
    \ of an automated system, because they become complacent and overly deferent to\
    \ its directions and cues. Decision-automation bias may also lead to over-compliance\
    \ or errors of commission where implementers defer to the perceived infallibility\
    \ of the system and thereby become unable to detect problems emerging from its\
    \ use for reason of a failure to hold the results against available information.\
    \ Both over-reliance and over-compliance may lead to what is known as out-of-loop\
    \ syndrome where the degradation of the role of human reason and the deskilling\
    \ of critical thinking hampers the user\u2019s ability to complete the tasks that\
    \ have been automated. This condition may bring about a loss of the ability to\
    \ respond to system failure and may lead both to safety hazards and to dangers\
    \ of discriminatory harm. To combat risks of decision-automation bias, you should\
    \ operationalise strong regimes of accountability at the site of user deployment\
    \ to steer human decision-agents to act on the basis of good reasons, solid inferences,\
    \ and critical judgment. \n- Automation-Distrust Bias: At the other extreme, users\
    \ of an automated decision-support system may tend to disregard its salient contributions\
    \ to evidence-based reasoning either as a result of their distrust or skepticism\
    \ about AI technologies in general or as a result of their over-prioritisation\
    \ of the importance of prudence, common sense, and human expertise. An aversion\
    \ to the non-human and amoral character of automated systems may also influence\
    \ decision subjects\u2019 hesitation to consult these technologies in high impact\
    \ contexts such as healthcare, transportation, and law. In order to secure and\
    \ safeguard fair implementation of AI systems by users well-trained to utilise\
    \ the algorithmic outputs as tools for making evidence-based judgements, you should\
    \ consider the following measures: \n- Training of implementers should include\
    \ the conveyance of basic knowledge about the statistical and probabilistic character\
    \ of machine learning and about the limitations of AI and automated decision-support\
    \ technologies. This training should avoid any anthropomorphic (or human-like)\
    \ portrayals of AI systems and should encourage users to view the benefits and\
    \ risks of deploying these systems in terms of their role in assisting human judgment\
    \ rather than replacing it. \n- Forethought should be given in the design of the\
    \ user-system interface about human factors and about possibilities for implementation\
    \ biases. The systems should be designed into processes that encourage active\
    \ user judgment and situational awareness. The interface between the user and\
    \ the system should be designed to make clear and accessible to the user the system\u2019\
    s rationale, compliance to fairness standards, and confidence level. Ideally this\
    \ should happen in a \u2018runtime\u2019 manner. \u2022 Training of implementers\
    \ should include a pre-emptive exploration of the cognitive and judgmental biases\
    \ that may occur across deployment contexts. This should be done in a use case\
    \ based manner that highlights the particular misjudgements that may occur when\
    \ people weigh statistical evidence. Examples of the latter may include overconfidence\
    \ in prediction based on the historical consistency of data, illusions that any\
    \ clustering of data points necessarily indicates significant insights, and discounting\
    \ of societal patterns that exist beyond the statistical results.\nPutting the\
    \ principle of discriminatory non-harm into action When you are considering how\
    \ to put the principle of discriminatory non-harm into action, you should come\
    \ together with all the managers on the project team to map out team member involvement\
    \ at each stage of the AI project pipeline from alpha through beta. Considering\
    \ fairness aware design and implementation from a workflow perspective will allow\
    \ you, as a team, to concretise and make explicit end-to-end paths of accountability\
    \ in a clear and peer-reviewable manner. This is essential for establishing a\
    \ robust accountability framework.\nConsidering fairness aware design and implementation\
    \ from such a workflow perspective will also assist you in pinpointing risks of\
    \ bias or downstream discrimination and streamlining possible solutions in a proactive,\
    \ pre-emptive, and anticipatory way. At each stage of the AI project pipeline\
    \ (i.e. at each column of the above table), you and the relevant members of your\
    \ team should carry out a collaborative self-assessment with regard to the applicable\
    \ dimension of fairness. This is a three-step process:\n1. Step 1: Identify the\
    \ fairness and bias mitigation dimensions that apply to the specific stage under\
    \ consideration (for example, at the data pre-processing stage, dimensions of\
    \ data fairness, design fairness, and outcome fairness may be at issue). \n2.\
    \ Step 2: Scrutinise how your particular AI project might pose risks or have unintended\
    \ vulnerabilities in each of these areas. \n3. Step 3: Take action to correct\
    \ any existing problems that have been identified, strengthen areas of weakness\
    \ that have possible discriminatory consequences, and take proactive bias-prevention\
    \ measures in areas that have been identified to pose potential future risks.\n\
    (Leslie, 2019, p. 20-23)\n"
  Linked Principles:
  - reczVPIH1y2OMpAJH
  - rec42P8U9usfYCtv9
  - rec7n2TGrH9RHYpQj
  Linked Sources:
  - recfYC5jjPmpLfSlM
  Tags:
  - reflection-discussion
  airtable_createdTime: '2023-05-19T11:13:37.000Z'
  airtable_id: recAhcg8OdusJvk43
  title: Principle of Discriminatory non-harm for fairness, key considerations for
    implementation fairness
- Description: "\u201CThe aim, purpose, and perceived benefits of a project need to\
    \ be clearly stated as a crucial first step of an ethical analysis. Emerging risks\
    \ will be judged against these in a balancing exercise. Some AI scholars would\
    \ claim that stating a too narrow aim would prevent the model from performing\
    \ properly as the strength of AI lies in processing large amounts of data inductively\
    \ (Alpaydin, 2016; Shirky, 2005). Other critical algorithmic researchers would\
    \ claim that we need to disclose potential power structures in these loose spaces\
    \ of operation (Bechmann & Bowker, 2019; boyd & Crawford, 2012; Crawford & Calo,\
    \ 2016; Sandvig, Hamilton, Karahalios & Langbort, 2016). \n- Can the researcher\
    \ articulate what their work aims to uncover?\n- How will this research contribute\
    \ to the state of the art in understanding Internetrelated phenomena? \n- How\
    \ will the research benefit society and specific stakeholders? \n- Will the research\
    \ aims create potential risks of harm for the individuals and groups involved\
    \ directly or indirectly? \n- If a generic aim is used how may the researchers/developers\
    \ influence the field and subjects directly and indirectly involved?\u201D \n\
    (franzke et al., 2020, p. 37)\n"
  Linked Principles:
  - recOHnq45Fq7YWsRO
  - recmzjcGKv3yNOxbl
  Linked Sources:
  - rec6r8OkE2Q2EdiM3
  Tags:
  - reflection-questions
  - project-design
  airtable_createdTime: '2023-05-19T12:20:36.000Z'
  airtable_id: recAnT7HDpWYvdJ1V
  title: Questions to consider in key stages of AI and machine learning based research,
    regarding aims and risks
- Description: "\u201CData collection for the model training and the research can\
    \ be done either by using open data repositories or by directly recruiting participants\
    \ to make a new dataset. The use of existing datasets raises issues around its\
    \ intended openness, consent for reuse, and the change of context for which the\
    \ data is used (Nissenbaum, 2001, 2009). \nCollecting new data raises issues around\
    \ meaningful informed consent, whether the subjects are aware of what their data\
    \ and the resulting research outputs will be used for, how this will affect them\
    \ and others, and the representation of humans by a necessarily more limited model.\
    \ More general questions arise about privacy as a concept to allow data subjects\
    \ self-determination and control over how data about them is used. Further, respect\
    \ for autonomy ensures an individual\u2019s ability to make decisions for themselves,\
    \ and to act upon them. Modern digital data collection (e.g. Application Programming\
    \ Interfaces) and processing techniques have put the various concepts of privacy\
    \ and autonomy under significant strain. It is therefore important for researchers\
    \ to be mindful of ways to minimize the risk to research subjects\u2019 and any\
    \ violations of privacy and autonomy by third parties. Further, applying technological\
    \ solutions such as encryption are often mistakenly classed as efforts to improve\
    \ privacy, while they instead provide more security. Similarly, not disclosing\
    \ information is called confidentiality, not necessarily privacy. \n**General\
    \ Data Collection: **\n- Are the identified data points necessary, relevant and\
    \ not excessive in relation to the research aim? \n- To what extent will data\
    \ in the database identify individuals directly, or indirectly through inference?\
    \ \n- Do the datasets contain classifiers that are particularly sensitive or even\
    \ protected classes? If so, what purpose do they serve? Can data points be used\
    \ as proxies to reconstruct sensitive and protected classes? Is it possible to\
    \ prevent the re-construction of sensitive and protected classes? \n- How does\
    \ the researcher protect the privacy of its users beyond security measures? For\
    \ example, is data deleted after a certain amount of time? Is data that is not\
    \ used for the purpose of the model deleted upon its inadvertent collection? Existing\
    \ Datasets: \n- Is the existing dataset explicitly open for public or research,\
    \ or was this dataset found without its reuse permissions being specified? \n\
    - Is the use of the existing dataset restricted by legal or other means? \n- Could\
    \ the data subjects (whether anonymized or not) in the existing dataset conceivably\
    \ object to the new use of their data? Does the initial consent (informed or proxy)\
    \ cover the intended re-use of the dataset? \n- What are the limitations in the\
    \ knowledge derived from the data in modeling individual and collective behaviour\
    \ in its totality? How does this limit the generalizability of the findings of\
    \ the study or the applicability of the precision and/or predictors found? \n\n\
    **New Data Collection: **\n- Have data subjects consented to the collection of\
    \ their data with a full understanding of what is being collected, for which purposes,\
    \ and with an understanding of how the data will be used by the researcher? If\
    \ not, have the collection processes gone through an ethical review board? And/or\
    \ how has the research team reflected on how to otherwise gain proxy consent and\
    \ the potential consequences of the proxy status? \n- How could potential risks\
    \ of harm be communicated to the research participants before entering the study?\
    \ \n- To what extent can researchers confirm whether people understand the consequences\
    \ of derivative uses of their data in AI and ML, knowing from existing literature\
    \ that the concept of \u2018informed\u2019 consent may not be meaningful for the\
    \ data subjects? \n- Has the organization decided how the privacy of data subjects\
    \ is safeguarded? \n- Does the system collect more information than it needs \n\
    - Are data subjects empowered to decide which data is collected and which inferences\
    \ are made about them? \n- Can the data subject have access to their data? Can\
    \ they choose to withdraw their data from the system?\u201D\n\n(franzke et al.,\
    \ 2020, p. 38-40)\n"
  Linked Principles:
  - rec42P8U9usfYCtv9
  - reczVPIH1y2OMpAJH
  Linked Sources:
  - rec6r8OkE2Q2EdiM3
  Tags:
  - reflection-questions
  - project-design
  airtable_createdTime: '2023-05-19T12:24:51.000Z'
  airtable_id: recB7MF7TeUKH3chO
  title: Questions to consider in key stages of AI and machine learning based research,
    regarding data collection
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \nTechnical robustness and safety\n\u201CAccuracy \n\uF0FC Did you\
    \ assess what level and definition of accuracy would be required in the context\
    \ of the AI system and use case? \n\uF0A7 Did you assess how accuracy is measured\
    \ and assured? \n\uF0A7 Did you put in place measures to ensure that the data\
    \ used is comprehensive and up to date? \n\uF0A7 Did you put in place measures\
    \ in place to assess whether there is a need for additional data, for example\
    \ to improve accuracy or to eliminate bias? \n\uF0FC Did you verify what harm\
    \ would be caused if the AI system makes inaccurate predictions? \n\uF0FC Did\
    \ you put in place ways to measure whether your system is making an unacceptable\
    \ amount of inaccurate predictions? \n\uF0FC Did you put in place a series of\
    \ steps to increase the system's accuracy?\u201D (High-Level Expert Group on AI,\
    \ 2019, p. 27)\n"
  Linked Principles:
  - rec42P8U9usfYCtv9
  - reclPiw2VvNOSTzv5
  - recOHnq45Fq7YWsRO
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:28:01.000Z'
  airtable_id: recBUdrJ8n1tIJLTz
  title: Considerations in assessing trustworthy AI - Accuracy
- Description: "## Recommendations\nOrganizations should clarify the relationship\
    \ between professional ethics and applied\_A/IS ethics by helping or enabling\
    \ designers, engineers, and other company representatives to discern the differences\
    \ between these kinds of ethics and where they complement each other.\nCorporate\
    \ ethical review boards, or comparable mechanisms, should be formed to address\_\
    ethical and behavioral concerns in relation to\_A/IS design, development and deployment.\
    \ Such boards should seek an appropriately diverse composition and use relevant\
    \ criteria, including both research ethics and product ethics, at the appropriate\
    \ levels of advancement of research and development. These boards should examine\
    \ justifications of research or industrial projects.\n## Further Resources\n\u2022\
    \ HH van der Kloot Meijberg and RHJ ter Meulen, \u201C[Developing Standards for\
    \ Institutional](https://jme.bmj.com/content/27/suppl_1/i36.full) [Ethics Committees:\
    \ Lessons from the Netherlands,](https://jme.bmj.com/content/27/suppl_1/i36.full)\u201D\
    \ _Journal of Medical Ethics_ 27 i36-i40, 2001.\"\n\np.130\n"
  Linked Challenges:
  - rec9qgAZS5U8GL137
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T10:38:42.000Z'
  airtable_id: recCN9eqkgKT3KjCB
  title: Establish corporate ethics review boards
- Description: "\u2022 Whom can you talk to about the above questions? How can you\
    \ create occasions for discussion and reflection about them with colleagues? \u2022\
    \ What are you learning from your project that can inform future practice related\
    \ to ethical issues in the scholarship of teaching and learning?\n(Fedoruk, 2017,\
    \ p. 3)\n"
  Linked Principles:
  - recjViPnz3atRIOpD
  Linked Sources:
  - recQzldmBLByP78Uu
  Tags:
  - reflection-questions
  - education-research
  airtable_createdTime: '2023-05-19T13:04:12.000Z'
  airtable_id: recDOGXIsqtXrCuSz
  title: Questions to consider in SoTL research professional learning
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CCommunication: \n\uF0FC Did you communicate to (end-)users\
    \ \u2013 through a disclaimer or any other means \u2013 that they are interacting\
    \ with an AI system and not with another human? Did you label your AI system as\
    \ such? \n\uF0FC Did you establish mechanisms to inform (end-)users on the reasons\
    \ and criteria behind the AI system\u2019s outcomes? \n\uF0A7 Did you communicate\
    \ this clearly and intelligibly to the intended audience? \n\uF0A7 Did you establish\
    \ processes that consider users\u2019 feedback and use this to adapt the system?\
    \ \n\uF0A7 Did you communicate around potential or perceived risks, such as bias?\
    \ \n\uF0A7 Depending on the use case, did you consider communication and transparency\
    \ towards other audiences, third parties or the general public? \n\uF0FC Did you\
    \ clarify the purpose of the AI system and who or what may benefit from the product/service?\
    \ \n\uF0A7 Did you specify usage scenarios for the product and clearly communicate\
    \ these to ensure that it is understandable and appropriate for the intended audience?\
    \ \n\uF0A7 Depending on the use case, did you think about human psychology and\
    \ potential limitations, such as risk of confusion, confirmation bias or cognitive\
    \ fatigue? \n\uF0FC Did you clearly communicate characteristics, limitations and\
    \ potential shortcomings of the AI system? \n\uF0A7 In case of the system's development:\
    \ to whoever is deploying it into a product or service? \n\uF0A7 In case of the\
    \ system's deployment: to the (end-)user or consumer?\u201D (High-Level Expert\
    \ Group on AI, 2019, p. 29)\n"
  Linked Principles:
  - recxcFmvPG5wrCqpO
  - recQEiU22Qy1E0YuA
  - recU6u0AZbcNj1ik9
  - recGb4WZzr34RXKr0
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:35:32.000Z'
  airtable_id: recEHwRaLD44KPk8v
  title: Considerations in assessing trustworthy AI - Communication
- Description: "## Conflicts of Interest\_and Power Relationships\n\u201C**Key Principle:**\
    \ When you are acting as both instructor and researcher, mitigate undue influence,\
    \ coercion, or power imbalances by basing decisions first and foremost on your\
    \ role as an Instructor (which sometimes may be at odds with your goals as a researcher)\
    \ and by being sensitive to the inherent power differential between instructor\
    \ and student. \n**Strategies for Ethical Practice **\n- As you begin to design\
    \ your study, describe your research plans to colleagues and former students,\
    \ and invite them to help you identify blind spots you might have in terms of\
    \ influence, coercion, or power imbalances, so that you can address them in your\
    \ planning. For a list of discussion questions to facilitate the identification\
    \ of blind spots, see Table 2 \u2013 Questions to Consider When Planning SoTL\
    \ Research on page 16 of this Guide. \n- Provide student participants with information\
    \ about contacting the university\u2019s REB with ethical questions or concerns.\
    \ \n**Definitions:**\n- Third party Someone who does not have grading authority\
    \ or perceived power over potential participants who can act as an intermediary\
    \ or buffer between you and the students. This person will be the only one who\
    \ knows which students are participating in the research (you will not), and students\
    \ will be informed of this person\u2019s role before deciding whether to participate.\
    \ With your direction, the third party may do any or all the following: introduce/present\
    \ the study to potential participants, collect and store consent forms, field\
    \ participants\u2019 questions and/ or de-identify then direct their questions\
    \ your way and convey your response(s) to them, provide prospective participants\
    \ with updates/ongoing information about the study, conduct interviews and/or\
    \ lead focus groups with student participants, etc. \n- Identifying information\
    \ Information that can or may reveal which students are participating in the research.\
    \ This information might include first and/ or last names, student ID numbers,\
    \ physical descriptions, or other references to or about people that might reasonably\
    \ identify them as research participants.\u201D \n(Fedoruk, 2017, p. 5)\n\n\u201C\
    If possible: \n- Use a third party to assist with participant recruitment, information\
    \ provision, and data generation and analysis. This approach protects participating\
    \ and non-participating students\u2019 identities by ensuring that they can become\
    \ informed about the study, raise questions, participate in, and/or withdraw from\
    \ the study without revealing their identities as research participants (or not)\
    \ to you. \n- Collect data (e.g., conduct interviews or focus groups, distribute\
    \ surveys) after final grades have been submitted and released to the students,\
    \ and after the appeal deadline has passed. \n- Analyze student work after identifying\
    \ information has been removed. \n- Conduct the research using students in a school\
    \ or classroom other than your own.\u201D\n(Fedoruk, 2017, p. 6)\n\n## **Consent\
    \ process**\n### Key Principle\nEnsure that each student\u2019s decision to participate\
    \ in your research (or not) is voluntary, and that their privacy is protected\
    \ when offering or declining consent.\_\n### Strategies for Ethical Practice\n\
    \u2022\_\_\_\_\_Use a third party to facilitate consent/withdrawal processes to\
    \ protect students\u2019 privacy.\n\u2022\_\_\_\_\_Clearly communicate to students\
    \ that there are no repercussions for their refusal to consent.\n\u2022\_\_\_\_\
    \_When conducting surveys, use web-based survey tools (e.g., Qualtrics, etc.)\
    \ that allow students to participate anonymously. Anonymous online participation\
    \ eliminates personal identifiers and peer pressure, and allows students who are\
    \ not interested in participating to decline privately.\n\u2022\_\_\_\_\_When\
    \ collecting consent forms from student participants in class, design the forms\
    \ so that all students must sign and hand in the paper form in order to prevent\
    \ knowledge of who is and is not participating (e.g., explain that everyone signs\
    \ the consent form, but those who do NOT want to participate can then draw two\
    \ lines through their signatures).\n\_\n\u2022\_\_\_\_\_When offering incentives,\
    \ keep them to a minimum to avoid undue influence (e.g., $25 bookstore gift card,\
    \ a draw for a $50 gift card), and provide students with clear timelines during\
    \ which they may opt-in or out of participation in the study. If the incentive\
    \ includes a small percentage of their grade (1 to 5%), give students not participating\
    \ in the study an opportunity to earn the same incentive through an alternate\
    \ option, such as an additional assignment equivalent in time and effort.\nIf\
    \ you are collecting and analyzing your own data, where applicable, inform students:\n\
    \u2022\_\_\_\_\_About where or to whom they might direct questions about the study,\
    \ before, during, and after the study.\n\u2022\_\_\_\_\_That you will not know\
    \ who has agreed to participate until students\u2019 grades are submitted and\
    \ released, and the appeal deadline has passed.\n\u2022\_\_\_\_\_That you will\
    \ not analyze data until after grades are submitted and released to students and\
    \ the appeal deadline has passed.\n(Fedoruk, 2017, p. 7)\n\n### Key Principle\n\
    Ensure that students\u2019 decisions to participate in your research (or not)\
    \ are informed by telling them about the purpose, benefits, risks, and consequences\
    \ of your research before asking for their consent.\n### Strategies for Ethical\
    \ Practice\n\u2022\_\_\_\_\_Describe and discuss (or have a third party describe\
    \ and discuss) the research with students before seeking their consent to participate.\n\
    \u2022\_\_\_\_\_Include clear and transparent descriptions of the project on consent\
    \ forms (even if the project has already been described in detail to participants).\n\
    \u2022\_\_\_\_\_Commit to students at the onset that results will be shared with\
    \ them upon completion.\n\u2022\_\_\_\_\_When conducting focus groups, ensure\
    \ that the consent process asks that each member of the focus group respect the\
    \ confidentiality of other members, but that you cannot guarantee confidentiality.\
    \ Because of this, in REB applications, it may be advisable to attend to why group\
    \ data (rather than individual interview data) is preferable in your research\
    \ design.\nWhen video or audio recording, because this method of collecting data\
    \ can inadvertently capture material produced by students who have not consented\
    \ to participate in the research process, it is advisable that researchers articulate\
    \ to their institutional REBs why this data collection method (as opposed to others)\
    \ is essential to the research design. If using video, give consenting students\
    \ the option to choose whether the research team will only view their presence\
    \ on the video (a), or (b) may be viewed by the research team and shared during\
    \ the dissemination of research findings, and indicating their choice on the informed\
    \ consent form.\nWhen possible, include a brief explanation of the research on\
    \ your course outline or syllabus (if advisable within your campus context). For\
    \ example: \u201CPlease be advised that within this course, you will have the\
    \ opportunity to volunteer as a research participant in a study that examines\
    \ the reading comprehension of second year, undergraduate students as they progress\
    \ through Language and Literature 201. Details will be provided at the start of\
    \ the course.\n### Key Principle\nMake sure students have the autonomy to freely\
    \ and privately choose to participate, refuse to participate,\_or withdraw from\
    \ participation at any time during or after the research (provided that it has\
    \ not already been disseminated).\n### Strategies for Ethical Practice\n\u2022\
    \_\_\_\_\_Provide students with the option to withdraw from the research simply\
    \ (e.g., by sending an email) and at any time prior to dissemination. Indicate\
    \ what will happen to their data after they have withdrawn from the research (e.g.,\
    \ wherever possible, it will be extracted and destroyed).\nIn cases where the\
    \ research timeline needs to be extended, whenever possible, seek students\u2019\
    \ consent regarding these extensions. \n## **Fairness and Equity in\_Research\
    \ Participation **\n###   Key Principle\nAs much as possible, within the research\
    \ project\u2019s goals, be inclusive, fair, and equitable when\_selecting participants.\_\
    \n### Strategies for Ethical Practice\n\u2022\_\_\_\_\_Have a clear rationale\
    \ for participant inclusion and exclusion criteria connected to your project\u2019\
    s goals and specific research question.\_For instance, if you are not including\
    \ seniors, or men, or non-majors, or non-native English speakers, explain how\
    \ this exclusion is relevant to your specific project (e.g., \u201CBecause this\
    \ research is specifically focused on the female experience of power relationships,\
    \ and because we are linking\nto gender theory, only women-identified people will\
    \ be included for participation\u201D).\n\u2022\_\_\_\_\_Consider your assumptions\
    \ about potential participants in your study. For example, do not assume that\
    \ students with physical disabilities should be excluded from a study that uses\
    \ physical activity games to assess comprehension of biomechanics principles.\
    \ Invite colleagues and/or former students to help you identify assumptions that\
    \ you might be making about participants, to ensure that your inclusion/ exclusion\
    \ criteria do not suffer from blind spots.\n\u2022\_\_\_\_\_If there is a language\
    \ barrier between you (or your third party) and participant(s), involve an intermediary\
    \ who is competent in both languages to assist with communication between you\
    \ (or your third party) and participant(s).\n### Key Principle\nEnsure that the\
    \ benefits of participating in your study are equitably distributed among participants.\n\
    ### Strategies for Ethical Practice\n\u2022\_\_\_\_\_Discuss potential research\
    \ benefits with students at the onset of the study.\n\u2022\_\_\_\_\_Ensure an\
    \ \u201Cequitable distribution of research benefits\u201D (TCPS2, 2014, p. 55)\
    \ by avoiding circumstances in which the conditions of some participants are significantly\
    \ more beneficial than the conditions for other participants or nonparticipants.\
    \ If you are using a comparison group or a differential experience for non-participants,\
    \ you should closely monitor the impact of an intervention to guard against one\
    \ group (e.g., research participants, or non-participants) experiencing significantly\
    \ more benefits over the other group(s). You are responsible for gauging if the\
    \ discrepancy between groups becomes unethical and could have negative implications\
    \ for the other group(s), in which case contingencies and modifications to a study\
    \ may be needed.\n\u2022\_\_\_\_\_For instance, an instructor teaching two sections\
    \ of the same course might use a \u201Cflipped classroom\u201D model for one section\
    \ and leave the other unchanged (as a control group). If students in the flipped\
    \ classroom are showing significant gains, the students in the control group may\
    \ be disadvantaged, and the instructor may decide to flip both sections to mitigate\
    \ an unethical disparity between groups. This type of contingency should be included\
    \ in the research design, and student success should be prioritized.\n## Research\
    \ Results\n### Key Principle\nUpon completing the study, make the results available,\
    \ accessible, and understandable to all participants.\n### Strategies for Ethical\
    \ Practice\n\u2022\_\_\_\_\_Inform students that you will share the outcomes of\
    \ your research and in what format (e.g., a one-page brief, a paper, etc.) as\
    \ soon as they become available. Invite students to provide contact information\
    \ during the consent process to indicate how to reach them with research outcomes\
    \ (e.g., an email address to which the outcomes can be sent).\nWhen the outcomes\
    \ are available, provide participants with the citation of the journal in which\
    \ it is published or copies of the publication and a written summary of results\
    \ written in clear, understandable language.\n## **Privacy and Confidentiality**\n\
    ### Key Principle\nProtect the participants\u2019 information and the integrity\
    \ of the research project.\n### Strategies for Ethical Practice\n\u2022\_\_\_\_\
    \_Discuss the practical implications of confidentiality with all members of your\
    \ research team, and where relevant, have all members sign a confidentiality agreement.\n\
    \u2022\_\_\_\_\_Do not share any specific identifying information about the data\
    \ collected with anyone other than your research team.\n\u2022\_\_\_\_\_If information\
    \ sharing with government agencies, community research partners, research sponsors,\
    \ or regulatory agencies may occur during the study, describe and include this\
    \ possibility as part of the information provided to students before they decide\
    \ whether to participate.\n\u2022\_\_\_\_\_If confidentiality is unexpectedly\
    \ breached, let participants know immediately and advise them of the steps you\
    \ have taken to address the situation and prevent further breaches.\nIf possible:\n\
    \u2022\_\_\_\_\_De-identify\_Student data, or have a third party de-identify the\
    \ data for you.\n\u2022\_\_\_\_\_When groups are small (e.g., fewer than 10 or\
    \ 15 members of a particular type), aggregate or combine data and remove identifying\
    \ information to diminish the possibility that the responses of specific identifiable\
    \ groups will be deduced.\nIn an online presentation about REBs, Babcock and Henry\
    \ (2014) reproduce the Risk Matrix below. The matrix demonstrates a two-fold relationship\
    \ between disclosure and harm reduction. Figure 2 demonstrates that privacy protection\
    \ and ethics are warranted in cases where the data consists of identifiable, confidential\
    \ information, where the\_risk of disclosure and harm are most pronounced. The\
    \ light orange cells in the table indicate that\_risk of disclosure and harm still\
    \ exists in cases in which the data has been de-identified (here,\_referred to\
    \ as anonymized), thus requiring\_privacy protection and REB review.\n### Key\
    \ Principle\nDuring data collection and analysis, use\_appropriate safeguards\
    \ and security measures\_to protect participant information and data.\n### Strategies\
    \ for Ethical Practice\n\u2022\_\_\_\_\_Use encryption software and/or passwordprotected\
    \ digital documents, folders, and/ or systems to limit access to data and protect\
    \ participant confidentiality.\n\u2022\_\_\_\_\_Store all hard copies of participant-identifying\
    \ data, including signed consent forms, in a locked cabinet with a key and protect\
    \ that key.\n\u2022\_\_\_\_\_Keep an up-to-date list of all persons with access\
    \ to participant information, ensuring they have signed a non-disclosure or confidentiality\
    \ agreement (e.g., You can find an example of such an agreement at the University\
    \ of Calgary CFREB website: https://ucalgary.ca/research/ researchers/ethics-compliance/cfreb).\n\
    \u2022\_\_\_\_\_If appropriate, destroy all identifying participant information\
    \ and identifying data upon completion of the research project (e.g., by shredding\
    \ hardcopy materials and/or by reformatting/wiping digital storage devices). It\
    \ may be inappropriate to destroy data when, for example, the data consists of\
    \ course material (e.g., assignments, papers, exams) or evaluative material (e.g.,\
    \ course and/or teaching evaluations) that may be otherwise used and retained\
    \ for other purposes (e.g., course redesign).\nBabcock and Henry (2014) also outline\
    \ a \u201Cdata hierarchy\u201D that helps researchers attend to the disclosure\
    \ risks that can arise concerning confidentiality and human participants. This\
    \ fourtiered represents risk as least to most pronounced by information type.\
    \ Specifically, the anonymous information is represented as lowest in risk in\
    \ terms of disclosing confidentiality, and identifiable information is positioned\
    \ as the kind of information with the highest risk of disclosure.\n\n\n## Overarching\
    \ questions\n  \t**Conflicts of Interest and Power **\n**Relationships**\n Mitigate\
    \ undue influence, coercion, or power imbalances by: a. basing decisions first\
    \ and foremost on an instructor\u2019s goals (which sometimes may be at odds with\
    \ research goals), and b. being sensitive to the inherent power differential between\
    \ instructor and student.\n \u2022\_\_\_\_\_\u201CCould any part of the research\
    \ design interfere with the effectiveness and/or credibility an instructor and/or\
    \ with students\u2019 interests and/or ability to learn?\u201D\n\u2022\_\_\_\_\
    \_\u201CAre there ways in which participating in this research - or not - might\
    \ be something that students feel like they had to do? If so, why?\u201D\n\u2022\
    \_\_\_\_\_\u201CCould a third-party help with the consent and data collection\
    \ process to mitigate powerdifferentials?\u201D\n   **Consent Processes**\n Ensure\
    \ that students\u2019 decisions to participate in the research (or not) is informed\
    \ and voluntary by: a. telling them about the purpose, benefits, risks, and consequences\
    \ of the research before asking for their consent, and b. making sure they have\
    \ the autonomy to freely and privately choose to participate, refuse to participate,\
    \ or withdraw from participation at any time during the research.\n \u2022\_\_\
    \_\_\_\u201CWhat else would you want to know before making a decision about participating\
    \ in this research?\u201D\n\u2022\_\_\_\_\_\u201CIn what ways might students feel\
    \ compelled to participate or compromised in their ability to withdraw from the\
    \ study without consequence?\u201D\n    **Fairness and Equity **\n**in Research\
    \ **\n**Participation **\n Within the research project\u2019s goals, be inclusive,\
    \ fair, and equitable when selecting participants by:\na. recognizing and respecting\
    \ the vulnerability of individuals or groups, and b. making the results available,\
    \ accessible, and understandable to\_all participants upon completion\_of the\
    \ study.\n \u2022\n\u2022\n \u201CAre there any individuals or groups that this\
    \ research might directly or indirectly exclude?\u201D\n\u201CHow can I be sure\
    \ that the results of this study can be accessible to\_all participants?\u201D\
    \n   **Privacy and Confidentiality**\n Protect the participants\u2019 information\
    \ and the integrity of the research project by: a. meeting confidentiality obligations\
    \ in the research,\nb. implementing appropriate institutional safeguards and security\
    \ measures to protect participant information and data, and c. if the research\
    \ involves identifiable secondary use of data (e.g., former students\u2019 work\
    \ or other identifiable materials collected before seeking REB approval), seeking\
    \ students\u2019 informed consent and applying the above principles of ethical\
    \ practice to this secondary use of data.\n-  \u201CAre there ways in which this\
    \ research design might, in any way, compromise participants\u2019 confidentiality?\u201D\
    \n- \u201CAre there adequate safeguards\_to protect participants\u2019 information\
    \ and data?\u201D\n- \u201CHave I obtained informed consent from all students\
    \ whose data I am using in this study, regardless of when said data was collected?\u201D\
    \n \nAll drawn from Fedoruk (2017)\n"
  Linked Principles:
  - recKdujFoPJr4ZAhZ
  - recQ9DIFEsOEkCx3O
  Linked Sources:
  - recQzldmBLByP78Uu
  Tags:
  - reflection-questions
  - education-research
  airtable_createdTime: '2023-05-19T13:11:32.000Z'
  airtable_id: recELo3u36oZuujxN
  title: Key discussion points for planning SoTL research regarding power relationships
- Description: "\"Algorithmic agents should be developed for individuals to curate\
    \ and share their personal data. Specifically:\n\u2022\_\_\_\_\_For purposes of\
    \ privacy, a person must be able to set up complex permissions that reflect a\
    \ variety of wishes.\n\u2022\_\_\_\_\_The agent should help a person foresee and\
    \ mitigate potential ethical implications of specific machine learning data exchanges.\n\
    \u2022\_\_\_\_\_A user should be able to override his/her personal agents should\
    \ he/she decide that the service offered is worth the conditions imposed.\n\u2022\
    \_\_\_\_\_An agent should enable machine-to-machine processing of information\
    \ to compare, recommend, and assess offers and services.\n\u2022\_\_\_Institutional\
    \ systems should ensure support for and respect the ability of individuals to\
    \ bring their own agent to the relationship without constraints that would make\
    \ some guardians inherently incompatible or subject to censorship.\n\u2022\_\_\
    \_\_\_Vulnerable parts of the population will need protection in the process of\
    \ granting access.\"\np.111 IEEE report\n"
  Linked Challenges:
  - recAoyMGCCLhaSqEb
  Linked Principles:
  - recPg7Ov0priGGtLm
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T09:36:00.000Z'
  airtable_id: recG86YsfXnKY9kNc
  title: AI assistants should be created to help users understand and manage how their
    data is being used
- Description: "\"Sustainability\n\nDesigners and users of AI systems should remain\
    \ aware that these technologies may have transformative and long-term effects\
    \ on individuals and society. In order to ensure that the deployment of your AI\
    \ system remains sustainable and supports the sustainability of the communities\
    \ it will affect, you and your team should proceed with a continuous sensitivity\
    \ to the real-world impacts that your system will have.\n\nStakeholder Impact\
    \ Assessment\n\nYou and your project team should come together to evaluate the\
    \ social impact and sustainability of your AI project through a Stakeholder Impact\
    \ Assessment (SIA), whether the AI project is being used to deliver a public service\
    \ or in a back-office administrative capacity. When we refer to \u2018stakeholders\u2019\
    \ we are referring primarily to affected individual persons, but the term may\
    \ also extend to groups and organisations in the sense that individual members\
    \ of these collectives may also be impacted as such by the design and deployment\
    \ of AI systems. Due consideration to stakeholders should be given at both of\
    \ these levels.\n\nThe purpose of carrying out an SIA is multidimensional. SIAs\
    \ can serve several purposes, some of which include:\n\n(1)\tHelp to build public\
    \ confidence that the design and deployment of the AI system by the public sector\
    \ agency has been done responsibly\n(2)\tFacilitate and strengthen your accountability\
    \ framework\n(3)\tBring to light unseen risks that threaten to affect individuals\
    \ and the public good\n \n(4)\tUnderwrite well-informed decision-making and transparent\
    \ innovation practices\n(5)\tDemonstrate forethought and due diligence not only\
    \ within your organisation but also to the wider public\n\nYour team should convene\
    \ to evaluate the social impact and sustainability of your AI project through\
    \ the SIA at three critical points in the project delivery lifecycle:\n\n1\\.\t\
    Alpha Phase (Problem Formulation): Carry out an initial Stakeholder Impact Assessment\
    \ (SIA) to determine the ethical permissibility of the project. Refer to the SUM\
    \ Values as a starting point for the considerations of the possible effects of\
    \ your project on individual wellbeing and public welfare. In cases where you\
    \ conclude that your AI project will have significant ethical and social impacts,\
    \ you should open your initial SIA to the public so that their views can be properly\
    \ considered. This will bolster the inclusion of a diversity of voices and opinions\
    \ into the design and development process through the participation of a more\
    \ representative range of stakeholders. You should also consider consulting with\
    \ internal organisational stakeholders, whose input will likewise strengthen the\
    \ openness, inclusivity, and diversity of your project.\n\n2\\.\tFrom Alpha to\
    \ Beta (Pre-Implementation): Once your model has been trained, tested, and validated,\
    \ you and your team should revisit your initial SIA to confirm that the AI system\
    \ to be implemented is still in line with the evaluations and conclusions of your\
    \ original assessment. This check-in should be logged on the pre-implementation\
    \ section of the SIA with any applicable changes added and discussed. Before the\
    \ launch of the system, this SIA should be made publicly available. At this point\
    \ you must also set a timeframe for re- assessment once the system is in operation\
    \ as well as a public consultation which predates and provides input for that\
    \ re-assessment. Timeframes for these re-assessments should be decided by your\
    \ team on a case-by-case basis but should be proportional to the scale of the\
    \ potential impact of the system on the individuals and communities it will affect.\n\
    \n3\\.\tBeta Phase (Re-Assessment): After your AI system has gone live, your team\
    \ should intermittently revisit and re-evaluate your SIA. These check-ins should\
    \ be logged on the re- assessment section of the SIA with any applicable changes\
    \ added and discussed. Re- assessment should focus both on evaluating the existing\
    \ SIA against real world impacts and on considering how to mitigate the unintended\
    \ consequences that may have ensued in the wake of the deployment of the system.\
    \ Further public consultation for input at the beta stage should be undertaken\
    \ before the re-assessment so that stakeholder input can be included in re-assessment\
    \ deliberations.\n\nYou should keep in mind that, in its specific focus on social\
    \ and ethical sustainability, your Stakeholder Impact Assessment constitutes just\
    \ one part of the governance platform for your AI project and should be a complement\
    \ to your accountability framework and other auditing and activity-monitoring\
    \ documentation.\n\nYour SIA should be broken down into four sections of questions\
    \ and responses. In the 1st section, there should be general questions about the\
    \ possible big-picture social and ethical impacts of the use of the AI system\
    \ you plan to build. In the 2nd section, your team should collaboratively formulate\
    \ relevant sector-specific and use case-specific questions about the impact of\
    \ the AI system on affected stakeholders. The 3rd section should provide answers\
    \ to the additional questions relevant to pre-implementation evaluation. The 4th\
    \ section should provide the opportunity for members of your team to reassess\
    \ the system in light of its real-world impacts, public input, and possible unintended\
    \ consequences.\n\n## Stakeholder impact assessment\n**Stakeholder Impact Assessment\
    \ for (Project Name)**\n   \n**1. Alpha Phase (Problem Formulation) General Questions**\n\
    - \_Completed on this Date:\n1. Identifying Affected Stakeholders: Who are the\
    \ stakeholders that this AI project is most likely to affect? What groups of these\
    \ stakeholders are most vulnerable? How might the project negatively impact them?\n\
    2. Goal-Setting and Objective-Mapping: How are you defining the outcome (the target\
    \ variable) that the system is optimising for? Is this a fair, reasonable, and\
    \ widely acceptable definition?\n3. Does the target variable (or its measurable\
    \ proxy) reflect a reasonable and justifiable translation of the project\u2019\
    s objective into the statistical frame? Is this translation justifiable given\
    \ the general purpose of the project and the potential impacts that the outcomes\
    \ of its implementation will have on the communities involved?\n4. Possible Impacts\
    \ on the Individual: How might the implementation of your AI system impact the\
    \ abilities of affected stakeholders to make free, independent, and well-informed\
    \ decisions about their lives? How might it enhance or diminish their autonomy?\
    \ How might it affect their capacities to flourish and to fully develop themselves?\
    \ How might it do harm to their physical or mental integrity? Have risks to individual\
    \ health and safety been adequately considered and addressed? How might it infringe\
    \ on their privacy rights, both on the data processing end of designing the system\
    \ and on the implementation end of deploying it?\n5. Possible Impacts on Society\
    \ and Interpersonal Relationships How might the implementation of your AI system\
    \ adversely affect each stakeholder\u2019s fair and equal treatment under the\
    \ law? Are there any aspects of the project that expose vulnerable communities\
    \ to possible discriminatory harm? How might the use of your system affect the\
    \ integrity of interpersonal dialogue, meaningful human connection, and social\
    \ cohesion? Have the values of civic participation, inclusion, and diversity been\
    \ adequately considered in articulating the purpose and setting the goals of the\
    \ project? If not, how might these values be incorporated into your project design?\
    \ Does the project aim to advance the interests and well-being of as many affected\
    \ individuals as possible? Might any disparate socioeconomic impacts result from\
    \ its deployment? Have you sufficiently considered the wider impacts of the system\
    \ on future generations and on the planet as a whole?\n\n**2. Alpha Phase (Problem\
    \ Formulation) Sector-Specific and Use Case-Specific Questions**\n\_\nCompleted\
    \ on this Date:\n In this section you should consider the sector-specific and\
    \ use case-specific issues surrounding the social and ethical impacts of your\
    \ AI project on affected stakeholders. Compile a list of the questions and concerns\
    \ you anticipate. State how your team is attempting to address these questions\
    \ and concerns.\n   \_\n**3. From Alpha to Beta (Pre-Implementation)**\n\_\nCompleted\
    \ on this Date:\nAfter reviewing the results of your initial SIA, answer the following\
    \ questions:\n\_\n- Are the trained model\u2019s actual objective, design, and\
    \ testing results still in line with the evaluations and conclusions contained\
    \ in your original assessment? If not, how does your assessment now differ?\n\
    - Have any other areas of concern arisen with regard to possibly harmful social\
    \ or ethical impacts as you have moved from the alpha to the beta phase?\n- You\
    \ must also set a reasonable timeframe for public consultation and beta phase\
    \ re-assessment: Dates of Public Consultation on Beta-Phase Impacts: Date of Planned\
    \ Beta Phase Re-Assessment: \n  \_\n**4. Beta Phase (Re-Assessment)**\n\_Completed\
    \ on this Date:\nOnce you have reviewed the most recent version of your SIA and\
    \ the results of the public consultation, answer the following questions:\n\n\
    - How does the content of the existing SIA compare with the real-world impacts\
    \ of the AI system as measured by available evidence of performance, monitoring\
    \ data, and input from implementers and the public?\n- What steps can be taken\
    \ to rectify any problems or issues that have emerged?\n- Have any unintended\
    \ harmful consequences ensued in the wake of the deployment of the system? If\
    \ so, how might these negative impacts be mitigated and redressed?\n- Have the\
    \ maintenance processes for your AI model adequately taken into account the possibility\
    \ of distributional shifts in the underlying population? Has the model been properly\
    \ retuned and retrained to accommodate changes in the environment?\_\nDates of\
    \ Public Consultation on Beta-Phase Impacts: Date of Next Planned Beta Phase Re-Assessment:\"\
    \n(Leslie, 2019, p.23-24)\n"
  Linked Challenges:
  - recvWM2glArsVhaye
  Linked Principles:
  - recZToVrPeFlFq0Aw
  - recmzjcGKv3yNOxbl
  - recOHnq45Fq7YWsRO
  Linked Sources:
  - recfYC5jjPmpLfSlM
  Tags:
  - reflection-discussion
  - impact-assessment
  airtable_createdTime: '2023-05-19T11:43:19.000Z'
  airtable_id: recGS9OVQ7qAjvIYE
  title: Principle of Sustainability, key considerations
- Description: "\u201CTensions may arise between the above principles, for which there\
    \ is no fixed solution. In line with the EU fundamental commitment to democratic\
    \ engagement, due process and open political participation, methods of accountable\
    \ deliberation to deal with such tensions should be established. For instance,\
    \ in various application domains, the principle of prevention of harm and the\
    \ principle of human autonomy may be in conflict. Consider as an example the use\
    \ of AI systems for \u2018predictive policing\u2019, which may help to reduce\
    \ crime, but in ways that entail surveillance activities that impinge on individual\
    \ liberty and privacy. Furthermore, AI systems\u2019 overall benefits should substantially\
    \ exceed the foreseeable individual risks. While the above principles certainly\
    \ offer guidance towards solutions, they remain abstract ethical prescriptions.\
    \ AI practitioners can hence not be expected to find the right solution based\
    \ on the principles above, yet they should approach ethical dilemmas and trade-offs\
    \ via reasoned, evidence-based reflection rather than intuition or random discretion.\
    \ There may be situations, however, where no ethically acceptable trade-offs can\
    \ be identified. Certain fundamental rights and correlated principles are absolute\
    \ and cannot be subject to a balancing exercise (e.g. human dignity).\u201D (\\\
    [High-Level Expert Group on AI, 2019, p. 13]\\(zotero://select/groups/4907410/items/XPCD8D3T))\
    \ (\\[pdf]\\(zotero://open-pdf/groups/4907410/items/CPFPH28M?page=15&annotation=5CFBE37J))\n"
  Linked Principles:
  - reckb3cgfeDh1EeUP
  Linked Sources:
  - recnCULdYQ36cpZR7
  airtable_createdTime: '2023-05-28T18:58:31.000Z'
  airtable_id: recHEUOqkzQTNsAMw
  title: Consider tensions and absolute rights
- Description: "## \"Recommendations\nAssuming that well-designed affective systems\
    \ have a minimum subset of configurable norms incorporated in their knowledge\
    \ base:\n1\\.\_\_\_Affective systems should have capabilities to identify differences\
    \ between the values they are designed with and the differing values of those\
    \ with whom the systems are interacting.\n2\\.\_\_\_Where appropriate, affective\
    \ systems will adapt accordingly over time to better fit the norms of their users.\
    \ As societal values change, there needs to be a means to detect and accommodate\
    \ such cultural change in affective systems.\n3\\.\_\_\_Those actions undertaken\
    \ by an affective system that are most likely to generate an emotional response\
    \ should be designed to be easily changed in appropriate ways by the user without\
    \ being easily hacked by actors with malicious intentions. Similar to how software\
    \ today externalizes the language and vocabulary to be easily changeable based\
    \ on location, affective systems should externalize some\_of the core aspects\
    \ of their actions.\n## Further Resources\_\_\n- J. Bielby, \u201CComparative\
    \ Philosophies in Intercultural Information Ethics.\u201D _Confluence: Online\
    \ Journal of World Philosophies _2, no. 1, pp. 233\u2013253, 2015.\_\_\_\_\_\_\
    \_\_\_\_\n- M. Velasquez, C. Andre, T. Shanks, and M. J. Meyer. \u201C[Ethical\
    \ Relativism.](https://www.scu.edu/ethics/ethics-resources/ethical-decision-making/ethical-relativism/)\u201D\
    \ Markkula Center for Applied Ethics, Santa Clara, CA: Santa Clara University,\
    \ August 1, 1992.\n- Culture reflects the moral values and ethical norms governing\
    \ how people should behave and interact with others. \u201C[Ethics, an Overview](https://courses.lumenlearning.com/boundless-management/chapter/ethics-an-overview/).\u201D\
    \ Boundless Management.\n- T. Donaldson, \u201C[Values in Tension: Ethics Away\
    \ from Home Away from Home](https://hbr.org/1996/09/values-in-tension-ethics-away-from-home).\u201D\
    \ _Harvard Business Review. _September\u2013 October 1996.\n\n"
  Linked Challenges:
  - recIUf0R50ogibwe6
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T11:34:16.000Z'
  airtable_id: recHsUV9w4HKBA1w1
  title: Affective systems should be configurable to cultural context
- Description: "Including the principles (which are drawn from the Canadian model)\n\
    ## TCPS2 | \u201CReasons to Conduct Secondary Analyses of Data\u201D\n\u201CReasons\
    \ to conduct secondary analyses of data include: avoidance of duplication in primary\
    \ collection and the associated reduction of burdens on participants; corroboration\
    \ or criticism of the conclusions of the original project; comparison of change\
    \ in a research sample over time; application\_of new tests of hypotheses that\
    \ were not available\_at the time of original data collection; and confirmation\
    \ that the data are authentic. Privacy concerns and questions about the need to\
    \ seek consent arrive, however, when information provided for secondary use in\
    \ research can be linked to individuals, and when the possibility exists that\
    \ individuals can be identified in published reports,\_or through data linkage\u201D\
    \ (TCPS2, Chapter 5, D. Consent and Secondary Use of Identifiable Information\
    \ for Research Purposes).\n## TCPS2 | Article 5.5A\n\u201CIf a researcher satisfies\
    \ all the conditions in Article 5.5A (a) to (f), the REB may approve the research\
    \ without requiring consent from the individuals to whom the information relates.\n\
    **a.\_\_\_\_\_\_\_**identifiable information is essential to the\nresearch;\n\
    **b.\_\_\_\_\_\_**the use of identifiable information without the participants\u2019\
    \ consent is unlikely to adversely\_affect the welfare of individuals to whom\
    \ the information relates;\n**c.\_\_\_\_\_\_\_**the researchers will take appropriate\
    \ measures\_\nto protect the privacy of individuals, and to safeguard the identifiable\
    \ information;\n**d.\_\_\_\_\_\_**the researchers will comply with any known\n\
    preferences previously expressed by individuals about any use of their information;\n\
    **e.\_\_\_\_\_\_\_**it is impossible or impracticable to seek consent\nfrom individuals\
    \ to whom the information relates; and\n**f.\_\_\_\_\_\_\_\_**the researchers\
    \ have obtained any other necessary permission for secondary use of information\
    \ for research purposes\u201D (TCPS2, Chapter 5, D. Consent and Secondary Use\
    \ of Identifiable Information for Research Purposes).\n### Secondary analyses\
    \ of data \nSecondary analyses of data, also referred to herein as secondary use\
    \ of data consists of information originally collected for other purposes. Such\
    \ information might consist of student work, information obtained for program\
    \ evaluation, school records, or other identifiable materials collected for educational\
    \ or administrative purposes.\n**\_**\n## TCPS2 | Article 5.5B\n\u201CResearchers\
    \ shall seek REB review, but are not required to seek participant consent, for\
    \ research that relies exclusively on the secondary use of non-identifiable information\u201D\
    \ (TCPS2, Chapter 5, D. Consent and Secondary Use of Identifiable Information\
    \ for Research Purposes).\n### Key Principle\nApply the above principles of privacy,\
    \ and seek\_REB review even if your research involves data initially collected\
    \ for other reasons (e.g.,\_\u201Csecondary use of data\u201D).\n### Strategies\
    \ for Ethical Practice\n\u2022\_\_\_\_\_When possible, use anonymous data.\n\u2022\
    \_\_\_\_\_If generating anonymous data conflicts with your research question and\
    \ design, when possible, use data that has been de-identified.\n\u2022\_\_\_\_\
    \_Although seeking participant consent is not required for non-identifiable data\
    \ (Article 5.5B above), it is still good practice to seek students\u2019 consent\
    \ to use their data again.\n\u2022\_\_\_\_\_If the data you want to use is identifiable\
    \ and\_the REB requires that you seek students\u2019\_consent anyway, apply the\
    \ principles and strategies in \u201CConsent Processes,\u201D starting\_on page\
    \ 6 of this Guide.\n\u2022\_\_\_\_\_If you are emailing former students to seek\
    \ consent to use their previously generated information as data or for additional\
    \ information that may serve as data, be sensitive to general overuse of email\
    \ and full inboxes.\_\n\u2022\_\_\_\_\_Use a third party to collect consent for\
    \ research participants who are not your current students. Although this is not\
    \ required, using a third party is a good practice if these students want to enroll\
    \ in a future course you teach or ask you to serve on an advisory committee or\
    \ write a reference letter for them, etc.\n\u2022\_\_\_\_\_If you are contacting\
    \ former students to seek their consent to use their previously generated information\
    \ as data or for additional information that may serve as data (e.g., \u201Csecondary\
    \ use of data\u201D), be prepared to explain to the REB:\n**\u203A **why you want\
    \ to contact these former students,\n**\u203A **how the potential benefits of\
    \ this follow-up or additional data outweigh any drawbacks of contacting them,\n\
    **\u203A **who will be contacting the individuals and the nature of their relationship\
    \ with those students (e.g., a third party), and** **how they will be contacted\
    \ (Article 5.6).\n \nAll drawn from Fedoruk (2017)\n"
  Linked Principles:
  - recKdujFoPJr4ZAhZ
  - recQ9DIFEsOEkCx3O
  Linked Sources:
  - recQzldmBLByP78Uu
  Tags:
  - reflection-questions
  - education-research
  airtable_createdTime: '2023-05-19T13:25:25.000Z'
  airtable_id: recI5BV6v0BCK03VN
  title: Key discussion points for planning SoTL research regarding secondary analysis
    of data
- Description: "## Recommendations\nThe IEEE and other standards-setting bodies should\
    \ draw upon existing standards, empirical research, and expertise to identify\
    \ priorities\_and develop standards for the governance of\_A/IS research and partner\
    \ with relevant national agencies, and international organizations,\_when possible.\n\
    ## Further Resources\n\u2022\_\_\_\_\_S. R. Jordan, \u201CThe Innovation Imperative.\u201D\
    \ _Public Management Review _16, no. 1,\_pp. 67\u201389, 2014.\n\u2022\_\_\_\_\
    \_B. Schneiderman, \u201C[The Dangers of Faulty, Biased, or Malicious Algorithms\
    \ Requires Independent Oversight.](http://www.pnas.org/content/113/48/13538.long)\u201D\
    \ _Proceedings of the National Academy of Sciences of the United States of America\
    \ _113, no. 48, 13538\u201313540, 2016.\n\u2022\_\_\_\_\_J. Metcalf and K. Crawford,\
    \ \u201C[Where are Human Subjects in Big Data Research? The Emerging Ethics Divide](http://papers.ssrn.com/abstract%3D2779647).\u201D\
    \ _Big Data & Society,_ May 14, 2016._ _[Online]. Available: SSRN: [https://ssrn.\
    \ com/abstract=2779647](https://ssrn.com/abstract=2779647). [Accessed Nov. 1,\
    \ 2018].\n\u2022\_\_\_\_\_R. Calo, \u201C[Consumer Subject Review Boards: A Thought\
    \ Experiment](https://www.stanfordlawreview.org/online/privacy-and-big-data-consumer-subject-review-boards/).\u201D\
    \ _Stanford Law Review Online _66 97, Sept. 2013.\n"
  Linked Challenges:
  - recELObWGfkhXzFG2
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T10:23:45.000Z'
  airtable_id: recI7WdfNhrQxRt7F
  title: Identify priorities and develop standards for the governance of AI research
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CSocietal and environmental well-being \n\u201CSocial impact:\
    \ \n\uF0FC In case the AI system interacts directly with humans:\n\uF0A7 Did you\
    \ assess whether the AI system encourages humans to develop attachment and empathy\
    \ towards the system? \n\uF0A7 Did you ensure that the AI system clearly signals\
    \ that its social interaction is simulated and that it\u201D (High-Level Expert\
    \ Group on AI, 2019, p. 30)\n"
  Linked Principles:
  - recmzjcGKv3yNOxbl
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:45:48.000Z'
  airtable_id: recJMrMEZnz9rYlnD
  title: Considerations in assessing trustworthy AI - Social impact
- Description: "\u201CHow are findings presented? \xA7 What immediate or future risk\
    \ might occur by using exact-quoted material in published reports? (For example,\
    \ while a participant might not think his or her information is sensitive now,\
    \ this might change in five years. What protections might be put in place to anticipate\
    \ changing perceptions?22) \xA7 Are individuals adequately protected in pre-publication\
    \ reports, such as workshops, conferences, or informal meetings?23 \xA7 Could\
    \ materials be restricted because of copyright? (For example, many countries have\
    \ strong restrictions on using screenshots or images taken from the web without\
    \ permission. Certain sites have restrictions in their terms of service. Whereas\
    \ there may be allowances for the scholarly use of copyrighted materials without\
    \ permission, such as the U.S. doctrine of fair use, this is not a guarantee of\
    \ protection against copyright infringement.)\u201D (Markham and Buchanan, 2012,\
    \ p. 10)\n"
  Linked Principles:
  - rec42P8U9usfYCtv9
  - recSqx6wklVpDzx3s
  - recPg7Ov0priGGtLm
  Linked Sources:
  - recQiVQ7CTC72xp6O
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T08:22:58.000Z'
  airtable_id: recJQF6QfQGlcSzDm
  title: Questions to consider regarding re-identification and issues of justice in
    IP
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CAccountability \n\u201CMinimising and reporting negative Impact:\
    \ \n\uF0FC Did you carry out a risk or impact assessment of the AI system, which\
    \ takes into account different stakeholders that are (in)directly affected? \n\
    \uF0FC Did you provide training and education to help developing accountability\
    \ practices? \n\uF0A7 Which workers or branches of the team are involved? Does\
    \ it go beyond the development phase?\n\uF0A7 Do these trainings also teach the\
    \ potential legal framework applicable to the AI system?\n\uF0A7 Did you consider\
    \ establishing an \u2018ethical AI review board\u2019 or a similar mechanism to\
    \ discuss overall accountability and ethics practices, including potentially unclear\
    \ grey areas? \n\uF0FC Did you foresee any kind of external guidance or put in\
    \ place auditing processes to oversee ethics and accountability, in addition to\
    \ internal initiatives? \n\uF0FC Did you establish processes for third parties\
    \ (e.g. suppliers, consumers, distributors/vendors) or workers to report potential\
    \ vulnerabilities, risks or biases in the AI system?\u201D (High-Level Expert\
    \ Group on AI, 2019, p. 31)\n\n## IEEE recommendations\n**Issue:** Oversight for\
    \ algorithms\n## Background\nThe algorithms behind A/IS are not subject to consistent\
    \ oversight. This lack of assessment causes concern because end users have no\
    \ account of how a certain algorithm or system came to its conclusions. These\
    \ recommendations are similar to those made in the \u201CGeneral Principles\u201D\
    \ and \u201CEmbedding Values into Autonomous and Intelligent Systems\u201D chapters\
    \ of _Ethically Aligned Design_, but here the recommendations are used as they\
    \ apply to the narrow scope of this chapter .\n## Recommendations\nAccountability:\
    \ As touched on in the General Principles chapter of _Ethically Aligned Design_,\
    \ algorithmic transparency is an issue of concern. It is understood that specifics\
    \ relating to algorithms or systems contain intellectual property that cannot,\
    \ or will not, be released to the general public. Nonetheless, standards providing\
    \ oversight of the manufacturing process of A/IS technologies need to be created\
    \ to avoid harm and negative consequences. We can look to other technical domains,\
    \ such as biomedical, civil, and aerospace engineering, where commercial protections\
    \ for proprietary technology are routinely and effectively balanced with the need\
    \ for appropriate oversight standards and mechanisms to safeguard the public.Human\
    \ rights and algorithmic impact assessments should be explored as a meaningful\
    \ way to improve the accountability of A/IS.\_These need to be paired with public\
    \ consultations, and the final impact\_assessments must be made public.\n## Further\
    \ Resources\n\u2022\_\_\_\_\_F. Pasquale, The Black Box Society: The Secret Algorithms\
    \ That Control Money and Information. Cambridge, MA: Harvard University Press,\
    \ 2016.\n\u2022\_\_\_\_\_R. Calo, \u201CArtificial Intelligence Policy: A Primer\
    \ and Roadmap,\u201D _UC Davis Law Review,_ 52: pp. 399\u2013435, 2017.\n\u2022\
    \_\_\_\_\_ARTICLE 19. \u201CPrivacy and Freedom of Expression in the Age of Artificial\
    \ Intelligence,\u201D Privacy International, April 2018. [Online]. Available:\
    \ [https://www.article19.org/wpcontent/uploads/2018/04/Privacy-andFreedom-of-Expression-In-the-Age-of-ArtificialIntelligence-1.pdf.](https://www.article19.org/wp-content/uploads/2018/04/Privacy-and-Freedom-of-Expression-In-the-Age-of-Artificial-Intelligence-1.pdf)\
    \ [Accessed October 28, 2018].\n\np.132-133\n"
  Linked Challenges:
  - recX6r1O4jcsp0nIM
  Linked Principles:
  - recKdujFoPJr4ZAhZ
  - recxcFmvPG5wrCqpO
  - rec42P8U9usfYCtv9
  - reczVPIH1y2OMpAJH
  Linked Sources:
  - recnCULdYQ36cpZR7
  - recpXl48pJdKDhc6f
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:48:37.000Z'
  airtable_id: recJyzFLE9ry4YEbb
  title: Considerations in assessing trustworthy AI - Minimising and reporting negative
    impacts
- Description: "\u201CWhat are potential benefits associated with this study? \xA7\
    \ Who benefits from the study - do the potential participants? If not, what greater\
    \ benefit justifies the potential risks? \xA7 Is the research aiming at a good\
    \ or desirable goal? \xA7 Can we be sure the data collected from online sites,\
    \ fora, communities, is \u201Clegitimate\u201D and \u201Cvaluable\u201D?27\u201D\
    \ (Markham and Buchanan, 2012, p. 11)\n"
  Linked Principles:
  - rec6O9e1nYBJtQUTj
  - recMGB4iC5oaCtr5x
  - recgDkzdE9dfpTxCK
  Linked Sources:
  - recQiVQ7CTC72xp6O
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T08:27:06.000Z'
  airtable_id: recKDgUEDD2f1egyd
  title: Questions to consider in assessing benefits
- Description: "\"Beyond safeguarding the sustainability of your AI project as it\
    \ relates to its social impacts on individual wellbeing and public welfare, your\
    \ project team must also confront the related challenge of technical sustainability\
    \ or safety. A technically sustainable AI system is safe, accurate, reliable,\
    \ secure, and robust. Securing these goals, however, is a difficult and unremitting\
    \ task.\n\_\nBecause AI systems operate in a world filled with uncertainty, volatility,\
    \ and flux, the challenge of building safe and reliable AI can be especially daunting.\
    \ This job, however, must be met head-on. Only by making the goal of producing\
    \ safe and reliable AI technologies central to your project, will you be able\
    \ to mitigate risks of your system failing at scale when faced with real-world\
    \ unknowns and unforeseen events. The issue of AI safety is of paramount importance,\
    \ because these potential failures may both produce harmful outcomes and undermine\
    \ public trust.\n\_\nIn order to safeguard that your AI system functions safely,\
    \ you must prioritise the technical objectives of accuracy, reliability, security,\
    \ and robustness. This requires that your technical team put careful forethought\
    \ into how to construct a system that accurately and dependably operates in accordance\
    \ with its designers\u2019 expectations even when confronted with unexpected changes,\
    \ anomalies, and perturbations. Building an AI system that meets these safety\
    \ goals also requires rigorous testing, validation, and re-assessment as well\
    \ as the integration of adequate mechanisms of oversight and control into its\
    \ real-world operation.\n\_\n_Accuracy, reliability, security, and robustness_\n\
    _\__\nIt is important that you gain a strong working knowledge of each of the\
    \ safety relevant operational objectives (accuracy, reliability, security, and\
    \ robustness):\n\_\n\xB7\_\_\_\_\_\_Accuracy and Performance Metrics: In machine\
    \ learning, the accuracy of a model is the proportion of examples for which it\
    \ generates a correct output. This performance measure is also sometimes characterised\
    \ conversely as an error rate or the fraction of cases for which the model produces\
    \ an incorrect output. Keep in mind that, in some instances, the choice of an\
    \ acceptable error rate or accuracy level can be adjusted in accordance with the\
    \ use case specific needs of the application. In other instances, it may be largely\
    \ set by a domain established benchmark.\n\n As a performance metric, accuracy\
    \ should be a central component to establishing and nuancing your team\u2019s\
    \ approach to safe AI. That said, specifying a reasonable performance level for\
    \ your system may also often require you to refine or exchange your measure of\
    \ accuracy. For instance, if certain errors are more significant or costly than\
    \ others, a metric for total cost can be integrated into your model so that the\
    \ cost of one class of errors can be weighed against that of another. Likewise,\
    \ if the precision and sensitivity of the system in detecting uncommon events\
    \ is a priority (say, in instances of the medical diagnosis of rare cases of a\
    \ disease), you can use the technique of precision and recall. This method of\
    \ addressing imbalanced classification would allow you to weigh the proportion\
    \ of the system\u2019s correct detections\u2014both of frequent and of rare outcomes\u2014\
    against the proportion of actual detections of the rare event (i.e. the ratio\
    \ of the true detections of the rare outcome to the sum of the true detections\
    \ of that outcome and the missed detections or false negatives for that outcome).\n\
    \_\nIn general, measuring accuracy in the face of uncertainty is a challenge that\
    \ must be given significant thought. The confidence level of your AI system will\
    \ depend heavily on problems inherent in attempts to model a chaotic and changing\
    \ reality. Concerns about accuracy must cope with issues of unavoidable noise\
    \ present in the data sample, architectural uncertainties generated by the possibility\
    \ that a given model is missing relevant features of the underlying distribution,\
    \ and inevitable changes in input data over time.\n\_\n\xB7\_\_\_\_\_\_Reliability:\
    \ The objective of reliability is that an AI system behaves exactly as its designers\
    \ intended and anticipated. A reliable system adheres to the specifications it\
    \ was programmed to carry out. Reliability is therefore a measure of consistency\
    \ and can establish confidence in the safety of a system based upon the dependability\
    \ with which it operationally conforms to its intended functionality.\n\_\n\xB7\
    \_\_\_\_\_\_Security: The goal of security encompasses the protection of several\
    \ operational dimensions of an AI system when confronted with possible adversarial\
    \ attack. A secure system is capable of maintaining the integrity of the information\
    \ that constitutes it. This includes protecting its architecture from the unauthorised\
    \ modification or damage of any of its component parts. A secure system also remains\
    \ continuously functional and accessible to its authorised users and keeps confidential\
    \ and private information secure even under hostile or adversarial conditions.\n\
    \_\n\xB7\_\_\_\_\_\_Robustness: The objective of robustness can be thought of\
    \ as the goal that an AI system functions reliably and accurately under harsh\
    \ conditions. These conditions may include adversarial intervention, implementer\
    \ error, or skewed goal-execution by an automated learner (in reinforcement learning\
    \ applications). The measure of robustness is therefore the strength of a system\u2019\
    s integrity and the soundness of its operation in response to difficult conditions,\
    \ adversarial attacks, perturbations, data poisoning, and undesirable reinforcement\
    \ learning behaviour.\n\_\n\_\n_Risks posed to accuracy and reliability:_\n\n\
    \ Concept Drift: Once trained, most machine learning systems operate on static\
    \ models of the world that have been built from historical data which have become\
    \ fixed in the systems\u2019 parameters. This freezing of the model before it\
    \ is released \u2018into the wild\u2019 makes its accuracy and reliability especially\
    \ vulnerable to changes in the underlying distribution of data. When the historical\
    \ data that have crystallised into the trained model\u2019s architecture cease\
    \ to reflect the population concerned, the model\u2019s mapping function will\
    \ no longer be able to accurately and reliably transform its inputs into its target\
    \ output values. These systems can quickly become prone to error in unexpected\
    \ and harmful ways.\n\_\nThere has been much valuable research done on methods\
    \ of detecting and mitigating concept and distribution drift, and you should consult\
    \ with your technical team to ensure that its members have familiarised themselves\
    \ with this research and have sufficient knowledge of the available ways to confront\
    \ the issue. In all cases, you should remain vigilant to the potentially rapid\
    \ concept drifts that may occur in the complex, dynamic, and evolving environments\
    \ in which your AI project will intervene. Remaining aware of these transformations\
    \ in the data is crucial for safe AI, and your team should actively formulate\
    \ an action plan to anticipate and to mitigate their impacts on the performance\
    \ of your system.\n\_\nBrittleness: Another possible challenge to the accuracy\
    \ and reliability of machine learning systems arises from the inherent imitations\
    \ of the systems themselves. Many of the high- performing machine learning models\
    \ such as deep neural nets (DNN) rely on massive amounts of data and brute force\
    \ repetition of training examples to tune the thousands, millions, or even billions\
    \ of parameters, which collectively generate their outputs.\n\_\nHowever, when\
    \ they are actually running in an unpredictable world, these systems may have\
    \ difficulty processing unfamiliar events and scenarios. They may make unexpected\
    \ and serious mistakes, because they have neither the capacity to contextualise\
    \ the problems they are programmed to solve nor the common-sense ability to determine\
    \ the relevance of new\n\u2018unknowns\u2019. Moreover, these mistakes may remain\
    \ unexplainable given the high- dimensionality and computational complexity of\
    \ their mathematical structures. This fragility or brittleness can have especially\
    \ significant consequences in safety-critical applications like fully automated\
    \ transportation and medical decision support systems where undetectable changes\
    \ in inputs may lead to significant failures. While progress is being made in\
    \ finding ways to make these models more robust, it is crucial to consider safety\
    \ first when weighing up their viability.\n\_\n_Risks posed to security and robustness_\n\
    _\__\nAdversarial Attack: Adversarial attacks on machine learning models maliciously\
    \ modify input data\u2014often in imperceptible ways\u2014to induce them into\
    \ misclassification or incorrect prediction. For instance, by undetectably altering\
    \ a few pixels on a picture, an adversarial attacker can mislead a model into\
    \ generating an incorrect output (like identifying a panda as a gibbon or a \u2018\
    stop\u2019 sign as a \u2018speed limit\u2019 sign) with an extremely high confidence.\
    \ While a good amount of attention has been paid to the risks that adversarial\
    \ attacks pose in deep learning applications like computer vision, these kinds\
    \ of perturbations are also effective across a vast range of machine learning\
    \ techniques and uses such as spam filtering and malware detection.\n\n These\
    \ vulnerabilities of AI systems to adversarial examples have serious consequences\
    \ for AI safety. The existence of cases where subtle but targeted perturbations\
    \ cause models to be misled into gross miscalculation and incorrect decisions\
    \ have potentially serious safety implication for the adoption of critical systems\
    \ like applications in autonomous transportation, medical imaging, and security\
    \ and surveillance. In response to concerns about the threats posed to a safe\
    \ and trusted environment for AI technologies by adversarial attacks a field called\
    \ adversarial machine learning has emerged over the past several years.\nWork\
    \ in this area focuses on securing systems from disruptive perturbations at all\
    \ points of vulnerability across the AI pipeline.\n\_\nOne of the major safety\
    \ strategies that has arisen from this research is an approach called model hardening,\
    \ which has advanced techniques that combat adversarial attacks by strengthening\
    \ the architectural components of the systems. Model hardening techniques may\
    \ include adversarial training, where training data is methodically enlarged to\
    \ include adversarial examples. Other model hardening methods involve architectural\
    \ modification, regularisation, and data pre-processing manipulation. A second\
    \ notable safety strategy is run- time detection, where the system is augmented\
    \ with a discovery apparatus that can identify and trace in real-time the existence\
    \ of adversarial examples. You should consult with members of your technical team\
    \ to ensure that the risks of adversarial attack have been taken into account\
    \ and mitigated throughout the AI lifecycle. A valuable collection of resources\
    \ to combat adversarial attack can be found at [https://github.com/IBM/adversarial-](https://github.com/IBM/adversarial-robustness-toolbox)\
    \ [robustness-toolbox](https://github.com/IBM/adversarial-robustness-toolbox)\
    \ .\n\_\nData Poisoning: A different but related type of adversarial attack is\
    \ called data poisoning. This threat to safe and reliable AI involves a malicious\
    \ compromise of data sources at the point of collection and pre-processing. Data\
    \ poisoning occurs when an adversary modifies or manipulates part of the dataset\
    \ upon which a model will be trained, validated, and tested. By altering a selected\
    \ subset of training inputs, a poisoning attack can induce a trained AI system\
    \ into curated misclassification, systemic malfunction, and poor performance.\
    \ An especially concerning dimension of targeted data poisoning is that an adversary\
    \ may introduce a\n\u2018backdoor\u2019 into the infected model whereby the trained\
    \ system functions normally until it\nprocesses maliciously selected inputs that\
    \ trigger error or failure.\n\_\nIn order to combat data poisoning, your technical\
    \ team should become familiar with the state of the art in filtering and detecting\
    \ poisoned data. However, such technical solutions are not enough. Data poisoning\
    \ is possible because data collection and procurement often involves potentially\
    \ unreliable or questionable sources. When data originates in uncontrollable environments\
    \ like the internet, social media, or the Internet of Things, many opportunities\
    \ present themselves to ill-intentioned attackers, who aim to manipulate training\
    \ examples. Likewise, in third- party data curation processes (such as \u2018\
    crowdsourced\u2019 labelling, annotation, and content identification), attackers\
    \ may simply handcraft malicious inputs.\nYour project team should focus on the\
    \ best practices of responsible data management, so that they are able to tend\
    \ to data quality as an end-to-end priority.\n\_\n\xB7\_\_\_\_\_\_Misdirected\
    \ Reinforcement Learning Behaviour: A different set of safety risks emerges from\
    \ the approach to machine learning called reinforcement learning (RL). In the\
    \ more widely\n\n applied methods of supervised learning that have largely been\
    \ the focus of this guide, a model transforms inputs into outputs according to\
    \ a fixed mapping function that has resulted from its passively received training.\
    \ In RL, by contrast, the learner system actively solves problems by engaging\
    \ with its environment through trial and error. This exploration and\n\u2018problem-solving\u2019\
    \ behaviour is determined by the objective of maximising a reward function that\
    \ is defined by its designers.\n\_\nThis flexibility in the model, however, comes\
    \ at the price of potential safety risks. An RL system, which is operating in\
    \ the real-world without sufficient controls, may determine a reward-optimising\
    \ course of action that is optimal for achieving its desired objective but harmful\
    \ to people. Because these models lack context-awareness, common sense, empathy,\
    \ and understanding, they are unable to identify, on their own, scenarios that\
    \ may have damaging consequences but that were not anticipated and constrained\
    \ by their programmers. This is a difficult problem, because the unbounded complexity\
    \ of the world makes anticipating all of its pitfalls and detrimental variables\
    \ veritably impossible.\n\_\nExisting strategies to mitigate such risks of misdirected\
    \ reinforcement learning behaviour include:\no\_\_Running extensive simulations\
    \ during the testing stage, so that appropriate measures of constraint can be\
    \ programmed into the system\no\_\_Continuous inspection and monitoring of the\
    \ system, so that its behaviour can be better predicted and understood\no\_\_\
    Finding ways to make the system more interpretable so that its decisions can be\
    \ better assessed\no\_\_Hard-wiring mechanisms into the system that enable human\
    \ override and system shut-down\n\_\nEnd-to-End AI Safety\n\_\nThe safety risks\
    \ you face in your AI project will depend, among other factors, on the sort of\
    \ algorithm(s) and machine learning techniques you are using, the type of applications\
    \ in which those techniques are going to be deployed, the provenance of your data,\
    \ the way you are specifying your objective, and the problem domain in which that\
    \ specification applies. As a best practice, regardless of this variability of\
    \ techniques and circumstances, safety considerations of accuracy, reliability,\
    \ security, and robustness should be in operation at every stage of your AI project\
    \ lifecycle.\n\_\nThis should involve both rigorous protocols of testing, validating,\
    \ verifying, and monitoring the safety of the system and the performance of AI\
    \ safety self-assessments by relevant members of your team at each stage of the\
    \ workflow. Such self-assessments should evaluate how your team\u2019s design\
    \ and implementation practices line up with the safety objectives of accuracy,\
    \ reliability, security, and robustness. Your AI safety self-assessments should\
    \ be logged across the workflow on a single document in a running fashion that\
    \ allows review and re-assessment.\"\n(Leslie, 2019, p26-30)\n\n### IEEE Report\n\
    \"Operational failures and, in particular, violations of a system\u2019s embedded\
    \ community norms, are unavoidable, both during system testing and during deployment.\
    \ Not only are implementations never perfect, but A/IS with embedded norms will\
    \ update or expand their norms over time (see Section 1, Issue 2) and interactions\
    \ in the social world are particularly complex and uncertain. Thus, prevention\
    \ and mitigation strategies must be adopted, and we sample four possible ones.\n\
    First, anticipating the process of evaluation during the implementation phase\
    \ requires defining criteria and metrics for such evaluation, which in turn better\
    \ allows the detection and mitigation of failures. Metrics will include:\n\u2022\
    \_\_\_\_\_Technical variables, such as traceability and verifiability,\n\u2022\
    \_\_\_\_\_User-level variables such as reliability, understandable explanations,\
    \ and responsiveness to feedback, and\n\u2022\_\_\_\_\_Community-level variables\
    \ such as justified trust (see Issue 2) and the collective belief that A/IS are\
    \ generally creating social benefits rather than, for example, technological unemployment.\n\
    Second, a systematic risk analysis and management approach can be useful (Oetzel\
    \ and Spiekermann 201443) for an application to privacy norms. This approach tries\
    \ to anticipate potential points of failure, e.g., norm violations, and, where\
    \ possible, develops some ways to reduce or remove the effects of failures. Successful\
    \ behavior, and occasional failures, can then iteratively improve predictions\
    \ and\_mitigation attempts.Third, because not all risks and failures are predictable\
    \ (Brundage et al 201844; Vanderelst and Winfield 201845), especially in complex\
    \ human-machine interactions in social contexts, additional mitigation mechanisms\
    \ must be made available. Designers are strongly encouraged to augment the architectures\
    \ of their systems with components that handle unanticipated norm violations with\
    \ a fail-safe, such as the symbolic \u201Cgateway\u201D agents discussed in Section\
    \ 2, Issue 1. Designers should identify a number of strict laws, that is, task-\
    \ and community-specific norms that should never be violated, and the failsafe\
    \ components should continuously monitor operations against possible violations\
    \ of these laws. In case of violations, the higher-order gateway agent should\
    \ take appropriate actions, such as safely disabling the system\u2019s operation,\
    \ or greatly limiting its scope of operation, until\nthe source of failure is\
    \ identified. The failsafe components need to be understandable, extremely reliable,\
    \ and protected against security breaches, which can be achieved, for example,\
    \ by validating them carefully and not letting them adapt their parameters during\
    \ execution.\nFourth, once failures have occurred, responsible entities, e.g.,\
    \ corporate, government, science, and engineering, shall create a publicly accessible\
    \ database with undesired outcomes caused by specific A/IS systems. The database\
    \ would include descriptions of the problem, background information on how the\
    \ problem was detected, which context it occurred in, and how it was addressed.\n\
    In summary, we offer the following recommendation.\n## Recommendation\nBecause\
    \ designers and developers cannot anticipate all possible operating conditions\
    \ and potential failures of A/IS, multiple strategies to mitigate the chance and\
    \ magnitude of harm\_must be in place.\n### Further Resources\n\u2022\_\_\_\_\_\
    M. Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garfunkel, A. Dafoe,\
    \ P. Scharre, T. Zeitzo, et al. \" \u201CThe Malicious Use of Artificial Intelligence:\
    \ Forecasting, Prevention, and Mitigation,\u201D CoRR abs/1802.07228 [cs.AI].\
    \ 2018. <https://arxiv.org/abs/1802.07228>\n\u2022\_\_\_\_\_M. C. Oetzel and S.\
    \ Spiekermann, \u201CA Systematic Methodology for Privacy Impact Assessments:\
    \ A Design Science Approach.\u201D _European Journal of Information Systems_,\
    \ vol._ _23, pp. 126\u2013150, 2014. [https://link.springer. com/article/10.1057/ejis.2013.18](https://link.springer.com/article/10.1057/ejis.2013.18)\n\
    \u2022\_\_\_\_\_D. Vanderelst and A.F. Winfield, 2018 \u201CThe Dark Side of Ethical\
    \ Robots,\u201D In Proc. The First AAAI/ACM Conf. on Artificial Intelligence,\
    \ Ethics and Society, New Orleans, LA, Feb. 1 -3, 2018.\"\np.180-181\n"
  Linked Principles:
  - recOHnq45Fq7YWsRO
  - recy6hrMpKZ7TOn3Q
  Linked Sources:
  - recfYC5jjPmpLfSlM
  - recpXl48pJdKDhc6f
  Tags:
  - reflection-discussion
  airtable_createdTime: '2023-05-19T11:51:22.000Z'
  airtable_id: recKWkdYO98PeEKDz
  title: Principle of Safety, key considerations
- Description: "## **\"**Recommendations\nThe potential for A/IS to contribute to\
    \ humanitarian action to save and improve lives should be prioritized for research\
    \ and development, including by organizing global research challenges, while also\
    \ building in safeguards to protect the creation, collection, processing, sharing,\
    \ use, and disposal of information, including data from and about individuals\
    \ and populations. Specific recommendations include:\n\u2022\_\_\_\_\_Promoting\
    \ awareness of the vulnerable condition of certain communities around the globe\
    \ and the need to develop and use A/IS applications for humanitarian purposes.\n\
    \u2022\_\_\_\_\_Elaborating competitions and challenges in high impact conferences\
    \ and university hackathons to engage both technical and nontechnical communities\
    \ in the development of A/IS for humanitarian purposes and to address social issues.\n\
    \u2022\_\_\_\_\_Support civil society groups who organize themselves for the purpose\
    \ of A/IS research and advocacy to develop applications to benefit humanitarian\
    \ causes.39\n\u2022\_\_\_\_\_Developing and applying ethical standards for the\
    \ collection, use, sharing, and disposal of data in fragile settings.\n\u2022\_\
    \_\_\_\_Following privacy protection frameworks for pressing humanitarian situations\
    \ that ensure the most vulnerable are protected.40\_\n\u2022\_\_\_\_\_Setting\
    \ up clear ethical frameworks for exceptional use of A/IS technologies in lifesaving\
    \ humanitarian situations, compared\_to \"normal\" situations.41\n\u2022\_\_\_\
    \_\_Stimulating the development of low-cost\_and open source solutions based on\
    \ A/IS\_to address specific humanitarian problems.\n\u2022\_\_\_\_\_Training A/IS\
    \ experts in humanitarian action and norms, and humanitarian practitioners\_to\
    \ catalyze collaboration in designing,\_piloting, developing, and implementing\_\
    A/IS technologies for humanitarian purposes. Forging public-private A/IS participant\
    \ alliances that develop crisis scenarios in advance.\n\u2022\_\_\_\_\_Working\
    \ on cultural and contextual acceptance of any A/IS introduced during emergencies.\n\
    \u2022\_\_\_\_\_Documenting and developing quantifiable metrics for evaluating\
    \ the outcomes of humanitarian digital projects, and educating the humanitarian\
    \ ecosystem on the same.\n## Further resources\n\u2022\_\_\_\_\_E. Prestes et\
    \ al., \"The 2016 Humanitarian Robotics and Automation Technology Challenge [Competitions],\"\
    \ in _IEEE Robotics & Automation Magazine_, vol. 23, no. 3, pp. 23-24, Sept. 2016.\
    \ [http://ieeexplore.ieee.org/stamp/ stamp.jsp?tp=&arnumber=7565695&isnumber=7565655](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7565695&isnumber=7565655)\n\
    \u2022\_\_\_\_\_L. Marques et al., \"Automation of humanitarian demining: The\
    \ 2016 Humanitarian Robotics and Automation Technology Challenge,\" _2016 International\
    \ Conference on Robotics and Automation for Humanitarian Applications (RAHA)_,\
    \ Kollam, 2016, pp. 1-7. <http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7931893&isnumber=7931858>\n\
    \u2022\_\_\_\_\_CYBATHLON 2020 Preliminary Race Task Descriptions [http://www.cybathlon.ethz.\
    \ ch/cybathlon-2020/preliminary-race-taskdescriptions.html](http://www.cybathlon.ethz.ch/cybathlon-2020/preliminary-race-task-descriptions.html)\n\
    \u2022\_\_\_\_\_CYBATHLON Scientific Publications\_<http://www.cybathlon.ethz.ch/>\n\
    \u2022\_\_\_\_\_Immigration Policy Lab (IPL), \u201CHarnessing Big Data to Improve\
    \ Refugee Resettlement\u201D [https://immigrationlab.org/project/harnessingbig-data-to-improve-refugee-resettlement/](https://immigrationlab.org/project/harnessing-big-data-to-improve-refugee-resettlement/)\n\
    \u2022\_\_\_\_\_Harvard Humanitarian Initiative, _The Signal Code_, [https://signalcode.org](https://signalcode.org/)\n\
    \_\u2022\_\_\_\_\_J.A. Quinn, et al., \u201CHumanitarian applications of machine\
    \ learning with remote-sensing data: review and case study in refugee settlement\
    \ mapping\u201D Philosophical Transactions of the Royal Society A, 376 20170363;\
    \ DOI:\n10.1098/rsta.2017.0363. Aug. 6, 2018.\n\u2022\_\_\_\_\_Humanitarian Innovation\
    \ Guide: [https:// higuide.elrha.org/,](https://higuide.elrha.org/) 2019.\n\u2022\
    \_\_\_\_\_P. Meier, [Digital Humanitarians: How Big Data is Changing the Face\
    \ of Humanitarian Response](http://cds.cern.ch/record/2123110). Florida: CRC Press,\
    \ 2015.\n\u2022\_\_\_\_\_\u201CTechnology for human rights: UN Human Rights Office\
    \ announces landmark partnership with Microsoft\u201D [https://www.ohchr.org/\
    \ EN/NewsEvents/Pages/DisplayNews. aspx?NewsID=21620&LangID=E](https://www.ohchr.org/EN/NewsEvents/Pages/DisplayNews.aspx?NewsID=21620&LangID=E)\n\
    \u2022\_\_\_\_\_M. Luengo-Oroz, \u201C10 big data science challenges facing humanitarian\
    \ organizations,\u201D UNHCR, Nov. 22, 2016. [http://www. unhcr.org/innovation/10-big-data-sciencechallenges-facing-humanitarian-organizations/](http://www.unhcr.org/innovation/10-big-data-science-challenges-facing-humanitarian-organizations/)\n\
    \u2022\_\_\_\_\_Optic Technologies, Press Release, Vatican Hack 2018\u2014Results,\
    \ 18 March 2018, which announced winning AI applications to benefit migrants and\
    \ refugees as well as social inclusion and interfaith dialogue,\_[http://optictechnology.org/index.php/en/news-en/151-vhack-2018winners-en\
    \ ](http://optictechnology.org/index.php/en/news-en/151-vhack-2018winners-en)\"\
    \n\np157-159\n"
  Linked Challenges:
  - recmRF2P1OOASMfNF
  airtable_createdTime: '2023-06-05T12:03:31.000Z'
  airtable_id: recMgYQ7EqHAyDfi1
  title: Consider developing strategies for specific context of humanitarian action
- Description: "\u201CHow is the context (venue/participants/data) being accessed?\
    \ \xA7 How are participants / authors situated in the context? \xA7 How are participants/authors\
    \ approached by the researcher? \xA7 How is the researcher situated in the context?15\
    \ \xA7 If access to an online context is publicly available, do members/participants/authors\
    \ perceive the context to be public? What considerations might be necessary to\
    \ accommodate \u2018perceived privacy\u201916 or the notion that individuals might\
    \ care more about the appropriate flow of information as defining it as public\
    \ or private17?\u201D (Markham and Buchanan, 2012, p. 8-9)\n"
  Linked Principles:
  - recKdujFoPJr4ZAhZ
  Linked Sources:
  - recQiVQ7CTC72xp6O
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-18T13:54:19.000Z'
  airtable_id: recN2Lw4yXDBWLSYv
  title: Questions to consider regarding access to a context (and data) and the perceptions
    and autonomy of those related to that context regarding access to it
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION)\n 1\\. Human agency and oversight \nFundamental rights: \n\uF0FC Did\
    \ you carry out a fundamental rights impact assessment where there could be a\
    \ negative impact on fundamental rights? \nDid you identify and document potential\
    \ trade-offs made between the different principles and rights? \n\uF0FC Does the\
    \ AI system interact with decisions by human (end) users (e.g. recommended actions\
    \ or decisions to take, presenting of options)? \n\uF0A7 Could the AI system affect\
    \ human autonomy by interfering with the (end) user\u2019s decision-making process\
    \ in an unintended way? \n\uF0A7 Did you consider whether the AI system should\
    \ communicate to (end) users that a decision, content, advice or outcome is the\
    \ result of an algorithmic decision? \n\uF0A7 In case of a chat bot or other conversational\
    \ system, are the human end users made aware that they are interacting with a\
    \ non-human agent?\u201D (High-Level Expert Group on AI, 2019, p. 26)\n\u201C\uF0FC\
    \ Is there is a self-learning or autonomous AI system or use case? If so, did\
    \ you put in place more specific mechanisms of control and oversight? \n\uF0A7\
    \ Which detection and response mechanisms did you establish to assess whether\
    \ something could go wrong\u201D (High-Level Expert Group on AI, 2019, p. 26)\n\
    \u201C\uF0A7 Did you ensure a stop button or procedure to safely abort an operation\
    \ where needed? Does this procedure abort the process entirely, in part, or delegate\
    \ control to a human?\u201D (High-Level Expert Group on AI, 2019, p. 27)\n"
  Linked Principles:
  - recKdujFoPJr4ZAhZ
  - recQEiU22Qy1E0YuA
  - rec6tz9Phzck0hvT8
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:21:28.000Z'
  airtable_id: recNHvwgyFPXERuJ8
  title: Considerations in assessing trustworthy AI - Human oversight
- Description: "\u201CAs part of this minimum safeguarding of discriminatory non-harm,\
    \ forethought and well-informed consideration must be put into how you are going\
    \ to define and measure the fairness of the impacts and outcomes of the AI system\
    \ you are developing.\nThere is a great diversity of beliefs in the area of outcome\
    \ fairness as to how to properly classify what makes the consequences of an algorithmically\
    \ supported decision equitable, fair, and allocatively just. Different approaches\u2014\
    detailed below\u2014stress different principles: some focus on demographic parity,\
    \ some on individual fairness, others on error rates equitably distributed across\
    \ subpopulations.\nYour determination of outcome fairness should heavily depend\
    \ both on the specific use case for which the fairness of outcome is being considered\
    \ and the technical feasibility of incorporating your chosen criteria into the\
    \ construction of the AI system. (Note that different fairness-aware methods involve\
    \ different types of technical interventions at the pre-processing, modelling,\
    \ or postprocessing stages of production). Again, this means that determining\
    \ your fairness definition should be a cooperative and multidisciplinary effort\
    \ across the project team.\nYou will find below a summary table of some of the\
    \ main definitions of outcome fairness that have been integrated by researchers\
    \ into formal models as well as a list of current articles and technical resources,\
    \ which should be consulted to orient your team to the relevant knowledge base.\
    \ (Note that this is a rapidly developing field, so your technical team should\
    \ keep updated about further advances.) The first four fairness types fall under\
    \ the category of group fairness and allow for comparative criteria of non-discrimination\
    \ to be considered in model construction and evaluation. The final two fairness\
    \ types focus instead on cases of individual fairness, where context-specific\
    \ issues of effective bias are considered and assessed at the level of the individual\
    \ agent.\nTake note, though, that these technical approaches have limited scope\
    \ in terms of the bigger picture issues of algorithmic fairness that we have already\
    \ stressed. Many of the formal approaches work only in use cases that have distributive\
    \ or allocative consequences. In order to carry out group comparisons, these approaches\
    \ require access to data about sensitive/protected attributes (that may often\
    \ be unavailable or unreliable) as well as accurate demographic information about\
    \ the underlying population distribution. Furthermore, there are unavoidable trade-offs\
    \ and inconsistences between these technical definitions that must be weighed\
    \ in determining which of them are best fit for your use case. Consult those on\
    \ your project team with the technical expertise to consider the use case appropriateness\
    \ of a desired formal approach.\u201D (Leslie, 2019, p. 18)\n\nSome Formalisable\
    \ Definitions of Outcome Fairness:\n- **Demographic/ Statistical Parity (group\
    \ fairness): **An outcome is fair if each group in the selected set receives benefit\
    \ in equal or similar proportions, i.e. if there is no correlation between a sensitive\
    \ or protected attribute and the allocative result. This approach is intended\
    \ to prevent disparate impact, which occurs when the outcome of an algorithmic\
    \ process disproportionately harms members of disadvantaged or protected groups.\n\
    - **True positive rate (group fairness): **An outcome is fair if the \u2018true\
    \ positive\u2019 rates of an algorithmic prediction or classification are equal\
    \ across groups. This approach is intended to align the goals of bias mitigation\
    \ and accuracy by ensuring that the accuracy of the model is equivalent between\
    \ relevant population subgroups. This method is also referred to as \u2018equal\
    \ opportunity\u2019 fairness because it aims to secure equalised odds of an advantageous\
    \ outcome for qualified individuals in a given population regardless of the protected\
    \ or disadvantaged groups of which they are members\n- **False positive rate parity\
    \ (group fairness): **An outcome is fair if it does not disparately mistreat people\
    \ belonging to a given social group by misclassifying them at a higher rate than\
    \ the members of a second social group, for this would place the members of the\
    \ first group at an unfair disadvantage. This approach is motivated by the position\
    \ that sensitive groups and advantaged groups should have similar error rates\
    \ in outcomes of algorithmic decisions.\n- **Positive predictive value parity\
    \ (group fairness): **An outcome is fair if the rates of positive predictive value\
    \ (the fraction of correctly predicted positive cases out of all predicted positive\
    \ cases) are equal across sensitive and advantaged groups. Outcome fairness is\
    \ defined here in terms of a parity of precision, where the probability of members\
    \ from different groups actually having the quality they are predicted to have\
    \ is the same across groups.\n- **Individual fairness (individual fairness): **An\
    \ outcome is fair if it treats individuals with similar relevant qualifications\
    \ similarly. This approach relies on the establishment of a similarity metric\
    \ that shows the degree to which pairs of individuals are alike with regard to\
    \ a specific task.\n- **Counterfactual fairness (individual fairness): **An outcome\
    \ is fair if an automated decision made about an individual belonging to a sensitive\
    \ group would have been the same were that individual a member of a different\
    \ group in a closest possible alternative (or counterfactual) world. Like the\
    \ individual fairness approach, this method of defining fairness focuses on the\
    \ specific circumstances of an affected decision subject, but, by using the tools\
    \ of contrastive explanation, it moves beyond individual fairness insofar as it\
    \ brings out the causal influences behind the algorithmic output. It also presents\
    \ the possibility of offering the subject of an automated decision knowledge of\
    \ what factors, if changed, could have influenced a different outcome. This could\
    \ provide them with actionable recourse to change an unfavourable decision.\n\
    (Leslie, 2019, p. 19)\n\n\"Once you and your project team have thoroughly considered\
    \ the use case appropriateness as well as technical feasibility of the formal\
    \ models of fairness most relevant for your system and have incorporated the model\
    \ into your application, you should prepare a Fairness Position Statement (FPS)\
    \ in which the fairness criteria being employed in the AI system is made explicit\
    \ and explained in plain and non-technical language. This FPS should then be made\
    \ publicly available for review by all affected stakeholders.\" (Leslie, 2019,\
    \ p.20)\n"
  Linked Principles:
  - reczVPIH1y2OMpAJH
  - rec42P8U9usfYCtv9
  - rec7n2TGrH9RHYpQj
  Linked Sources:
  - recfYC5jjPmpLfSlM
  Tags:
  - reflection-discussion
  airtable_createdTime: '2023-05-19T11:13:24.000Z'
  airtable_id: recNzXJwCfbwBLmVU
  title: Principle of Discriminatory non-harm for fairness, key considerations for
    outcome fairness
- Description: "\u201C\u2022 Are there mechanisms to ensure that sensitive data is\
    \ kept anonymous? Are there procedures in place to limit access to the data only\
    \ to those who need it? \u2022 Is access to learner data protected and stored\
    \ in a secure location and used only for the purposes for which the data was collected?\
    \ \u2022 Is there a mechanism to allow teachers and school leaders to flag issues\
    \ related to privacy or data protection? \u2022 Are learners and teachers informed\
    \ about what happens with their data, how it is used and for what purposes? \u2022\
    \ Is it possible to customise the privacy and data settings? \u2022 Does the AI\
    \ system comply with General Data Protection Regulation?\u201D (European Commission,\
    \ 2022, p. 21)\n"
  Linked Cases:
  - recrVkbG0XGe2Ca0v
  - recSXcY4cnofb4zTP
  - recAlOHJhEy5nDwA6
  Linked Principles:
  - rec42P8U9usfYCtv9
  - recKdujFoPJr4ZAhZ
  Linked Sources:
  - rec9jnxuHOioQn4DC
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T13:37:43.000Z'
  airtable_id: recOo7Pmo4FBYcu7P
  title: Guiding questions for educators regarding Privacy and Data Governance of
    AI in Education
- Description: "\u201CWhat particular issues might arise around the issue of minors\
    \ or vulnerable persons? \xA7 Are minors being excluded from the study because\
    \ of the difficulties of getting ethical permission to study them? \xA7 In situations\
    \ where identity, age, and ability of the participant is unknown or hidden, and\
    \ harm cannot be determined as an a priori category based on known vulnerability\
    \ of participant, how will harm be considered as an ethical concern and operationalized\
    \ in the study? \xA7 How are minors identified as \u2018minors\u2019 in contexts\
    \ where demographic information is not required? What harm might result from asking\
    \ (or not asking) for participants to reveal their age? \xA7 How will parental\
    \ or guardian consent be obtained in addition to assent where required by research\
    \ regulations? What risks might arise in this particular consent process? (for\
    \ any or all parties, including the minor, the parents, and the researcher)?\u201D\
    \ (Markham and Buchanan, 2012, p. 11)\n"
  Linked Principles:
  - reczVPIH1y2OMpAJH
  Linked Sources:
  - recQiVQ7CTC72xp6O
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T08:28:55.000Z'
  airtable_id: recPynxbe7x5wOs5E
  title: Questions to consider regarding protection of vulnerable populations
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CDiversity, non-discrimination and fairness \n\u201CStakeholder\
    \ participation: \n\uF0FC Did you consider a mechanism to include the participation\
    \ of different stakeholders in the AI system\u2019s development and use? \n\uF0FC\
    \ Did you pave the way for the introduction of the AI system in your organisation\
    \ by informing and involving impacted workers and their representatives in advance?\u201D\
    \ (High-Level Expert Group on AI, 2019, p. 30)\n\n### IEEE recommendation\n\"\
    To ensure representation of stakeholders, organizations should enact a planned\
    \ and controlled set of activities to account for the interests of the full range\
    \ of stakeholders or practitioners who will be working alongside\_A/IS and incorporating\
    \ their insights to build upon, rather than circumvent or ignore, the\_social\
    \ and practical wisdom of involved practitioners and other stakeholders.\n## Further\
    \ Resources\n\u2022\_\_\_\_\_C. Schroeter, et al., \u201C[Realization and User\
    \ Evaluation of a Companion Robot for People with Mild Cognitive Impairments](http://www.tu-ilmenau.de/fileadmin/media/neurob/publications/conferences_int/2013/Schroeter-ICRA-2013-fin.pdf),\u201D\
    \ _Proceedings of IEEE International Conference on Robotics and Automation (ICRA\
    \ 2013)_, Karlsruhe, Germany 2013. pp. 1145\u20131151.\n\u2022\_\_\_\_\_T. L.\
    \ [Chen, et al. ](http://ieeexplore.ieee.org/abstract/document/6476704/)\u201C\
    [Robots for Humanity: Using Assistive Robotics to Empower People with Disabilities](http://ieeexplore.ieee.org/document/6476704/),\u201D\
    \ _IEEE Robotics and Automation Magazine, _vol. 20, no. 1, pp. 30\u201339, 2013.\n\
    R. Hartson, and P. S. Pyla. _The UX Book: Process and Guidelines for Ensuring\
    \ a Quality User Experience_. Waltham, MA: Elsevier, 2012\"\n\np.130-131\n"
  Linked Challenges:
  - recnYgPiGULdHLunC
  Linked Principles:
  - recU6u0AZbcNj1ik9
  - rec6tz9Phzck0hvT8
  - recB9JaNSRmLbD8eE
  Linked Sources:
  - recnCULdYQ36cpZR7
  - recpXl48pJdKDhc6f
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:39:27.000Z'
  airtable_id: recQPwyiQbPcN0G47
  title: Considerations in assessing trustworthy AI - Stakeholder participation
- Description: "\u201C\u2022 Who is responsible for the ongoing monitoring of results\
    \ produced by the AI system and how the results are being used to enhance teaching,\
    \ learning and assessment? \u2022 How is the effectiveness and impact of the AI\
    \ system being evaluated and how does this evaluation consider key values of education?\
    \ \u2022 Who is responsible and accountable for final decisions made regarding\
    \ the procurement and implementation of the AI system? \u2022 Is there a Service\
    \ Level Agreement in place, clearly outlining the support and maintenance services\
    \ and steps to be taken to address reported problems?\u201D (European Commission,\
    \ 2022, p. 21)\n"
  Linked Cases:
  - recrVkbG0XGe2Ca0v
  - recmS3zSMbR3ofAR5
  - recSXcY4cnofb4zTP
  Linked Principles:
  - recKdujFoPJr4ZAhZ
  - recmzjcGKv3yNOxbl
  - recOHnq45Fq7YWsRO
  Linked Sources:
  - rec9jnxuHOioQn4DC
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T13:37:49.000Z'
  airtable_id: recRTVqtvPcBS6zps
  title: Guiding questions for educators regarding Accountability of AI in Education
- Description: "\u2022 Is there sufficient security in place to protect against data\
    \ breaches? \u2022 Is there a strategy to monitor and test if the AI system is\
    \ meeting the goals, purposes and intended applications? \u2022 Are the appropriate\
    \ oversight mechanisms in place for data collection, storage, processing, minimisation\
    \ and use? \u2022 Is information available to assure learners and parents of the\
    \ system\u2019s technical robustness and safety?\n"
  Linked Cases:
  - recOOmVQviRyJGvea
  - recAlOHJhEy5nDwA6
  Linked Principles:
  - recgDkzdE9dfpTxCK
  - recQ9DIFEsOEkCx3O
  - recy4stJ6Y4e2Fezp
  - recint2IxoR8aILCp
  Linked Sources:
  - rec9jnxuHOioQn4DC
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T13:37:48.000Z'
  airtable_id: recTWhZ88TbQLcNaQ
  title: Guiding questions for educators regarding Technical Robustness and Safety
    of AI in Education
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CDiversity, non-discrimination and fairness \nUnfair bias avoidance:\
    \ \n\uF0FC Did you establish a strategy or a set of procedures to avoid creating\
    \ or reinforcing unfair bias in the AI system, both regarding the use of input\
    \ data as well as for the algorithm design? \n\uF0A7 Did you assess and acknowledge\
    \ the possible limitations stemming from the composition of the used data sets?\
    \ \uF0A7 Did you consider diversity and representativeness of users in the data?\
    \ Did you test for specific populations or problematic use cases? \n\uF0A7 Did\
    \ you research and use available technical tools to improve your understanding\
    \ of the data, model and performance\u201D (High-Level Expert Group on AI, 2019,\
    \ p. 29)\n\u201C30 \n\uF0A7 Did you put in place processes to test and monitor\
    \ for potential biases during the development, deployment and use phase of the\
    \ system? \n\uF0FC Depending on the use case, did you ensure a mechanism that\
    \ allows others to flag issues related to bias, discrimination or poor performance\
    \ of the AI system? \n\uF0A7 Did you establish clear steps and ways of communicating\
    \ on how and to whom such issues can be raised? \n\uF0A7 Did you consider others,\
    \ potentially indirectly affected by the AI system, in addition to the (end)users?\
    \ \n\uF0FC Did you assess whether there is any possible decision variability that\
    \ can occur under the same conditions? \n\uF0A7 If so, did you consider what the\
    \ possible causes of this could be? \n\uF0A7 In case of variability, did you establish\
    \ a measurement or assessment mechanism of the potential impact of such variability\
    \ on fundamental rights? \n\uF0FC Did you ensure an adequate working definition\
    \ of \u201Cfairness\u201D that you apply in designing AI systems? \n\uF0A7 Is\
    \ your definition commonly used? Did you consider other definitions before choosing\
    \ this one? \n\uF0A7 Did you ensure a quantitative analysis or metrics to measure\
    \ and test the applied definition of fairness? \n\uF0A7 Did you establish mechanisms\
    \ to ensure fairness in your AI systems? Did you consider other potential mechanisms?\u201D\
    \ (High-Level Expert Group on AI, 2019, p. 30)\n"
  Linked Challenges:
  - recHHr97jsyNDnlsJ
  Linked Principles:
  - recmzjcGKv3yNOxbl
  - recScYLR2TNiv7iKf
  - reclPiw2VvNOSTzv5
  - rec42P8U9usfYCtv9
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:39:18.000Z'
  airtable_id: recTjwhqfrJoaRxYo
  title: Considerations in assessing trustworthy AI - Unfair bias avoidance
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION)\n 1\\. Human agency and oversight \nFundamental rights: \n\uF0FC Did\
    \ you carry out a fundamental rights impact assessment where there could be a\
    \ negative impact on fundamental rights? \nDid you identify and document potential\
    \ trade-offs made between the different principles and rights? \n\uF0FC Does the\
    \ AI system interact with decisions by human (end) users (e.g. recommended actions\
    \ or decisions to take, presenting of options)? \n\uF0A7 Could the AI system affect\
    \ human autonomy by interfering with the (end) user\u2019s decision-making process\
    \ in an unintended way? \n\uF0A7 Did you consider whether the AI system should\
    \ communicate to (end) users that a decision, content, advice or outcome is the\
    \ result of an algorithmic decision? \n\uF0A7 In case of a chat bot or other conversational\
    \ system, are the human end users made aware that they are interacting with a\
    \ non-human agent?\u201D (High-Level Expert Group on AI, 2019, p. 26)\n"
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:10:10.000Z'
  airtable_id: recTr6cDE4OBKV0wv
  title: Considerations in assessing trustworthy AI - Fundamental rights
- Description: "## Recommendations\nCompanies should create roles for senior-level\
    \ marketers, engineers, and lawyers who can collectively and pragmatically implement\
    \ ethically aligned design. There is also a need for more in-house ethicists,\
    \ or positions that fulfill similar roles. One potential way to ensure values\
    \ are on the agenda in A/IS development is to have a Chief Values Officer (CVO),\
    \ a role first suggested by Kay Firth-Butterfield, see \u201CFurther Resources\u201D\
    . However, ethical responsibility should not be delegated solely to CVOs. They\
    \ can support the creation of ethical knowledge in companies, but in the end,\
    \ all members of an organization will need to act responsibly throughout the design\
    \ process.\nCompanies need to ensure that their understanding of values-based\
    \ system innovation is based on _de jure _and _de facto _international human rights\
    \ standards.\n\u2022\_\_\_\_\_K. Firth-Butterfield, \u201C[How IEEE Aims to Instill\
    \ Ethics in Artificial Intelligence Design,](http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design)\u201D\
    \ The Institute. Jan. 19, 2017. [Online]. Available: [http://theinstitute.ieee.org/ieee-roundup/\
    \ blogs/blog/how-ieee-aims-to-instill-ethicsin-artificial-intelligence-design](http://theinstitute.ieee.org/ieee-roundup/blogs/blog/how-ieee-aims-to-instill-ethics-in-artificial-intelligence-design).\
    \ [Accessed October 28, 2018]. \u2022\_\_\_\_\_United Nations, [Guiding Principles\
    \ on Business and Human Rights: Implementing the United Nations \u201CProtect,\
    \ Respect and Remedy\u201D Framework,](http://www.ohchr.org/Documents/Publications/GuidingPrinciplesBusinessHR_EN.pdf)\
    \ New York and Geneva: UN, 2011.\n\u2022\_\_\_\_\_Institute for Human Rights and\
    \ Business\n(IHRB), and Shift, ICT [Sector Guide on ](https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf)\n\
    [Implementing the UN Guiding Principles on Business and Human Rights,](https://www.ihrb.org/pdf/eu-sector-guidance/EC-Guides/ICT/EC-Guide_ICT.pdf)\
    \ 2013.\n\u2022\_\_\_\_\_C. Cath, and L. Floridi, \u201C[The Design of the Internet\u2019\
    s Architecture by the Internet ](http://europepmc.org/abstract/med/27255607)\n\
    [Engineering Task Force (IETF) and Human Rights](http://europepmc.org/abstract/med/27255607).\u201D\
    \ _Science and Engineering Ethics, _vol._ _23, no. 2, pp. 449\u2013468, Apr. 2017.\"\
    \n\np.128\n"
  Linked Challenges:
  - recaBNAcact8Bz6dg
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T10:29:17.000Z'
  airtable_id: recV8WjuiXlfbVN5d
  title: Roles regarding ethics should have leadership capacity, to foster ethical
    culture across the organisation
- Description: "\u201CAI models in edtech will be approximations of reality and, thus,\
    \ constituents can always ask these questions: How precise are the AI models?\
    \ Do they accurately capture what is most important? How well do the recommendations\
    \ made by an AI model fit educational goals? What are the broader implications\
    \ of using AI models at scale in educational processes? Building on what was heard\
    \ from constituents, the sections of this report develop the theme of evaluating\
    \ the quality of AI systems and tools using multiple dimensions as follows: \n\
    \u25CF About AI: AI systems and tools must respect data privacy and security.\
    \ Humans must be in the loop. \n\u25CF Learning: AI systems and tools must align\
    \ to our collective vision for high-quality learning, including equity. \n\u25CF\
    \ Teaching: AI systems and tools must be inspectable, explainable, and provide\
    \ human alternatives to AI-based suggestions; educators will need support to exercise\
    \ professional judgment and override AI models, when necessary.\u201D (Cardona\
    \ et al., 2023, p. 9)\n\u201C\u25CF Formative Assessment: AI systems and tools\
    \ must minimize bias, promote fairness, and avoid additional testing time and\
    \ burden for students and teachers. \n\u25CF Research and Development: AI systems\
    \ and tools must account for the context of teaching and learning and must work\
    \ well in educational practice, given variability in students, teachers, and settings.\
    \ \n\u25CF Recommendations: Use of AI systems and tools must be safe and effective\
    \ for students. They must include algorithmic discrimination protections, protect\
    \ data privacy, provide notice and explanation, and provide a recourse to humans\
    \ when problems arise. The people most affected by the use of AI in education\
    \ must be part of the development of the AI model, system, or tool, even if this\
    \ slows the pace of adoption.\u201D (Cardona et al., 2023, p. 10)\n\n"
  Linked Principles:
  - recKdujFoPJr4ZAhZ
  Linked Sources:
  - recY4zreDoWrsbsv7
  airtable_createdTime: '2023-05-29T07:49:51.000Z'
  airtable_id: recWm6C6wOuVr6UCX
  title: 'Foundational issue: Promotion of Transparency'
- Attachments:
  - filename: p30loops.png
    height: 1404
    id: attNj1iymmOkpnRP3
    size: 1294010
    thumbnails:
      full:
        height: 3000
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/XdwS6_RL3bnZRGaRjMuRcA/3wDqs0Nyt0cIPvHVKrnVoM5CGAWoGDIsRnUwJrN5y59UFaELS7pkyIMg2IyNyVblQfsNJpYwVHUN4cy2s1UJzg/jaLp-ZZB_avF1lDj5zOvLSHOdNFDlQ8num3t7Bbby40
        width: 3000
      large:
        height: 512
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/Gy4qzgdT167umpjRp0NfrA/3TOcmv6XrmuR6H9QkWFcih3IA929bxk4tPot46O0f9hpR7GMYeqGIwbe0TMrfq4QWGh03sLZmE6RDFCDit56UQ/ct1aiLyoivc0AgEMlnN-ZJQHPOvSiXyddsSJnDM96DM
        width: 563
      small:
        height: 36
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/rPxlm3ykV48OutOabn08uw/mjy_-6OGbFOpC0jrXl3o0yazqBL-jeRQatLii9cuSU0zZkaxAd6xxPTL2L6Ful_JSMRvHxKRHyg1B6_EOvu_HQ/aJoZL0ISYFYZ9xnMWpW9oOFJJasHnn_OL1OtVb__7v0
        width: 40
    type: image/png
    url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/Hn2HENTUpPtiKAEN8MdrZA/SzUZbGP1OMwvncj1zFGOPyMC4Iyfmm4KMVl5PNSuxhk8mjv94IzLfOe2KAxMDzHoGpHhfBi_dYxTHibwNwdumXxBIHcfDw6CapxqxK7qluw/U5R82ohV_kVuTXkaREU2muZy-6gxtOzbfjliKf010d4
    width: 1545
  Description: "\u201CTo succeed with AI as an enhancement to learning and teaching,\
    \ we need to always center educators (ACE). Practically speaking, practicing \u201C\
    ACE in AI\u201D means keeping a humanistic view of teaching front and center.\
    \ \nACE is not just about making teachers\u2019 jobs easier but also making it\
    \ possible to do what most teachers want to do. That includes, for example, understanding\
    \ their students more deeply and having more time to respond in creative ways\
    \ to teachable moments.\nTo bring more precision to how and where we should center\
    \ educators, we return to our advocacy for human in the loop AI and ask, what\
    \ are the loops in which teachers should be centered? Figure 5 suggests three\
    \ key loops (inspired by research on adaptivity loops34):\u201D (Cardona et al.,\
    \ 2023, p. 25)\n\u201C1. The loop in which teachers make moment-to-moment decisions\
    \ as they do the immediate work of teaching. \n2\\. The loop in which teachers\
    \ prepare for, plan, and reflect on teaching, which includes professional development.\
    \ \n3\\. The loop in which teachers participate in decisions about the design\
    \ of AI-enabled technologies, participate in selecting the technologies, and shape\
    \ the evaluation of technologies\u2014thus setting a context for not only their\
    \ own classroom but those of fellow teachers as well.\u201D (Cardona et al., 2023,\
    \ p. 26)\n"
  Linked Principles:
  - recKdujFoPJr4ZAhZ
  Linked Sources:
  - recY4zreDoWrsbsv7
  airtable_createdTime: '2023-05-29T08:05:27.000Z'
  airtable_id: recX4wcNFSw9YljDD
  title: Always Center Educators in Instructional Loops
- Description: "\u201CWhat are the potential harms or risks associated with this study?\
    \ \xA7 What is the potential harm or risk for individuals, for online communities,\
    \ for researchers, for research? \xA7 Are risks being assessed throughout the\
    \ study as well as in advance of the study? (Harm is only certain after it occurs.\
    \ Thus, a priori assessments of risk might be useful but inadequate24). \xA7 How\
    \ are the concepts of \u2018vulnerability\u2019 and \u2018harm\u2019 being defined\
    \ and operationalized in the study? How are risks to the community/author/participant\
    \ being assessed? \xA7 How is vulnerability determined in contexts where this\
    \ categorization may not be apparent? \xA7 Would a mismatch between researcher\
    \ and community/participant/author definitions of \u2018harm\u2019 or \u2018vulnerability\u2019\
    \ create an ethical dilemma? If so, how would this be addressed? \xA7 What harms--to\
    \ life, to career, to reputation--may occur from the research? (e.g., would the\
    \ research \u201Cout\u201D an LGBTQ individual who is not publicly out and perhaps\
    \ cause them to lose their jobs? Would the research cause someone to face criminal\
    \ or civil penalties?) \xA7 What possible privacy-related harms may occur? For\
    \ example, might online groups disband or individuals cease to use an online support\
    \ group or withdraw from blogging activities because of the presence of researchers;25\
    \ Might individuals be upset that their perceived privacy has been violated;26\
    \ might individuals object to having their writing or speech anonymised, preferring\
    \ to remain known and public in any published results? \xA7 Who or what else could\
    \ cause harm to the author/participant beyond the researcher? \u201C\xA7 Are we\
    \ acting in ways that minimizes risk? \xA7 Does our research adequately protect\
    \ the researcher as well as the community/author/participant?\u201D (Markham and\
    \ Buchanan, 2012, p. 10-11)\n"
  Linked Principles:
  - rec42P8U9usfYCtv9
  - reczVPIH1y2OMpAJH
  - recU6u0AZbcNj1ik9
  - recjViPnz3atRIOpD
  - recPg7Ov0priGGtLm
  Linked Sources:
  - recQiVQ7CTC72xp6O
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T08:25:30.000Z'
  airtable_id: recY9yr9vYcOAUSEA
  title: Questions to consider in assessing risk of harms
- Description: "## \"Recommendations\nA/IS creators should work to better understand\
    \ and apply well-being metrics in the algorithmic age. Specifically:\n\u2022\_\
    \_\_\_\_A/IS creators should work directly with experts, researchers, and practitioners\
    \ in wellbeing concepts and metrics to identify existing metrics and combinations\
    \ of indicators that would bring support a \u201Ctriple bottom line\u201D, i.e.,\
    \ accounting for economic, social, and environmental impacts, approach to wellbeing.\
    \ However, well-being metrics should only be used with consent, respect for privacy,\
    \ and with strict standards for collection and use of these data.\n\u2022\_\_\_\
    \_\_For A/IS to promote human well-being, the well-being metrics should be chosen\
    \ in collaboration with the populations most affected by those systems\u2014the\
    \ A/IS stakeholders\u2014including both the intended end-users or beneficiaries\
    \ and those groups whose lives might be unintentionally transformed by them. This\
    \ selection process should be iterative and through a learning\_and continually\
    \ improving process. In addition, \u201Cmetrics of well-being\u201D should be\
    \ treated as vehicles for learning and potential mid- course corrections. The\
    \ effects of A/IS on human well-being should be monitored continuously throughout\
    \ their life cycles, by\_A/IS creators and stakeholders, and both A/IS creators\
    \ and stakeholders should be prepared to significantly modify, or even roll back,\
    \ technology that is shown to reduce well-being, as defined by affected populations.\n\
    \u2022\_\_\_\_\_A/IS creators in the business or academic, engineering, or policy\
    \ arenas are advised to review the additional resources on standards development\
    \ models and frameworks at the end of this chapter to familiarize themselves with\
    \ existing indicators relevant to their work.\n## Further Resources\n\u2022\_\_\
    \_\_\_PricewaterhouseCoopers (PwC). [Managing and Measuring Total Impact: A New\
    \ Language for Business Decisions](https://www.pwc.com/gx/en/services/sustainability/total-impact-measurement-management/measuring-and-managing-total-impact-a-new-language-for-business-decisions.html),\
    \ 2017.\n\u2022\_\_\_\_\_World Economic Forum. [The Inclusive Growth and Development\
    \ Report 2017](https://www.weforum.org/reports/the-inclusive-growth-and-development-report-2017),\
    \ Geneva, Switzerland: World Economic Forum, January 16, 2017.\n\u2022\_\_\_\_\
    \_[OECD Guidelines on Measuring Subjective Well-being,](http://www.oecd.org/statistics/oecd-guidelines-on-measuring-subjective-well-being-9789264191655-en.htm)\
    \ 2013.\n\u2022\_\_\_\_\_National Research Council. [Subjective WellBeing: Measuring\
    \ Happiness, Suffering, and Other Dimensions of Experience. D](https://www.nap.edu/catalog/18548/subjective-well-being-measuring-happiness-suffering-and-other-dimensions-of)C:\
    \ The National Academies Press, 2013.\"\np73-74\n\n\"Create technical standards\
    \ for representing goals, metrics, and evaluation guidelines for well-being metrics\
    \ and their precursors and components within A/IS that include:\n\u2022\_\_\_\_\
    \_[O](https://en.wikipedia.org/wiki/Ontology_(information_science))ntologies for\
    \ representing technological requirements.\n\u2022\_\_\_\_\_A testing framework\
    \ for validating adherence to well-being metrics and ethical principles such as\
    \ [IEEE P7010\u2122 Standards Project for Wellbeing Metric for Autonomous and\
    \ Intelligent Systems](https://standards.ieee.org/project/7010.html).\nabove as\
    \ well as others as a basis for a wellbeing metrics standard for A/IS creators.\
    \ _(See page 191, [Additional Resources: Additional Resources: Standards Development\
    \ Models and Frameworks)](https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/ead1e_standards_development_models_frameworks.pdf)_\n\
    \u2022\_\_\_\_\_The development of a well-being metrics standard for A/IS that\
    \ encompasses an understanding of well-being as holistic and interlinked to social,\
    \ economic, and ecological systems.\n\_\"\np.79-80\n\n## \"Recommendation\nAppoint\
    \ a lead team or person, \u201Cleads\u201D, to facilitate stakeholder engagement\
    \ and to serve as a resource for A/IS creators who use stakeholderbased processes\
    \ to establish well-being indicators. Specifically:\n\u2022\_\_\_\_\_Leads should\
    \ solicit and collect lessons learned from specific applications of stakeholder\
    \ engagement and deliberation in order to continually refine its guidance.\n\u2022\
    \_\_\_\_\_When determining well-being indicators, the leads should enlist the\
    \ help of experts in public participation and deliberation. With expert guidance,\
    \ facilitators can provide guidance for how to: take steps to mitigate the effects\
    \ of unequal power in deliberative processes; incorporate appropriately trained\
    \ facilitators and coaching participants in deliberations; recognize and curb\
    \ disproportionate influence by morepowerful groups; use techniques to maximize\
    \ the voices of less-powerful groups.\u2022\_\_\_\_\_Leads should use their convening\
    \ power to bring together A/IS creators and stakeholders, including critics of\
    \ A/IS, for deliberations on well-being indicators, impacts, and other considerations\
    \ for specific contexts and settings. Leads\u2019 involvement would help bring\
    \ actors to the table with a balance of power and encourage all actors to remain\
    \ in conversation until robust, mutually agreeable definitions\_are found.\n##\
    \ Further Resources\n\u2022\_\_\_\_\_D. E. Booher and J. E. Innes. Planning with\
    \ Complexity: An Introduction to Collaborative Rationality for Public Policy.\
    \ London:\_Routledge, 2010.\n\u2022\_\_\_\_\_J. A. Leydens and J. C. Lucena. Engineering\
    \ Justice: Transforming Engineering Education and Practice_. _Wiley-IEEE Press,\
    \ 2018.\n\u2022\_\_\_\_\_G. Ottinger. [Assessing Community Advisory ](https://www.sciencehistory.org/sites/default/files/studies-in-sustainability-ottinger2009.pdf)\n\
    [Panels: A Case Study from Louisiana\u2019s Industrial Corridor.](https://www.sciencehistory.org/sites/default/files/studies-in-sustainability-ottinger2009.pdf)\
    \ Center for Contemporary History and Policy, 2008.\n\u2022\_\_\_\_\_[Expert and\
    \ Citizen Assessment of Science and ](https://ecastnetwork.org/about/)\n[Technology\
    \ (ECAST) Network ](https://ecastnetwork.org/about/)\"\np.82-83\n"
  Linked Challenges:
  - rec1QZHHMARBQcZoo
  - recOpn4I30te1qiKl
  - reciNc7OeTUmnsLqg
  - recA7Kh502s4UKWGo
  - recGRpoF7DA23ODSj
  - rec5nbWC0ZbVwnmxJ
  - recaFyWRROaJ1ZFyY
  Linked Principles:
  - receFm7cGasHwpJZO
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T11:22:05.000Z'
  airtable_id: recZI7HDWKBU5T22v
  title: Wellbeing measures and promotion should be part of AI evaluation
- Description: "\"The demand for sensitivity to human factors should inform your approach\
    \ to devising delivery and implementation processes from start to finish. To provide\
    \ clear and effective explanations about the content and rationale of algorithmic\
    \ outputs, you will have to begin by building _from the human ground up_. You\
    \ will have to pay close attention to the circumstances, needs, competences, and\
    \ capacities of the people whom your AI project aims to assist and serve.\nThis\
    \ means that _context will be critical_. By understanding your use case well and\
    \ by drawing upon solid domain knowledge, you will be better able to define roles\
    \ and relationships. You will better be able to train the users and implementers\
    \ of your system. And, you will be better able to establish an effectual implementation\
    \ platform, to clarify content, and to facilitate understanding of outcomes for\
    \ users and affected stakeholders alike. Here is a diagram of what securing human-centred\
    \ implementation protocols and practices might look like:\n \n  \n   \n    \n\
    \ \_\nLet us consider these steps in turn by building a checklist of essential\
    \ actions that should be taken to help ensure the human-centred implementation\
    \ of your AI project. Because the specifics of your approach will depend so heavily\
    \ on the context and potential impacts of your project, we\u2019ll assume a\n\n\
    \ generic case and construct the checklist around a hypothetical algorithmic decision-making\
    \ system that will be used for predictive risk assessment.\n\_\nStep 1: Consider\
    \ aspects of application type and domain context to define roles and determine\
    \ user needs\n\_\n\xFF\_\_\_(1) Assess which members of the communities you are\
    \ serving will be most affected by the implementation of your AI system. Who are\
    \ the most vulnerable among them? How will their socioeconomic, cultural, and\
    \ education backgrounds affect their capacities to interpret and understand the\
    \ explanations you intend to provide? How can you fine-tune your explanatory strategy\
    \ to accommodate their requirements and provide them with clear and non-technical\
    \ details about the rationale behind the algorithmically supported result?\n\_\
    \nWhen thinking about providing explanations to affected stakeholders, you should\
    \ start with the needs of the most disadvantaged first. Only in this way, will\
    \ you be able to establish an acceptable baseline for the equitable delivery of\
    \ interpretable AI.\n\_\n\xFF\_\_\_(2) After reviewing [Guideline 1](#_bookmark33)\
    \ above, make a list of and define all the roles that will potentially be involved\
    \ at the delivery stage of your AI project. As you go through each role, specify\
    \ levels of technical expertise and domain knowledge as well as possible goals\
    \ and objectives for each role. For instance, in our predictive risk assessment\
    \ case:\n\_\no\_\_Decision Subject (DS)-\n\xA7\_Role: Subject of the predictive\
    \ analytics.\n\xA7\_Possible Goals and Objectives: To receive a fair, unbiased,\
    \ and reasonable determination, which makes sense; to discover which factors might\
    \ be changed to receive a different outcome.\n\xA7\_Technical and Domain Knowledge:\
    \ Most likely low to average technical expertise and average domain knowledge.\n\
    o\_\_Advocate for the DS-\n\xA7\_Role: Support for the DS (for example, legal\
    \ counsel or care worker) and concerned party to the automated decision.\n\xA7\
    \_Possible Goals and Objectives: To make sure the best interests of the DS are\
    \ safeguarded throughout the process; to help make clear to the DS what is going\
    \ on and how and why decisions are being made.\n\xA7\_Technical and Domain Knowledge:\
    \ Most likely average technical expertise and high level of domain knowledge.\n\
    o\_\_Implementer-\n\xA7\_Role: User of the AI system as decision support.\n\xA7\
    \_Possible Goals and Objectives: To make an objective and fair decision that is\
    \ sufficiently responsive to the particular circumstances of the DS and that is\
    \ anchored in solid reasoning and evidence-based judgement.\n\xA7\_Technical and\
    \ Domain Knowledge: Most likely average technical expertise and high level of\
    \ domain knowledge.\no\_\_System Operator/Technician-\n\xA7\_Role: Provider of\
    \ support and maintenance for the AI system and its use.\n\n \xA7\_Possible Goals\
    \ and Objectives: To make sure the machine learning system is performing well\
    \ and running in accordance with its intended design; to handle the technical\
    \ dimension of information processing for the DS\u2019s particular case; to answer\
    \ technical questions about the system and its results as they arise.\n\xA7\_\
    Technical and Domain Knowledge: Most likely high level of technical expertise\
    \ and average domain knowledge.\no\_\_Delivery Manager-\n\xA7\_Role: Member of\
    \ the implementation team who oversees its operation and responds to problems\
    \ as they arise.\n\xA7\_Possible Goals and Objectives: To ensure that the quality\
    \ of the automation- supported assessment process is high and that the needs of\
    \ the decision subject are being served as intended by the project; to oversee\
    \ the overall quality of the relationships within the implementation team and\
    \ between the members of that team and the communities they serve.\n\xA7\_Technical\
    \ and Domain Knowledge: Most likely average technical expertise and good to high\
    \ level domain knowledge\n\_\nStep 2: Define delivery relations and map delivery\
    \ processes\n\_\n\xFF\_\_\_(1) Assess the possible relationships between the defined\
    \ roles that will have significant\nbearing on your project\u2019s implementation\
    \ and formulate a descriptive account of this relationship with an eye to the\
    \ part it will play in the delivery process. For the predictive risk assessment\
    \ example:\n\_\no\_\_Decision Subject/Advocate to Implementer: This is the primary\
    \ relationship of the implementation process. It should be information-driven\
    \ and dialogue-driven with the implementer\u2019s exercise of unbiased judgment\
    \ and the DS\u2019s comprehension of the outcome treated as the highest priorities.\
    \ Implementers should be prepared to answer questions and to offer evidence-based\
    \ clarifications and justifications for their determinations. The achievement\
    \ of well-informed mutual understanding is a central aim.\no\_\_Implementer to\
    \ System Operator: This is the most critical operational relationship within the\
    \ implementation team. Communication levels should be kept high from case to case,\
    \ and the shared goal of the two parties should be to optimise the quality of\
    \ the decisions by optimising the use of the algorithmic decision-support system\
    \ in ways that are accessible both to the user and to the DS. The conversations\
    \ between implementers and system operators should be problem-driven and should\
    \ avoid, as much as possible, focus on the specialised vocabularies of each party\u2019\
    s domain of expertise.\no\_\_Delivery Manager to Operator to Implementer: The\
    \ quality of this cross-disciplinary relationship within the implementation team\
    \ will have direct bearing on the overall quality of the delivery of the algorithmically\
    \ supported decisions. Safeguarding the latter will require that open and easily\
    \ accessible lines of communication be maintained between delivery managers, operators,\
    \ and implementers, so that unforeseen implementation problems can be tackled\
    \ from multiple angles and in ways that anticipate and stem future difficulties.\
    \ Additionally, different use cases may present different explanatory challenges\
    \ that are best addressed by multidisciplinary\n\n team input. Good communications\
    \ within the implementation team will be essential to enable that such challenges\
    \ are addressed in a timely and efficient manner.\n\_\n\xFF\_\_\_(2) Start building\
    \ a map of the delivery process. This should involve incorporating your understanding\
    \ of the needs, roles, and relationships of relevant actors involved in the implementation\
    \ of your AI system into the wider objective of providing clear, informative,\
    \ and understandable explanations of algorithmically supported decisions.\n\_\n\
    It is vital to recognise, at this implementation-planning stage of your project,\
    \ that the principal goal of the delivery process is two-fold: _to translate statistically\
    \ expressed results into humanly significant reasons and to translate algorithmic\
    \ outputs into socially meaningful outcomes_.\n\_\nThese overlapping objectives\
    \ should have a direct bearing on the way you build a map for\nyour project\u2019\
    s delivery process, because they organise the duties of implementation into two\
    \ task-specific components:\n\_\n_1.\_\_\_\_\__A technical component, which involves\
    \ determining the most effective way to convey and communicate to users and decision\
    \ subjects the statistical results of your model\u2019s information processing\
    \ so that the factors that figured into the logic and rationale of those results\
    \ can be translated into understandable reasons that can be subjected to rational\
    \ evaluation and critical assessment; and\n\_\n_2.\_\_\_\_\__A social component,\
    \ which involves clarifying the socially meaningful content of the outcome of\
    \ a given algorithmically assisted decision by translating that model\u2019s technical\
    \ machinery\u2014its input and output variables, parameters, and functional rationale\u2014\
    _back _into the everyday language of the humanly relevant categories and relationships\
    \ that informed the formulation of its purpose, objective, and intended elements\
    \ of design in the first place. Only through this re-translation will the effects\
    \ of\nthat model\u2019s output on the real human life it impacts be understandable\
    \ in terms of the specific social and individual context of that life and be conveyable\
    \ as such.\n\_\nThese two components of the delivery process will be fleshed out\
    \ in turn.\n\_\nTechnical component of responsible implementation: As a general\
    \ rule, we use the results of statistical analysis to guide our actions, because,\
    \ when done properly, this kind of analysis offers a solid basis of empirically\
    \ derived evidence that helps us to exercise sound and well- supported judgment\
    \ about the matters it informs.\n\_\nHaving a good understanding of the factors\
    \ that are at work in producing the result of a particular statistical analysis\
    \ (such as in an algorithmic decision-support system) means that we are able to\
    \ grasp these factors (for instance, input features that weigh heavily in determining\
    \ a given algorithmically generated decision) as reasons that may warrant the\
    \ rational acceptability of that result. After all, seen from the perspective\
    \ of the interpretability of such an analysis, these factors are, in fact, nothing\
    \ other than _reasons that are operating to support its conclusions_.\n\n Clearly\
    \ understood, these factors that lie behind the logic of the result or decision\
    \ are not \u2018causes\u2019 of it. Rather, they form the evidentiary basis of\
    \ its rational soundness and of the goodness of the inferences that support it.\
    \ Whether or not we ultimately agree with the decision or the result of the analysis,\
    \ the reasons that work together to comprise its conclusions make _claims to validity\
    \ _and can _as such _be called before a tribunal of _rational criticism_. These\
    \ reasons, in other words, must bear the burden of continuous assessment, evaluation,\
    \ and contestation.\n\_\nThis is an element especially crucial for the responsible\
    \ implementation of AI systems: Because they serve surrogate cognitive functions\
    \ in society, their decisions and results are in no way immune from these demands\
    \ for rational justification and thus must be delivered to be optimally responsive\
    \ to such demands.\n\_\nThe results of algorithmic decision support systems, in\
    \ this sense, serve as stand-ins for acts of speech and representation and therefore\
    \ bear the justificatory burdens of those cognitive functions. They must establish\
    \ the validity of their conclusions and operate under the constraint of being\
    \ surrogates of the dialogical goal to convince through good reasons.\n\_\nThis\
    \ charge to be responsive to the demands of rational justification should be essential\
    \ to the way you map out your delivery strategy. When you devise how best to relay\
    \ and explain the statistical results of your AI systems, you need to start from\
    \ the role they play in supporting evidence-based reasoning.\n\_\nThis, however,\
    \ is no easy job. Interpreting the results of data scientific analysis is, more\
    \ often than not, a highly technical activity and can depart widely from the conventional,\
    \ everyday styles of reasoning that are familiar to most. Moreover, the various\
    \ performance metrics deployed in AI systems can be confusing and, at times, seem\
    \ to be at cross-purposes with each other, depending upon the metrics chosen.\
    \ There is also an unavoidable dimension of uncertainty that must be accounted\
    \ for and expressed in confidence intervals and error bars which may only bring\
    \ further confusion to users and decision subjects.\n\_\nBe that as it may, by\
    \ taking a deliberate and human-centred approach to the delivery process, you\
    \ should be able to find the most effective way to convey your model\u2019s statistical\
    \ results to users and decision subjects in non-technical and socially meaningful\
    \ language that enables them to understand and evaluate the rational justifiability\
    \ of those results. A good point of departure for this is to divide your map-building\
    \ task into the _means of content delivery _and the _substance of the content\
    \ to be delivered_.\n\_\n_Means of content delivery: _When you start mapping out\
    \ serviceable ways of presenting and communicating your model\u2019s results,\
    \ you should consider the users\u2019 and decision subjects\u2019 perspectives\
    \ to be of primary importance. Here are a few guiding questions to ask as you\
    \ sketch out this dimension of your delivery process as well as some provisional\
    \ answers to them:\n\_\no\_\_How can the delivery process of explaining the AI\
    \ system\u2019s results aid and augment the user\u2019s and decision subject\u2019\
    s _mental models _(their ways of organising and filtering information), so that\
    \ they can get a clear picture of the technical meaning of the\n\n assessment\
    \ or explanation? What is the best way to frame the statistical inferences and\
    \ meanings so that they can be effectively integrated into each user\u2019s own\
    \ cognitive _space of concepts and beliefs_?\n\_\nWhile answering these questions\
    \ will largely depend both on your use case and on the type of AI application\
    \ you are building, it is just as important that you start responding to them\
    \ by concentrating on the differing needs and capabilities of your explainees.\
    \ To do this properly, you should first seek input from domain experts, users,\
    \ and affected stakeholders, so that you can suitably scan the horizons of existing\
    \ needs and capabilities. Likewise, you should take a human-centred approach to\
    \ exploring the types of explanation delivery methods that would best be suited\
    \ for each of your target groups. Much valuable research has been done on this\
    \ in the field of human-computer interaction and in the study of human factors.\
    \ This work should be consulted when mapping delivery means.\n\_\nOnce you have\
    \ gathered enough background information, you should begin to plan out how you\
    \ are going to line up your means of delivery with the varying levels of technical\
    \ literacy, expertise, and cognitive need possessed by the relevant stakeholder\
    \ groups, who will be involved in the implementation of your project. Such a _multi-tiered\
    \ approach _minimally requires that individual attention be paid to the explanatory\
    \ needs and capacities of implementers, system operators, and decision subjects\
    \ and their advocates. This multi-tiered approach will pose different challenges\
    \ at each different level.\n\_\nFor instance, the mental models of implementers\u2014\
    i.e. their ways of conceptualising the information they are receiving from the\
    \ algorithmic decision-support system\u2014 may, in some cases, largely be shaped\
    \ by their accumulation of domain know-how and by the filter of on-the-job expertise\
    \ that they have developed over long periods of practice. These users may have\
    \ a predisposition to automation distrust or aversion bias, and this should be\
    \ taken into account when you are formulating appropriate means of explanation\
    \ delivery.\n\_\nIn other contexts, the opposite may be the case. Where implementers\
    \ tend to over- rely on or over-comply with automated systems, the means of explanation\
    \ delivery must anticipate a different sort of mental model and adjust the presentation\
    \ of information accordingly.\n\_\nIn any event, you will need to have a good\
    \ empirical understanding of your\nimplementer\u2019s decision-making context\
    \ and maintain such knowledge through ongoing assessment. In both bias risk areas,\
    \ the conveyance and communication of the assessments generated by algorithmic\
    \ decision-support systems should attempt\nto bolster each user\u2019s practical\
    \ judgment in ways that mitigate the possibility of either sort of bias. These\
    \ assessments should present results as evidence-based reasons that support and\
    \ better capacitate the objectivity of these implementers\u2019 reasoning processes.\n\
    \n The story is different with regard to the cognitive life of the technically\
    \ inclined user. The mental models of system operators, who are natives in the\
    \ technical vocabulary and epistemic representations of the statistical results,\
    \ may be adept at the model- based problem-solving tasks that arise during implementation\
    \ but less familiar with identifying and responding to the cognitive needs and\
    \ limitations of non-technical stakeholders. Incorporating ongoing communication\
    \ exercises and training into their roles in the delivery process may capacitate\
    \ them to better facilitate implementers\u2019 and decision subjects\u2019 understanding\
    \ of the technical details of the assessments generated by algorithmic decision-support\
    \ systems. These ongoing development\nactivities will not only helpfully enrich\
    \ operators\u2019 mental models, they may also inspire them to develop deeper,\
    \ more responsive, and more effective ways of communicating the technical yields\
    \ of the analytics they oversee.\n\_\nFinally, the mental models of decision subjects\
    \ and their advocates will show the broadest range of conceptualisation capacities,\
    \ so your delivery strategy should (1) prioritise the facilitation of optimal\
    \ explanation at the baseline level of the needs of the most disadvantaged of\
    \ them and (2) build the depth of your multi-tiered approach to providing effective\
    \ explanations into the delivery options presented to decision subjects and their\
    \ advocates. This latter suggestion entails that, beyond provision of the baseline\
    \ explanation of the algorithmically generated result, options should be given\
    \ to decision subjects and their advocates to view more detailed and technical\
    \ presentations of the sort available to implementers and operators (with the\
    \ proviso that reasonable limitations be placed on transparency in accordance\
    \ with the need to protect the confidential personal and organisational information\
    \ and to prevent gaming of the system).\n\_\no\_\_How can non-technical stakeholders\
    \ be adequately prepared to gain baseline knowledge of the kinds of statistical\
    \ and probabilistic reasoning that have factored into the technical interpretation\
    \ of the system\u2019s output, so that they are able to comprehend it on its own\
    \ technical terms? How can the technical components be presented in a way that\
    \ will enable explainees to easily translate the statistical inferences and meanings\
    \ of the results into understandable and rationally assessable terms? What are\
    \ the best available media for presenting the technical results in engaging and\
    \ comprehensible ways?\n\_\nTo meet these challenges, you should consider supplementing\
    \ your implementation platform with knowledge-building and enrichment resources\
    \ that will provide non- technical stakeholders with access to basic technical\
    \ concepts and vocabulary. At a minimum, you should consider building a plain\
    \ language glossary of basic terms and concepts that will include all of the technical\
    \ ideas covered by the algorithmic component of a given explanation. If your explanation\
    \ platforms are digital, you should also make them as user friendly as possible\
    \ by hyperlinking the technical terms used in the explanations to their plain\
    \ language glossary elaborations.\n\_\nWhere possible, explanatory demonstrations\
    \ of technical concepts (like performance metrics, formal fairness criteria, confidence\
    \ intervals, etc.) should be provided to users and decision subjects in an engaging\
    \ and easy-to-comprehend way, and\n\n graphical and visualisation techniques should\
    \ be consistently used to make potentially difficult ideas more accessible. Moreover,\
    \ the explanation interfaces themselves should be as simple, learnable, and usable\
    \ as possible. They should be tested to measure the ease with which those with\
    \ neither technical experience nor domain knowledge are able to gain proficiency\
    \ in their use and in understanding their content.\n\_\nSubstance of the technical\
    \ content to be delivered: The overall interpretability of your AI system will\
    \ largely hinge on the effectiveness and even-handedness of your technical content\
    \ delivery. You will have to strike a balance between (1) determining how best\
    \ to convey and communicate the rationale of the statistical results so that they\
    \ may be treated appropriately as decision supporting and clarifying reasons and\
    \ (2) being clear about the limitations of and potential uncertainties in the\
    \ statistical results themselves so that the explanations you offer will not mislead\
    \ implementers and decision subjects. These are not easy tasks and will require\
    \ substantial forethought as you map out the content clarification aspect of your\
    \ delivery process.\n\_\nTo assist you in this, here is a non-exhaustive list\
    \ of recommendations that you should consider as you map out the execution of\
    \ the technical content delivery component of the responsible implementation of\
    \ your AI project (This list will, for the sake of specificity, assume the predictive\
    \ risk assessment example):\n\_\n\xB7\_\_\_\_\_\_Each explanation should be presented\
    \ in plain, non-technical language and in an optimally understandable way so that\
    \ the results provided can enable the affordance of better judgment on the part\
    \ of implementers and optimal understanding on the part of decision subjects.\
    \ On the implementer\u2019s side, the primary goal of the explanation should be\
    \ to support the user\u2019s ability to offer solid, coherent, and reasonable\
    \ justifications of\ntheir determinations of decision outcomes. On the decision\
    \ subject\u2019s side, the primary goal of the explanation should be to make maximally\
    \ comprehensible the rationale behind the algorithmic component of the decision\
    \ process, so that the decision subject can undertake a properly informed critical\
    \ evaluation of the decision outcome as a whole.\n\_\n\xB7\_\_\_\_\_\_Each explanation\
    \ should present its results as facts or evidence in as sparse but complete and\
    \ sound a manner as possible with a clear indication of what components in the\
    \ explanation are operating as premises, what components are operating as conclusions,\
    \ and what the inferential rationale is that is connecting the premises to the\
    \ conclusions. Each explanation should therefore make explicit the rational criteria\
    \ for its determination whether this be, for example, global inferences drawn\
    \ from the population-based reasoning of a demographic analysis or more locally\
    \ or instance-based inferences drawn from the indication of feature significance\
    \ by a proxy model. In all cases, the optimisation criteria of the operative algorithmic\
    \ system should be specified, made explicit, and connected to the logic and rationale\
    \ of the decision.\n\_\n\xB7\_\_\_\_\_\_Each explanation should make available\
    \ the records and activity-monitoring results that the design and development\
    \ processes of your AI project yielded. Building this link between the process\
    \ transparency dimension of your project and its outcome transparency will help\
    \ to make its result, as a whole, more sufficiently interpretable. This\n\n can\
    \ be done by simply linking or including the public-facing component of the process\
    \ log of your PBG Framework.\n\_\n\xB7\_\_\_\_\_\_Each explanation provided to\
    \ an implementer should come with a standard Implementation Disclaimer that may\
    \ read as follows:\n \n  \n   \n    \n \_\n\xB7\_\_\_\_\_\_Each explanation should\
    \ specify and make explicit its governing performance metrics together with the\
    \ acceptability criteria used to select those metrics and any standard benchmarks\
    \ followed in establishing that criteria. Where appropriate and possible, fuller\
    \ information about model validation measurement (including confusion matrix and\
    \ ROC curve results) and any external validation results should be made available.\n\
    \_\n\xB7\_\_\_\_\_\_Each explanation should provide confirmatory information that\
    \ the formal fairness criteria\nspecified in your project\u2019s Fairness Policy\
    \ Statement has been met.\n\_\n\xB7\_\_\_\_\_\_Each explanation should include\
    \ clear representations of confidence intervals and error bars. These certainty\
    \ estimates should make as quantitatively explicit as possible the confidence\
    \ range of specific predictions, so that users and decision subjects can more\
    \ fully understand their reliability and the levels of uncertainty surrounding\
    \ them.\n\_\n\xB7\_\_\_\_\_\_When an explanation offers categorically ordered\
    \ scores (for instance, risk scores on a scale of 1 to 10), that explanation must\
    \ also explicitly indicate the actual raw numerical probabilities for the labels\
    \ (predicted outcomes) that have been placed into those categories. This will\
    \ help your delivery process avoid producing confusion about the relative magnitudes\
    \ of the categorical groupings under which the various scores fall. Information\
    \ should also be provided about the relative distances between the risk scores\
    \ of specific cases if the risk categories under which they are placed are unevenly\
    \ distributed. It may be possible, for example, for two cases, which fall under\
    \ the same high risk category (say, 9) to be farther apart in terms of the actual\
    \ values of their risk probabilities than two other cases in two different categories\
    \ (say 1 and 4). This may be misleading to the user.\n\n \xB7\_\_\_\_\_\_Each\
    \ explanation should, where possible, include a counterfactual explanatory tool,\
    \ so that implementers and affected individuals have the opportunity to gain a\
    \ better contrastive understanding of the logic of the outcome and its alternative\
    \ possibilities.\n\_\nSocial component of responsible implementation: We have\
    \ now established the first step in the delivery of a responsible implementation\
    \ process: making clear the rationale behind the technical content of an algorithmic\
    \ model\u2019s statistical results and determining how best to convey and communicate\
    \ it so that these results may be appropriately treated as decision supporting\
    \ and clarifying reasons. This leaves us with a second related task of content\
    \ clarification, which is only implicit in the first step but must be made explicit\
    \ and treated reflectively in a second.\n\_\nBeyond translating statistically\
    \ expressed results into humanly significant reasons, you will have to make sure\
    \ that their _socially meaningful content _is clarified by implementers, so that\
    \ they are able to thoughtfully apply these results to the real human lives they\
    \ impact in terms of the specific societal and individual contexts in which those\
    \ lives are situated.\n\_\nThis will involve explicitly translating that model\u2019\
    s technical machinery\u2014its input and output variables, parameters, and functional\
    \ rationale\u2014_back _into the everyday language of the humanly relevant meanings,\
    \ categories, and relationships that informed the formulation of its purpose,\
    \ objectives, and intended elements of design in the first place. It will also\
    \ involve training and preparing implementers to intentionally assist in carrying\
    \ out this translation in each particular case, so that due regard for the dignity\
    \ of decision subjects can be supported by the interpretive charity, reasonableness,\
    \ empathy, and context-specificity of the determination of the outcomes that affect\
    \ them.\n\_\nOnly through this re-translation will the internals, mechanisms,\
    \ and output of the model become _useably interpretable _by implementers: Only\
    \ then will they be able to apply input features of relevance to the specific\
    \ situations and attributes of decision subjects. Only then will they be able\
    \ to critically assess the manner of inference-making that led to its conclusion.\
    \ And only then will they be able to adequately weigh the normative considerations\
    \ (such as prioritising public interest or safeguarding individual well-being)\
    \ that factored into the\nsystem\u2019s original objectives.\n\_\nHaving clarified\
    \ the socially meaningful content of the model\u2019s results, the implementer\
    \ will be able to more readily apply its evidentiary contribution to a more holistic\
    \ and wide-angled consideration of the particular circumstances of the decision\
    \ subject while, at the same time, weighing these circumstances against the greater\
    \ purpose of the algorithmically assisted assessment. It is important to note\
    \ here that the understanding enabled by the clarification of the social context\
    \ and stakes of an algorithmically supported decision-making process goes hand-in-glove\
    \ with fuller considerations of the moral justifiability of the outcome of that\
    \ process.\n\_\nA good starting point for considering how to integrate this clarification\
    \ of the socially meaningful content of an algorithmic model\u2019s output into\
    \ your map of the delivery process is to consider what you might think of as your\
    \ AI project\u2019s content lifecycle.\n\n The content lifecycle: The output of\
    \ an algorithmic system does not begin and end with the computation. Rather, it\
    \ begins with the very human purposes, ideas, and initiatives that lay behind\
    \ the conceptualisation and design of that system. Creating technology is a shared\
    \ public activity, and it is animated by human objectives and beliefs. An algorithmic\
    \ system is brought into the world as the result of this collective enterprise\
    \ of ingenuity, intention, action, and collaboration.\n\_\nHuman choices and values\
    \ therefore punctuate the design and implementation of AI systems. These choices\
    \ and values are inscribed in algorithmic models:\n\_\n\xB7\_\_\_\_\_\_At the\
    \ very inception of an AI project, human choices and values come into play when\
    \ we formulate the goals and objectives to be achieved by our algorithmic technologies.\
    \ They come into play when we define the optimal outcome of our use of such technologies\
    \ and when we translate these goals and objectives into target variables and their\
    \ measurable proxies.\n\_\n\xB7\_\_\_\_\_\_Human choices and values come into\
    \ play when decisions are made about the sufficiency, fit-for-purpose, representativeness,\
    \ relevance, and appropriateness of the data sampled. They come into play in how\
    \ we curate our data\u2014in how we label, organise, and annotate them.\n\_\n\xB7\
    \_\_\_\_\_\_Such choices and values operate as well when we make decisions about\
    \ how we craft a feature space\u2014how we select or omit and aggregate or segregate\
    \ attributes. Determinations of what is relevant, reasonable, desirable, or undesirable\
    \ will factor into what kinds of inputs we are going to include in the processing\
    \ and how we are going to group and separate them.\n\_\n\xB7\_\_\_\_\_\_Moreover,\
    \ the data points themselves are imbued with residua of human choices and values.\
    \ They carry forward historical patterns of social and cultural activity that\
    \ may contain configurations of discrimination, inequality, and marginalisation\u2014\
    configurations that must be thoughtfully and reflectively considered by implementers\
    \ as they incorporate the analytics into their reasoned determinations.\n\_\n\
    Whereas all of these human choices and values are translated in to the algorithmic\
    \ systems we build, the responsible implementation of these systems requires that\
    \ they be translated out. The rationale and logic behind an algorithmic model\u2019\
    s output can be properly understood as it affects the real existence of a decision\
    \ subject only when we transform its variables, parameters, and analytical structures\
    \ back into the human currency of values, choices, and norms that shaped the construction\
    \ of its purpose, its intended design, and its optimisation logic from the start.\n\
    \_\nIt is only in virtue of this re-translation that an algorithmically supported\
    \ outcome can afford stakeholders the degree of deliberation, dialogue, assessment,\
    \ and mutual understanding that is necessary to make it fully comprehensible and\
    \ justifiable to them. And, it is likewise only in virtue of this re-translation\
    \ that the implementation process itself can, at once, secure end-to-end accountability\
    \ and give due regard to the SUM values.\n\n The content lifecycle of algorithmic\
    \ systems therefore has three phases: (1) The translation in of human purposes,\
    \ values, and choices during the design process; (2) The digital processing of\
    \ the quantified/mechanised proxies of these purposes, values, and choices in\
    \ the statistical frame; (3) The translation out of the purposes, values, and\
    \ choices in clarifying the socially meaningful content of the result as it affects\
    \ the life of the decision subject through the implementation process. Here is\
    \ a visualisation of these three phases of the content lifecycle:\n \n  \n   \n\
    \    \n \_\nThe translation rule: A beneficial result of framing the implementation\
    \ process in terms of the content lifecycle is that it gives us a clear and context-sensitive\
    \ measure by which to identify the explanatory needs of any given AI application.\
    \ We can think of this measurement as the translation rule. It states that:\n\_\
    \nWhat is _translated in _to an algorithmic system with regard to the human choices\
    \ and societal values that determine its content and purpose is directly proportional\
    \ to what, in terms of the explanatory needs of clarification and justification,\
    \ must be _translated out_.\n\_\nThe translation rule organically makes two distinctions\
    \ that have great bearing on the delivery process for responsible implementation.\
    \ First, it divides the question of what needs explaining into two parts: (1)\
    \ issues of socially meaningful content in need of clarification (i.e., the explanatory\
    \ need that comes from the translation in to the AI model of the categories, meanings,\
    \ and relations that originate in social practices, beliefs, and intentions)\n\
    (2) issues of normative rightness in need of justification (i.e. the explanatory\
    \ need that comes from translation in to the AI model of choices and considerations\
    \ that have bearing on its ethical permissibility, discriminatory non-harm, and\
    \ public trustworthiness). These two parts line up with what we have above called\
    \ interpretable AI and justifiable AI respectively, and what we have also identified\
    \ as [tasks 2 and 3](#_bookmark26) of delivering transparent AI.\n\_\nSecondly,\
    \ the translation rule divides the two dimensions of translation (translation\
    \ in and translation out) into aspects of intention-in-design and intention-in-application.\
    \ _Translating in_\n\n has to do with _intention-in-design_. It involves an active\
    \ awareness of the human purposes, objectives, and intentions that factor into\
    \ the construction of AI systems. _Translating out, _on the other hand, has directly\
    \ to do with _intention-in-application_, or put differently, the intentional dimension\
    \ of the implementation of an AI system by a user in a specific context and with\
    \ direct consequences for a subject affected by its outcome.\n\_\nIn human beings,\
    \ intention-in-design and intention-in-application are _united in intelligent\
    \ action_, and it is precisely this unity that enables people to reciprocally\
    \ hold each other accountable for the consequences of what they say and what they\
    \ do. By contrast, in artificial intelligence systems, which fulfil surrogate\
    \ cognitive functions in society but are themselves neither intentional nor accountable,\
    \ design and application are divided. In these systems, intention-in-design and\
    \ intention-in-application are and must remain _punctuation points of human involvement\
    \ and responsibility _that manifest on either side of the vacant mechanisms of\
    \ data processing. This is why translation is so important, and this is why enabling\
    \ the\nimplementer\u2019s capacity to _intentionally translate out the social\
    \ and normative content _of the model\u2019s results is such a critical element\
    \ of the responsible delivery of your AI project.\n\_\nIt might be helpful to\
    \ think more concretely about the translation rule by considering it in action.\
    \ Let\u2019s compare two hypothetical examples: (1) a use case about an early\
    \ cancer detection system in radiomics (a machine learning application that uses\
    \ high throughput\ncomputing to identify features of pathology that are undetectable\
    \ to the trained radiological eye); and (2) a use case about a predictive risk\
    \ assessment application that supports decision- making in child social care.\n\
    \_\nIn the radiomics case, the _translating in _dimension involves minimal social\
    \ content: the\nclinical goal inscribed in the model\u2019s objective is that\
    \ of lesion detection and the features of relevance are largely voxels extracted\
    \ from PET and CT scanner images. However, the normative aspect of _translating\
    \ in _is, in this case, significant. Ethical considerations about looking after\
    \ patient wellbeing and clinical safety are paramount and wider justice concerns\
    \ about improving healthcare for all and health equity factor in as well.\n\_\n\
    The explanatory needs of the physician/implementer receiving clinical decision\
    \ support and of the clinical decision subject will thus lean less heavily on\
    \ the dimension of the clarification of socially meaningful content than it will\
    \ on the normative dimension of justifying the safety of the system, the priority\
    \ of the patient\u2019s wellbeing, and the issues of improved delivery and equitable\
    \ access. The technical content of the decision support may be crucial here (Issues\
    \ surrounding the reproducibility of the results and the robustness of the system\
    \ may, in fact, be of great concern in the assessment of the validity of the outcome.),\
    \ but the _translating out _component of the implementation remains directly proportional\
    \ to the minimal social content and to the substantial ethical concerns and objectives\
    \ that were _translated in _and that thus inform the explanatory and justificatory\
    \ needs of the result in general.\n\_\nThe explanatory demands in the child social\
    \ care risk assessment use case are entirely different. The social content of\
    \ the _translating in _dimension is intricate, multi-layered, and extensive. The\
    \ chosen target variable may be child safety or the prevention of severe mistreatment\
    \ and the measurable proxy, home removal of at-risk children within a certain\
    \ timeframe. Selected features that are deemed relevant may include the age of\
    \ the at-risk\n\n children, public health records, previous referrals, family\
    \ history of violent crime, welfare records, juvenile criminal records, demographic\
    \ information, and mental health records. Complex socioeconomic and cultural formations\
    \ may additionally influence the representativeness and quality of the dataset\
    \ as well as the substance of the data itself.\n\_\nThe normative aspect of _translating\
    \ in _here is also subtle and complicated. Ethical considerations about protecting\
    \ the welfare of children at risk are combined with concerns that parents and\
    \ guardians be treated fairly and without discrimination. Objectives of providing\
    \ evidence-based decision support are also driven by hopes that accurate results\
    \ and well-reasoned determinations will preserve the integrity and sanctity of\
    \ familial relations where just, safe, and appropriate. Other goals and purposes\
    \ may be at play as well such as making an overburdened system of service provision\
    \ more efficient or accelerating real-time decision-making without harming the\
    \ quality of the decisions themselves.\n\_\nIn this case of predictive risk assessment,\
    \ the _translating out _burdens of the frontline social worker are immense both\
    \ in terms of clarifying content and in terms of moral justification. If, for\
    \ example, analytical results yielding a high risk score were based on the relative\
    \ feature importance of demographic information, welfare records, mental health\
    \ records, and criminal history, the implementer would have to scrutinise the\
    \ particular decision subject\u2019s situation, so that the socially meaningful\
    \ content of these factors could be clarified in terms of the living context,\
    \ relevant relationships, and behavioural patterns of the stakeholders directly\
    \ affected. Only then could the features of relevance be thoroughly and deliberatively\
    \ assessed.\n\_\nThe effective interpretability of the model\u2019s result would,\
    \ in this case, heavily depend on the implementer\u2019s ability to apply domain-knowledge\
    \ in order to reconstruct the meaningful social formations, intentions, and relationships\
    \ that constituted the concrete form of life in which the predictive risk modelling\
    \ applies. The implementer\u2019s well-reasoned decision here would involve a\
    \ careful weighing of this socially clarified content against the wider predictive\
    \ patterns in the data distribution yielded by the model\u2019s results\u2014\
    patterns that may have otherwise gone unnoticed.\n\_\nSuch a weighing process\
    \ would, in turn, be informed by the normative-explanatory need to translate out\
    \ the morally implicating choices, concerns, and objectives that influenced and\
    \ informed the predictive risk assessment model\u2019s development in the first\
    \ place. Again, the interpretive burden of the frontline social worker would be\
    \ immense here. First, this\nimplementer would have to deliberate with a critically\
    \ informed awareness of the legacies of discrimination and inequity that tend\
    \ to feed forward in the kinds of evidentiary sources drawn upon by the analytics.\
    \ Such an active reflexivity is crucial for retaining the punctuating role of\
    \ human involvement and responsibility in these sensitive and high-stakes environments.\n\
    \_\nJust as importantly, the frontline social worker would have to evaluate the\
    \ real impact of ethical objectives at the point of delivery. Not only would the\
    \ results of the analytics have to be aligned with the ethical concerns and purposes\
    \ that fostered the construction of the model, this implementer would have to\
    \ reflectively align their own potentially diverging ethical point of view both\
    \ with those results and with those objectives. This _normative_\n\n _triangulation\
    \ _between the original intention-in-design, the implementer\u2019s intention-in-\
    \ application, and the content clarification of the AI system\u2019s results is,\
    \ in fact, a crucial safeguard to the delivery of justifiable AI. It again enables\
    \ a reanimation of moral involvement and responsibility at the most critical juncture\
    \ of the content lifecycle.\n\_\nStep 3: Build an ethical implementation platform:\n\
    \_\n\xFF\_\_\_(1) Train ethical implementation. The continuous challenges of translation,\
    \ content clarification, and normative explanation should inform how you set up\
    \ your implementation training to achieve optimal outcome transparency. In addition\
    \ to the necessary [training to](#_bookmark13) [prevent implementation biases\
    \ in the users of your AI system](#_bookmark13) (discussed above), you should\
    \ prepare and train the implementers to be stewards of interpretable and justifiable\
    \ AI. This entails that they be able to:\n\_\no\_\_Rationally evaluate and critically\
    \ assess the logic and rationale behind the outputs of the AI systems;\n\_\no\_\
    \_Convey and communicate their algorithmically assisted decisions to the individuals\
    \ affected by them in plain language. This includes explaining to them in an everyday,\
    \ non-technical, and accessible way how and why the decision-supporting model\
    \ performed the way it did in a specific context and how that result factored\
    \ into the final outcome of the implementation;\n\_\no\_\_Apply the conclusions\
    \ reached by the AI model to a more focused consideration of the particular social\
    \ circumstances and life context of the decision subject and other affected parties;\n\
    \_\no\_\_Treat the inferences drawn from the results of the model\u2019s computation\
    \ as evidentiary contributions to a broader, more rounded, and coherent understanding\
    \ of the individual situations of the decision subject and other affected parties;\n\
    \_\no\_\_Weigh the interpretive understanding gained by integrating the model\u2019\
    s insights into this rounded picture of the life context of the decision subject\
    \ against the greater purpose and societal objective of the algorithmically assisted\
    \ assessment;\n\_\no\_\_Justify the ethical permissibility, the discriminatory\
    \ non-harm, and the public\ntrustworthiness both of the AI system\u2019s outcome\
    \ and of the processes behind its design and use\n\_\n\xFF\_\_\_(2) Make your\
    \ implementation platform a relevant part and capstone of the sustainability track\
    \ of your project. An important element of gauging the impacts of your AI technology\
    \ on the individuals and communities it touches is having access to the frontlines\
    \ of its potentially transformative and long-term effects. Your implementation\
    \ platform should assist you in gaining this access by being a _two-way medium\
    \ of application and communication_. It should both enable you to sustainably\
    \ achieve the objectives and goals you set for your project through responsible\
    \ implementation, but it should also be a sounding board as well as a site for\
    \ feedback and cooperative sense-checking about the real-life effects of your\
    \ system\u2019s use.\n\n Your implementation platform should be dialogically and\
    \ collaboratively connected to the stakeholders it effects. It should be bound\
    \ to the communities it serves as part of a shared project to advance their immediate\
    \ and long-run wellbeing.\n\_\n\xFF\_\_\_(3) Provide a model sheet to implementers\
    \ and establish protocols for implementation reporting. As part of the roll-out\
    \ of your AI project, you should prepare a summary/model sheet for implementers,\
    \ which includes summation information about the system\u2019s technical specifications\
    \ and all of the relevant details indicated above in the section on _substance\
    \ of the technical content to be delivered_. This should include relevant information\
    \ about performance metrics, formal fairness criteria and validation, the implementation\
    \ disclaimer, links or summaries to the relevant information from the process\
    \ logs of your PBG Framework, and links or summary information from the Stakeholder\
    \ Impact Assessment.\n\_\nYou should also set up protocols for implementation\
    \ reporting that are proportional to the\npotential impacts and risks of the system\u2019\
    s use.\n\_\n\xFF\_\_\_\_(4) Foster outcome understanding through dialogue. Perhaps\
    \ the single most important aspect of building a platform for ethical implementation\
    \ is the awareness that the realisation of interpretable and justifiable AI is\
    \ a dialogical and collaborative effort. Because all types of explanation are\
    \ mediated by language, each and every explanatory effort is a participatory enterprise\
    \ where understanding can be reached only through acts of communication. The interpretability\
    \ and justifiability of AI systems depend on this shared human capacity to give\
    \ and ask for reasons in the ends of reaching mutual understanding. Implementers\
    \ and decision subjects are, in this respect, first and foremost participants\
    \ in an explanatory dialogue, and the success of their exchange will hinge both\
    \ on a reciprocal readiness take the other\u2019s perspective and on a willingness\
    \ to enlarge their respective mental models in accordance with new, communicatively\
    \ achieved, insights and understandings.\n\_\nFor these reasons, your implementation\
    \ platform should encourage open, mutually respectful, sincere, and well-informed\
    \ dialogue. Reasons from all affected voices must be heard and considered as demands\
    \ for explanation arise, and manners of response and expression should remain\
    \ clear, straightforward, and optimally accessible. Deliberations that have been\
    \ inclusive, unfettered, and impartial tend to generate new ideas and insights\
    \ as well as better and more inferentially sound conclusions, so approaching the\
    \ interpretability and justifiability of your AI project in this manner will not\
    \ only advance its responsible implementation, it will likely encourage further\
    \ improvements in its design, delivery, and performance.\n\nLeslie, 2019, p.54-68\n"
  Linked Principles:
  - recOHnq45Fq7YWsRO
  - recZToVrPeFlFq0Aw
  Linked Sources:
  - recfYC5jjPmpLfSlM
  Tags:
  - reflection-discussion
  airtable_createdTime: '2023-05-19T12:03:02.000Z'
  airtable_id: recZPU5yTmrjPXlF2
  title: Principle of responsible delivery through human-centred implementation, key
    considerations
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CAccountability \n\u201CAbility to redress: \n\uF0FC Did you\
    \ establish an adequate set of mechanisms that allows for redress in case of the\
    \ occurrence of any harm or adverse impact? \n\uF0FC Did you put mechanisms in\
    \ place both to provide information to (end-)users/third parties about opportunities\
    \ for redress?\u201D (High-Level Expert Group on AI, 2019, p. 31)\n"
  Linked Principles:
  - recKdujFoPJr4ZAhZ
  - rec42P8U9usfYCtv9
  - reclPiw2VvNOSTzv5
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:50:07.000Z'
  airtable_id: recaTTvWN1olqx608
  title: Considerations in assessing trustworthy AI - Minimising and reporting negative
    impacts
- Description: "\u201CHow are we recognizing the autonomy of others and acknowledging\
    \ that they are of equal worth to ourselves and should be treated so? \xA7 Will\
    \ informed consent be required from participants? \xA7 If so, what procedures\
    \ to obtain consent will be followed? (E.g., print or digital signatures, virtual\
    \ consent tokens, click boxes or waiver of documented consent)28 \xA7 Will consent\
    \ be obtained just from individuals or from communities and online system administrators?\
    \ \xA7 In situations whereby consent is desired but written informed consent is\
    \ impossible (or in regulatory criteria, impracticable) or potentially harmful,\
    \ will procedures or requirements be modified? \xA7 What harm might result from\
    \ asking for consent, or through the process of asking for consent?29 \xA7 What\
    \ ethical concerns might arise if informed consent is not obtained? \xA7 If an\
    \ ethics board deems no consent is required, will the researcher still seek subjects\u2019\
    /participants\u2019 consent in a non-regulatory manner? \xA7 If informed consent\
    \ is warranted, how will the researcher ensure that participants are truly informed?\u201D\
    \ (Markham and Buchanan, 2012, p. 11)\n"
  Linked Principles:
  - recsvi4LnhEEPyQ1h
  Linked Sources:
  - recQiVQ7CTC72xp6O
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T08:28:21.000Z'
  airtable_id: recazD3B5XpqgOCGV
  title: Questions to consider regarding autonomy and informed consent
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CAccountability \n\u201CDocumenting trade-offs:\n\uF0FC Did\
    \ you establish a mechanism to identify relevant interests and values implicated\
    \ by the AI system and potential trade-offs between them? \n\uF0FC How do you\
    \ decide on such trade-offs? Did you ensure that the trade-off decision was documented?\u201D\
    \ (High-Level Expert Group on AI, 2019, p. 31) c\n"
  Linked Principles:
  - recKdujFoPJr4ZAhZ
  - rec42P8U9usfYCtv9
  - recmzjcGKv3yNOxbl
  - recSqx6wklVpDzx3s
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:49:50.000Z'
  airtable_id: reccAKmFQRH7IiuJi
  title: Considerations in assessing trustworthy AI - Documenting trade-offs
- Description: "## Recommendation\nEstablish a leading role for [intercultural information\
    \ ethics ](http://www.capurro.de/iie.html)(IIE) practitioners in ethics committees\
    \ informing technologists, policy makers, and engineers. Clearly demonstrate through\
    \ examples how cultural variation informs not only information flows and information\
    \ systems, but also algorithmic decision-making and value by design.\n## Further\
    \ Resources\n\u2022\_\_\_\_\_D. J. Pauleen, et al. \u201C[Cultural Bias in Information\
    \ Systems Research and Practice: Are You Coming From the Same Place I Am? ](http://aisel.aisnet.org/cais/vol17/iss1/17/)\u201D\
    \ _Communications of the Association for Information Systems, _vol._ _17, no.\
    \ 17, 2006.\n\u2022\_\_\_\_\_J. Bielby, \u201C[Comparative Philosophies in Intercultural\
    \ Information Ethics](https://scholarworks.iu.edu/iupjournals/index.php/confluence/article/view/540),\u201D\
    \ _Confluence: Online Journal of World Philosophies _2, no. 1, pp. 233\u2013253,\
    \ 2016.\n\_\np.124\n\n## Recommendation\nTo develop A/IS capable of following\
    \ social and moral norms, the first step is to identify the norms of the specific\
    \ community in which the\_A/IS are to be deployed and, in particular, norms relevant\
    \ to the kinds of tasks and roles that the A/IS are designed for. This norm identification\
    \ process must use appropriate scientific methods and continue through the system's\
    \ life cycle.\n## Further Resources\n\u2022\_\_\_\_\_Mack, Ed., \u201CChanging\
    \ social norms.\u201D _Social Research: An International Quarterly,_ 85, no.1,\
    \ 1\u2013271, 2018.\n\u2022\_\_\_\_\_I. Misra, C. L. Zitnick, M. Mitchell, and\
    \ R. Girshick, (2016). Seeing through the human reporting bias: Visual Classifiers\
    \ from Noisy Human-Centric Labels. In _Proceedings of the 2016 IEEE Conference\
    \ on Computer Vision and Pattern Recognition_ (CVPR), pp. 2930\u20132939. doi[:10.1109/CVPR.2016.320](https://doi.org/10.1109/CVPR.2016.320)\n\
    \u2022\_\_\_\_\_I. van de Poel, \u201C[An Ethical Framework for Evaluating Experimental\
    \ Technology,](https://link.springer.com/article/10.1007/s11948-015-9724-3)\u201D\
    \ _Science and Engineering Ethics_, 22, no. 3,pp. 667686, 2016.\"\np.168-169\n\
    \n## Recommendation\nA/IS developers should identify the ways in which people\
    \ resolve norm conflicts and the ways in which they expect A/IS to resolve similar\
    \ norm conflicts. A system\u2019s resolution of norm conflicts must be transparent\u2014\
    that is, documented by the system and ready to be made available to users, the\
    \ relevant community of deployment, and third-party evaluators.\n## Further resources\n\
    \u2022\_\_\_\_\_M. Velasquez, C. Andre, T. Shanks, S.J., and M. J. Meyer, \u201C\
    [The Common](https://legacy.scu.edu/ethics/publications/iie/v5n1/common.html)\
    \ [Good](https://legacy.scu.edu/ethics/publications/iie/v5n1/common.html).\u201D\
    \ _Issues in Ethics_, vol._ _5, no. 1, 1992.\n\u2022\_\_\_\_\_J. Van den Hoven,\
    \ \u201CEngineering and the Problem of Moral Overload.\u201D _Science and Engineering\
    \ Ethics, vol. _18, no. 1, pp.\_143\u2013155, 2012.\n\u2022\_\_\_\_\_D. Abel,\
    \ J. MacGlashan, and M. L. Littman. \u201CReinforcement Learning as a Framework\
    \ for Ethical Decision Making.\u201D _AAAI Workshop AI, Ethics, and Society, Volume\
    \ WS-16-02 of 13th AAAI Workshops_. Palo Alto, CA: AAAI\_Press, 2016.\n\u2022\_\
    \_\_\_\_O. Bendel, Die Moral in der Maschine: Beitr\xE4ge zu Roboter- und Maschinenethik.\
    \ Hannover, Germany: Heise Medien, 2016.\_\_\_Accessible popular-science contributions\
    \ to philosophical issues and technical implementations of machine ethics\n\u2022\
    \_\_\_\_\_S. V. Burks, and E. L. Krupka. [\u201CA Multimethod Approach to Identifying\
    \ Norms and Normative Expectations within a Corporate Hierarchy: ](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1110.1478)\n\
    [Evidence from the Financial Services Industry.\u201D](https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1110.1478)\
    \ _Management Science, _vol. 58, pp. 203\u2013217, 2012.\_\_\_\_Illustrates surveys\
    \ and incentivized coordination games as methods to elicit norms in a large financial\
    \ services firm\n\u2022\_\_\_\_\_F. Cushman, V. Kumar, and P. Railton, \u201C\
    Moral Learning,\u201D _Cognition_, vol._ _167, pp. 1\u2013282, 2017.\n\u2022\_\
    \_\_\_\_M. Flanagan, D. C. Howe, and H. Nissenbaum, \u201CEmbodying Values in\
    \ Technology: Theory and Practice.\u201D _Information Technology and Moral _\n\
    _Philosophy_, J. van den Hoven and J. Weckert, Eds., Cambridge University Press,\
    \ 2008, pp. 322\u201353. Cambridge Core, _Cambridge University Press._ Preprint\
    \ available at\_[http://www.nyu.edu/projects/nissenbaum/ papers/Nissenbaum-VID.4-25.pdf](http://www.nyu.edu/projects/nissenbaum/papers/Nissenbaum-VID.4-25.pdf)\n\
    \u2022\_\_\_\_\_B. Friedman, P. H. Kahn, A. Borning, and A. Huldtgren. \u201C\
    Value Sensitive Design and Information Systems,\u201D in _Early Engagement and\
    \ New Technologies: Opening up the Laboratory, _N. Doorn, Schuurbiers, I. van\
    \ de Poel, and M. Gorman, Eds., vol. 16, pp. 55\u201395. Dordrecht: Springer,\
    \ 2013.\_\_\_A comprehensive introduction into Value Sensitive Design and three\
    \ sample applications\n\u2022\_\_\_\_\_G. Mackie, F. Moneti, E. Denny, and H.\
    \ Shakya. \u201CWhat Are Social Norms? How Are They Measured?\u201D UNICEF Working\
    \ Pape[r. ](http://dmeforpeace.org/sites/default/files/4%2009%2030%20Whole%20What%20are%20Social%20Norms.pdf)University\
    \ of California at San Diego: UNICEF, Sept. 2014. [https://dmeforpeace.org/sites/\
    \ default/files/4%2009%2030%20Whole%20 What%20are%20Social%20Norms.pdf](https://dmeforpeace.org/sites/default/files/4%2009%2030%20Whole%20What%20are%20Social%20Norms.pdf)\_\
    \_A broad survey of conceptual and measurement questions regarding social norms.\n\
    \u2022\_\_\_\_\_J. A. Leydens and J. C. Lucena. Engineering Justice: Transforming\
    \ Engineering Education and Practice. Hoboken, NJ: John Wiley & Sons, 2018.\_\_\
    Identifies principles of engineering for social justice.\n\u2022\_\_\_\_\_B. F.\
    \ Malle, \u201CIntegrating Robot Ethics and Machine Morality: The Study and Design\
    \ of Moral Competence in Robots.\u201D _Ethics and Information Technology, _vol._\
    \ _18, no. 4, pp. 243\u2013256, 2016.\_\_Discusses how a robot\u2019s norm capacity\
    \ fits in the larger vision of a robot with moral competence.\n\u2022\_\_\_\_\_\
    K. W. Miller, M. J. Wolf, and F. Grodzinsky, \u201CThis \u2018Ethical Trap\u2019\
    \ Is for Roboticists, Not Robots: On the Issue of Artificial Agent Ethical DecisionMaking.\u201D\
    \ _Science and Engineering Ethics, _vol._ _23, pp. 389\u2013401, 2017.\_\_\_This\
    \ article raises doubts about the possibility of imbuing artificial agents with\
    \ morality, or of claiming to have done so.\n\u2022\_\_\_\_\_Open Roboethics Initiative:\
    \ [www.openroboethics.org](http://www.openroboethics.org/). A series of poll results\
    \ on differences in human moral decision-making and changes in priority order\
    \ of values for autonomous systems (e.g., [on care](http://www.openroboethics.org/results-should-a-carebot-bring-an-alcoholic-a-drink-poll-says-it-depends-on-who-owns-the-robot/)\
    \ [robots)](http://www.openroboethics.org/results-should-a-carebot-bring-an-alcoholic-a-drink-poll-says-it-depends-on-who-owns-the-robot/),\
    \ 2019.\n\u2022\_\_\_\_\_A. Rizzo and L. L. Swisher, \u201CComparing the Stewart\u2013\
    Sprinthall Management Survey and the Defining Issues Test-2 as Measures of Moral\
    \ Reasoning in Public Administration.\u201D _Journal of Public Administration\
    \ Research\_and Theory, _vol._ _14, pp. 335\u2013348, 2004. Describes two assessment\
    \ instruments of moral reasoning (including norm maintenance) based on Kohlberg\u2019\
    s theory\_of moral development.\n\u2022\_\_\_\_S. H. Schwartz, \u201CA[n Overview\
    \ of the Schwartz](https://scholarworks.gvsu.edu/orpc/vol2/iss1/11/)\_[Theory\
    \ of Basic Values.\u201D _Online Readings in Psychology and Culture _2, 2012](https://scholarworks.gvsu.edu/orpc/vol2/iss1/11/).\
    \ \u2022\_\_\_\_\_Comprehensive overview of a specific theory of values, understood\
    \ as motivational orientations toward abstract outcomes (e.g., self-direction,\
    \ power, security).\n\u2022\_\_\_\_\_S. H. Schwartz and K. Boehnke. \u201C[Evaluating\
    \ the Structure of Human Values with Confirmatory Factor Analysis.\u201D _Journal\
    \ of Research in Personality, _vol. 38, ](http://www.sciencedirect.com/science/article/pii/S0092656603000692?via%3Dihub)pp.\
    \ 230\u2013255, 2004.\n\u2022\_\_\_\_\_Describes an older method of subjective\
    \ judgments of relations among valued outcomes and a newer, formal method of analyzing\
    \ these relations.\n\u2022\_\_\_\_\_W. Wallach and C. Allen. _Moral Machines:\
    \ Teaching Robots Right from Wrong_. New York: Oxford University Press, 2008.\
    \ This book describes some of the challenges of having a one-size-fits-all approach\
    \ to embedding human values in autonomous systems. \"\np.172-174\n"
  Linked Challenges:
  - recxjc79LvLdKa4rl
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T10:21:20.000Z'
  airtable_id: reccMqFCLOgQwWM5Q
  title: Community norms should be identified, and expertise from intercultural information
    ethics practitioners embedded in ethics committees
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CTransparency \nTraceability: \n\uF0FC Did you establish measures\
    \ that can ensure traceability? This could entail documenting the following methods:\
    \ \n\uF0D8 Methods used for designing and developing the algorithmic system: \n\
    o Rule-based AI systems: the method of programming or how the model was built;\
    \ \no Learning-based AI systems; the method of training the algorithm, including\
    \ which input data was gathered and selected, and how this occurred.\n\uF0D8 Methods\
    \ used to test and validate the algorithmic system: o Rule-based AI systems; the\
    \ scenarios or cases used in order to test and validate; \no Learning-based model:\
    \ information about the data used to test and validate. \n\uF0D8 Outcomes of the\
    \ algorithmic system: \no The outcomes of or decisions taken by the algorithm,\
    \ as well as potential other decisions that would result from different cases\
    \ (for example, for other subgroups of users).\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 28-29)\n"
  Linked Principles:
  - recxcFmvPG5wrCqpO
  - recQEiU22Qy1E0YuA
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:33:40.000Z'
  airtable_id: recdtW2BdWmY6WSP5
  title: Considerations in assessing trustworthy AI - Traceability
- Description: "- Is the teacher role clearly defined so as to ensure that there is\
    \ a teacher in the loop while the AI system is being used? How does the AI system\
    \ affect the didactical role of the teacher?\n- Are the decisions that impact\
    \ students conducted with teacher agency and is the teacher able to notice anomalies\
    \ or possible discrimination? \n- Are procedures in place for teachers to monitor\
    \ and intervene, for example in situations where empathy is required when dealing\
    \ with learners or parents? \n- Is there a mechanism for learners to opt-out if\
    \ concerns have not been adequately addressed? \n- Are there monitoring systems\
    \ in place to prevent overconfidence in or overreliance on the AI system? \n-\
    \ Do teachers and school leaders have all the training and information needed\
    \ to effectively use the system and ensure it is safe and does not cause harms\
    \ or violate rights of students?\u201D \n(European Commission, 2022, p. 19)\n"
  Linked Cases:
  - reciNqxyfUgE5XM7t
  - recOOmVQviRyJGvea
  Linked Principles:
  - recLHILkx2JDFsLbX
  Linked Sources:
  - rec9jnxuHOioQn4DC
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T13:32:23.000Z'
  airtable_id: recegTm800wJXGjO1
  title: Guiding questions for educators regarding Human Agency and Oversight of AI
    in Education
- Description: "\u201CWho is involved in the study? \xA7 What are the ethical expectations\
    \ of the community/participants/authors? \xA7 What is the ethical stance of the\
    \ researcher? (For example, a mismatch between the ethical stance of the researcher\
    \ and the community/participant/author may create ethical complications).18 \xA7\
    \ What are the ethical traditions of researchers\u2019 and/or author/participants\u2019\
    \ cultures or countries? \xA7 If research data is housed in a repository for reuse,\
    \ how might individuals or communities be affected later? For example, data collected\
    \ for one purpose might be reused later for a different purpose but the researcher\u2019\
    s relationship with the community from which the data came no longer exists. What\
    \ possible risk or harm might result from reuse and publication of this information?1\u201D\
    \ (Markham and Buchanan, 2012, p. 9)\n"
  Linked Principles:
  - recKdujFoPJr4ZAhZ
  Linked Sources:
  - recQiVQ7CTC72xp6O
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-18T13:54:19.000Z'
  airtable_id: recf50wvya0NXxdxz
  title: Questions to consider regarding the norms and values of those involved in
    the study and alignment of these to the research
- Description: "## Recommendations\nThe building blocks of such practices include\
    \ top-down leadership, bottom-up empowerment, ownership, and responsibility, along\
    \ with the need to consider system deployment contexts and/or ecosystems. Corporations\
    \ should identify stages in their processes in which ethical considerations, \u201C\
    ethics filters\u201D, are in place before products are further developed and deployed.\
    \ For instance, if an ethics review board comes in at the right time during the\
    \ A/IS creation process, it would help mitigate the likelihood of creating ethically\
    \ problematic designs. The institution of an ethical A/IS corporate culture would\
    \ accelerate the adoption of the other recommendations within this section focused\
    \ on business practices.\n## Further Resources\n\u2022\_\_\_\_\_[ACM Code of Ethics\
    \ and Professional Ethics,](https://ethics.acm.org/2018-code-draft-2/) which includes\
    \ various references to human well-being and human rights, 2018.\n\u2022\_\_\_\
    \_\_Report of UN Special Rapporteur on [Freedom of Expression. _AI and Freedom\
    \ of Expression_.](http://undocs.org/A/73/348) 2018.\n\u2022\_\_\_\_\_The [website\
    \ of the Benefit corporations ](https://www.bcorporation.net/)(B-corporations)\
    \ provides a good overview of a range of companies that personify this type of\
    \ culture.\n\u2022\_\_\_\_\_R. Sisodia, J. N. Sheth and D. Wolfe, [Firms of ](http://www.firmsofendearment.com/)\n\
    [Endearment](http://www.firmsofendearment.com/)_, _2nd edition. Upper Saddle River,\
    \ NJ: FT Press, 2014. This book showcases how companies embracing values and a\
    \ stakeholder approach outperform their competitors in the long run.\"\np.127\n\
    \n\"Organizations should identify points for formal review during product development.\
    \ These reviews can focus on \u201Cred flags\u201D that have been identified in\
    \ advance as indicators of risk. For example, if the datasets involve minors or\
    \ focus on users from protected classes, then it may require additional justification\
    \ or alterations to the research or development protocols.\"\n\n## Further Resources\n\
    \u2022\_\_\_\_\_A. Sinclair, \u201C[Approaches to Organizational Culture and Ethics,](https://doi.org/10.1007/BF01845788)\u201D\
    \ _Journal of Business Ethics, _vol._ _12, no. 1, pp. 63\u201373, 1993.\n\u2022\
    \_\_\_\_\_Al Y. S. Chen, R. B. Sawyers, and P. F. Williams. \u201C[Reinforcing\
    \ Ethical Decision Making Through Corporate Culture,](https://link.springer.com/article/10.1023/A:1017953517947)_\u201D\
    \ Journal of Business Ethics _16, no. 8, pp. 855\u2013865, 1997.\n \u2022 K. Crawford\
    \ and R. Calo, \u201C[There Is a Blind Spot in AI Research,](http://www.nature.com/news/there-is-a-blind-spot-in-ai-research-1.20805)\u201D\
    \ _Nature _538, pp. 311\u2013313, 2016.\n \np.132\n"
  Linked Challenges:
  - recqpYMjEFJLmyNaN
  - rect310qem9li3HKK
  - recA7Kh502s4UKWGo
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T10:26:39.000Z'
  airtable_id: recfcXzM3foqFNNGN
  title: Corporations should implement "ethics filters" throughout the development
    process
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CSocietal and environmental well-being \n\u201CSociety and democracy:\
    \ \n\uF0FC Did you assess the broader societal impact of the AI system\u2019s\
    \ use beyond the individual (end-)user, such as potentially indirectly affected\
    \ stakeholders?\u201D (High-Level Expert Group on AI, 2019, p. 31)\n"
  Linked Principles:
  - reckb3cgfeDh1EeUP
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:46:30.000Z'
  airtable_id: recffbHgJO0d179DG
  title: Considerations in assessing trustworthy AI - Society and democracy
- Description: "## \"Recommendations\nTo thrive in the A/IS age, workers must be provided\
    \ training in skills that improve their adaptability to rapid technological changes;\
    \ programs should be available to any worker, with special attention to the low-skilled\
    \ workforce. Those programs can be private, that is, sponsored by the employer,\
    \ or publicly and freely offered through specific public channels and government\
    \ policies, and should be available regardless of whether the worker is in between\
    \ jobs or still employed. Specific measures include:\n\u2022\_\_\_\_\_Offering\
    \ new technical programs, possibly earlier than high school, to increase the workforce\
    \ capacity to close the skills gap and thrive in employment alongside A/IS. \n\
    \u2022\_\_\_\_\_Creating opportunities for apprenticeships, pilot programs, and\
    \ scaling up data-driven evidence-based solutions that increase employment and\
    \ earnings.\n\u2022\_\_\_\_\_Supporting new forms of public-private partnerships\
    \ involving civil society, as well as new outcome-oriented financial mechanisms,\
    \ e.g., social impact bonds, that help scale up successful innovations.\n\u2022\
    \_\_\_\_\_Supporting partnerships between universities, innovation labs in corporations,\
    \ and governments to research and incubate startups for A/IS graduates.23\n\u2022\
    \_\_\_\_\_Developing regulations to hold corporations responsible for employee\
    \ retraining necessary due to increased automation and other technological applications\
    \ having impact\_on the workforce.\n\u2022\_\_\_\_\_Facilitating private sector\
    \ initiatives by public policy for co-investment in training and retraining programs\
    \ through tax incentives.\n\u2022\_\_\_\_\_Establishing and resourcing public\
    \ policies that assure the survival and well-being of workers, displaced by A/IS\
    \ and automation, who cannot be retrained.\n\u2022\_\_\_\_\_Researching complementary\
    \ areas, to lay solid foundations for the transformation outlined above.\n\u2022\
    \_\_\_\_\_Requiring more policy research on the dynamics of professional transitions\
    \ in different labor market conditions.\n\u2022\_\_\_\_\_Researching the fairest\
    \ and most efficient public-private options for financing labor force transformation\
    \ due to A/IS.\n\u2022\_\_\_\_\_Developing national and regional future of work\
    \ strategies based on sound research and strategic foresight.\n## Further Resources\n\
    \u2022\_\_\_\_\_V. Cerf and D. Norfors, The People-centered Economy: The New Ecosystem\
    \ for Work. California: IIIJ Foundation, 2018.\n\u2022\_\_\_\_\_Executive Office\
    \ of the President. _Artificial Intelligence, Automation, and the Economy._ December\
    \ 20, 2016.\n\u2022\_\_\_\_\_S. Kilcarr, \u201CDefining the American Dream for\
    \ Trucking ... and the Nation, Too,\u201D _FleetOwner_, April 26, 2016.\n\u2022\
    \_\_\_\_\_M. Mason, \u201CMillions of Californians\u2019 Jobs could be Affected\
    \ by Automation\u2014a Scenario the next Governor has to Address,\u201D_Los Angeles\
    \ Times_, October 14, 2018.\n\u2022\_\_\_\_\_OECD, \u201CLabor Market Programs:\
    \ Expenditure and Participants,\u201D _OECD Employment and Labor Market Statistics\
    \ _(database), 2016.\n\u2022\_\_\_\_\_M. Vivarelli, \u201CInnovation and Employment:\
    \ ASurvey,\u201D Institute for the Study of Labor (IZA) Discussion Paper No. 2621,\
    \ February 2007.\n\"\np.147-149\n\n## Recommendations\nWhile there is evidence\
    \ that robots and automation are taking jobs away in various sectors, a more balanced,\
    \ granular, analytical, and objective treatment of A/IS impact on the workforce\
    \ is needed to effectively inform policy making and essential workforce reskilling.\
    \ Specifics to accomplish this include:\n\u2022\_\_\_\_\_Creating an international\
    \ and independent agency able to properly disseminate objective statistics and\
    \ inform the media, as well as the general public, about the impact of robotics\
    \ and A/IS on jobs, tax revenue, growth,26 and well-being.\n\u2022\_\_\_\_\_Analyzing\
    \ and disseminating data on how current task content of jobs have changed, based\
    \ on a clear assessment of the automatability of the occupational\_description\
    \ of such jobs.\n\u2022\_\_\_\_\_Promoting automation with augmentation, as recommended\
    \ in the _Future of Jobs Report 2018_ (see chart on page 154), to maximize the\
    \ benefit of A/IS to employment and meaningful work.\n\u2022\_\_\_\_\_Integrating\
    \ more granulated dynamic mapping of the future jobs, tasks, activities, workplace-structures,\
    \ associated work-habits, and skills base spurred by the A/IS revolution, in order\
    \ to innovate, align, and synchronize skill development and training programs\
    \ with future requirements. This workforce mapping is needed at the macro, but\
    \ also crucially at the micro, levels where labor market programs\_are deployed.\n\
    \u2022\_\_\_\_\_Considering both product and process innovation, and looking at\
    \ them from a global perspective in order to understand properly the global impact\
    \ of A/IS on employment.\n\u2022\_\_\_\_\_Proposing mechanisms for redistribution\
    \ of productivity increases and developing an adaptation plan for the evolving\
    \ labor market.\n## Further Resources\n\u2022\_\_\_\_\_E. Brynjolfsson and A.\
    \ McAfee. The Second Age of Machine Intelligence: Work Progress and Prosperity\
    \ in a Time of Brilliant Technologies. New York, NY: W. W. Norton & Company, 2014.\n\
    \u2022\_\_\_\_\_P.R. Daugherty, and H.J. Wilson, Human + Machine: Reimagining\
    \ Work in the Age of AI_. _Watertown, MA:_ _Harvard Business Review Press, 2018.\n\
    \u2022\_\_\_\_\_International Federation of Robotics. \u201CThe Impact of Robots\
    \ on Productivity, Employment and Jobs,\u201D A positioning paper by the International\
    \ Federation of Robotics, April 2017.\n\u2022\_\_\_\_\_RockEU. \u201CRobotics\
    \ Coordination Action for Europe Report on Robotics and Employment,\u201D Deliverable\
    \ D3.4.1, June 30, 2016.\n\u2022\_\_\_\_\_World Economic Forum, Centre for the\
    \ New Economy and Society, _The Future of Jobs 2018_, Geneva: WEF 2018.\"\n150-152\n"
  Linked Challenges:
  - recb50cfuQUWDAHZW
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T11:49:40.000Z'
  airtable_id: recg2u6ZXSRiUO8uF
  title: Training in skills for adaptability to rapid technological changes informed
    by improved data regarding labour pattern shifts
- Description: "The close of the project includes data archiving and model storage\
    \ for future access, deployment and development, and for third parties to replicate\
    \ similar results on other datasets or reuse the dataset (e.g. with permission)\
    \ on other research questions. Established repositories such as data archives\
    \ can assist researchers in post-research data governance.\_\n_\__\nPost-research\
    \ Data Governance \nSystems and research design will never be as robust as intended.\
    \ To mitigate unforeseen risks, researchers must be prepared and manage the unknown,\
    \ also after the project has been completed. For example, when a dataset containing\
    \ sensitive information is disclosed by a third-party unexpectedly, researchers\
    \ must alert data subjects so they can take precautions.\_\n\_\n\u25CF\_\_\_\_\
    \_Are the datasets and models stored securely? \n\u25CF\_\_\_\_\_Are some datasets\
    \ more sensitive than others and do they warrant special security precautions?\
    \ \n\u25CF\_\_\_\_\_Will the data be destroyed at a specific date? How will this\
    \ data be destroyed? Or will they be anonymized and archived at a specific date?\
    \ \n\u25CF\_\_\_\_\_How might the data and model be accessed through an application\
    \ process and what potential harm to data subjects and/or society might this future\
    \ access have? \n\u25CF\_\_\_\_\_Is there a containment policy for unexpected\
    \ breaches or malicious uses and what does it oblige the researcher to do or will\
    \ this responsibility go to the archival organization? \n\u25CB Will researchers\
    \ contact the data subjects and/or the relevant privacy regulator directly about\
    \ a breach?\_\n\u25CB To what extent does this depend on the seriousness of the\
    \ disclosure or the sensitivity of the data? \n\u25CB How will harmed data subjects\
    \ or stakeholders be compensated? \n\n(franzke, 2020, p.46-47)\n"
  Linked Principles:
  - rec42P8U9usfYCtv9
  Linked Sources:
  - rec6r8OkE2Q2EdiM3
  Tags:
  - reflection-questions
  - data-governance
  - internet-research
  airtable_createdTime: '2023-05-19T12:45:30.000Z'
  airtable_id: recgNdOlcMTiussUk
  title: Questions to consider in key stages of AI and machine learning based research,
    regarding project data governance
- Description: "\u201CHow are data being managed, stored, and represented? \xA7 What\
    \ method is being used to secure and manage potentially sensitive data? \xA7 What\
    \ unanticipated breaches might occur during or after the collection and storage\
    \ of data or the production of reports?20 (For example, if an audience member\
    \ recorded and posted sensitive material presented during an in-house research\
    \ presentation, what harms might result? \xA7 If the researcher is required to\
    \ deposit research data into a repository for future use by other researchers\
    \ (or wishes to do so), what potential risks might arise? What steps should be\
    \ taken to ensure adequate anonymity of data or to unlink this data from individuals?\
    \ \xA7 What are the potential ethical consequences of stripping data of personally\
    \ identifiable information? \xA7 How might removal of selected information from\
    \ a dataset distort it such that it no longer represents what it was intended\
    \ to represent? \xA7 If future technologies (such as automated textual analysis\
    \ or facial recognition software) make it impossible to strip personally identifiable\
    \ information from data sets in repositories, what potential risks might arise\
    \ for individuals? Can this be addressed by the original researcher? If so, how?\
    \ How will this impact subsequent researchers and their data management?\u201D\
    \ (Markham and Buchanan, 2012, p. 9-10)\n"
  Linked Principles:
  - recsvi4LnhEEPyQ1h
  - recPg7Ov0priGGtLm
  Linked Sources:
  - recQiVQ7CTC72xp6O
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T07:41:10.000Z'
  airtable_id: recgWFCdfcVaeaPQO
  title: Questions to consider regarding data management and participant re-identification
- Attachments:
  - filename: transparencymap.png
    height: 846
    id: atttvNqZNvtd43PlK
    size: 787184
    thumbnails:
      full:
        height: 3000
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/yRs_qllKONiZRqtCvil3lw/pdZVls7dbsDcg3CzD-eBtacOaJ0o29VR6sq-cD8KiEa8DocOGk0nRKYRRflZoXOaUE-B2ns8smQPcSz45WJH8A/OFtH4MEEvofut23te7lTL3H_qHi6JfuU8lHnBDpDTVc
        width: 3000
      large:
        height: 512
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/qAIl3dIsS7YPondk_A0tZA/cXnscCZyN2eFM-tOkiPR8E8an1XgJ3kZS-UkAFzrkEhCAf-wjtzgEgVpcW3hJYLsdtBVuQ8P2gGqtiHIIZa9hw/QjKTtD42TJb8MJgopU2SE8Fz5EAva4sZGiR2AMaKExI
        width: 901
      small:
        height: 36
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/6JDO36Is1O8ZrBrxjcVQag/_ixWNamCYeyPer8QfePBFoFgw0YImMZ4NBjavo7OS2JJSRrzXH9Z1aBfA6otiR8JmRNQcwYVNDlGm4BZRAj20g/9sks0EguxFD3NuVtZzBjkkh7wntIYq7jlwg3V4qlEBk
        width: 63
    type: image/png
    url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/J9tXjk9R-zdeklWwAC_uuw/2O72AvBInXLMDvSvL_VuSec15uxl3okr1lfLF6geepcl0EVzui5RmRHB4Do8WT-LKdmndiFBsgYDKiQNKSgIVa4M-KO0pwNFBfMm2j7GNMk/Nfs_K3sKVjrbb7jlT6Mxi4PqQ4nW3Vl047h7aelh5Kw
    width: 1488
  - filename: explanatorystrategies.png
    height: 754
    id: attEZqLJa2IyrkwSr
    size: 962099
    thumbnails:
      full:
        height: 3000
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/P7YJcC16xGchyu9d1bDi0g/wfWjkx-W8ApfJp-0TNm3Pud49iT7EmP8n_h4CSzMzuECsW05emJSvXy0ovnNXiVu-y1EQCjGe7BdW4D6qir2tA/Alt-3NbQjKsSFOuflTcxRZvosfA8rtSPcdoCBdCfukQ
        width: 3000
      large:
        height: 512
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/4d7zp4GdvuyMnq6kqtH3ow/iazfhqD3wz3wg-zcwtkGEnqSxKtpwcku8BPEVuWEMyFIl9AWfnohMvMKhhv0S_rL1Tpw7adJwSxHT5zTv2uLPQ/JDb4hgXEhj3Xi_OPO3UgZVBHYuBR5OR6NkaP1JgGCC4
        width: 987
      small:
        height: 36
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/oJhV0mD9o0LCCAAf_D_NwQ/hkBb_1sfoqjt3BG-MeP-GpxJN4FGhW2djWMUvRg87UYtyno3FML7e7cD6ILTd7C_ADm1aeuttMvP6svHbI5OTQ/yBtaw3SXQfb9NMzCJ5k4-LcNPEcYABgli2H3xnvAtKc
        width: 69
    type: image/png
    url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/Q59wFhQIes-Nq8fqJjw5Gw/FUpv-E_c0LKSQN3z9mlyVj_2qBch1kpo4uS_tbJVV4NVgS1JFp6SXvQ_WwKKR7dD44fhYoBEw2dfkF6NMjjvD9wHIvArKP3mn5RKHmARm7uff75X47cTXeu43ELZ7FX4/Oym8AauPejEUmkcmtP0JkxH6B4MazQz3AQu6ulWrA8w
    width: 1454
  Description: "\"It is important to remember that _transparency as a principle of\
    \ AI ethics _differs a bit in meaning from the everyday use of the term. The common\
    \ dictionary understanding of transparency defines it as _either _(1) the quality\
    \ an object has when one can see clearly through it or (2) the quality of a situation\
    \ or process that can be clearly justified and explained because it is open to\
    \ inspection and free from secrets.\n\_\nTransparency as a principle of AI ethics\
    \ encompasses _both _of these meanings:\n\_\nOn the one hand, transparent AI involves\
    \ the interpretability of a given AI system, i.e. the ability to know how and\
    \ why a model performed the way it did in a specific context and therefore to\
    \ understand the rationale behind its decision or behaviour. This sort of transparency\
    \ is often referred to by way of the metaphor of \u2018opening the black box\u2019\
    \ of AI. It involves _content clarification and intelligibility _or explicability.\n\
    \_\nOn the other hand, transparent AI involves the justifiability both of the\
    \ processes that go into its design and implementation and of its outcome. It\
    \ therefore involves the _soundness of the justification of its use_. In this\
    \ more normative meaning, transparent AI is _practically justifiable _in an unrestricted\
    \ way if one can demonstrate that both the design and implementation processes\
    \ that have gone into the particular decision or behaviour of a system and the\
    \ decision or behaviour itself are ethically permissible, non-discriminatory/fair,\
    \ and worthy of public trust/safety-securing.\n\_\n_Three critical tasks for designing\
    \ and implementing transparent AI_\n_\__\nThis two-pronged definition of transparency\
    \ as a principle of AI ethics asks that you to think about transparent AI both\
    \ in terms of the _process _behind it (the design and implementation practices\
    \ that lead to an algorithmically supported outcome) and in terms of its _product\
    \ _(the content and justification of that outcome). Such a process/product distinction\
    \ is crucial, because it clarifies the three tasks that your team will be responsible\
    \ for in safeguarding the transparency of your AI project:\n\_\n\xB7\_\_\_\_\_\
    \_Process Transparency, Task 1: Justify Process. In offering an explanation to\
    \ affected stakeholders, you should be able to demonstrate that considerations\
    \ of ethical permissibility, non-discrimination/fairness, and safety/public trustworthiness\
    \ were operative end-to-end in the design and implementation processes that lead\
    \ to an automated decision or behaviour. This task will be supported both by following\
    \ the best practices outlined herein throughout the AI project lifecycle and by\
    \ putting into place robust auditability measures through an accountability-by-design\
    \ framework.\n\_\n\xB7\_\_\_\_\_\_Outcome Transparency, Task 2: Clarify Content\
    \ and Explain Outcome. In offering an explanation to affected stakeholders, you\
    \ should be able to show in plain language that is understandable to non-specialists\
    \ how and why a model performed the way it did in a specific decision-making or\
    \ behavioural context. You should therefore be able to clarify and communicate\
    \ the rationale behind its decision or behaviour. This explanation should be _socially\
    \ meaningful _in the sense that the terms and logic of the explanation should\
    \ not simply reproduce the formal characteristics or the technical meanings and\
    \ rationale of the mathematical model but should rather be translated into the\
    \ everyday language of human practices and therefore be understandable in terms\
    \ of the societal factors and relationships that the decision or behaviour implicates.\n\
    \_\n\xB7\_\_\_\_\_\_Outcome Transparency, Task 3: Justify Outcome. In offering\
    \ an explanation to affected stakeholders, you should be able to demonstrate that\
    \ a specific decision or behaviour of your system is ethically permissible, non-discriminatory/fair,\
    \ and worthy of public trust/safety- securing. This outcome justification should\
    \ take the content clarification/explicated outcome from task 2 as its starting\
    \ point and weigh that explanation against the justifiability criteria adhered\
    \ to throughout the design and use pipeline: ethical permissibility, non- discrimination/fairness,\
    \ and safety/public trustworthiness. Undertaking an optimal approach to process\
    \ transparency from the start should support and safeguard this demand for normative\
    \ explanation and outcome justification.\"\nLeslie, 2019, p.30-31\n\n## \"Process\
    \ Transparency: Establishing a Process-Based Governance Framework\n\_\nThe central\
    \ importance of the end-to-end operability of good governance practices should\
    \ guide your strategy to build out responsible AI project workflow processes.\
    \ Three components are essential to creating a such a responsible workflow: (1)\
    \ Maintaining strong regimes of professional and institutional transparency; (2)\
    \ Having a clear and accessible Process-Based Governance\n\n Framework (PBG Framework);\
    \ (3) Establishing a well-defined auditability trail in your PBG Framework through\
    \ robust activity logging protocols that are consolidated digitally in a process\
    \ log.\n\_\n1\\.\_\_\_\_\_Professional and Institutional Transparency: At every\
    \ stage of the design and implementation of your AI project, team members should\
    \ be held to rigorous standards of conduct that secure and maintain professionalism\
    \ and institutional transparency. These standards should include the core values\
    \ of integrity, honesty, sincerity, neutrality, objectivity and impartiality.\
    \ All professionals involved in the research, development, production, and implementation\
    \ of AI technologies are, first and foremost, acting as fiduciaries of the public\
    \ interest and must, in keeping with these core values of the Civil Service, put\
    \ the obligations to serve that interest above any other concerns.\n\_\nFurthermore,\
    \ from start to finish of the AI project lifecycle, the design and implementation\
    \ process should be as transparent and as open to public scrutiny as possible\
    \ with restrictions on accessibility to relevant information limited to the reasonable\
    \ protection of justified public sector confidentiality and of analytics that\
    \ may tip off bad actors to methods of gaming the system of service provision.\n\
    \_\n2\\.\_\_\_\_\_Process-Based Governance Framework: So far, this guide has presented\
    \ some of the main steps that are necessary for establishing responsible innovation\
    \ practices in your AI project. Perhaps the most vital of these measures is the\
    \ effective operationalisation of the values and principles that underpin the\
    \ development of ethical and safe AI. By organising all of your governance considerations\
    \ and actions into a PBG Framework, you will be better able to accomplish this\
    \ task.\n\_\nThe purpose of a PBG Framework is to provide a template for the integrations\
    \ of the norms, values, and principles, which motivate and steer responsible innovation,\
    \ with the actual processes that characterise the AI design and development pipeline.\
    \ While the accompanying Guide has focused primarily on the Cross Industry Standard\
    \ Process for Data Mining (CRISP-DM), keep in mind that such a structured integration\
    \ of values and principles with innovation processes is just as applicable in\
    \ other related workflow models like Knowledge Discovery in Databases (KDD) and\
    \ Sample, Explore, Modify, Model, and Assess (SEMMA).\n\_\n\tYour PBG Framework\
    \ should give you a landscape view of the governance procedures and protocols\
    \ that are organising the control structures of your project workflow. Constructing\
    \ a good PBG Framework will provide you and your team with a big picture of:\n\
    \xB7\_\_\_\_\_\_The relevant team members and roles involved in each governance\
    \ action.\n\xB7\_\_\_\_\_\_The relevant stages of the workflow in which intervention\
    \ and targeted consideration are necessary to meet governance goals\n\xB7\_\_\_\
    \_\_\_Explicit timeframes for any necessary follow-up actions, re-assessments,\
    \ and continual monitoring\n\xB7\_\_\_\_\_\_Clear and well-defined protocols for\
    \ logging activity and for instituting mechanisms to assure end-to-end auditability\"\
    \n(Leslie, 2019, p31-33)\n\n\"1.\_\_\_\_\_Enabling Auditability with a Process\
    \ Log: With your controls in place and your governance framework organised, you\
    \ will be better able to manage and consolidate the information necessary to assure\
    \ end-to-end auditability. This information should include both the records and\
    \ activity-monitoring results that are yielded by your PBG Framework and the model\
    \ development data gathered across the modelling, training, testing, verifying,\
    \ and implementation phases.\n\_\nBy centralising your information digitally in\
    \ a process log, you are preparing the way for optimal process transparency. A\
    \ process log will enable you to make available, in one place, information that\
    \ may assist you in demonstrating to concerned parties and affected decision subjects\
    \ both the responsibility of design and use practices and the justifiability of\
    \ the outcomes of your system\u2019s processing behaviour.\n\_\nSuch a log will\
    \ also allow you to differentially organise the accessibility and presentation\
    \ of the information yielded by your project. Not only is this crucial to preserving\
    \ and protecting data that legitimately should remain unavailable for public view,\
    \ it will afford your team the capacity to cater the presentation of results to\
    \ different tiers of stakeholders with different interests and levels of expertise.\
    \ This ability to curate your explanations with the user- receiver in mind will\
    \ be vital to achieving the goals of interpretable and justifiable AI.\n\_\n##\
    \ Outcome transparency: Explaining outcomeand clarifying content\n\_\nBeyond enabling\
    \ process transparency through your PBG Framework, you must also put in place\
    \ standards and protocols to ensure that clear and understandable explanations\
    \ of the outcomes of your AI system\u2019s decisions, behaviours, and problem-solving\
    \ tasks can:\n\n 1\\.\_\_\_\_\_Properly inform the evidence-based judgments of\
    \ the implementers that they are designed to support;\n2\\.\_\_\_\_\_Be offered\
    \ to affected stakeholders and concerned parties in an accessible way.\n\_\nThis\
    \ is a multifaceted undertaking that will demand careful forethought and participation\
    \ across your entire project team.\n\_\nThere is no simple technological solution\
    \ for how to effectively clarify and convey the rationale\nbehind a model\u2019\
    s output in a particular decision-making or behavioural context. Your team will\
    \ have to use sound judgement and common sense in order to bring together the\
    \ technical aspects of choosing, designing, using a sufficiently interpretable\
    \ AI system and the delivery aspects of being able to clarify and communicate\
    \ in plain, non-technical, and socially meaningful language how and why that system\
    \ performed the way it did in a specific decision-making or behavioural context.\n\
    \_\nHaving a good grasp of the rationale and criteria behind the decision-making\
    \ and problem-solving behaviour of your system is essential for producing safe,\
    \ fair, and ethical AI. If your AI model is not sufficiently interpretable\u2014\
    if you aren\u2019t able to draw from it humanly understandable explanations of\
    \ the factors that played a significant role in determining its behaviours\u2014\
    then you may not be able to tell how and why things go wrong in your system when\
    \ they do.\n\_\nThis is a crucial and unavoidable issue for reasons we have already\
    \ explored. Ensuring the safety of high impact systems in transportation, medicine,\
    \ infrastructure, and security requires human verification that these systems\
    \ have properly learned the critical tasks they are charged to complete. It also\
    \ requires confirmation that when confronted with unfamiliar circumstances, anomalies,\
    \ and perturbations, these systems will not fail or make unintuitive errors. Moreover,\
    \ ensuring that these systems operate without causing discriminatory harms requires\
    \ effective ways to detect and to mitigate sources of bias and inequitable influence\
    \ that may be buried deep within their feature spaces, inferences, and architectures.\
    \ Without interpretability each one of these tasks necessary for delivering safe\
    \ and morally justifiable AI will remain incomplete.\n\_\nDefining Interpretable\
    \ AI\n\_\nTo gain a foothold in both the technical and delivery dimensions of\
    \ AI interpretability, you will first need a solid working definition of what\
    \ interpretable AI is. To this end, it may be useful to recall once again the\
    \ definition of AI offered in the accompanying Guide: \u2018Artificial Intelligence\
    \ is the science of _making computers do things that require intelligence when\
    \ done by humans_.\u2019\n\_\nThis characterisation is important, because it brings\
    \ out an essential feature of the explanatory demands of interpretable AI: to\
    \ do things that require intelligence when done by humans means to do things that\
    \ require _reasoning processes and cognitive functioning_. This cognitive dimension\
    \ has a direct bearing on how you should think about offering suitable explanations\
    \ about algorithmically generated outcomes:\n\_\nExplaining an algorithmic model\u2019\
    s decision or behaviour should involve making explicit how the particular set\
    \ of factors which determined that outcome can play the role of evidence in supporting\n\
    \n the conclusion reached. It should involve making intelligible to affected individuals\
    \ the rationale behind that decision or behaviour as if it had been produced by\
    \ a reasoning, evidence-using, and inference-making person.\n\_\nWhat makes this\
    \ explanation-giving task so demanding when it comes to AI systems is that reasoning\
    \ processes do not occur, for humans, at just one level. Rather, human-scale reasoning\
    \ and interpreting includes:\n\_\n1\\.\_\_\_\_\_Aspects of logic (applying the\
    \ basic principles of validity that lie behind and give form to sound thinking):\
    \ _This aspect aligns with the need for formal or logical explanations of AI systems._\n\
    _\__\n2\\.\_\_\_\_\_Aspects of semantics (gaining an understanding of how and\
    \ why things work the way they do and what they mean): _This aspect aligns with\
    \ the need for explanations of the technical rationale behind the outcomes AI\
    \ systems._\n_\__\n3\\.\_\_\_\_\_Aspects of the social understanding of practices,\
    \ beliefs, and intentions (clarifying the content of interpersonal relations,\
    \ societal norms, and individual objectives): _This aspect aligns with the need\
    \ for the clarification of the socially meaningful content of the outcomes of\
    \ AI systems._\n_\__\n4\\.\_\_\_\_\_Aspects of moral justification (making sense\
    \ of what should be considered right and wrong in our everyday activities and\
    \ choices): _This aspect aligns with the justifiability of AI systems._\n_\__\n\
    There are good reasons why _all four of these dimensions of human reasoning processes\
    \ _must factor in to explaining the decisions and behaviours of AI systems: First\
    \ and most evidently, understanding the logic and technical innerworkings (i.e.\
    \ semantic content) of these systems is a precondition for ensuring their safety\
    \ and fairness. Secondly, because they are designed and used to achieve human\
    \ objectives and to fulfil surrogate cognitive functions _in the everyday social\
    \ world_, we need to make sense of these systems in terms of the consequential\
    \ roles that their decisions and behaviours play in that human reality. The social\
    \ context of these outcomes matters greatly. Finally, because they actually affect\
    \ individuals and society in direct and morally consequential ways, we need to\
    \ be able to understand and explain their outcomes not just in terms of their\
    \ mathematical logic, technical rationale, and social context but also in terms\
    \ of the justifiability of their impacts on people.\n\_\nDelving more deeply into\
    \ the technical and delivery aspects of interpretable AI will show how these four\
    \ dimensions of human reasoning directly line up with the different levels of\
    \ demand for explanations of the outcomes of AI systems. In particular, the logical\
    \ and semantic dimensions will weigh heavily in technical considerations whereas\
    \ the social and moral dimensions will be significant at the point of delivery.\n\
    \_\nNote here, though, that these different dimensions of human reasoning are\
    \ not necessarily mutually exclusive but build off and depend upon each other\
    \ in significant and cascading ways. Approaching explanations of interpretable\
    \ AI should therefore be treated holistically and inclusively. Technical explanation\
    \ of the logic and rationale of a given model, for instance, should be seen as\
    \ a support for the context-based clarification of its socially meaningful content,\
    \ just as that socially meaningful content should be viewed as forming the basis\
    \ of explaining an outcome\u2019s moral justifiability. When\n\n considering how\
    \ to make the outcomes of decision-making and problem-solving AI systems maximally\
    \ transparent to affected stakeholders, you should take this rounded view of human\
    \ reasoning into account, because it will help you address more effectively the\
    \ spectrum of concerns that these stakeholders may have.\n\_\nTechnical aspects\
    \ of choosing, designing, and using an interpretable AI system\n\_\nKeep in mind\
    \ that, while, on the face of it, the task of choosing between the numerous AI\
    \ and machine learning algorithms may seem daunting, it need not be so. By sticking\
    \ to the priority of outcome transparency, you and your team will be able to follow\
    \ some straightforward and simple guidelines for selecting sufficiently interpretable\
    \ but optimally performing algorithmic techniques.\n\_\nBefore exploring these\
    \ guidelines, it is necessary to provide you with some background information\
    \ to help you better understand what facets of explanation are actually involved\
    \ in technically interpretable AI. A good grasp of what is actually needed from\
    \ such an explanation will enable you to effectively target the interpretability\
    \ needs of your AI project.\n\_\nFacets of explanation in technically interpretable\
    \ AI: A good starting point for understanding how the technical dimension of explanation\
    \ works in interpretable AI systems is to remember that these systems are largely\
    \ mathematical models that carry out step-by-step computations in transforming\
    \ sets of statistically interacting or independent inputs into sets of target\
    \ outputs. Machine learning is, at bottom, just applied statistics and probability\
    \ theory fortified with several other mathematical techniques. As such, it is\
    \ subject to same methodologically rigorous requirements of logical validation\
    \ as other mathematical sciences.\n\_\nSuch a demand for rigour informs the facet\
    \ of formal and logical explanation of AI systems that is sometimes called the\
    \ _mathematical glass box_. This characterisation refers to the transparency of\
    \ strictly formal explanation: No matter how complicated it is (even in the case\
    \ of a deep neural net with a hundred million parameters), an algorithmic model\
    \ is a closed system of effectively computable operations where rules and transformations\
    \ are mechanically applied to inputs to determine outputs. In this restricted\
    \ sense, all AI and machine learning models are fully intelligible and mathematically\
    \ transparent if only _formally and logically _so.\n\_\nThis is an important characteristic\
    \ of AI systems, because it makes it possible for supplemental and eminently interpretable\
    \ computational approaches to model, approximate, and simplify even the most complex\
    \ and high dimensional among them. In fact, such a possibility fuels some of the\
    \ technical approaches to interpretable AI that will soon be explored.\n\_\nThis\
    \ formal way of understanding the technical explanation of AI and machine learning\
    \ systems, however, has immediate limitations. It can tell us that a model is\
    \ mathematically intelligible because it operates according to a collection of\
    \ fixed operations and parameters, but it cannot tell us much about how or why\
    \ the components of the model transformed a specified group of inputs into their\
    \ corresponding outputs. It cannot tell us anything about the _rationale behind\
    \ the algorithmic generation of a given outcome_.\n\_\nThis second dimension of\
    \ technical explanation has to do with the _semantic facet _of interpretable AI.\
    \ A semantic explanation offers an interpretation of the functions of the individual\
    \ parts of the\n\n algorithmic system in the generation of its output. Whereas\
    \ formal and logical explanation presents an account of the stepwise application\
    \ of the procedures and rules that comprise the formal framework of the algorithmic\
    \ system, semantic explanation helps us to understand the meaning of those procedures\
    \ and rules in terms of their purpose in the input-output mapping operation of\
    \ the system, i.e. what role they play in determining the outcome of the model\u2019\
    s computation.\n\_\nThe difficulties surrounding the interpretability of algorithmic\
    \ decisions and behaviours arise in this semantic dimension of technical explanation.\
    \ It is easiest to illustrate this by starting from the simplest case.\n\_\nWhen\
    \ a machine learning model is very basic, the task of following the rationale\
    \ of how it transforms a given set of inputs into a given set of outputs can be\
    \ relatively unproblematic. For instance, in the simple linear regression, _y\
    \ = a + bx + e_, with a single predictor variable _x _and a response variable\
    \ _y_, the predictive relationship of x to y is directly expressed in a regression\
    \ coefficient _b_, representing the rate and direction at which _y _is predicted\
    \ to change as x changes. This hypothetical model is completely interpretable\
    \ from the technical perspective for the following reasons:\n\_\n\xB7\_\_\_\_\_\
    \_Linearity: Any change in the value of the predictor variable is directly reflected\
    \ in a change in the value of the response variable at a constant rate _b_. The\
    \ interpretable prediction yielded by the model can therefore be directly inferred_.\
    \ _This linearity dimension of predictive models has been an essential feature\
    \ of the automated decision-making systems in many heavily regulated and high-impact\
    \ sectors, because the predictions yielded have high inferential clarity and strength.\n\
    \_\n\xB7\_\_\_\_\_\_Monotonicity: When the value of the predictor changes in a\
    \ given direction, the value of the response variable changes consistently either\
    \ in the same or opposite direction. The interpretable prediction yielded by the\
    \ model can thus be directly inferred. This monotonicity dimension is also a highly\
    \ desirable interpretability condition of predictive models in many heavily regulated\
    \ sectors, because it incorporates reasonable expectations about the consistent\
    \ application of sector specific selection constraints into automated decision-making\
    \ systems. So, for example, if the selection criteria to gain employment at an\
    \ agency or firm includes taking an exam, a reasonable expectation of outcomes\
    \ would be that if candidate A scored better than candidate B, then candidate\
    \ B, all other things being equal, would not be selected for employment when A\
    \ is not. A monotonic predictive model that uses the exam score as the predictor\
    \ variable and application success as the response variable would, in effect,\
    \ guarantee this expectation is met by disallowing situations where A scores better\
    \ than B but B gets selected and A does not.\n\_\n\xB7\_\_\_\_\_\_Non-Complexity:\
    \ The number of features (dimensionality) and feature interactions is low enough\
    \ and the mapping function is simple enough to enable a clear \u2018global\u2019\
    \ understanding of the function of each part of the model in relation to its outcome.\n\
    \_\nWhile, all three of these desirable interpretability characteristics of the\
    \ imagined model allow for direct and intuitive reasoning about the relation of\
    \ the predictor and response variables, the model itself is clearly too minimal\
    \ to capture the density of relationships and interactions between attributes\
    \ in complex real-world situations where some degree of noisiness is unavoidable\
    \ and the task of apprehending the subtleties of underlying data distributions\
    \ is tricky.\n\n In fact, one of the great strides forward that has been enabled\
    \ by the contemporary convergence of expanding computing power and big data availability\
    \ with more advanced machine learning models has been exactly this capacity to\
    \ better capture and model the intricate and complicated dynamics of real-world\
    \ situations. Still, this incorporation of the complexity of scale into the models\
    \ themselves has also meant significant challenges to the semantic dimension of\
    \ the technical explanation of AI systems.\n\_\nAs machine learning systems have\
    \ come to possess both ever greater access to big data and increasing computing\
    \ power, their designers have correspondingly been able both to enlarge the feature\
    \ spaces (the number of input variables) of these systems and to turn to gradually\
    \ more complex mapping functions. In many cases, this has meant vast improvements\
    \ in the predictive and classificatory performance of more accurate and expressive\
    \ models, but this has also meant the growing prevalence of non-linearity, non-monotonicity,\
    \ and high-dimensional complexity in an expanding array of so-called \u2018black-box\u2019\
    \ models.\n\_\nOnce high-dimensional feature spaces and complex functions are\
    \ introduced into machine learning systems, the effects of changes in any given\
    \ input become so entangled with the values and interactions of other inputs that\
    \ understanding how individual components are transformed into outputs becomes\
    \ extremely difficult. The complex and unintuitive curves of the decision functions\
    \ of many of these models preclude linear and monotonic relations between their\
    \ inputs and outputs.\nLikewise, the high-dimensionality of their optimisation\
    \ techniques\u2014frequently involving millions of parameters and complex correlations\u2014\
    ranges well beyond the limits of human-scale cognition and understanding.\"\n\
    Leslie, 2019, p33-39\n\n\"These rising tides of computational complexity and algorithmic\
    \ opacity consequently pose a key challenge for the responsible design and deployment\
    \ of safe, fair, and ethical AI systems: how should the potential to advance the\
    \ public interest through the implementation of high performing but increasingly\
    \ uninterpretable machine learning models be weighed against the tangible risks\
    \ posed by the lack of interpretability of such systems?\n\n A careful answer\
    \ to this question is, in fact, not so simple. While the trade-off between performance\
    \ and interpretability may be real and important in _some domain-specific applications_,\
    \ in others there exist increasingly sophisticated developments of standard interpretable\
    \ techniques such as regression extensions, decision trees, and rule lists that\
    \ may prove just as effective for use cases where the need for transparency is\
    \ paramount. Furthermore, supplemental interpretability tools, which function\
    \ to make \u2018black box\u2019 models more semantically and qualitatively explainable\
    \ are rapidly advancing day by day.\n\_\nThese are all factors that you and your\
    \ team should consider as you work together to decide on which models to use for\
    \ your AI project. As a starting point for those considerations, let us now turn\
    \ to some basic guidelines that may help you to steer that dialogue toward points\
    \ of relevance and concern.\n\_\nGuidelines for designing and delivering a sufficiently\
    \ interpretable AI system\n\_\nYou should use the table below to begin thinking\
    \ about how to integrate interpretability into your AI project. While aspects\
    \ of this topic can become extremely technical, it is important to make sure that\
    \ dialogue about making your AI system interpretable remains multidisciplinary\
    \ and inclusive.\nMoreover, it is crucial that key stakeholders be given adequate\
    \ consideration when deciding upon the delivery mechanisms of your project. These\
    \ should include policy or operational design leads, the technical personnel in\
    \ charge of operating the trained models, the implementers of the models, and\
    \ the decision subjects, who are affected by their outcomes.\n\_\nNote that the\
    \ first three guidelines focus on the big picture issues you will need to consider\
    \ in order to incorporate interpretability needs into your project planning and\
    \ workflow, whereas the last two guidelines shift focus to the user-centred requirements\
    \ of designing and implementing a sufficiently interpretable AI system.\" (Leslie,\
    \ 2019, p.39-40)\n\n## Guidelines for designing and delivering a sufficiently\
    \ interpretable AI system\n \n  \_\nGuideline 1: Look first to context, potential\
    \ impact, and domain-specific need when determining the interpretability requirements\
    \ of your project\n\_\nThere are several related factors that should be taken\
    \ into account as you formulate your project\u2019s\napproach to interpretability:\n\
    \_\n1\\. Type of application: Start by assessing both the kind of tool you are\
    \ building and the environment in which it will apply. Clearly there is a big\
    \ difference between a computer vision system that sorts handwritten employee\
    \ feedback forms and one that sorts safety risks at a security checkpoint. Likewise,\
    \ there is a big difference between a random forest model that triages applicants\
    \ at a licencing agency and one that triages sick patients in an emergency department.\n\
    \_\nUnderstanding your AI system\u2019s purpose and context of application will\
    \ give you a\nbetter idea of the stakes involved in its use and hence also a good\
    \ starting point to think\nabout the scope of its interpretability needs. For\
    \ instance, low-stakes AI models that are\n  \n\n   not safety-critical, do not\
    \ directly impact the lives of people, and do not process potentially sensitive\
    \ social and demographic data will likely have a lower need for extensive resources\
    \ to be dedicated to a comprehensive interpretability platform.\n\_\n2\\.\_\_\_\
    \_\_Domain specificity: By acquiring solid domain knowledge of the environment\
    \ in which your AI system will operate, you will gain better insight into any\
    \ potential sector-specific standards of explanation or benchmarks of justification\
    \ which should inform your approach to interpretability. Through such knowledge,\
    \ you may also obtain useful information about organisational and public expectations\
    \ regarding the scope, content, and depth of explanations that have been previously\
    \ offered in relevant use cases.\n\_\n3\\.\_\_\_\_\_Existing technology: If one\
    \ of the purposes of your AI project is to replace an existing algorithmic technology\
    \ that may not offer the same sort of expressive power or performance level as\
    \ the more advanced machine learning techniques that you are planning to deploy,\
    \ you should carry out an assessment of the performance and interpretability levels\
    \ of the existing technology. Acquiring this knowledge will provide you with an\
    \ important reference point when you are considering possible trade-offs between\
    \ performance and interpretability that may occur in your own prospective system.\
    \ It will also allow you to weigh the costs and benefits of building a more complex\
    \ system with higher interpretability-support needs in comparison to the costs\
    \ and benefits of using a simpler model.\n   \_\nGuideline 2: Draw on standard\
    \ interpretable techniques when possible\n\_\nIn order to actively integrate the\
    \ aim of sufficient interpretability into your AI project, your team should approach\
    \ the model selection and development process with the goal of finding the right\
    \ fit between (1) domain-specific risks and needs, (2) available data resources\
    \ and domain knowledge, and (3) task appropriate machine learning techniques.\
    \ Effectively assimilating these three aspects of your use case requires open-mindedness\
    \ and practicality.\n\_\nOften times, it may be the case that high-impact, safety-critical,\
    \ or other potentially sensitive environments heighten demands for the thoroughgoing\
    \ accountability and transparency of AI projects. In some of these instances,\
    \ such demands may make choosing standard but sophisticated non-opaque techniques\
    \ an overriding priority. These techniques may include decisions trees, linear\
    \ regression and its extensions like generalised additive models, decision/rule\
    \ lists, case-based reasoning, or logistic regression. In many cases, reaching\
    \ for the \u2018black box\u2019 model first may not be appropriate and may even\
    \ lead to inefficiencies in project development, because more interpretable models,\
    \ which perform very well but do not require supplemental tools and techniques\
    \ for facilitating interpretable outcomes, are also available.\n\_\nAgain, solid\
    \ domain knowledge and context awareness are key components here. In use cases\
    \ where data resources lend to well-structured, meaningful representations and\
    \ domain expertise can be incorporated into model architectures, interpretable\
    \ techniques may often be more desirable than opaque ones. Careful data pre-processing\
    \ and iterative model development can, in these cases, hone the accuracy of such\
    \ interpretable systems in ways that may make the advantages gained by the combination\
    \ of their performance and transparency outweigh the\nbenefits of more semantically\
    \ intransparent approaches.\n  \n\n   \_\nIn other use cases, however, data processing\
    \ needs may disqualify the deployment of these sorts of straightforward interpretable\
    \ systems. For instance, when AI applications are sought for classifying images,\
    \ recognising speech, or detecting anomalies in video footage, the most effective\
    \ machine learning approaches will likely be opaque. The feature spaces of these\
    \ kinds of AI systems grow exponentially to hundreds of thousands or even millions\
    \ of dimensions. At this scale of complexity, conventional methods of interpretation\
    \ no longer apply. Indeed, it is the unavoidability of hitting such an interpretability\
    \ wall for certain important applications of supervised, unsupervised, and reinforcement\
    \ learning that has given rise to an entire subfield of machine learning research\
    \ which focuses on providing technical tools to facilitate interpretable and explainable\
    \ AI.\n\_\nWhen the use of \u2018black box\u2019 models best fits the purpose\
    \ of your AI project, you should proceed diligently and follow the procedures\
    \ recommended in Guideline 3. For clarity, let us define a \u2018black box\u2019\
    \ model as any AI system whose innerworkings and rationale are opaque or inaccessible\
    \ to human understanding. These systems may include neural networks (including\
    \ recurrent, convolutional, and deep neural nets), ensemble methods (an algorithmic\
    \ technique such as the random forest method that strengthens an overall prediction\
    \ by combining and aggregating the results of several or many different base models),\
    \ and support vector machines (a classifier that uses a special type of mapping\
    \ function to build a divider between two sets of features in a high dimensional\
    \ feature space).\n   \_\nGuideline 3: When considering the use of \u2018black\
    \ box\u2019 AI systems, you should:\n\_\n1\\.\_\_\_\_\_Thoroughly weigh up impacts\
    \ and risks;\n\_\n2\\.\_\_\_\_\_Consider the options available for supplemental\
    \ interpretability tools that will ensure a level of semantic explanation which\
    \ is both _domain appropriat_e and _consistent with the design and implementation\
    \ of safe, fair, and ethical AI_;\n\_\n3\\.\_\_\_\_\_Formulate an interpretability\
    \ action plan, so that you and your team can put adequate forethought into how\
    \ explanations of the outcomes of your system\u2019s decisions, behaviours, or\
    \ problem-solving tasks can be optimally provided to users, decision subjects,\
    \ and other affected parties.\n\_\nIt may be helpful to explore each of these\
    \ three suggested steps of assessing the viability of the\nresponsible design\
    \ and implementation of a \u2018black box\u2019 model in greater detail.\n\_\n\
    (1)\_Thoroughly weigh up impacts and risks: Your first step in evaluating the\
    \ feasibility of using a complex AI system should be to focus on issues of ethics\
    \ and safety. As a general policy, you and your team should utilise \u2018black\
    \ box\u2019 models only:\n\_\n\xB7\_\_\_\_\_\_where their potential impacts and\
    \ risks have been thoroughly considered in advance, and you and your team have\
    \ determined that your use case and domain specific needs support the responsible\
    \ design and implementations of these systems;\n  \n\n \xB7\_\_\_\_\_\_\n\nwhere\
    \ supplemental interpretability tools provide your system with a domain appropriate\
    \ level of semantic explainability that is reasonably sufficient to mitigate its\
    \ potential risks and that is therefore consistent with the design and implementation\
    \ of safe, fair, and ethical AI.\n\_\n(2) Consider the options available for supplemental\
    \ interpretability tools: Next, you and your team should assess whether there\
    \ are technical methods of explanation-support that _both _satisfy the specific\
    \ interpretability needs of your use case as determined by the deliberations suggested\
    \ in Guideline 1 _and _are appropriate for the algorithmic approach you intend\
    \ to use. You should consult closely with your technical team at this stage of\
    \ model selection. The exploratory processes of trial-and-error, which often guide\
    \ this discovery phase in the innovation lifecycle, should be informed and constrained\
    \ by a solid working knowledge of the technical art of the possible in the domain\
    \ of available and useable interpretability approaches.\n\_\nThe task of lining\
    \ up the model selection process with the demands of interpretable AI requires\
    \ a few conceptual tools that will enable thoughtful evaluation of whether proposed\
    \ supplemental interpretability approaches sufficiently meet your project\u2019\
    s explanatory needs. First and most importantly, you should be prepared to ask\
    \ the right questions when evaluating any given interpretability approach. This\
    \ involves establishing with as much clarity as possible how the explanatory results\
    \ of that approach can contribute to the user\u2019s ability to offer solid, coherent,\
    \ and reasonable accounts of the rationale behind any given algorithmically generated\
    \ output. Relevant questions to ask that can serve this end are:\n\_\n\xB7\_\_\
    \_\_\_\_What sort of explanatory resources will the interpretability tool provide\
    \ users and implementers in order (1) to enable them to exercise better-informed\
    \ evidence-based judgments and (2) to assist them in offering plausible, sound,\
    \ and reasonable accounts of the logic behind algorithmically generated output\
    \ to affected individuals and concerned parties?\n\_\n\xB7\_\_\_\_\_\_Will the\
    \ explanatory resources that the interpretability tool offers be useful for providing\
    \ affected stakeholders with a sufficient understanding of a given outcome?\n\_\
    \n\xB7\_\_\_\_\_\_How, if at all, might the explanatory resources offered by the\
    \ tool be misleading or confusing?\n\_\nYou and your team should take these questions\
    \ as a starting point for evaluating prospective interpretability tools. These\
    \ tools should be assessed in terms of their capacities to render the reasoning\
    \ behind the decisions and behaviours of the uninterpretable \u2018black box\u2019\
    \ systems sufficiently intelligible to users and affected stakeholders given use\
    \ case and domain specific interpretability needs.\n\_\nKeeping this in mind,\
    \ there are two technical dimensions of supplemental interpretability approaches\
    \ that should be systematically incorporated into evaluation processes at this\
    \ stage of the innovation workflow.\n\n \nThe first involves the possible explanatory\
    \ strategies you choose to pursue over the course of the design and implementation\
    \ lifecycle. Such strategies will largely determine the paths to understanding\
    \ you will be able to provide for its users and decision subjects. They will largely\
    \ define _how you explain your model and its outcomes _and hence _what kinds of\
    \ explanation you are able offer_.\n\_\nThe second involves the coverage and scope\
    \ of the actual explanations themselves. The choices you make about explanatory\
    \ coverage will determine the extent to which the kinds of explanations you are\
    \ planning to pursue will address _single instances _of the model\u2019s outputs\
    \ or range more broadly to cover the _underlying rationale of its behaviour in\
    \ general and across instances_. Choices you make about explanatory coverage will\
    \ largely govern the extent to which your AI system is locally and/or globally\
    \ interpretable.\n\_\nThe very broad-brushed overview of these two dimensions\
    \ that follows is just meant to orient you to some of the basic concepts in an\
    \ expanding field of research, so that you are more prepared for working with\
    \ your technical team to think through the strengths and weaknesses of various\
    \ approaches. Note, additionally, that this is a rapidly developing area. Relevant\
    \ members of your team should keep abreast of the latest developments in the field\
    \ of interpretable AI or XAI (Explainable AI):\n\_\nTwo technical dimensions of\
    \ supplemental interpretability approaches:\n\_\n1\\.\_\_\_\_\_Determining explanatory\
    \ strategies: To achieve the goal of securing a sufficiently interpretable AI\
    \ system, you and your team will need to get clear on how to explain your model\
    \ and its outcomes. The explanatory strategies you decide to pursue will shape\
    \ the paths to understanding you are able to provide for the users of your model\
    \ and for its decision subjects.\n\_\nThere are four such explanatory strategies\
    \ to which you should pay special attention:\n\_\na)\_\_\_\_\__Internal explanation:\
    \ _Pursuing the internal explanation of an opaque model involves making intelligible\
    \ how the components and relationships within it function. There are two ways\
    \ that such a goal of internal explanation can be interpreted. On the one hand,\
    \ it can be seen as an endeavour to explain the operation of the model by considering\
    \ it globally _as a comprehensible whole_. Here, the aspiration is to \u2018pry\
    \ open the black box\u2019 by building an explanatory model that enables a full\
    \ grasp of the opaque system\u2019s internal contents. The strengths and weaknesses\
    \ of such an approach will be discussed in the next section on global interpretability.\n\
    \_\nOn the other hand, the search for internal explanation can indicate the pursuit\
    \ a kind of _engineering insight_. In this sense, internal explanation can be\
    \ seen as attempting to shed descriptive and inferential light on the parts and\
    \ operation of the system as a whole in order to try to make it work better. Acquiring\
    \ this sort of internal understanding of the more general relationships that the\
    \ working parts of a trained model have with patterns of its responses can allow\
    \ researchers to advance step-by-step in gaining a better data scientific grasp\
    \ on\n\n \nwhy it does what it does and how to improve it. Similarly, this type\
    \ of internal explanation can be seen as attempting to shed light on an opaque\
    \ model\u2019s operation by breaking it down into more understandable, analysable,\
    \ and digestible parts (for instance, in the case of a DNN: into interpretable\
    \ characteristics of its vectors, features, layers, parameters, etc.).\n\_\nFrom\
    \ a practical point of view, this kind of aspiration to _engineering insight _in\
    \ the ends of data scientific advancement should inform the goals of your technical\
    \ team throughout the model selection and design workflow.\nNumerous methods exist\
    \ to help provide informative representations of the innerworkings of various\
    \ \u2018black box\u2019 systems. Gaining a clearer descriptive understanding of\
    \ the internal composition of your system will contribute greatly to your project\u2019\
    s ability to achieve a higher degree of outcome transparency and to its capacity\
    \ to foster best practices in the pursuit of responsible data science in general.\n\
    \_\nb)\_\_\_\_\__External or post-hoc explanation: _External or post-hoc explanation\
    \ attempts to capture essential attributes of the observable behaviour of a \u2018\
    black box\u2019 system by subjecting it to a number of different techniques that\
    \ reverse engineer explanatory insight. Some post-hoc approaches test the sensitivity\
    \ of the outputs of an opaque model to perturbations in its inputs; others allow\
    \ for the interactive probing of its behavioural characteristics; others, still,\
    \ build proxy- based models that utilise simplified interpretable techniques to\
    \ gain a better understanding of particular instances of its predictions and classifications.\n\
    \_\nThis external or post-hoc approach has, at present, established itself in\
    \ machine learning research as a go-to explanatory strategy and for good reason.\
    \ It allows data scientists to pose mathematical questions to their opaque systems\
    \ by testing them and by building supplemental models which enable greater insight\
    \ through the inferences drawn from their experimental interventions. Such a post-hoc\
    \ approach allows them, moreover, to seek out evidence for the reasoning behind\
    \ a given opaque model\u2019s prediction or classification by utilising maximally\
    \ interpretable techniques like linear regression, decision trees, rule lists,\
    \ or case-based reasoning. Several examples of post-hoc explanation will be explored\
    \ below in the section on local interpretability.\n\_\nTake note initially though\
    \ that, as some critics have rightly pointed out, because they are approximations\
    \ or simplified supplemental models of the more complex originals, many post-hoc\
    \ explanations can fail to accurately represent certain areas of the opaque model\u2019\
    s feature space. This deterioration of\naccuracy in parts of the original model\u2019\
    s domain can frequently produce\nmisleading and uncertain results in the post-hoc\
    \ explanations of concern.\n\_\nc)\_\_\_\_\_\__Supplemental explanatory infrastructure_:\
    \ A different kind of explanatory strategy involves actually incorporating secondary\
    \ explanatory facilities into the system you are building. For instance, an image\
    \ recognition system could have a primary component, like a convolutional neural\
    \ net, that extracts features from\n\n \nits inputs and classifies them while\
    \ a secondary component, like a built-in recurrent neural net with an \u2018attention-directing\u2019\
    \ mechanism, translates the extracted features into a natural language representation\
    \ that produces a sentence-long explanation of the result to the user. In other\
    \ words, a system like this is designed to provide simple explanations of its\
    \ own data processing results.\n\_\nResearch into integrating \u2018attention-based\u2019\
    \ interfaces like this in AI systems is continuing to advance toward making their\
    \ implementations more sensitive to user needs, more explanation-forward, and\
    \ more human-understandable. For instance, multimodal methods of combining visualisation\
    \ tools and textual interface are being developed that may make the provision\
    \ of explanations more interpretable for both implementers and decision subjects.\
    \ Furthermore, the incorporation of domain knowledge and logic-based or convention-based\
    \ structures into the architectures of complex models are increasingly allowing\
    \ for better and more user-friendly representations and prototypes to be built\
    \ into them. This is gradually enabling more sophisticated explanatory infrastructures\
    \ to be integrated into opaque systems and makes it essential to think about building\
    \ explanation-by-design into your AI projects.\n\_\nd)\_\_\_\_\__Counterfactual\
    \ explanation_: While counterfactual explanation is a kind of post- hoc approach,\
    \ it deserves special attention insofar as it moves beyond other post-hoc explanations\
    \ to provide affected stakeholders with clear and precise options for actionable\
    \ recourse and practical remedy.\n\_\nCounterfactual explanations are contrastive\
    \ explanations: They offer succinct computational reckonings of how specific factors\
    \ that influenced an algorithmic decision can be changed so that better alternatives\
    \ can be realised by the subject of that decision. Incorporating counterfactual\
    \ explanations into your AI system at its point of delivery would allow stakeholders\
    \ to see what input variables of the model can be modified, so that the outcome\
    \ could be altered to their benefit. Additionally, from a responsible design perspective,\
    \ incorporating counterfactual explanation into the development and testing phases\
    \ of your system would allow your team to build a model that incorporates _actionable\
    \ variables_, i.e. input variables that will afford decision subjects with concise\
    \ options for making practical changes that would improve their chances of obtaining\
    \ the desired outcome. Counterfactual explanatory strategies can be used as way\
    \ to incorporate reasonableness and the encouragement of agency into the design\
    \ and implementation of your AI project.\n\_\nAll that said, it is important to\
    \ recognise that, while counterfactual explanation does offer an innovative way\
    \ to contrastively explore how feature importance may influence an outcome, it\
    \ is not a complete solution to the problem of AI interpretability. In certain\
    \ cases, for instance, the sheer number of potentially significant features that\
    \ could be at play in counterfactual explanations of a given result can make a\
    \ clear and direct explanation difficult to obtain and selected sets of explanations\
    \ seem potentially arbitrary. Moreover, there are as\n\n \nyet limitations on\
    \ the types of datasets and functions to which these kinds of explanations are\
    \ applicable. Finally, because this kind of explanation concedes the opacity of\
    \ the algorithmic model outright, it is less able to address concerns about potentially\
    \ harmful feature interactions and multivariate relationships\nthat may be buried\
    \ deep within the model\u2019s architecture.\n\_\nHere is an at-a-glance view\
    \ of a typology of these explanatory strategies:\n\_\n\_\n\_\n\_\n\_\n\_\n\_\n\
    \_\n\_\n\_\n\_\n\_\n\_\n\_\n\_\n\_\n\_\n\_\n\_\n2\\.\_\_\_\_\_Coverage and Scope:\
    \ The main questions you will need to broach in the dimension of the coverage\
    \ and scope of your supplemental interpretability approach are: To what extent\
    \ does our interpretability approach cover the explanation of _singe predictions\
    \ or classifications _of the model and to what extent does it cover the explanation\
    \ of the _innerworkings and rationale of the model as a whole and across predictions_?\
    \ To what extent does it cover both?\n\_\nThis distinction between single instance\
    \ and total model explanation is often characterised as the difference between\
    \ local interpretability and the global interpretability. Both types of explanation\
    \ offer potentially helpful support for the provision of significant information\
    \ about the rationale behind an algorithmic decision or behaviour, but both, in\
    \ their own ways, also face difficulties.\n\_\nLocal Interpretability: A local\
    \ semantic explanation aims to enable the interpretability of individual cases.\
    \ The general idea behind attempts to explain a \u2018black box\u2019 system in\
    \ terms of specific instances is that, regardless of how complex the architecture\
    \ or decision function of that system may be, it is possible to gain interpretive\
    \ insight into its innerworkings by focusing on single data points or neighbourhoods\
    \ in its feature space. In other words, even if the high dimensionality and curviness\
    \ of a model makes it opaque _as a whole_, there is an expectation that insight-generating\n\
    \n \ninterpretable methods can be applied _locally _to smaller sections of the\
    \ model, where changes in isolated or grouped variables are more manageable and\
    \ understandable.\n\_\nThis general explanatory perspective has yielded several\
    \ different interpretive\nstrategies that have been successfully applied in significant\
    \ areas of \u2018black box\u2019 machine learning. One family of such strategies\
    \ has zeroed in on neural networks (DNNs, in particular) by identifying what features\
    \ of an input vector\u2019s data points make it representative of the target concept\
    \ that a given model is trying to classify. So, for example, if we have a digital\
    \ image of a dog that is converted into a vector of pixel values and then processed\
    \ it through a dog-classifying deep neural net, this interpretive approach will\
    \ endeavour to tell us why the system yielded a \u2018dog- positive\u2019 output\
    \ by isolating the slices of this set of data points that are most relevant to\
    \ its successful classification by the model.\n\_\nThis can be accomplished in\
    \ several related ways. What is called sensitivity analysis identifies the most\
    \ relevant features of an input vector by calculating local gradients to determine\
    \ how a data point has to be moved to change the output label. Here, an output\u2019\
    s sensitivity to such changes in input values identifies the most relevant features.\
    \ Another method to identify feature relevance that is downstream from sensitivity\
    \ analysis is called salience mapping, where a strategy of moving backward through\
    \ the layers of a neural net graph allows for the mapping of patterns of high\
    \ activation in the nodes and ultimately generates interpretable groupings of\
    \ salient input variables that can be visually represented in a heat or pixel\
    \ attribution map.\n\_\nA second local interpretive strategy also seeks to explain\
    \ feature importance in a single prediction or classification by perturbing input\
    \ variables. However, instead of using these nudges in the feature space to highlight\
    \ areas of saliency, it uses them to prod the opaque model in the area around\
    \ the relevant prediction, so that a supplemental interpretable model can be constructed\
    \ which establishes the relative importance of features in the black box model\u2019\
    s output.\n\_\nThe most well-known example of this strategy is called LIME (Local\
    \ Interpretable Model-Agnostic Explanation). LIME works by fitting an interpretable\
    \ model to a specific prediction or classification produced by the opaque system\
    \ of concern. It does this by sampling data points at random around the target\
    \ prediction or classification and then using them to build a local approximation\
    \ of the decision boundary that can account for the features which figure prominently\
    \ in the specific prediction or classification under scrutiny.\n\_\nThe way this\
    \ works is relatively uncomplicated: LIME generates a simple linear regression\
    \ model by weighting the values of the data points, which were produced by randomly\
    \ perturbing the opaque model, according to their proximity to the original prediction\
    \ or classification. The closest of these values to the instance being explained\
    \ are weighted the heaviest, so that the supplemental model can produce an explanation\
    \ of feature importance that is locally faithful to that instance. Note that the\
    \ type of model that LIME uses most prominently is a sparse linear regression\n\
    \n \nfunction for reasons of semantic transparency that were discussed above.\
    \ Other interpretable models such as decision trees can likewise be employed.\n\
    \_\nWhile LIME does indeed appear to be a step in the right direction for the\
    \ future of interpretable AI, a host of issues that present challenges to the\
    \ approach remains unresolved. For instance, the crucial aspect of how to properly\
    \ define the proximity measure for the \u2018neighbourhood\u2019 or \u2018local\
    \ region\u2019 where the explanation applies remains unclear, and small changes\
    \ in the scale of the chosen measure can lead to greatly diverging explanations.\
    \ Likewise, the explanation produced by the supplemental linear model can quickly\
    \ become unreliable even with small and virtually unnoticeable perturbations of\
    \ the system it is attempting to approximate. This challenges the basic assumption\
    \ that that there is always some simplified linear model that successfully approximates\
    \ the underlying model reasonably well near any given data point.\n\_\nLIME\u2019\
    s creators have largely acknowledged these shortcomings and have recently offered\
    \ a new explanatory approach that they call \u2018anchors\u2019. These \u2018\
    high precision rules\u2019 incorporate into their formal structures \u2018reasonable\
    \ patterns\u2019 that are\noperating within the underlying model (such as the\
    \ implicit linguistic conventions that are at work in a sentiment prediction model),\
    \ so that they can establish suitable and faithful boundaries of their explanatory\
    \ coverage of its predictions or classifications.\n\_\nA related and equally significant\
    \ local interpretive strategy is called SHAP (Shapley Additive exPlanations).\
    \ SHAP uses concepts from game theory to define a \u2018Shapley value\u2019 for\
    \ a feature of concern that provides a measurement of its influence on the underlying\
    \ model\u2019s prediction. Broadly, this value is calculated for a feature by\n\
    averaging its marginal contribution to _every possible prediction _for the instance\
    \ under consideration.\n\_\nThis might seem impossible, but the strategy is straightforward.\
    \ SHAP calculates the marginal contribution of the relevant feature for all possible\
    \ combinations of inputs in the feature space of the instance. So, if the opaque\
    \ model that it is explaining has 15 features, SHAP would calculate the marginal\
    \ contribution of the feature under consideration 32,768 times (i.e. one calculation\
    \ for each combination of all possible combinations of features: 215, or 2_k _when\
    \ _k = _15).\n\_\nThis method then allows SHAP to estimate the Shapley values\
    \ for all input features in the set to produce the complete distribution of the\
    \ prediction for the instance. In our example, this would entail 491,520 calculations.\
    \ While such a procedure is computationally burdensome and becomes intractable\
    \ beyond a certain threshold, this means that _locally_, that is, for the calculation\
    \ of the specific instance, SHAP can axiomatically guarantee the consistency and\
    \ accuracy of its reckoning of the marginal effect of the feature. (Note that\
    \ the SHAP platform does offer methods of approximation to avoid this excessive\
    \ computational expense.)\n\_\nDespite this calculational robustness, SHAP also\
    \ faces some of the same kinds of difficulties that LIME does. The way SHAP calculates\
    \ marginal contributions is by\n\n \nconstructing two instances: the first instance\
    \ includes the feature being measured while the second leaves it out. After calculating\
    \ the prediction for each of these instances by plugging their values into the\
    \ underlying model, the result of the second is subtracted from that of the first\
    \ to determine the marginal contribution of the feature. This procedure is then\
    \ repeated for all possible combinations of features so that the weighted average\
    \ of all of the marginal contributions of the feature of concern can be computed.\n\
    \_\nThe contestable part of this process comes with how SHAP defines the _absence\
    \ _of variables under consideration. To leave out a feature\u2014whether it\u2019\
    s the one being directly measured or one of the others not included in the combination\
    \ under consideration\u2014SHAP replaces it with a _stand-in feature value _drawn\
    \ from a selected donor sample (that is itself drawn from the existing dataset).\
    \ This method of sampling values assumes feature independence (i.e. that values\
    \ sampled are not correlated in ways that might significantly affect the output\
    \ for a particular calculation). As a consequence, the interaction effects engendered\
    \ by and between stand-in variables are necessarily unaccounted for when conditional\
    \ contributions are approximated.\nThe result is the introduction of uncertainty\
    \ into the explanation that is produced because the complexity of multivariate\
    \ interactions in the underlying model may not be sufficiently captured by the\
    \ simplicity of this supplemental interpretability technique. This drawback in\
    \ sampling (as well as a certain degree of arbitrariness in domain definition)\
    \ can cause SHAP to become unreliable even with minimal perturbations of the model\
    \ it is approximating.\n\_\nDespite these limitations in the existing tools of\
    \ local interpretability, it is important that you think \u2018local-first\u2019\
    \ when considering the issue of the coverage and scope of the explanatory approaches\
    \ you plan to incorporate into your project. Being able to provide explanations\
    \ of specific predictions and classifications is of paramount\nimportance both\
    \ to securing optimal outcome transparency and also to ensuring that your AI system\
    \ will be implemented responsibly and reasonably.\n\_\nGlobal interpretability:\
    \ The motivation behind the creation of local interpretability tools like LIME\
    \ or SHAP (as well as many others not mentioned here) has derived, at least in\
    \ part, from a need to find a way of avoiding the kind of difficult _double bind\
    \ _faced by the alternative approach to the coverage and scope of interpretable\
    \ AI: global interpretability.\n\_\nOn the prevailing view, providing a global\
    \ explanation of a \u2018black box\u2019 model entails offering an alternative\
    \ interpretable model that captures the innerworkings and logic of a \u2018black\
    \ box\u2019 model _in sum _and across predictions or classifications. The difficulty\
    \ faced by global interpretability arises in the seemingly unavoidable trade-off\
    \ between the need for the global explanatory model to be sufficiently simple\
    \ so that it is understandable by humans and the need for that model to be sufficiently\
    \ complex so that it can capture the intricacies of how the mapping function of\
    \ a \u2018black box\u2019 model works as a whole.\n\n \nWhile this is clearly\
    \ a real problem that appears to be theoretically inevitable, it is important\
    \ to keep in mind that, _from a practical standpoint_, a serviceable notion of\
    \ global interpretability need not be limited to such a conceptual puzzle. There\
    \ are at least two less ambitious but more constructive ways to view global interpretability\
    \ as a potentially meaningful contributor to the responsible design and implementation\
    \ of interpretable AI.\n\_\nFirst, many useful attempts have already been made\
    \ at building explanatory models that employ interpretable methods (like decision\
    \ trees, rule lists, and case-based classification) to globally approximate neural\
    \ nets, tree ensembles, and support vector machines. These results have enabled\
    \ a deeper understanding of the way human interpretable logics and conventions\
    \ (like if-then rules and representationally generated prototypes) can be measured\
    \ against or mapped onto high dimensional computational structures and even allow\
    \ for some degree of targeted comprehensibility of the logic of their parts.\n\
    \_\nThis capacity to \u2018peek into the black box\u2019 is of great practical\
    \ importance in domains where trust, user-confidence, and public acceptance are\
    \ critical for the realisation optimal outcomes. Moreover, this ability to move\
    \ back and forth between interpretable architectures and high-dimensional processing\
    \ structures can enable knowledge discovery as well as insights into the kinds\
    \ of dataset-level and population-level patterns, which are crucial for well-informed\
    \ macroscale decision- making in areas ranging from public health and economics\
    \ to the science of climate change.\n\_\nBeing able to uncover global effects\
    \ and relationships between complex model behaviour and data distributions at\
    \ the demographic and ecological level may prove vital for establishing valuable\
    \ and practically useful knowledge about unobservable but significant biophysical\
    \ and social configurations. Hence, although these models have not solved the\
    \ understandability-complexity puzzle as such, they have opened up new pathways\
    \ for innovative thinking in the applied data sciences that may be of immense\
    \ public benefit in the future.\n\_\nSecondly, as mentioned above, under the auspices\
    \ of the aspiration to engineering insight, a _descriptive and analytical kind\
    \ of global interpretability _can be seen as a driving force of data scientific\
    \ advancement. When seen through a practitioner- centred lens, this sort of global\
    \ interpretability allows data scientists to take a wide- angled and discovery-oriented\
    \ view of a \u2018black box\u2019 model\u2019s relationship to patterns that arise\
    \ across the range of its predictions. Figuring out how an opaque system works\
    \ and how to make it work better by more fully understanding these patterns is\
    \ a continuous priority of good research. So too is understanding the relevance\
    \ of features and of their complex interactions through dataset level measurement\
    \ and analysis. These dimensions of incorporating the explanatory aspirations\
    \ of global interpretability into best practices of research and innovation should\
    \ be encouraged in your AI project.\n\n   (3) Formulate an interpretability action\
    \ plan: The final step you will need to take to ensure a\nresponsible approach\
    \ to using \u2018black box\u2019 models is to formulate an interpretability action\
    \ plan so that you and your team can put adequate forethought into how explanations\
    \ of the outcomes of your system\u2019s decisions, behaviours, or problem-solving\
    \ tasks can be optimally provided to users, decision subjects, and other affected\
    \ parties.\n\_\nThis action plan should include the following:\n\_\n\xB7\_\_\_\
    \_\_\_A clear articulation of the explanatory strategies your team intends to\
    \ use and a detailed plan that indicates the stages in the project workflow when\
    \ the design and development of these strategies will need to take place.\n\_\n\
    \xB7\_\_\_\_\_\_A succinct formulation of your explanation delivery strategy,\
    \ which addresses the special provisions for clear, simple, and user-centred explication\
    \ that are called for when\nsupplemental interpretability tools for \u2018black\
    \ box\u2019 models are utilised. See more about delivery and implementation in\
    \ Guideline 5.\n\_\n\xB7\_\_\_\_\_\_A detailed timeframe for evaluating your team\u2019\
    s progress in executing its interpretability action plan and a role responsibility\
    \ list, which maps in detail the various task-specific responsibilities that will\
    \ need to be fulfilled to execute the plan.\n   \_\nGuideline 4: Think about interpretability\
    \ in terms of the capacities of human understanding\n\_\nWhen you begin to deliberate\
    \ about the specific scope and content of your interpretability platform, it is\
    \ important to reflect on what it is that you are exactly aiming to do in making\
    \ your model sufficiently interpretable. A good initial step to take in this process\
    \ is to think about what makes even the simplest explanations clear and understandable.\
    \ In other words, you should begin by thinking about interpretability in terms\
    \ of the capacities and limitations of human cognition.\n\_\nFrom this perspective,\
    \ it becomes apparent that even the most straightforward model like a linear regression\
    \ function or a decision tree can become uninterpretable when its dimensionality\
    \ presses beyond the cognitive limits of a thinking human. Recall our example\
    \ of the simple linear regression: \U0001D466 = \U0001D44E + \U0001D44F\U0001D465\
    \ + \U0001D716. In this instance, only one feature _x _relates to the response\
    \ variable _y_, so understanding the predictive relationship is easy. The model\
    \ is parsimonious.\n\_\nHowever, if we started to add more features as covariates,\
    \ even though the model would remain linear and hence intuitively predictable,\
    \ being able to understand the relationship between the response variable and\
    \ all the predictors and their coefficients (feature weights) would quickly become\
    \ difficult. So, say we added ten thousand features and trained the model: \U0001D466\
    \ = \U0001D44E + \U0001D44F0\U0001D4650 +\n\U0001D44F1\U0001D4651 + \u22EF + \U0001D44F\
    10000\U0001D46510000 + \U0001D716. Understanding _how _this model\u2019s prediction\
    \ comes about\u2014what role each of the individual parts play in producing the\
    \ prediction\u2014would become difficult because of a certain cognitive limit\
    \ in the quantity of entities that human thinking can handle at any given time.\
    \ This model would lose a significant degree of interpretability.\n\_\nSeeing\
    \ interpretability as a continuum of comprehensibility that is dependent on the\
    \ capacities and limits of the individual human interpreter should key you in\
    \ to what is needed in order to\ndeliver an interpretable AI system. Such limits\
    \ to consider should include not only cognitive boundaries but also varying levels\
    \ of access to relevant vocabularies of explanation; an explanation about the\
    \ results of a trained model that uses a support vector machine to divide a 26-dimensional\
    \ feature space with a planar separator, for instance, may be easy to understand\
    \ for a technical operator or auditor but entirely inaccessible to a non-specialist.\
    \ Offering good explanations should take expertise level into account. Your interpretability\
    \ platform should be cognitively equitable.\"\n(Leslie, 2019, p.40-53)\n  \n##\
    \ IEEE report\n\"for transparency from implementation to deployment\n## Background\n\
    When A/IS become part of social communities and behave according to the norms\
    \ of their communities, people will want to understand the A/IS decisions and\
    \ actions, just as they want to understand each other\u2019s decisions and actions.\
    \ This is particularly true for morally significant actions or omissions: an ethical\
    \ reasoning system should be able to explain its own reasoning to a user on request.\
    \ Thus, transparency, or \u201Cexplainability\u201D, of A/IS is paramount (Chaudhuri\
    \ 201727; Wachter, Mittelstadt, and Floridi 201728), and it will allow a community\
    \ to understand, predict, and modify the A/IS (see Section 1, Issue 2; for a nuanced\
    \ discussion see Selbst and Barocas29). Moreover, as the norms embedded in A/IS\
    \ are continuously updated and refined (see Section 1, Issue 2), transparency\
    \ allows for appropriate trust to be developed (Grodzinsky, Miller, and Wolf 201130),\
    \ and, where necessary, allows the community to modify a system\u2019s norms,\
    \ reasoning, and behavior.\nTransparency can occur at multiple levels, e.g., ordinary\
    \ language or coder verification, and for multiple stakeholders, e.g., user, engineer,\
    \ and attorney. (See [IEEE P7001](https://standards.ieee.org/develop/project/7001.html)\u2122\
    , IEEE Standards Project for Transparency of Autonomous Systems). It should be\
    \ noted that transparency to all parties may not always be advisable, such as\
    \ in the case of security programs that prevent a system from being hacked (Kroll\
    \ et al. 201631). Here we briefly illustrate the broad range of transparency by\
    \ reference to four ways in which systems can be transparent\u2014traceability,\
    \ verifiability, honest design, and intelligibility\u2014and apply these considerations\
    \ to the implementation of norms in A/IS.\n_Transparency as traceability_\u2014\
    Most relevant for the topic of implementation is the transparency of the software\
    \ engineering process during implementation (Cleland-Huang, Gotel, and Zisman201232).\
    \ It allows for the originally identified norms (Section 1, Issue 1) to be traced\
    \ through to the final system. This allows technical inspection of which norms\
    \ have been implemented, for which contexts, and how norm conflicts are resolved,\
    \ e.g., priority weights given to different norms. Transparency in the implementation\
    \ process may also reveal biases that were inadvertently built into systems, such\
    \ as racism and sexism, in search engine algorithms (Noble 201333). (See Section\
    \ 3, Issue 2.) Such traceability in turn calibrates a community\u2019s trust about\
    \ whether A/IS are conforming to the norms and values relevant in their use contexts\
    \ (Fleischmann and Wallace 200534).\n_Transparency as verifiability_\u2014Transparency\
    \ concerning how normative reasoning is approached in the implementation is important\
    \ as we wish to verify that the normative decisions the system makes match the\
    \ required norms and values. Explicit and exact representations of these normative\
    \ decisions can then provide the basis for a range of strong mathematical techniques,\
    \ such as formal verification (Fisher, Dennis, and Webster 201335). Even if a\
    \ system cannot explain every single reasoning step in understandable human terms,\
    \ a log of ethical reasoning should be available for inspection of later evaluation\
    \ purposes (Hind et al. 201836).\n_Transparency as honest design_\u2014German\
    \ designer Dieter Rams coined the term \u201Chonest design\u201D to refer to design\
    \ that \u201Cdoes not make a product more innovative, powerful or valuable than\
    \ it really is\u201D (Vitsoe 201837; see also Donelli 201538; Jong 201739). Honest\
    \ design of A/IS is one aspect of their transparency, because it allows the user\
    \ to \u201Csee through\u201D the outward appearance and accurately infer the A/IS\u2019\
    \ actual capacities. At times, however, the physical appearance of a system does\
    \ not accurately represent what the system is capable of doing\u2014e.g., the\
    \ agent displays signs of a certain human-like emotion but its internal state\
    \ does not represent that human emotion. Humans are quick to make strong inferences\
    \ from outward appearances of human-likeness to the mental and social capacities\
    \ the A/IS might have. Demands for transparency in design therefore put a responsibility\
    \ on the designer to \u201Cnot attempt to manipulate the consumer with promises\
    \ that cannot be kept\u201D (Vitsoe 201840).\n_Transparency as intelligibility_\u2014\
    As mentioned above, humans will want to understand the\_A/IS\u2019 decisions and\
    \ actions, especially the morally significant ones. A clear requirement for an\
    \ ethical A/IS is that the system be able to explain its own reasoning to a user,\
    \ when asked\u2014or, ideally, also when suspecting the user\u2019s confusion,\
    \ and the system should do so at a level of ordinary human reasoning, not with\
    \ incomprehensible technical detail (Tintarev and Kutlak 201441). Furthermore,\
    \ when the system cannot explain some of its actions, technicians or designers\
    \ should be available to make those actions intelligible. Along these lines, the\
    \ European Union\u2019s General Data Protection Regulation (GDPR), in effect since\
    \ May 2018, states that, for automated decisions based on personal data, individuals\
    \ have a right to \u201Can explanation of the [algorithmic] decision reached after\
    \ such assessment and to challenge the decision\u201D. (See boyd [sic] 201642,\
    \ for a critical discussion of this regulation.)\n## Recommendation\nA/IS, especially\
    \ those with embedded norms, must have a high level of transparency, shown as\
    \ traceability in the implementation process, mathematical verifiability of their\
    \ reasoning, honesty in appearance-based signals,\_and intelligibility of the\
    \ systems\u2019 operation\_and decisions.\n## Further Resources\n\u2022\_\_\_\_\
    \_d. boyd, \u201C[Transparency \u2260 Accountability](https://points.datasociety.net/transparency-accountability-3c04e4804504).\u201D\
    \__Data & Society: Points_, November 29, 2016.\n\u2022\_\_\_\_\_A. Chaudhuri,\
    \ \u201C Philosophical Dimensions of Information and Ethics in the Internet of\
    \ Things (IoT) Technology,\u201DThe EDP Audit, Control, and Security\nNewsletter,\
    \ vol. 56, no. 4, pp. 7-18, DOI:\n10.1080/07366981.2017.1380474, 2017.\n\u2022\
    \_\_\_\_\_J. Cleland-Huang, O. Gotel, and A. Zisman, eds. _Software and Systems\
    \ Traceability_. London: Springer, 2012. doi:10.1007/978-\n1-4471-2239-5\n\u2022\
    \_\_\_\_\_G. Donelli, \u201CGood design is honest.\u201D (blog). March 13, 2015.\
    \ Accessed Oct 22, 2018. [https://blog.astropad.com/good-design-ishonest/ ](https://blog.astropad.com/good-design-is-honest/)\n\
    \u2022\_\_\_\_\_M. Fisher, L. A. Dennis, and M. P. Webster. \u201CVerifying Autonomous\
    \ Systems.\u201D _Communications of the ACM_, vol. 56,\_no. 9, pp. 84\u201393,\
    \ 2013.\n\u2022\_\_\_\_\_K. R. Fleischmann and W. A. Wallace. \u201CA\nCovenant\
    \ with Transparency: Opening the Black Box of Models.\u201D _Communications of\
    \ the ACM_, vol. 48, no. 5, pp. 93\u201397, 2005.\n\u2022\_\_\_\_\_F. S. Grodzinsky,\
    \ K. W. Miller, and M. J. Wolf.\n\u201CDeveloping Artificial Agents Worthy of\
    \ Trust:\nWould You Buy a Used Car from This Artificial Agent?\u201D _Ethics and\
    \ Information Technology_, vol. 13, pp. 17\u201327, 2011.\n\u2022\_\_\_\_\_M.\
    \ Hind, et al. \u201CIncreasing Trust in AI Services through Supplier\u2019s Declarations\
    \ of Conformity.\u201D _ArXiv E-Prints_, Aug. 2018. [Online] Available: [https://arxiv.org/abs/1808.07261.](https://arxiv.org/abs/1808.07261)\
    \ [Accessed October 28, 2018].\n\u2022\_\_\_\_\_C. W. De Jong, ed., _Dieter Rams:\
    \ Ten Principles for Good Design_. New York, NY: Prestel Publishing, 2017.\n\u2022\
    \_\_\_\_\_J. A. Kroll, J. Huey, S. Barocas et al. \u201CAccountable Algorithms.\u201D\
    \ University of Pennsylvania Law Review 165 2017.\n\u2022\_\_\_\_\_S. U. Noble,\
    \ \u201CGoogle Search: Hyper-Visibility as a Means of Rendering Black Women and\
    \ Girls Invisible.\u201D InVisible Culture 19, 2013.\n\u2022\_\_\_\_\_ D. Selbst\
    \ and S. Barocas, \u201CThe Intuitive Appeal of Explainable Machines,\u201D _87Fordham\
    \ Law Review 1085_, Available at SSRN: [https:// ssrn.com/abstract=3126971 ](https://ssrn.com/abstract=3126971)or\
    \ [http://dx.doi.org/10.2139/ssrn.3126971,](http://dx.doi.org/10.2139/ssrn.3126971)\
    \ Feb. 19, 2018.\n\u2022\_\_\_\_\_N. Tintarev and R. Kutlak. \u201CDemo: Making\
    \ Plans Scrutable with Argumentation and Natural Language Generation.\u201D _Proceedings\
    \ of the Companion Publication of the 19th International Conference on Intelligent\
    \ User Interfaces, _pp. 29\u201332, 2014.\n\u2022\_\_\_\_\_Vitsoe. \u201CThe Power\
    \ of Good Design.\u201D _Vitsoe_, 2018. Retrieved Oct 22, 2018 from [https://\
    \ www.vitsoe.com/us/about/good-design](https://www.vitsoe.com/us/about/good-design).\n\
    \u2022\_\_\_\_\_S.Wachter, B. Mittelstadt, and L. Floridi, \u201CTransparent,\
    \ Explainable, and Accountable AI for Robotics.\u201D Science Robotics, vol. 2,\
    \ no. 6, eaan6080. doi:10.1126/scirobotics. aan6080, 2017.\"\np.177-179\n"
  Linked Challenges:
  - recdmBNNa98cN8Sda
  Linked Principles:
  - recLHILkx2JDFsLbX
  Linked Sources:
  - recfYC5jjPmpLfSlM
  - recpXl48pJdKDhc6f
  Tags:
  - reflection-discussion
  airtable_createdTime: '2023-05-19T11:54:47.000Z'
  airtable_id: recgn2UvSD4OhzGI4
  title: Principle of transparency, key considerations
- Description: "## Recommendation\nAn independent, internationally coordinated body\u2014\
    akin to ISO\u2014should be formed to oversee whether A/IS products actually meet\
    \ ethical criteria, both when designed, developed, deployed, and when considering\
    \ their evolution after deployment and during interaction with other products.\
    \ It should also include\_a certification process.\n## Further Resources\n\u2022\
    \_\_\_\_\_A. Tutt, \u201CAn FDA for Algorithms,\u201D _Administrative Law Review\
    \ _69, 83\u2013123, 2016.\n\u2022\_\_\_\_\_M. U. Scherer, \u201C[Regulating Artificial\
    \ Intelligence Systems: Risks, Challenges, Competencies, and Strategies,](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2609777)\u201D\
    \ _Harvard Journal of Law and Technology _vol._ _29, no. 2, 354\u2013400, 2016.\n\
    \u2022\_\_\_\_\_D. R. Desai and J. A. Kroll, \u201C[Trust But Verify: A Guide\
    \ to Algorithms and the Law](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2959472).\u201D\
    \ _Harvard Journal of Law and Technology_, Forthcoming; Georgia Tech Scheller\
    \ College of Business Research Paper No. 17-19, 2017.\n\np.133-134\n"
  Linked Challenges:
  - recrk0Tgfwe5J9xfI
  - rececsX8igwNqhhkC
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T10:57:02.000Z'
  airtable_id: recgqEckuFTILlOnz
  title: Independent standards body for ethical criteria
- Description: "\u201CBecause human beings have a hand in all stages of the construction\
    \ of AI systems, fairness-aware design must take precautions across the AI project\
    \ workflow to prevent bias from having a discriminatory influence:\n- Problem\
    \ Formulation: At the initial stage of problem formulation and outcome definition,\
    \ technical and non-technical members of your team should work together to translate\
    \ project goals into measurable targets. This will involve the use of both domain\
    \ knowledge and technical understanding to define what is being optimised in a\
    \ formalisable way and to translate the project\u2019s objective into a target\
    \ variable or measurable proxy, which operates as a statistically actionable rendering\
    \ of the defined outcome. At each of these points, choices must be made about\
    \ the design of the algorithmic system that may introduce structural biases which\
    \ ultimately lead to discriminatory harm. Special care must be taken here to identify\
    \ affected stakeholders and to consider how vulnerable groups might be negatively\
    \ impacted by the specification of outcome variables and proxies. Attention must\
    \ also be paid to the question of whether these specifications are reasonable\
    \ and justifiable given the general purpose of the project and the potential impacts\
    \ that the outcomes of the system\u2019s use will have on the individuals and\
    \ communities involved. These challenges of fairness aware design at the problem\
    \ formulation stage show the need for making diversity and inclusive participation\
    \ a priority from the start of the AI project lifecycle. This involves both the\
    \ collaboration of the entire team and the attainment of stakeholder input about\
    \ the acceptability of the project plan. This also entails collaborative deliberation\
    \ across the project team and beyond about the ethical impacts of the design choices\
    \ made.\n- Data Pre-Processing: Human judgment enters into the process of algorithmic\
    \ system construction at the stage of labelling, annotating, and organising the\
    \ training data to be utilised in building the model. Choices made about how to\
    \ classify and structure raw inputs must be taken in a fairness aware manner with\
    \ due consideration given to the sensitive social contexts that may introduce\
    \ bias into such acts of classification. Similar fairness aware processes should\
    \ be put in place to review automated or outsourced classifications. Likewise,\
    \ efforts should be made to attach solid contextual information and ample metadata\
    \ to the datasets, so that downstream analyses of data processing have access\
    \ to properties of concern in bias mitigation.\u201D\n- Feature Determination\
    \ and Model-Building: The constructive task of selecting the attributes or features\
    \ that will serve as input variables for your model involves human decisions be\
    \ made about what sorts of information may or may not be relevant or rationally\
    \ required to yield an accurate and unbiased classification or prediction. Moreover,\
    \ the feature engineering tasks of aggregating, extracting, or decomposing attributes\
    \ from datasets may introduce human appraisals that have biasing effects. For\
    \ this reason, discrimination awareness should play a large role at this stage\
    \ of the AI model-building workflow as should domain knowledge and policy expertise.\
    \ Your team should proceed in the modelling stage aware that choices made about\
    \ grouping or separating and including or excluding features as well as more general\
    \ judgements about the comprehensiveness or coarseness of the total set of features\
    \ may have significant consequences for vulnerable or protected groups. The process\
    \ of tuning hyperparameters and setting metrics at the modelling, testing, and\
    \ evaluation stages also involves human choices that may have discriminatory effects\
    \ in the trained model. Your technical team should proceed with an attentiveness\
    \ to bias risk, and continual iterations of peer review and project team consultation\
    \ should be encouraged to ensure that choices made in adjusting the dials and\
    \ metrics of the model are in line with bias mitigation and discriminatory non-harm.\n\
    - Evaluating Analytical Structures: Design fairness also demands close assessment\
    \ of the existence in the trained model of lurking or hidden proxies for discriminatory\
    \ features that may act as significant factors in its output. Including such hidden\
    \ proxies in the structure of the model may lead to implicit \u2018redlining\u2019\
    \ (the unfair treatment of a sensitive group on the basis of an unprotected attribute\
    \ or interaction of attributes that \u2018stands in\u2019 for a protected or sensitive\
    \ one). Designers must additionally scrutinise the moral justifiability of the\
    \ significant correlations and inferences that are determined by the model\u2019\
    s learning mechanisms themselves. In cases of the processing of social or demographic\
    \ data related to human features, where the complexity and high dimensionality\
    \ of machine learning models preclude the confirmation of the discriminatory non-harm\
    \ of these inferences (for reason of their uninterpretability by human assessors),\
    \ these models should be avoided. In AI systems that process and draw analytics\
    \ from data arising from human relationships, societal patterns, and complex socioeconomic\
    \ and cultural formations, designers must prioritise a degree of interpretability\
    \ that is sufficient to ensure that the inferences produced by these systems are\
    \ nondiscriminatory. In cases where this is not possible, a different, more transparent\
    \ and explainable model or portfolio of models should be chosen. Analytical structures\
    \ must also be confirmed to be procedurally fair. Any rule or procedure employed\
    \ in an AI system should be consistently and uniformly applied to every decision\
    \ subject whose information is being processed by that system. Your team should\
    \ be able to certify that when a rule or procedure has been used to render an\
    \ outcome for any given individual, the same rule or procedure will be applied\
    \ to any other individual in the same way regardless of that other subject\u2019\
    s similarities with or differences from the first. Implementers, in this respect,\
    \ should be able to show that any algorithmic output is replicable when the same\
    \ rules and procedures are applied to the same inputs. Such a uniformity of the\
    \ application of rules and procedures secures the equal procedural treatment of\
    \ decision subjects and precludes any rule-changes in the algorithmic processing\
    \ targeted at a specific person that may disadvantage that individual vis-\xE0\
    -vis any other.\n(Leslie, 2019, p. 16-18)\n"
  Linked Principles:
  - reczVPIH1y2OMpAJH
  - rec42P8U9usfYCtv9
  - rec7n2TGrH9RHYpQj
  Linked Sources:
  - recfYC5jjPmpLfSlM
  Tags:
  - reflection-discussion
  airtable_createdTime: '2023-05-19T11:12:50.000Z'
  airtable_id: recgswAsiepwEclOd
  title: Principle of Discriminatory non-harm for fairness, key considerations for
    design fairness
- Description: "\u2022 Are teachers and school leaders aware of the AI methods and\
    \ features being utilised by the system? \n\u2022 Is it clear what aspects AI\
    \ can take over and what not within the system? \n\u2022 Do teachers and school\
    \ leaders understand how specific assessment or personalisation algorithms work\
    \ within the AI system? \n\u2022 Are the system processes and outcomes focussed\
    \ on the expected learning outcomes for the learners? How reliable are the predictions,\
    \ assessments and classifications of the AI system in explaining and evaluating\
    \ the relevance of its use? \n\u2022 Are the instructions and information accessible\
    \ and presented in a way that is clear both for teachers and learners?\n(European\
    \ Commission, 2022, p. 19)\n"
  Linked Cases:
  - reciNqxyfUgE5XM7t
  - recmS3zSMbR3ofAR5
  Linked Challenges:
  - recdmBNNa98cN8Sda
  - recHsgB7ki6GknJnX
  Linked Principles:
  - recLHILkx2JDFsLbX
  - recmzjcGKv3yNOxbl
  - recgDkzdE9dfpTxCK
  Linked Sources:
  - rec9jnxuHOioQn4DC
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T13:36:31.000Z'
  airtable_id: rechaQXedBh3OsMjZ
  title: Guiding questions for educators regarding Transparency of AI in Education
- Description: "\u201CPurpose, Participation, and Consent \n- What is the question\
    \ or problem you want to investigate, and why is it essential to spend your own\
    \ and others\u2019 time and energy on it? \n- Whose consent, permission, cooperation,\
    \ involvement, or collaboration will be required to conduct your project? How\
    \ can roles and permissions be negotiated and renegotiated over time? \n- What\
    \ concerns might students have about your work and their participation in it?\
    \ What choices do students have if they are uncomfortable? \n- Whose perspectives\
    \ will be represented in the work? How can various perspectives be honoured? What\
    \ unique concerns do you have about representing individuals or groups with less\
    \ power in the educational system? \n- What power relationships need to be considered\
    \ when negotiating roles, permissions, and involvements by various participants\
    \ in your work? Are there issues of gender, race, culture, and status difference\
    \ that need to be considered?\u201D \n(Fedoruk, 2017, p. 2)\n"
  Linked Principles:
  - recint2IxoR8aILCp
  - recQ9DIFEsOEkCx3O
  - recLHILkx2JDFsLbX
  Linked Sources:
  - recQzldmBLByP78Uu
  Tags:
  - reflection-questions
  - education-research
  airtable_createdTime: '2023-05-19T12:56:14.000Z'
  airtable_id: recinajIdAe7hRxjN
  title: Questions to consider in SoTL research inception
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CPrivacy and data governance Respect for privacy and data Protection:\
    \ \n\u201CQuality and integrity of data: \n\uF0FC Did you align your system with\
    \ relevant standards (for example ISO, IEEE) or widely adopted protocols for daily\
    \ data management and governance? \n\uF0FC Did you establish oversight mechanisms\
    \ for data collection, storage, processing and use? \n\uF0FC Did you assess the\
    \ extent to which you are in control of the quality of the external data sources\
    \ used? \n\uF0FC Did you put in place processes to ensure the quality and integrity\
    \ of your data? Did you consider other processes? How are you verifying that your\
    \ data sets have not been compromised or hacked?\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 28)\n"
  Linked Principles:
  - recPg7Ov0priGGtLm
  - recOHnq45Fq7YWsRO
  - reczVPIH1y2OMpAJH
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:31:36.000Z'
  airtable_id: reciw4r1bRvx6A6Fq
  title: Considerations in assessing trustworthy AI - Quality and integrity of data
- Description: "\u2022\_\_\_\_\_Ethics training needs to be a core subject\_for all\
    \ those in the STEM field, beginning at\_the earliest appropriate level and for\
    \ all advanced degrees.\n\u2022\_\_\_\_\_Effective STEM ethics curricula should\
    \ be informed by experts outside the STEM community_ _from a variety of cultural\
    \ and educational backgrounds to ensure that students acquire sensitivity to a\
    \ diversity\_of robust perspectives on ethics and design.\n\u2022\_\_\_\_\_Such\
    \ curricula should teach aspiring engineers, computer scientists, and statisticians\
    \ about the relevance and impact of their decisions in designing A/IS technologies.\
    \ Effective ethics education in STEM contexts and beyond should span primary,\
    \ secondary, and postsecondary education, and include both universities and vocational\
    \ training schools.\n\u2022\_\_\_\_\_Relevant accreditation bodies should reinforce\
    \ this integrated approach as outlined above.\n\n## Further Resources\n\u2022\_\
    \_\_\_\_[IEEE P7000TM Standards Project for a Model ](https://standards.ieee.org/develop/project/7000.html)\n\
    [Process for Addressing Ethical Concerns During System Design.](https://standards.ieee.org/develop/project/7000.html)\
    \ IEEE P7000 aims to enhance corporate IT innovation practices by providing processes\
    \ for embedding a values- and virtue-based thinking, culture, and practice into\
    \ them.\n\u2022\_\_\_\_\_Z. Lipton and J. Steinhardt, [Troubling Trends in Machine\
    \ Learning Scholarship.](https://www.dropbox.com/s/ao7c090p8bg1hk3/Lipton%20and%20Steinhardt%20-%20Troubling%20Trends%20in%20Machine%20Learning%20Scholarship.pdf?dl=0)\
    \ ICML conference paper, July 2018.\n\u2022\_\_\_\_\_J. Holdren, and M. Smith.\
    \ \u201C[Preparing for the ](https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_future_of_ai.pdf)\n\
    [Future of Artificial Intelligence.\u201D](https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_future_of_ai.pdf)\
    \ Washington, DC: Executive Office of the President, National Science and Technology\
    \ Council, 2016.\n\u2022\_\_\_\_\_Comparing the UK, EU, and US approaches to AI\
    \ and ethics: C. Cath, S. Wachter, B. Mittelstadt, et al., \u201C[Artificial Intelligence\
    \ and the \u2018Good Society\u2019: The US, EU, and UK Approach.](https://link.springer.com/article/10.1007/s11948-017-9901-7)\u201D\
    \ _Science and Engineering Ethics, _vol._ _24_, _pp. 505-528, 2017.\n\np122-123\n"
  Linked Challenges:
  - recPqsTlFK76oGD9C
  Linked Principles:
  - recKWrfJzX52AXSIf
  - recQ9DIFEsOEkCx3O
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T10:14:39.000Z'
  airtable_id: reck6xeMUc9jtmLr3
  title: Ethics must be embedded in STEM education and professional accreditation
- Description: "Results and the Presentation of Results to Various Audiences \n- What\
    \ negative or embarrassing data can you anticipate emerging from your scholarship\
    \ of teaching and learning, and who might be harmed as a consequence? How can\
    \ you create a context for understanding \u201Cbad news\u201D? How in particular,\
    \ can examples of work by students who are novices or who are struggling with\
    \ new material be treated with respect? \n- Who will see the results and products\
    \ of your work? What conclusions might be drawn by various audiences: About students?\
    \ About teaching? About your department, discipline, or campus? About higher education?\
    \ About you? How is your choice of medium (e.g., video recording) related to those\
    \ concerns? \n- How can contributions to your work by various participants (including\
    \ both colleagues and students) be acknowledged and/or cited, while maintaining\
    \ appropriate confidentiality?\n(Fedoruk, 2017, p. 2-3)\n"
  Linked Principles:
  - recMGB4iC5oaCtr5x
  - rec42P8U9usfYCtv9
  - rec6O9e1nYBJtQUTj
  Linked Sources:
  - recQzldmBLByP78Uu
  Tags:
  - reflection-questions
  - education-research
  airtable_createdTime: '2023-05-19T13:01:27.000Z'
  airtable_id: reclplj55gpqdYGTr
  title: Questions to consider in SoTL research dissemination
- Description: "\"Educational data offer a unique opportunity to model individuals\u2019\
    \ thought processes and could be used to predict or change individuals\u2019 behavior\
    \ in many situations. Governments and organizations should classify educational\
    \ data\_as being sensitive and implement special protective standards.\nChildren\u2019\
    s data should be held in \u201Cescrow\u201D\_and not used for any commercial purposes\_\
    until a child reaches the age of majority and is able to authorize use as they\
    \ choose.\"\np.115 IEEE report\n"
  Linked Challenges:
  - recKzhZVabDuYM6rG
  - recroNon39TCBeC88
  Linked Sources:
  - recpXl48pJdKDhc6f
  Tags:
  - education-research
  airtable_createdTime: '2023-06-05T09:41:58.000Z'
  airtable_id: reclzrMrQSQls3PI9
  title: Educational data should be classified as sensitive and held in 'escrow' not
    available for commercial purposes
- Description: "\u201CPrinciple of Discriminatory Non-Harm: The designers and users\
    \ of AI systems, which process social or demographic data pertaining to features\
    \ of human subjects, societal patterns, or cultural formations, should prioritise\
    \ the mitigation of bias and the exclusion of discriminatory influences on the\
    \ outputs and implementations of their models. Prioritising discriminatory non-harm\
    \ implies that the designers and users of AI systems ensure that the decisions\
    \ and behaviours of their models do not generate discriminatory or inequitable\
    \ impacts on affected individuals and communities. This entails that these designers\
    \ and users ensure that the AI systems they are developing and deploying: \n1\\\
    . Are trained and tested on properly representative, relevant, accurate, and generalisable\
    \ datasets (Data Fairness) \n2\\. Have model architectures that do not include\
    \ target variables, features, processes, or analytical structures (correlations,\
    \ interactions, and inferences) which are unreasonable, morally objectionable,\
    \ or unjustifiable (Design Fairness)\n3\\. Do not have discriminatory or inequitable\
    \ impacts on the lives of the people they affect (Outcome Fairness) \n4\\. Are\
    \ deployed by users sufficiently trained to implement them responsibly and without\
    \ bias (Implementation Fairness)\u201D (Leslie, 2019, p. 14)\n\n\\*Data Fairness\\\
    *\n\"Responsible data acquisition, handling, and management is a necessary component\
    \ of algorithmic fairness. If the results of your AI project are generated by\
    \ biased, compromised, or skewed datasets, affected stakeholders will not adequately\
    \ be protected from discriminatory harm. Your project team should keep in mind\
    \ the following key elements of data fairness:\n- Representativeness: Depending\
    \ on the context, either underrepresentation or overrepresentation of disadvantaged\
    \ or legally protected groups in the data sample may lead to the systematic disadvantaging\
    \ of vulnerable stakeholders in the outcomes of the trained model. To avoid such\
    \ kinds of sampling bias, domain expertise will be crucial to assess the fit between\
    \ the data collected or procured and the underlying population to be modelled.\
    \ Technical team members should, if possible, offer means of remediation to correct\
    \ for representational flaws in the sampling. \n- Fit-for-Purpose and Sufficiency:\
    \ An important question to consider in the data collection and procurement process\
    \ is: Will the amount of data collected be sufficient for the intended purpose\
    \ of the project? The quantity of data collected or procured has a significant\
    \ impact on the accuracy and reasonableness of the outputs of a trained model.\
    \ A data sample not large enough to represent with sufficient richness the significant\
    \ or qualifying attributes of the members of a population to be classified may\
    \ lead to unfair outcomes. Insufficient datasets may not equitably reflect the\
    \ qualities that should rationally be weighed in producing a justified outcome\
    \ that is consistent with the desired purpose of the AI system. Members of the\
    \ project team with technical and policy competences should collaborate to determine\
    \ if the data quantity is, in this respect, sufficient and fit-for-purpose. \n\
    - Source Integrity and Measurement Accuracy: Effective bias mitigation begins\
    \ at the very commencement of data extraction and collection processes. Both the\
    \ sources and instruments of measurement may introduce discriminatory factors\
    \ into a dataset. When incorporated as inputs in the training data, biased prior\
    \ human decisions and judgments such as prejudiced scoring, ranking, interview-data\
    \ or evaluation\u2014will become the \u2018ground truth\u2019 of the model and\
    \ replicate the bias in the outputs of the system. In order to secure discriminatory\
    \ non-harm, you must do your best to make sure your data sample has optimal source\
    \ integrity. This involves securing or confirming that the data gathering processes\
    \ involved suitable, reliable, and impartial sources of measurement and sound\
    \ methods of collection. \n- Timeliness and Recency: If your datasets include\
    \ outdated data then changes in the underlying data distribution may adversely\
    \ affect the generalisability of your trained model. Provided these distributional\
    \ drifts reflect changing social relationship or group dynamics, this loss of\
    \ accuracy with regard to the actual characteristics of the underlying population\
    \ may introduce bias into your AI system. In preventing discriminatory outcomes,\
    \ you should scrutinise the timeliness and recency of all elements of the data\
    \ that constitute your datasets. \n- Relevance, Appropriateness and Domain Knowledge:\
    \ The understanding and utilisation of the most appropriate sources and types\
    \ of data are crucial for building a robust and unbiased AI system. Solid domain\
    \ knowledge of the underlying population distribution and of the predictive or\
    \ classificatory goal of the project is instrumental for choosing optimally relevant\
    \ measurement inputs that contribute to the reasonable determination of the defined\
    \ solution. You should make sure that domain experts collaborate closely with\
    \ your technical team to assist in the determination of the optimally appropriate\
    \ categories and sources of measurement.\"\n(Leslie, 2019, p. 15)\n\"To ensure\
    \ the uptake of best practices for responsible data acquisition, handling, and\
    \ management across your AI project delivery workflow, you should initiate the\
    \ creation of a Dataset Factsheet at the alpha stage of your project. This factsheet\
    \ should be maintained diligently throughout the design and implementation lifecycle\
    \ in order to secure optimal data quality, deliberate bias-mitigation aware practices,\
    \ and optimal auditability. It should include a comprehensive record of data provenance,\
    \ procurement, pre-processing, lineage, storage, and security as well as qualitative\
    \ input from team members about determinations made with regard to data representativeness,\
    \ data sufficiency, source integrity, data timeliness, data relevance, training/testing/validating\
    \ splits, and unforeseen data issues encountered across the workflow.\" (Leslie,\
    \ 2019, p. 16)\n"
  Linked Principles:
  - reczVPIH1y2OMpAJH
  - rec42P8U9usfYCtv9
  - rec7n2TGrH9RHYpQj
  Linked Sources:
  - recfYC5jjPmpLfSlM
  Tags:
  - reflection-discussion
  airtable_createdTime: '2023-05-19T11:03:50.000Z'
  airtable_id: recmhwo8kmYkBZ7Sy
  title: Principle of Discriminatory non-harm for fairness, key considerations for
    data fairness
- Description: "\"The research analytical process includes selecting the training\
    \ data, cleaning the data, developing the model through steps of training, evaluating,\
    \ adjusting, re-training the model. \n\_\nSource of Training Data \nThe inferences\
    \ and predictions of an AI system are closely connected to the source of the training\
    \ data and here especially issues on systemic discrimination or biases are interesting\
    \ to disclose and reflect upon as many previous studies have shown such effects\
    \ (Barocas & Selbst, 2016; Bechmann & Bowker, 2019; boyd & Crawford, 2012; Crawford\
    \ & Calo, 2016; Kroll et al., 2017; Sweeney, 2013). The use of AI systems to uncover\
    \ or predict social phenomena can thus be tainted by biases in the training data\
    \ set on certain demographics or proxies thereof, which may lead to unfair and\
    \ unjust outcomes.\_\n\_\n\u25CF\_\_\_\_\_What is the cultural and sociodemographic\
    \ profile of the datasets used by the researcher to train the models? \n\u25CF\
    \_\_\_\_\_To what extent does the cultural and sociodemographic profile of the\
    \ training data allow for generalizability of the resulting findings or predictors\
    \ from the research study? \n\u25CF\_\_\_\_\_Are there particular groups which\
    \ may be advantaged or disadvantaged, in the context in which the researcher is\
    \ deploying an AI-system? What is the potential damaging effect of uncertainty\
    \ and error-rates to different groups? \n\u25CF\_\_\_\_\_How has the demographic\
    \ profile of the researcher(s) affected the composition of the training data?\
    \ \n\u25CF\_\_\_\_\_How does the training data as \u2018ground truth\u2019 affect\
    \ different demographic profiles and proxies thereof?\_\n\_\nData Cleaning \n\
    Data cleaning is the process of detecting, correcting, replacing and even removing\
    \ inaccurate and incomplete records from a database and structuring the data in\
    \ a consistent way that makes it processable in the model. Researchers typically\
    \ find data cleaning a difficult, timeconsuming, though necessary and important\
    \ part of creating an AI-system. It is therefore tempting for some to cut corners\
    \ or otherwise speed up the process, which can lead to concerns about the rigor\
    \ and validity of the study because it is seldom accounted for in details. The\
    \ time spent on cleaning a dataset and the assumptions that go into this process\
    \ should be communicated more clearly in the resulting research paper. A descriptive\
    \ analysis of the study datasets may help to identify missing information, incorrect\
    \ coding, outliers, and misaligned data by the reader. \n\_\n\u25CF\_\_\_\_\_\
    How would you characterize the datasets and their cleaning processes? For which\
    \ variables was the cleaning process optimized? (Features, labels etc.) \n\u25CF\
    \_\_\_\_\_How have (small) adjustments to the training data to make data fit into\
    \ a model logic potentially influenced the outcome of the model calculations and\
    \ predictions? \n\u25CF\_\_\_\_\_If the researcher used the raw data to train\
    \ the model, to what extent could the resulting model be inaccurate, inappropriate,\
    \ or dysfunctional? \n\u25CF\_\_\_\_\_Specifically, which actions have been taken\
    \ by the research team in the process of cleaning the dataset and what potential\
    \ consequences do these choices have on the predictions and/or findings made in\
    \ the study? \n\u25CF\_\_\_\_\_How do the data cleaning actions normalize data\
    \ and what are the potential consequences of taking out outliers in terms of minority\
    \ representation in the model? \n\u25CF\_\_\_\_\_To what extent does the data\
    \ cleaning process reflect the character of the data collected and the context\
    \ in which it was provided? \n\u25CF\_\_\_\_\_What actions have been taken to\
    \ anonymize/pseudonymize the data and to what extent is it possible to de-identify\
    \ data subjects? Does the anonymization prevent certain types of analysis and\
    \ what is the argument for the decisions taken? \n\u25CF\_\_\_\_\_How has the\
    \ data been stored in order to safeguard the privacy of the data subjects? \n\u25CF\
    \_\_\_\_\_If the research team consists of multiple parties and/or distributed\
    \ calculations how has access to data been negotiated and established in a safe\
    \ space solution for data subjects? \n\_\nModel \nThe researcher\u2019s model,\
    \ based on cleaned training data, will likely have utility in predicting behaviours,\
    \ or finding correlations in datasets. Such inferences may not be tailored to\
    \ individuals or be based on anonymized data. Ethical issues may still remain,\
    \ however, with regard to (1) the privacy considerations of groups on their collective\
    \ behaviour and the resulting shifts of power balances, (2) the automation of\
    \ inferences or decision-making, and (3) biases as well as errors in the output\
    \ data. These issues may also arise if researchers choose to work with a pretrained\
    \ model on different datasets, for instance open source models.\_\n\_\n**Group\
    \ Privacy and Power Dynamics **\n\u25CF\_\_\_\_\_Can the knowledge that is generated\
    \ and inferred from the model shift power balances with regard to specific communities\
    \ and societies in the training data or as data subjects in terms of predictive\
    \ power over their behaviour?\_\n\u25CF\_\_\_\_\_Could the increased power be\
    \ operationalized maliciously if the model or inferred data was shared with a\
    \ third-party, and how could such problems be mitigated? \n\u25CF\_\_\_\_\_Could\
    \ the predictors identified by the model be operationalized maliciously by a third\
    \ party when published and how could such use potentially be mitigated? \n\u25CF\
    \_\_\_\_\_To what extent is the organization or the AI-system making decisions\
    \ for data subjects?\_\n\_\n**Automation **\n\u25CF\_\_\_\_\_To what extent is\
    \ human deliberation being replaced by automated systems and what consequences\
    \ does it have for the research results? \n\u25CF\_\_\_\_\_Can the researcher\
    \ override the automated inferences procedure, how will this be documented and\
    \ justified for later reproducibility?\_\n\u25CF\_\_\_\_\_Are the automated inferences\
    \ explainable? \n\u25CF\_\_\_\_\_Is there a strong incentive for the researcher\
    \ to take the automated inferences as a base truth? How was the ground-truth identified\
    \ and is this ground-truth adequate to predict the whole spectrum of the problem\
    \ and/or population behaviour? \n\u25CF\_\_\_\_\_Can the data subjects influence\
    \ the reach of the AI-system on their lives, their physical environments, and\
    \ their own decisions? Should the researchers provide such functionality? \n\_\
    \n**Biases and Errors **\n\u25CF\_\_\_\_\_To what extent has the researcher accounted\
    \ for false positives and false negatives in the output of the model, and to what\
    \ extent can the researcher mitigate their negative impacts? \n\u25CF\_\_\_\_\_\
    Can the researchers use their model to infer social biases and communicate them?\
    \ \n\u25CF\_\_\_\_\_How have steps of re-training the model to improve accuracy\
    \ influenced the outcome and what considerations on representation/non-representation\
    \ have been made in this practice? \n\u25CF\_\_\_\_\_If the research team uses\
    \ a pretrained model, are the datasets well-documented and how can the character\
    \ of the datasets influence the predictions of the research in question and the\
    \ study of another context/practice?\_\_\n\_\n**Model Training **\n\u25CF\_\_\_\
    \_\_How many instances of re-training have taken place, what was the reason for\
    \ each retraining and the result, what were the choices made for changing the\
    \ settings, and what was the specific type of data added to the training loop?\
    \ \n\u25CF\_\_\_\_\_How do the re-training choices align with the cultural and\
    \ sociodemographic profile of the research group, and how does this affect the\
    \ robustness/generalizability of the predictions and/or the findings of the study?\
    \ \n\u25CF\_\_\_\_\_What would be the consequences of manually tweaking certain\
    \ weights in the model and feeding the model with different training data? How\
    \ would this affect the predictions of the model? \"\n(franzke, et al., 2020 p.41-44)\n"
  Linked Principles:
  - recLHILkx2JDFsLbX
  - recmzjcGKv3yNOxbl
  - recSqx6wklVpDzx3s
  - recOHnq45Fq7YWsRO
  Linked Sources:
  - rec6r8OkE2Q2EdiM3
  Tags:
  - reflection-questions
  - project-analysis
  airtable_createdTime: '2023-05-19T12:34:45.000Z'
  airtable_id: recnFHf9V2VtAVJMx
  title: Questions to consider in key stages of AI and machine learning based research,
    regarding the Research Analytical Process
- Description: "## **\"**Recommendations\nA/IS benefits should be equally available\
    \ to populations in HIC and LMIC, in the interest of universal human dignity,\
    \ peace, prosperity, and planet protection. Specific measures for LMIC should\
    \ include:\n\u2022\_\_\_\_\_Deploying A/IS to detect fraud and corruption, to\
    \ increase the transparency of power structures, to contribute to a favorable\
    \ investment, governance, and innovation environment. \n\u2022\_\_\_\_\_Supporting\
    \ LMIC in the development of their own A/IS strategies, and in the retention or\
    \ return of their A/IS talent to prevent \u201Cbrain drain\u201D.\n\u2022\_\_\_\
    \_\_Encouraging global standardization/ harmonization and open source A/IS software.\n\
    \u2022\_\_\_\_\_Promoting distribution of knowledge and wealth generated by the\
    \ latest A/IS, including through formal public policy and financial mechanisms\
    \ to advance equity worldwide.\n\u2022\_\_\_\_\_Developing public datasets to\
    \ facilitate the access of people from LMIC to data resources to facilitate their\
    \ applied research, while ensuring the protection of personal data.\n\u2022\_\_\
    \_\_\_Creating A/IS international research centers in every continent, that promote\
    \ culturally appropriate research, and allow the remote access of LMIC's communities\
    \ to high-end technology.16\n\u2022\_\_\_\_\_Facilitating A/IS access in LMIC\
    \ through online courses in local languages.\n\u2022\_\_\_\_\_Ensuring that, along\
    \ with the use of A/IS, discussions related to identity, platforms, and blockchain\
    \ are conducted, such that core enabling technologies are designed to meet the\
    \ economic, social, and cultural needs of LMIC.\n\u2022\_\_\_\_\_Diminishing the\
    \ barriers and increase LMIC access to technological products, including the formation\
    \ of collaborative networks between developers in HIC and LMIC, supporting the\
    \ latter in attending global A/IS conferences.17\n\u2022\_\_\_\_\_Promoting research\
    \ into A/IS-based technologies, for example, mobile lightweight A/IS applications,\
    \ that are readily available\_in LMIC.\n\u2022\_\_\_\_\_Facilitating A/IS research\
    \ and development in LMIC through investment incentives, public-private partnerships,\
    \ and/or joint grants, and collaboration between international organizations,\
    \ government bodies, universities, and research institutes.\n\u2022\_\_\_\_\_\
    Prioritizing A/IS infrastructure in international development assistance, as necessary\
    \ to improve the quality and standard of living and advance progress towards the\
    \ SDGs in LMIC.\n\u2022\_\_\_\_\_Recognizing data issues that may be particular\
    \ to LMIC contexts, i.e., insufficient sample size for machine learning which\
    \ sometimes results in _de facto_ discrimination, and inadequate laws for, and\
    \ the practice of, data protection.\_\n\u2022\_\_\_\_\_Supporting research on\
    \ the adaptation of\_A/IS methods to scarce data environments\_and other remedies\
    \ that facilitate an optimal\_A/IS enabling environment in LMIC.\n## Further Resources\n\
    \u2022\_\_\_\_\_A. Akubue, \u201CAppropriate Technology for Socioeconomic Development\
    \ in Third World Countries.\u201D _The Journal of Technology Studies _26, no.\
    \ 1, pp. 33\u201343, 2000.\n\u2022\_\_\_\_\_O. Ajakaiye and M. S. Kimenyi. \u201C\
    Higher Education and Economic Development in Africa: Introduction and Overview.\u201D\
    \ _Journal of African Economies _20, no. 3, iii3\u2013iii13, 2011.\n\u2022\_\_\
    \_\_\_D. Allison-Hope and M. Hodge, \"Artificial Intelligence: A Rights-Based\
    \ Blueprint for Business,\u201D San Francisco: BSF, Aug. 28, 2018\n\u2022\_\_\_\
    \_\_D. E. Bloom, D. Canning, and K. Chan. _Higher Education and Economic Development\
    \ in Africa _(Vol. 102). Washington, DC: World Bank, 2006.\n\u2022\_\_\_\_\_N.\
    \ Bloom, \u201CCorporations in the Age of Inequality.\u201D _Harvard Business\
    \ Review, _April 21, 2017.\n\u2022\_\_\_\_\_C. Dahlman, _Technology, Globalization,\
    \ and Competitiveness: Challenges for Developing Countries. Industrialization\
    \ in the 21st Century_. New York: United Nations, 2006.\u2022\_\_\_\_\_M. Fong,\
    \ _Technology_ _Leapfrogging for Developing Countries. Encyclopedia of Information\
    \ Science and Technology_, 2nd ed. Hershey, PA: IGI Global, 2009 (pp. 3707\u2013\
    \ 3713).\n\u2022\_\_\_\_\_C. B. Frey and M. A. Osborne. \u201CThe Future of Employment:\
    \ How Susceptible Are Jobs to Computerisation?\u201D (working paper). Oxford,\
    \ U.K.: Oxford University, 2013.\n\u2022\_\_\_\_\_B. Hazeltine and C. Bull. _Appropriate\
    \ Technology: Tools, Choices, and Implications. _New York: Academic Press, 1999.\n\
    \u2022\_\_\_\_\_McKinsey Global Institute. \u201CDisruptive Technologies: Advances\
    \ That Will Transform Life, Business, and the Global Economy\u201D (report), May\
    \ 2013.\n\u2022\_\_\_\_\_D. Rotman, \u201CHow Technology Is Destroying Jobs.\u201D\
    \ _MIT Technology Review_, June 12, 2013.\n\u2022\_\_\_\_\_R. Sauter and J. Watson.\
    \ \u201CTechnology Leapfrogging: A Review of the Evidence, A Report for DFID.\u201D\
    \ Brighton, England: University of Sussex. October 3, 2008.\n\u2022\_\_\_\_\_\u201C\
    The Rich and the Rest.\u201D _The Economist. _October 13, 2012.\n\u2022\_\_\_\_\
    \_\u201CWealth without Workers, Workers without Wealth.\u201D _The Economist_.\
    \ October 4, 2014.\n\u2022\_\_\_\_\_World Bank. \u201CGlobal Economic Prospects\
    \ 2008: Technology Diffusion in the Developing World.\u201D Washington, DC: World\
    \ Bank, 2008.\n\u2022\_\_\_\_\_World Development Report 2016: Digital Dividends_.\
    \ _Washington, DC: World Bank. doi:10.1596/978-1-4648-0671-1.\n\u2022\_\_\_\_\_\
    World Wide Web Foundation \u201CArtificial Intelligence: The Road ahead in Low\
    \ and Middle-income Countries,\u201D webfoundation.org, June 2017.\"\n\np.145-147\n"
  Linked Challenges:
  - rec0BAjUqQrSIMAEG
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T11:46:23.000Z'
  airtable_id: recnLauSoXMQCSK5c
  title: 'Foster equitable benefits of AI through global standards, regional investment,
    and justice with respect to the burdens and benefits of AI development '
- Description: "\u201CHow is the context defined and conceptualized? \xA7 Does the\
    \ research definition of the context match the way owners, users, or members might\
    \ define it?13 (Parameters such as \u2018culture,\u2019 \u2018person,\u2019 \u2018\
    data set,\u2019 and \u2018public text\u2019 each carry different ethical expectations\
    \ for researchers) \xA7 Are there distinctions between local contextual norms\
    \ for how a venue is conceptualized and jurisdictional frameworks (e.g., Terms\
    \ of Service, other regulations)? For example, if the TOS defines the space as\
    \ off limits for researchers but the individuals want to participate in public\
    \ research of this space, what risk might exist for either the researcher or individuals\
    \ involved?14 \xA7 What are the ethical expectations users attach to the venue\
    \ in which they are interacting, particularly around issues of privacy? Both for\
    \ individual participants as well as the community as a whole?\u201D (Markham\
    \ and Buchanan, 2012, p. 8)\n"
  Linked Principles:
  - recU6u0AZbcNj1ik9
  Linked Sources:
  - recQiVQ7CTC72xp6O
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-18T13:54:19.000Z'
  airtable_id: recobVSYWj9jYvbgF
  title: Questions to consider regarding how a context is defined and conceptualised
    with respect to relevant stakeholder agency
- Description: "## **\"**Recommendations\nThe current range of A/IS applications in\
    \ sectors crucial to the SDGs, and to excluded populations everywhere, should\
    \ be studied, with the strengths, weaknesses, and potential of the most significant\
    \ recent applications analyzed, and the best ones developed at scale. Specific\
    \ objectives to consider include:\n\u2022\_\_\_\_\_Identifying and experimenting\
    \ with\_A/IS technologies relevant to the SDGs,\_such as: big data for development\
    \ relevant to, for example, agriculture and medical tele-diagnosis; geographic\
    \ information systems needed in public service planning, disaster prevention,\
    \ emergency planning, and disease monitoring; control systems used in, for example,\
    \ naturalizing intelligent cities through energy and traffic control and management\
    \ of urban agriculture; applications that promote human empathy focused on diminishing\
    \ violence and exclusion and increasing well-being.\n\u2022\_\_\_\_\_Promoting\
    \ the potential role of A/IS in sustainable development by collaboration between\
    \ national and international government agencies and non-governmental organizations\
    \ (NGOs) in technology sectors.\n\u2022\_\_\_\_\_Analyzing the cost of and proposing\
    \ strategies for publicly providing internet access for\_all, as a means of diminishing\
    \ the gap in\_A/IS\u2019 potential benefit to humanity, particularly between urban\
    \ and rural populations in HIC and LMIC alike.\n\u2022\_\_\_\_\_Investing in the\
    \ documentation and dissemination of innovative applications of\_A/IS that advance\
    \ the resolution of identified societal issues and the SDGs.\n\u2022\_\_\_\_\_\
    Researching sustainable energy to power A/IS computational capacity.\n\u2022\_\
    \_\_\_\_Investing in the development of transparent monitoring frameworks to track\
    \ the concrete results of donations by international organizations, corporations,\
    \ independent agencies, and the State, to ensure efficiency and accountability\
    \ in applied A/IS.\n\u2022\_\_\_\_\_Developing national legal, policy, and fiscal\
    \ measures to encourage competition in the\_A/IS domestic markets and the flourishing\_\
    of scalable A/IS applications.\n\u2022\_\_\_\_\_Integrating the SDGs into the\
    \ core of private sector business strategies and adding SDG indicators to companies\u2019\
    \ key performance indicators, going beyond corporate social responsibility (CSR).\n\
    \u2022\_\_\_\_\_Applying the well-being indicators10 to evaluate A/IS\u2019 impact\
    \ from multiple perspectives in HIC and LMIC alike.\n## Further reading\n\u2022\
    \_\_\_\_\_R. Van Est and J.B.A. Gerritsen, with assistance of L. Kool, Human Rights\
    \ in the Robot Age: Challenges arising from the use of Robots, Artificial Intelligence\
    \ and Augmented Reality Expert Report written for the Committee on Culture, Science,\
    \ Education and Media of the Parliamentary Assembly of the Council of Europe (PACE),\
    \ The Hague: Rathenau Instituut 2017.\n\u2022\_\_\_\_\_World Economic Forum Global\
    \ Future Council on Human Rights 2016-18, \u201CWhite Paper: How to Prevent Discriminatory\
    \ Outcomes in Machine Learning,\u201D World Economic Forum, March 2018.\n\u2022\
    \_\_\_\_\_United Nations General Assembly, _Transforming Our World: The 2030 Agenda\
    \ for Sustainable Development_ (A/RES/70/1: 21 October 2015) Preamble. [http://www.un.org/\
    \ en/development/desa/population/migration/ generalassembly/docs/globalcompact/\
    \ A\\_RES\\_70\\_1\\_E.pdf](http://www.un.org/en/development/desa/population/migration/generalassembly/docs/globalcompact/A_RES_70_1_E.pdf).\n\
    \u2022\_\_\_\_\_United Nations Global Pulse, Big Data for Development: Challenges\
    \ and Opportunities, 2012.\"\n\np139-140\n"
  Linked Challenges:
  - recj64vAVJSm5B2ba
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T11:16:58.000Z'
  airtable_id: recouZjokdKQz88z1
  title: Explicitly consider how AI may foster and hamper SDG progress
- Description: "\u201CWhat is the primary object of study? \xA7 What are the ethical\
    \ expectations commonly associated with these types of data? (For example, working\
    \ with aggregated, de-identified data carries different ethical expectations than\
    \ working with interview data.) \xA7 Does the object of analysis include persons\
    \ or texts beyond the immediate parameters outlined by the study? What are the\
    \ potential ethical consequences and how might these be addressed? (For example,\
    \ collecting data from a blog often includes comments; collecting data from one\
    \ social media stream reveals links to people or data outside the specific scope\
    \ of the study.) \xA7 If information collected in the course of a study can be\
    \ linked back to an individual by means of internet search or other technology,\
    \ what process will the researcher use to determine how that information will\
    \ be treated? (For example, many challenges surround the responsible use of images\
    \ and video). To what extent might data be considered by participants to be personal\
    \ and private, or public and freely available for analysis and republication?\
    \ \xA7 What other questions might arise as a result of the particular context\
    \ from which this data was collected?\u201D (Markham and Buchanan, 2012, p. 9)\n"
  Linked Principles:
  - recsvi4LnhEEPyQ1h
  - recPg7Ov0priGGtLm
  Linked Sources:
  - recQiVQ7CTC72xp6O
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T07:39:07.000Z'
  airtable_id: recpUrzG3GpRiwqnz
  title: Questions to consider regarding the nature of the data (as (dis)aggregated,
    private/public, reidentifiable, etc.)
- Description: "\u201CAssessing Alternative Methodologies and Scope of Research Internet\
    \ experimentation projects can be scaled to a worldwide level (e.g. Kramer, Guillory\
    \ & Hancock, 2014) and engineers are typically incentivized to deploy a project\
    \ as widely as possible to maximize their reach. It is sometimes also just easier\
    \ to let a project operate without limitations and to see later which data is\
    \ collected, rather than limiting its scope artificially. However, the knowledge\
    \ gained using this collection method can have exposed some problems in specific\
    \ political and cultural contexts. Risk levels can vary widely based on target\
    \ countries or for particular target groups (see also Dwork, 2006). Therefore,\
    \ trying to mitigate the risks and shifts in values in all areas will result in\
    \ a race to appease the lowest common denominator (or: reduce the utility of the\
    \ project to appease the context with the highest risk factors). \n- How can the\
    \ researcher limit the scope of the research questions and the project\u2019s\
    \ aim to avoid some risks of harm or negatively affected values? \n- How can the\
    \ researcher limit the scope of stakeholders, by excluding particular groups or\
    \ countries? If so, would the data collected still be a representative sample\
    \ to answer the research question? \n- Are any risks averted if the researcher\
    \ limits the project duration to a shorter amount of operation time? And does\
    \ this conflict with the ability of the researcher to conduct the research in\
    \ question?\u201D \n(franzke et al., 2020, p. 40)\n"
  Linked Principles:
  - recint2IxoR8aILCp
  - recheWZC64aZRgmpo
  - recgDkzdE9dfpTxCK
  Linked Sources:
  - rec6r8OkE2Q2EdiM3
  Tags:
  - reflection-questions
  - project-design
  airtable_createdTime: '2023-05-19T12:32:46.000Z'
  airtable_id: recpvGxQzTNQS1smH
  title: Questions to consider in key stages of AI and machine learning based research,
    regarding methodological scope
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CDiversity, non-discrimination and fairness \n\u201CAccessibility\
    \ and universal design: \n\uF0FC Did you ensure that the AI system accommodates\
    \ a wide range of individual preferences and abilities? \n\uF0A7 Did you assess\
    \ whether the AI system usable by those with special needs or disabilities or\
    \ those at risk of exclusion? How was this designed into the system and how is\
    \ it verified? \n\uF0A7 Did you ensure that information about the AI system is\
    \ accessible also to users of assistive technologies? \n\uF0A7 Did you involve\
    \ or consult this community during the development phase of the AI system? \n\uF0FC\
    \ Did you take the impact of your AI system on the potential user audience into\
    \ account? \n\uF0A7 Did you assess whether the team involved in building the AI\
    \ system is representative of your target user audience? Is it representative\
    \ of the wider population, considering also of other groups who might tangentially\
    \ be impacted? \n\uF0A7 Did you assess whether there could be persons or groups\
    \ who might be disproportionately affected by negative implications? \n\uF0A7\
    \ Did you get feedback from other teams or groups that represent different backgrounds\
    \ and experiences?\u201D (High-Level Expert Group on AI, 2019, p. 30)\n"
  Linked Principles:
  - recmzjcGKv3yNOxbl
  - recScYLR2TNiv7iKf
  - reclPiw2VvNOSTzv5
  - rec42P8U9usfYCtv9
  - recSqx6wklVpDzx3s
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:39:26.000Z'
  airtable_id: recq6GeKNegFQdzS9
  title: Considerations in assessing trustworthy AI - Accessibility and universal
    design
- Description: "## \"Recommendations\nTo protect democracy, respect fundamental rights,\
    \ and promote sustainable development, governments should implement a legislative\
    \ agenda which prevents the spread of misinformation and hate speech, by:\n\u2022\
    \_\_\_\_\_Ensuring more control and transparency in the use of A/IS techniques\
    \ for user profiling in order to protect privacy and prevent user manipulation.\n\
    \u2022\_\_\_\_\_Using A/IS techniques to detect untruthful information circulating\
    \ in the infrastructures, overseen by a democratic body to prevent potential censorship.\n\
    \u2022\_\_\_\_\_Obliging companies owning A/IS infrastructures to provide more\
    \ transparency regarding their algorithms, sources of funding, services, and clients.\n\
    \u2022\_\_\_\_\_Defining a new legal status somewhere between \"platforms\" and\
    \ \"content providers\" for A/IS infrastructures.\n\u2022\_\_\_\_\_Reformulating\
    \ the deontological codes of the journalistic profession to take into account\
    \ the intensive use of A/IS techniques foreseen\_in the future. \u2022\_\_\_\_\
    \_Promoting the right to information in official documents, and developing A/IS\
    \ techniques to automate journalistic tasks such as verification of sources and\
    \ checking the accuracy of the information in official documents, or in the selection,\
    \ hierarchy, assessment, and development of news, thereby contributing to objectivity\
    \ and reliability.\n## Further Resources\n\u2022\_\_\_\_\_M. Broussard, \u201C\
    Artificial Iintelligence for Investigative Reporting: Using an expert system to\
    \ enhance journalists\u2019 ability to discover original public affairs stories.\u201D\
    \ Digital Journalism, vol. 3, no. 6, pp. 814-831, 2015.\n\u2022\_\_\_\_\_M. Carlson,\
    \ \u201CThe robotic reporter: Automated journalism and the redefinition of labor,\
    \ compositional forms, and journalistic authority.\u201D Digital Journalism, vol.\
    \ 3, no. 3, pp. 416-431, 2015.\n\u2022\_\_\_\_\_A. L\xF3pez Barriuso, F. de la\
    \ Prieta Pintado, \xC1. Lozano Murciego, , D. Hern\xE1ndez de la Iglesia and J.\
    \ Revuelta Herrero, JOUR-MAS: A Multiagent System Approach to Help Journalism\
    \ Management, vol. 4, no. 4, 2015.\n\u2022\_\_\_\_\_P. Mozur, \u201DA Genocide\
    \ Incited on Facebook with Posts from Myanmar\u2019s Military,\u201D _The New\
    \ York Times,_ Oct. 15 2018. [https:// www.nytimes.com/2018/10/15/technology/\
    \ myanmar-faceboo.k-genocide.html](https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html)\n\
    \u2022\_\_\_\_\_UK Parliament, House of Commons, Digital, Culture, Media and Sport\
    \ Committee Disinformation and \u2018fake news\u2019: Interim Report, Fifth Report\
    \ of Session 2017\u201319UK Parliament, Published on July 29, 2018\"\np.142-144\n"
  Linked Challenges:
  - recJV8yTX0MnF3rGD
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T11:43:25.000Z'
  airtable_id: recqJAVTtqOUfypNI
  title: Legislate to prevent misinformation and hate speech spread
- Attachments:
  - filename: p59desiredqualities.png
    height: 1409
    id: attK9DA52owcxBtnF
    size: 1354532
    thumbnails:
      full:
        height: 3000
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/bI6914kgPisUuEPei5MJjA/Hn4svQe1FskeUZndeRA-l04DagVf8yNfRIgZi9-TWWNhH9YEr7nDBpe47pQ1sJDLWsfvcn-YEzQVhLgJpB4V2Q/-KOz6L9iGpbqffxkQy-1Y5-sKCl4Zt26sfTmD2AknBo
        width: 3000
      large:
        height: 512
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/SNtN99T5FNEcOnB953IkUA/8BzLd4jnNdu5jEsLeJCN7pMnikkCp4PvTtbNNb944XQQzG54Fc_6k_dmmYY0SEaDbyMwm933R3YdR5Cb7ZlnrA/CGaWs-iC-FPkm-bWW_k_-VSTi2CEH1rvVLrb6ARbzds
        width: 556
      small:
        height: 36
        url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/WJ7d1-2JxW1v1LVkX59lqg/FV9Hcs7BtnBgEY2GsYGFiVlj1F2eYj2LdjtkSwjukSzyuiTXhCgAVozGSznNZrOJ2jW1bA7nps7upkcNQ6P6og/ZJZx3M_-5VRnqRnQmIM4SdUh_mczFlf-SSBIvdHQOso
        width: 39
    type: image/png
    url: https://v5.airtableusercontent.com/v1/17/17/1686002400000/_07k9mjCkp6Ibxu6cMaEhA/OGzPJxcs2hJr-4t0OIC1LFk59tTYESuUqqcq967DCDvytA5MZ_xBb8E2Oc0qGnssGRptCwgt2Uqi0AVXfLS8KX7IKald1ik88jQaOUYKAz0/xzzAAaZj3UNCtqfeH3wJhH6h8VWWlrM9Uk4I3W0hnPA
    width: 1531
  Description: "\u201CAI technologies are grounded in models, and these models are\
    \ inevitably incomplete in some way. It is up to humans to name educational goals\
    \ and measure the degree to which models fit and are useful\u2014or don\u2019\
    t fit and might be harmful. Such an assessment of how well certain tools serve\
    \ educational priorities may seem obvious, but the romance of technology can lead\
    \ to a \u201Clet\u2019s see what the tech can do'' attitude, which can weaken\
    \ the focus on goals and cause us to adopt models that fit our priorities poorly.\u201D\
    \ (Cardona et al., 2023, p. 54)\n\u201Calign priorities, educational strategies,\
    \ and technology adoption decisions to place the educational needs of students\
    \ ahead of the excitement about emerging AI capabilities.\u201D (Cardona et al.,\
    \ 2023, p. 54)\n\u201CEvery conversation about AI (or any emerging technology)\
    \ should start with the educational needs and priorities of students front and\
    \ center and conclude with a discussion about the evaluation of effectiveness\
    \ re-centered on those needs and priorities. Equity, of course, is one of those\
    \ priorities that requires constant attention, especially given the worrisome\
    \ consequences of potentially biased AI models. We especially call upon leaders\
    \ to avoid romancing the magic of AI or only focusing on promising applications\
    \ or outcomes, but instead to interrogate with a critical eye how AI-enabled systems\
    \ and tools function in the educational environment.\u201D (Cardona et al., 2023,\
    \ p. 54)\n\u201Cwe center teaching and learning in all considerations about the\
    \ suitability of an AI model for an educational use. Humans remain in the loop\
    \ of defining, refining, and using AI models. We highlight the six desirable characteristics\
    \ of AI models for education\u201D (Cardona et al., 2023, p. 55)\n\u201C1. Alignment\
    \ of the AI Model to Educators\u2019 Vision for Learning: When choosing to use\
    \ AI in educational systems, decision makers prioritize educational goals, the\
    \ fit to all we know about how people learn, and alignment to evidence-based best\
    \ practices in education. 2. Data Privacy: Ensuring security and privacy of student,\
    \ teacher, and other human data in AI systems is essential. 3. Notice and Explanation:\
    \ Educators can inspect edtech to determine whether and how AI is being incorporated\
    \ within edtech systems. Educators\u2019 push for AI models can explain the basis\
    \ for detecting patterns and/or for making recommendations, and people retain\
    \ control over these suggestions. 4. Algorithmic Discrimination Protections: Developers\
    \ and implementers of AI in education take strong steps to minimizing bias and\
    \ promoting fairness in AI models.\u201D (Cardona et al., 2023, p. 55)\n\u201C\
    5. Safe and Effective Systems: The use of AI models in education is based on evidence\
    \ of efficacy (using standards already established in education for this purpose)\
    \ and work for diverse learners and in varied educational settings. 6. Human Alternatives,\
    \ Consideration and Feedback: AI models that support transparent, accountable,\
    \ and responsible use of AI in education by involving humans in the loop to ensure\
    \ that educational values and principles are prioritized.\u201D (Cardona et al.,\
    \ 2023, p. 56)\n"
  Linked Principles:
  - recOHnq45Fq7YWsRO
  Linked Sources:
  - recY4zreDoWrsbsv7
  airtable_createdTime: '2023-05-29T08:14:09.000Z'
  airtable_id: recqY2lEigz82Xmh5
  title: Align AI models to a shared vision of education
- Description: "## \"Recommendations\n1\\.\_\_\_It is important that human workers\u2019\
    \ interaction with other workers not always be intermediated by affective systems\
    \ (or other technology) which may filter out autonomy, innovation, and communication.\n\
    2\\.\_\_\_Human points of contact should remain available to customers and other\
    \ organizations when using A/IS.\n3\\.\_\_\_Affective systems should be designed\
    \ to support human autonomy, sense of competence, and meaningful relationships\
    \ as these are necessary to support a flourishing life.\n4\\.\_\_\_Even where\
    \ A/IS are less expensive, more predictable, and easier to control than human\
    \ employees, a core network of human employees should be maintained at every level\
    \ of decision-making in order to ensure preservation of human autonomy, communication,\
    \ and innovation.\n5\\.\_\_\_Management and organizational theorists should consider\
    \ appropriate use of affective and autonomous systems to enhance their business\
    \ models and the efficacy of their workforce within the limits of the preservation\
    \ of human autonomy.\n## Further reading\n\u2022\_\_\_\_\_J. J. Bryson, \u201C\
    Artificial Intelligence and Pro-Social Behavior,\u201D in _Collective Agency and\
    \ Cooperation in Natural and Artificial Systems, _C. Misselhorn, Ed., pp. 281\u2013\
    306, Springer, 2015.\n\u2022\_\_\_\_\_D. Peters, R.A. Calvo, and R.M. Ryan,\n\u201C\
    [Designing for Motivation, Engagement and ](https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00797/full)\n\
    [Wellbeing in Digital Experience](https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00797/full),\u201D\
    _ Frontiers in Psychology_\u2013 Human Media Interaction, vol. 9, pp 797, 2018.\"\
    \"\n\np.100-101\n"
  Linked Challenges:
  - rec3Fm8dyG49YU137
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T11:40:38.000Z'
  airtable_id: recrBZOfrDC2lKpRM
  title: Consider where AI-mediation is, and is not, appropriate with respect to human
    relationships and autonomy
- Description: "\"\u201CQuantitative research is typically guided by stated hypotheses,\
    \ which may be written down before data collection and analysis begins. In qualitative\
    \ research, researchers often do not work with clear hypotheses to be tested but\
    \ instead enter the field in order to learn something about the practice in a\
    \ particular social setting. They conduct self-reflection and state their assumptions\
    \ as well as the effect of their presence in the research domain. These processes\
    \ add accountability and transparency to the research process. In technical domains\
    \ and data science, even though relying on massive amounts of data, it is also\
    \ common for researchers to collect data over a period of time without a clear\
    \ stated goal in order to find strong predictors, correlations or interesting\
    \ cluster phenomena. We suggest that researchers who use AI systems in their research\
    \ consider a hybrid of an ex ante hypothesis (still making room for exploration)\
    \ as well as documenting actions taken and choices made throughout the process\
    \ along with self-reflections on how this research practice has affected their\
    \ findings and research questions.\n- How do the research questions or hypotheses\
    \ affect and control the outcome? \n- If the researcher did not store a fixed\
    \ hypothesis, 1) how has the researcher\u2019s choices been documented? And 2)\
    \ how has the researcher affected the outcome by choosing this practice (e.g.\
    \ discussing the presence of proxies and spurious correlations)?\u201D \n(franzke\
    \ et al., 2020, p. 38)\n"
  Linked Principles:
  - recgDkzdE9dfpTxCK
  Linked Sources:
  - rec6r8OkE2Q2EdiM3
  Tags:
  - reflection-questions
  - project-design
  airtable_createdTime: '2023-05-19T12:24:49.000Z'
  airtable_id: recsAAZKwCyesrHK6
  title: 'Questions to consider in key stages of AI and machine learning based research,
    regarding merit '
- Description: "## Recommendations\nFunding models and institutional incentive structures\
    \ should be reviewed and revised to prioritize projects with interdisciplinary\
    \ ethics components to encourage integration of ethics into projects at all levels.\n\
    ## Further Resources\n\u2022\_\_S. Barocas, Course Material for Ethics and Policy\
    \ in Data Science, Cornell University, 2017.\n\u2022\_\_L. Floridi, and M. Taddeo.\
    \ \u201CWhat Is Data Ethics?\u201D _Philosophical Transactions of the Royal Society,\
    \ _vol._ _374, no. 2083, 1\u20134. DOI[10.1098/ rsta.2016.0360,](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5124072/)\
    \ 2016.\n\u2022\_\_S. Spiekermann, Ethical IT Innovation: A ValueBased System\
    \ Design Approach. Boca Raton, FL: Auerbach Publications, 2015.\n\u2022\_\_K.\
    \ Crawford, \u201C[Artificial Intelligence\u2019s White Guy Problem](http://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html?_r=1)\u201D\
    , _New York Times_, July 25, 2016. [Online]. Available: [http://www.nytimes. com/2016/06/26/opinion/sunday/artificialintelligences-white-guy-problem.html?\\\
    _r=1](http://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html?_r=1).\
    \ [Accessed October 28, 2018].\"\n\np.123-124\n\_\n"
  Linked Challenges:
  - rech9vLLbQgO3OY0i
  - recaFyWRROaJ1ZFyY
  - recdlFwsToNXtcvGC
  Linked Principles:
  - recjViPnz3atRIOpD
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T10:19:10.000Z'
  airtable_id: recsonQhKLmX4D1ao
  title: Cross-disciplinary work should be incentivised
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION)\n\u201CHuman agency: \n\uF0FC Is the AI system implemented in work\
    \ and labour process? If so, did you consider the task allocation between the\
    \ AI system and humans for meaningful interactions and appropriate human oversight\
    \ and control? \n\uF0A7 Does the AI system enhance or augment human capabilities?\
    \ \n\uF0A7 Did you take safeguards to prevent overconfidence in or overreliance\
    \ on the AI system for work processes?\u201D (High-Level Expert Group on AI, 2019,\
    \ p. 26)\n"
  Linked Principles:
  - recU6u0AZbcNj1ik9
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:19:22.000Z'
  airtable_id: recspvYTySr0ANH6j
  title: Considerations in assessing trustworthy AI - human agency
- Description: "The scientific requirements of reproducibility and replicability demand\
    \ from researchers to describe their experiment in such a way that another person\
    \ could achieve at least similar results. For social science and humanities research\
    \ using AI tools, this includes for instance making available the training data,\
    \ the model and test prediction results if deemed safe for the data subjects (Zimmer,\
    \ 2010).\_\n\_\n\u25CF\_\_\_\_\_Can the researcher make datasets available without\
    \ violating the privacy of data subjects or revealing other sensitive information?\
    \ \n\u25CF\_\_\_\_\_To what extent would rigorous anonymization of research data\
    \ affect the utility of the data to allow for reproducibility and replicability?\
    \ \n\u25CF\_\_\_\_\_What exact version of the model did the researcher use, was\
    \ this model pre-trained and if so, what are the precise specifications of that\
    \ particular dataset(s), what is the cultural and sociodemographic profile of\
    \ the dataset?\_\n\u25CF\_\_\_\_\_Has the journal/conference in question established\
    \ procedures for uploading material to safe space solution or other process for\
    \ safe access for review purposes? Are there established repositories that offer\
    \ sharing solutions appropriate for the specific material that might be used?\
    \ \n\u25CF\_\_\_\_\_Does the journal/conference offer guidance for how to safely\
    \ and adequately document AI based research? \n(The NeurIPS conference for example\
    \ requires completion of a \u201Creproducibility checklist,\u201D https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf)\
    \ \n\n(franzke, 2020, p.44-45)\n"
  Linked Principles:
  - recOHnq45Fq7YWsRO
  Linked Sources:
  - rec6r8OkE2Q2EdiM3
  Tags:
  - reflection-questions
  - project-dissemination
  airtable_createdTime: '2023-05-19T12:34:49.000Z'
  airtable_id: recvVsXM40mlnmhqP
  title: Questions to consider in key stages of AI and machine learning based research,
    regarding reproducibility and replicability
- Description: "**\"**A/IS creators need to recognize and prioritize the stewardship\
    \ of the Earth\u2019s natural systems to promote human and ecological well-being.\
    \ Specifically:_ _\n\u2022\_\_\_\_\_Human well-being should be defined to encompass\
    \ ecological health, access to nature, safe climate and natural environments,\
    \ biosystem diversity, and other aspects of a healthy, sustainable natural environment.\n\
    \u2022\_\_\_\_\_A/IS systems should be designed to use, support, and strengthen\
    \ existing ecological sustainability standards with a certification or similar\
    \ system, e.g., [LEED,](https://new.usgbc.org/leed) [Energy Star,](https://www.energystar.gov/)\
    \ or [Forest Stewardship Council.](https://us.fsc.org/en-us) This directs automation\
    \ and machine intelligence to follow the principle of doing no harm and to safeguard\
    \ environmental, social, and economic systems.\n\u2022\_\_\_\_\_A/IS creators\
    \ should prioritize doing no harm to the Earth\u2019s natural systems, both intended\
    \ and unintended harm.\_\n\u2022\_\_\_\_\_A committee should be convened to issue\
    \ findings on ways in which A/IS can be used by business, NGOs, and governmental\
    \ agencies to promote stewardship and restoration of natural systems while reducing\
    \ the harmful impact of economic development on ecological sustainability and\
    \ environmental justice.\n## Further reading\n\u2022\_\_\_\_\_D. Austin and M.\
    \ Macauley. \"[Cutting Through Environmental Issues: Technology as a double-edged\
    \ sword.](https://www.brookings.edu/articles/cutting-through-environmental-issues-technology-as-a-double-edged-sword/)\u201D\
    \ The Brookings Institution, Dec. 2001 [Online]. Available: [https://www.brookings.edu/articles/cuttingthrough-environmental-issues-technology-asa-double-edged-sword/.](https://www.brookings.edu/articles/cutting-through-environmental-issues-technology-as-a-double-edged-sword/)\
    \ [Accessed Dec. 1, 2018].\n\u2022\_\_\_\_\_J. Newton, _[Well-being and the Natural\
    \ Environment: An Overview of the Evidence](http://resolve.sustainablelifestyles.ac.uk/sites/default/files/JulieNewtonPaper.pdf)_[.\
    \ ](http://resolve.sustainablelifestyles.ac.uk/sites/default/files/JulieNewtonPaper.pdf)August\
    \ 20, 2007.\n\u2022\_\_\_\_\_P. Dasgupta, [Human Well-Being and the Natural Environment.](https://books.google.com/books?id=OuMTDAAAQBAJ&amp;dq=wellbeing%2Band%2Bthe%2Bnatural%2Benvironment&amp;lr&amp;source=gbs_navlinks_s)\
    \ Oxford, U.K.: Oxford University Press, 2001.\n\u2022\_\_\_\_\_R. Haines-Young\
    \ and M. Potschin. \u201C[The Links ](https://www.pik-potsdam.de/news/public-events/archiv/alter-net/former-ss/2009/10.09.2009/10.9.-haines-young/literature/haines-young-potschin_2009_bes_2.pdf)\n\
    [Between Biodiversity, Ecosystem Services and Human Well-Being,](https://www.pik-potsdam.de/news/public-events/archiv/alter-net/former-ss/2009/10.09.2009/10.9.-haines-young/literature/haines-young-potschin_2009_bes_2.pdf)\u201D\
    \ in _Ecosystem Ecology: A New Synthesis_, D. Raffaelli, and C. Frid, Eds. Cambridge,\
    \ U.K.: Cambridge University Press, 2010.\n\u2022\_\_\_\_\_S. Hart, _[Capitalism\
    \ at the Crossroads: Next Generation Business Strategies for a PostCrisis World.](https://www.pearson.com/us/higher-education/program/Hart-Capitalism-at-the-Crossroads-Next-Generation-Business-Strategies-for-a-Post-Crisis-World-3rd-Edition/PGM9671.html)_\
    \ Upper Saddle River, NJ: Pearson Education, 2010.\n\u2022\_\_\_\_\_United Nations\
    \ Department of Economic and Social Affairs. \u201C[Call for New Technologies\
    \ to Avoid Ecological Destruction.](http://www.un.org/en/development/desa/news/policy/wess-2011.html)\u201D\
    \ Geneva, Switzerland, July 5, 2011.\n\u2022\_\_\_\_\_Pope Francis. [Encyclical\
    \ Letter Laudato Si\u2019 of the Holy Father Francis On the Care for Our Common\
    \ Home.](http://w2.vatican.va/content/francesco/en/encyclicals/documents/papa-francesco_20150524_enciclica-laudato-si.html)\
    \ May 24, 2015.\n\u2022\_\_\_\_\_\u201C[Environment,](https://www.dalailama.com/messages/environment)\u201D\
    \ The 14th Dalai Lama. Accessed Dec. 9, 2018. [https://www.dalailama.com/ messages/environment.](https://www.dalailama.com/messages/environment)\n\
    - Why Islam.org, Environment and Islam, 2018._ \"_\n\n_p.74-75_\n\n_\"_\n\u2022\
    \_\_\_\_\_To avoid potential negative, unintended consequences, and secure and\
    \ safeguard positive impacts, A/IS creators, end-users, and stakeholders should\
    \ be aware of possible well-being impacts when designing, using, and monitoring\
    \ A/IS systems. This includes being aware of existing cases and possible areas\
    \ of impact, measuring impacts on wellbeing outcomes, and developing regulations\
    \ to promote beneficent uses of A/IS. Specifically\"\n\u2022\_\_\_\_\_A/IS creators\
    \ should protect human dignity, autonomy, rights, and well-being of those directly\
    \ and indirectly affected by the technology. As part of this effort, it is important\
    \ to include multiple stakeholders, minorities, marginalized groups, and those\
    \ often without power or a voice in consultation.\n\u2022\_\_\_\_\_Policymakers,\
    \ regulators, monitors, and researchers should consider issuing guidance on areas\
    \ such as A/IS labor and the proper role of humans vs. A/IS in work transparency,\
    \ trust, and explainability; manipulation and deception; and other areas that\
    \ emerge.\n\u2022\_\_\_\_\_Ongoing literature review and analysis should be performed\
    \ by research and other communities to curate and aggregate information on positive\
    \ and negative A/IS impacts, along with demonstrated approaches to realize positive\
    \ ones and ameliorate\_negative ones.\n\u2022\_\_\_\_\_A/IS creators working toward\
    \ computational sustainability should integrate well-being concepts, scientific\
    \ findings, and indicators into current computational sustainability models. They\
    \ should work with well-being experts, researchers, and practitioners to conduct\
    \ research and develop and apply models in\_A/IS development that prioritize and\
    \ increase human well-being.\n\u2022\_\_\_\_\_Cross-pollination should be developed\
    \ between computational sustainability and well-being professionals to ensure\
    \ integration of well-being into computational sustainability frameworks, and\
    \ vice versa. Where feasible and reasonable, do the same for conceptual models\
    \ such as doughnut economics and systems thinking.\n## Further Resources\n\u2022\
    \_\_\_\_\_[AI Safety Research](https://futureoflife.org/ai-safety-research/) by\
    \ The Future of Life Institute\n\u2022\_\_\_\_\_D. Helbing, et al. \u201C[Will\
    \ Democracy Survive Big Data and Artificial Intelligence?\u201D](https://www.scientificamerican.com/article/will-democracy-survive-big-data-and-artificial-intelligence/)\
    \ _Scientific American_, February 25, 2017.\n\u2022\_\_\_\_\_J. L. Schenker, \u201C\
    [Can We Balance Human Ethics with Artificial Intelligence?](http://techonomy.com/2017/01/how-will-ai-decide-who-lives-and-who-dies/)\u201D\
    \ _Techonomy, _January 23, 2017_._\n\u2022\_\_\_\_\_M. Bulman, \u201C[EU to Vote\
    \ on Declaring Robots To Be \u2018Electronic Persons](http://www.independent.co.uk/life-style/gadgets-and-tech/robots-eu-vote-electronic-persons-european-union-ai-artificial-intelligence-a7527106.html)\u2019\
    .\u201D _Independent, _January 14, 2017.\n\u2022\_\_\_\_\_N. Nevejan, for the\
    \ European Parliament.\n\u201C[European Civil Law Rules in Robotics.](http://www.europarl.europa.eu/RegData/etudes/STUD/2016/571379/IPOL_STU(2016)571379_EN.pdf)\u201D\
    \ October 2016.\n\u2022\_\_\_\_\_University of Oxford. \u201CSocial media manipulation\
    \ rising globally, new report warns,\u201D [https://phys.org/news/2018-07social-media-globally.html.](https://phys.org/news/2018-07-social-media-globally.html)\
    \ July 20, 2018.\n\u2022\_\_\_\_\_\u201C[The AI That Pretends To Be Human,](http://lesswrong.com/lw/n99/the_ai_that_pretends_to_be_human/)\u201D\
    \ _LessWrong _blog post, February 2, 2016.\n\u2022\_\_\_\_\_C. Chan, \u201C[Monkeys\
    \ Grieve When Their Robot Friend Dies.](http://sploid.gizmodo.com/monkeys-grieve-when-their-robot-friend-dies-1791076966)\u201D\
    \ _Gizmodo_, January 11, 2017.\n\_\u2022\_\_\_\_\_Partnership on AI, \u201CAI,\
    \ Labor, and the\nEconomy\u201D Working Group launches in New York City,\u201D\
    \ [https://www.partnershiponai.org/ aile-wg-launch/.](https://www.partnershiponai.org/aile-wg-launch/)\
    \ April 25, 2018.\n\u2022\_\_\_\_\_C.Y. Johnson, \u201C[Children can be swayed\
    \ by robot peer pressure,study says,](https://www.washingtonpost.com/news/speaking-of-science/wp/2018/08/15/robot-overlords-may-sound-scary-but-robot-friends-could-be-just-as-bad/?utm_term=.d07ad598531c)\u201D\
    \ The Washington Post, August 15, 2018. [Online].\nAvailable: [www.WashingtonPost.com.](http://www.WashingtonPost.com/)\n\
    [Accessed 2018].\n## **Further Resources for\_**Computational Sustainability\n\
    \u2022\_\_\_\_\_Stanford Engineering Department, [Topics in Computational Sustainability\
    \ Course Presentation,](https://cs.stanford.edu/~ermon/cs325/slides/lecture1-S16.pdf)\
    \ 2016.\_\n\u2022\_\_\_\_\_Computational Sustainability, [Computational Sustainability:\
    \ Computational Methods for a Sustainable Environment, Economy, and Society Project\
    \ Summary. ](http://computational-sustainability.cis.cornell.edu/FILES/gomes-computational-sustainability-summary-NSF-Exp08.pdf)\_\
    \n\u2022\_\_\_\_\_C. P. Gomes, \u201C[Computational Sustainability: Computational\
    \ Methods for a Sustainable ](https://www.nae.edu/File.aspx?id=17673)\n[Environment,\
    \ Economy, and Society](https://www.nae.edu/File.aspx?id=17673)\u201D in The Bridge:\
    \ Linking Engineering and Society_.\"_\n\n_p.83-86_\n"
  Linked Challenges:
  - recvWM2glArsVhaye
  - recdZI38VrUaUKRYf
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T11:23:27.000Z'
  airtable_id: recwQrzqPp6C7jyJs
  title: Sustainability should be part of AI impact design and evaluation
- Description: "\"Individuals should be provided tools that produce machine-readable\
    \ terms and conditions that are dynamic in nature and serve to protect their data\
    \ and honor their preferences for its use.\_\n\u2022\_\_\_\_\_Personal data access\
    \ and consent should be managed by the individual using their curated terms and\
    \ conditions that provide notification and an opportunity for consent at the time\
    \ data are exchanged, versus outside actors being able to access personal data\
    \ without an individual\u2019s awareness or control.\_\_\n\u2022\_\_\_\_\_Terms\
    \ should be presented in a way that allows a user to easily read, interpret, understand,\
    \ and choose to engage with any A/IS. Consent should be both conditional and dynamic,\
    \ where \u201Cdynamic\u201D means downstream uses of a person\u2019s data must\
    \ be explicitly called out, allowing them to cancel a service and potentially\
    \ rescind or \u201Ckill\u201D any data they have shared with a service to date\
    \ via the use of a \u201CSmart Contract\u201D or specific conditions as described\
    \ in mutual terms and conditions between two parties at the time of exchange.\n\
    \u2022\_\_\_\_\_For further information on these issues, please see the following\
    \ section in regard to algorithmic agents and their application.\"\n\np.109 IEEE\
    \ report\n"
  Linked Challenges:
  - recO1L6GoFMkA6Lt4
  Linked Principles:
  - recPg7Ov0priGGtLm
  Linked Sources:
  - recpXl48pJdKDhc6f
  airtable_createdTime: '2023-06-05T09:31:04.000Z'
  airtable_id: recwWov6KzmwU0FQW
  title: Tools for individuals to create custom machine-readable dynamic terms and
    conditions that respect their preferences for data collection and use
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CPrivacy and data governance Respect for privacy and data Protection:\
    \ \n\uF0FC Depending on the use case, did you establish a mechanism allowing others\
    \ to flag issues related to privacy or data protection in the AI system\u2019\
    s processes of data collection (for training and operation) and data processing?\
    \ \n\uF0FC Did you assess the type and scope of data in your data sets (for example\
    \ whether they contain personal data)? \n\uF0FC Did you consider ways to develop\
    \ the AI system or train the model without or with minimal use of potentially\
    \ sensitive or personal data? \n\uF0FC Did you build in mechanisms for notice\
    \ and control over personal data depending on the use case (such as valid consent\
    \ and possibility to revoke, when applicable)? \n\uF0FC Did you take measures\
    \ to enhance privacy, such as via encryption, anonymisation and aggregation? \n\
    \uF0FC Where a Data Privacy Officer (DPO) exists, did you involve this person\
    \ at an early stage in the process?\u201D (High-Level Expert Group on AI, 2019,\
    \ p. 28)\n"
  Linked Principles:
  - recPg7Ov0priGGtLm
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:30:10.000Z'
  airtable_id: recypFGFqJzmgFmWV
  title: Considerations in assessing trustworthy AI - Respect for privacy and data
    protection
- Description: "\u201CCompliance with this assessment list is not evidence of legal\
    \ compliance, nor is it intended as guidance to ensure compliance with applicable\
    \ law. Given the application-specificity of AI systems, the assessment list will\
    \ need to be tailored to the specific use case and context in which the system\
    \ operates. In addition, this chapter offers a general recommendation on how to\
    \ implement the assessment list for Trustworthy AI though a governance structure\
    \ embracing both operational and management level.\u201D (High-Level Expert Group\
    \ on AI, 2019, p. 24)\n\u201CCompliance with this assessment list is not evidence\
    \ of legal compliance, nor is it intended as guidance to ensure compliance with\
    \ applicable law. Given the application-specificity of AI systems, the assessment\
    \ list will need to be tailored to the specific use case and context in which\
    \ the system operates. In addition, this chapter offers a general recommendation\
    \ on how to implement the assessment list for Trustworthy AI though a governance\
    \ structure embracing both operational and management level.\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 24)\n\n\u201CTRUSTWORTHY AI ASSESSMENT LIST (PILOT\
    \ VERSION) \n\u201CAccountability \nAuditability: \n\uF0FC Did you establish mechanisms\
    \ that facilitate the system\u2019s auditability, such as ensuring traceability\
    \ and logging of the AI system\u2019s processes and outcomes? \n\uF0FC Did you\
    \ ensure, in applications affecting fundamental rights (including safety-critical\
    \ applications) that the AI system can be audited independently?\u201D (High-Level\
    \ Expert Group on AI, 2019, p. 31)\n"
  Linked Principles:
  - recKdujFoPJr4ZAhZ
  - recxcFmvPG5wrCqpO
  Linked Sources:
  - recnCULdYQ36cpZR7
  Tags:
  - governance-question
  airtable_createdTime: '2023-05-28T19:47:29.000Z'
  airtable_id: reczFKqCos9f1opXO
  title: Considerations in assessing trustworthy AI - Auditability
- Description: "\u201CHow are texts/persons/data being studied? \xA7 Does one\u2019\
    s method of analysis require exact quoting and if so, what might be the ethical\
    \ consequence of this in the immediate or long term? (For example, would quoting\
    \ directly from a blog cause harm to the blogger and if so, could another method\
    \ of representation be less risky?21) \xA7 What are the ethical expectations of\
    \ the research community associated with a particular approach (e.g, ethnographic,\
    \ survey, linguistic analysis)? \xA7 Do one\u2019s disciplinary requirements for\
    \ collecting, analyzing, or representing information clash with the specific needs\
    \ of the context? If so, what are the potential ethical consequences?\u201D (Markham\
    \ and Buchanan, 2012, p. 10)\n"
  Linked Principles:
  - recsvi4LnhEEPyQ1h
  - recPg7Ov0priGGtLm
  - recy4stJ6Y4e2Fezp
  - rec42P8U9usfYCtv9
  Linked Sources:
  - recQiVQ7CTC72xp6O
  Tags:
  - reflection-questions
  airtable_createdTime: '2023-05-19T07:44:09.000Z'
  airtable_id: reczG9x5YfScZJdCo
  title: Questions to consider regarding re-identification and disciplinary methodological
    norms
